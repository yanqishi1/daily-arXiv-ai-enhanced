{"id": "2602.12155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12155", "abs": "https://arxiv.org/abs/2602.12155", "authors": ["Yeyao Ma", "Chen Li", "Xiaosong Zhang", "Han Hu", "Weidi Xie"], "title": "FAIL: Flow Matching Adversarial Imitation Learning for Image Generation", "comment": null, "summary": "Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.", "AI": {"tldr": "FAIL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u7684\u6d41\u5339\u914d\u6a21\u578b\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6216\u6210\u5bf9\u6bd4\u8f83\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7b56\u7565\u4e0e\u4e13\u5bb6\u5206\u5e03\u5dee\u5f02\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\u65e0\u6cd5\u7ea0\u6b63\u672a\u89c1\u72b6\u6001\u4e0b\u7684\u7b56\u7565\u6f02\u79fb\uff0c\u800c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u504f\u597d\u5bf9\u6216\u5956\u52b1\u5efa\u6a21\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6216\u6210\u5bf9\u6bd4\u8f83\u5c31\u80fd\u5bf9\u9f50\u8f93\u51fa\u5206\u5e03\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFlow Matching Adversarial Imitation Learning (FAIL)\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u6700\u5c0f\u5316\u7b56\u7565\u4e0e\u4e13\u5bb6\u5206\u5e03\u5dee\u5f02\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1aFAIL-PD\u5229\u7528\u53ef\u5fae\u5206ODE\u6c42\u89e3\u5668\u83b7\u5f97\u4f4e\u65b9\u5dee\u8def\u5f84\u68af\u5ea6\uff1bFAIL-PG\u4e3a\u79bb\u6563\u6216\u8ba1\u7b97\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u9ed1\u76d2\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u4ec5\u4f7f\u752813,000\u4e2aNano Banana pro\u6f14\u793a\u5fae\u8c03FLUX\u6a21\u578b\uff0c\u5728\u63d0\u793a\u8ddf\u968f\u548c\u7f8e\u5b66\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6846\u67b6\u80fd\u6709\u6548\u6cdb\u5316\u5230\u79bb\u6563\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff0c\u5e76\u53ef\u4f5c\u4e3a\u9c81\u68d2\u6b63\u5219\u5316\u5668\u7f13\u89e3\u57fa\u4e8e\u5956\u52b1\u4f18\u5316\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "FAIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6216\u6210\u5bf9\u6bd4\u8f83\u7684\u6d41\u5339\u914d\u6a21\u578b\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u591a\u79cd\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6709\u6548\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
