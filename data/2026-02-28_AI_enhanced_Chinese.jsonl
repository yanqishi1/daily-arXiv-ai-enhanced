{"id": "2602.22868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22868", "abs": "https://arxiv.org/abs/2602.22868", "authors": ["Yushi Ye", "Feng Hong", "Huangjie Zheng", "Xu Chen", "Zhiyong Chen", "Yanfeng Wang", "Jiangchao Yao"], "title": "Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference", "comment": null, "summary": "Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \\times$ inference speedup without any quality degradation.", "AI": {"tldr": "ReMix\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8fde\u7eed\u6df7\u5408\u72b6\u6001\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u89e3\u7801\u4e2d\u7684\u7ec4\u5408\u77db\u76fe\u95ee\u9898\uff0c\u5b9e\u73b02-8\u500d\u63a8\u7406\u52a0\u901f\u4e14\u65e0\u8d28\u91cf\u635f\u5931", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u627f\u8bfa\u5feb\u901f\u975e\u81ea\u56de\u5f52\u63a8\u7406\uff0c\u4f46\u5728\u5e76\u884c\u89e3\u7801\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u8d28\u91cf-\u901f\u5ea6\u6743\u8861\u95ee\u9898\uff0c\u8fd9\u6e90\u4e8e\"\u7ec4\u5408\u77db\u76fe\"\u73b0\u8c61\u2014\u2014\u5e76\u884c\u751f\u6210\u7684\u4ee4\u724c\u4f1a\u5f62\u6210\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u7ec4\u5408", "method": "\u63d0\u51faReMix\u6846\u67b6\uff0c\u5f15\u5165\u8fde\u7eed\u6df7\u5408\u72b6\u6001\u4f5c\u4e3a\u521d\u59cb\u63a9\u7801\u72b6\u6001\u548c\u6700\u7ec8\u89e3\u7801\u4ee4\u724c\u72b6\u6001\u4e4b\u95f4\u7684\u4e2d\u95f4\u72b6\u6001\uff0c\u5141\u8bb8\u4ee4\u724c\u8868\u793a\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u8fed\u4ee3\u7ec6\u5316\uff0c\u89e3\u51b3\u4e0e\u5176\u4ed6\u4ee4\u724c\u7684\u76f8\u4e92\u51b2\u7a81\uff1b\u540c\u65f6\u91c7\u7528\u62d2\u7edd\u89c4\u5219\u5c06\u4e0d\u786e\u5b9a\u8868\u793a\u4ece\u8fde\u7eed\u72b6\u6001\u56de\u9000\u5230\u63a9\u7801\u72b6\u6001\u91cd\u65b0\u5904\u7406", "result": "ReMix\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e862-8\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u6ca1\u6709\u4efb\u4f55\u8d28\u91cf\u4e0b\u964d", "conclusion": "ReMix\u901a\u8fc7\u5c06\u8fde\u7eed\u8868\u793a\u6574\u5408\u5230\u79bb\u6563\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7ec4\u5408\u77db\u76fe\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6"}}
{"id": "2602.22654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22654", "abs": "https://arxiv.org/abs/2602.22654", "authors": ["Bowen Cui", "Yuanbin Wang", "Huajiang Xu", "Biaolong Chen", "Aixi Zhang", "Hao Jiang", "Zhengzheng Jin", "Xu Liu", "Pipei Huang"], "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache", "comment": "Accepted by CVPR 2026", "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.", "AI": {"tldr": "DPCache\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8def\u5f84\u611f\u77e5\u6210\u672c\u5f20\u91cf\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u65f6\u95f4\u6b65\u9009\u62e9\uff0c\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u591a\u6b65\u8fed\u4ee3\u91c7\u6837\u7684\u8ba1\u7b97\u5f00\u9500\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u7f13\u5b58\u7684\u52a0\u901f\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u6216\u5c40\u90e8\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u6ca1\u6709\u8003\u8651\u53bb\u566a\u8f68\u8ff9\u7684\u5168\u5c40\u7ed3\u6784\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "DPCache\u5c06\u6269\u6563\u91c7\u6837\u52a0\u901f\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u9996\u5148\u4ece\u5c0f\u578b\u6821\u51c6\u96c6\u6784\u5efa\u8def\u5f84\u611f\u77e5\u6210\u672c\u5f20\u91cf\uff0c\u91cf\u5316\u5728\u7ed9\u5b9a\u524d\u4e00\u4e2a\u5173\u952e\u65f6\u95f4\u6b65\u6761\u4ef6\u4e0b\u8df3\u8fc7\u65f6\u95f4\u6b65\u7684\u8def\u5f84\u4f9d\u8d56\u8bef\u5dee\u3002\u7136\u540e\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u7684\u5173\u952e\u65f6\u95f4\u6b65\u5e8f\u5217\uff0c\u6700\u5c0f\u5316\u603b\u8def\u5f84\u6210\u672c\u540c\u65f6\u4fdd\u6301\u8f68\u8ff9\u4fdd\u771f\u5ea6\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u53ea\u5728\u5173\u952e\u65f6\u95f4\u6b65\u8fdb\u884c\u5b8c\u6574\u8ba1\u7b97\uff0c\u4e2d\u95f4\u8f93\u51fa\u5219\u4f7f\u7528\u7f13\u5b58\u7279\u5f81\u8fdb\u884c\u9ad8\u6548\u9884\u6d4b\u3002", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDPCache\u5728\u4fdd\u6301\u6700\u5c0f\u8d28\u91cf\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002\u5728FLUX\u4e0a\uff0c4.87\u500d\u52a0\u901f\u65f6ImageReward\u63d0\u5347+0.031\uff0c3.54\u500d\u52a0\u901f\u65f6\u751a\u81f3\u8d85\u8fc7\u5b8c\u6574\u6b65\u6570\u57fa\u7ebf+0.028 ImageReward\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u3002", "conclusion": "DPCache\u901a\u8fc7\u8def\u5f84\u611f\u77e5\u7684\u5168\u5c40\u8c03\u5ea6\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u52a0\u901f\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22871", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22871", "abs": "https://arxiv.org/abs/2602.22871", "authors": ["Roy Miles", "Aysim Toker", "Andreea-Maria Oncescu", "Songcen Xu", "Jiankang Deng", "Ismail Elezi"], "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching", "comment": null, "summary": "Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.", "AI": {"tldr": "\u63d0\u51faStitching Noisy Diffusion Thoughts\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u91c7\u6837\u591a\u63a8\u7406\u8f68\u8ff9\uff0c\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u5206\u6b65\u9aa4\uff0c\u8de8\u8f68\u8ff9\u62fc\u63a5\u9ad8\u8d28\u91cf\u6b65\u9aa4\u5f62\u6210\u590d\u5408\u63a8\u7406\uff0c\u518d\u7528\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff0c\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u805a\u5408\u7b56\u7565\u901a\u5e38\u662f\u8f68\u8ff9\u7ea7\u7684\uff08\u5982\u9009\u62e9\u6700\u4f73\u8f68\u8ff9\u6216\u6295\u7968\u6700\u7ec8\u7b54\u6848\uff09\uff0c\u4e22\u5f03\u4e86\u90e8\u5206\u6216\"\u63a5\u8fd1\u6b63\u786e\"\u5c1d\u8bd5\u4e2d\u6709\u7528\u7684\u4e2d\u95f4\u5de5\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u91cd\u7528\u6b65\u9aa4\u7ea7\u5019\u9009\u7684\u81ea\u6211\u4e00\u81f4\u6027\u6846\u67b6\u3002", "method": "1) \u4f7f\u7528\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u591a\u6837\u3001\u4f4e\u6210\u672c\u7684\u63a8\u7406\u8f68\u8ff9\uff1b2) \u7528\u73b0\u6210\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u8bc4\u5206\u6bcf\u4e2a\u4e2d\u95f4\u6b65\u9aa4\uff1b3) \u8de8\u8f68\u8ff9\u62fc\u63a5\u6700\u9ad8\u8d28\u91cf\u6b65\u9aa4\u5f62\u6210\u590d\u5408\u63a8\u7406\uff1b4) \u7528\u81ea\u56de\u5f52\u6a21\u578b\u57fa\u4e8e\u590d\u5408\u63a8\u7406\u91cd\u65b0\u8ba1\u7b97\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6b65\u9aa4\u7ea7\u91cd\u7ec4\u5bf9\u66f4\u96be\u95ee\u9898\u6700\u6709\u76ca\uff0c\u6700\u7ec8\u81ea\u56de\u5f52\u6c42\u89e3\u5668\u80fd\u5c06\u62fc\u63a5\u4f46\u4e0d\u5b8c\u7f8e\u7684\u63a8\u7406\u8f6c\u6362\u4e3a\u51c6\u786e\u7b54\u6848\u3002\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u5728\u516d\u4e2a\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8fbe23.8%\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u6269\u6563\u6a21\u578b\u548c\u7edf\u4e00\u67b6\u6784\u5b9e\u73b01.8\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6a21\u5757\u5316\u7ba1\u9053\u5c06\u63a2\u7d22\uff08\u6269\u6563\uff09\u4e0e\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6848\u5408\u6210\u5206\u79bb\uff0c\u907f\u514d\u5355\u4e00\u7edf\u4e00\u6df7\u5408\u67b6\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u5e7f\u6cdb\u641c\u7d22\u3002\u6b65\u9aa4\u7ea7\u91cd\u7ec4\u7279\u522b\u9002\u7528\u4e8e\u56f0\u96be\u95ee\u9898\uff0c\u81ea\u56de\u5f52\u6c42\u89e3\u5668\u5bf9\u786e\u4fdd\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.22918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22918", "abs": "https://arxiv.org/abs/2602.22918", "authors": ["Jonathan Steinberg", "Oren Gal"], "title": "Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u5206\u6790\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2dOCR\u4fe1\u606f\u7684\u5904\u7406\u8def\u5f84\uff0c\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u7684OCR\u74f6\u9888\u4f4d\u7f6e\u4e0d\u540c\uff0cOCR\u4fe1\u53f7\u5177\u6709\u4f4e\u7ef4\u7279\u6027\uff0c\u4e14\u5728\u6a21\u5757\u5316\u67b6\u6784\u4e2d\u79fb\u9664OCR\u53cd\u800c\u80fd\u63d0\u5347\u8ba1\u6570\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u56fe\u50cf\u4e2d\u8bfb\u53d6\u6587\u672c\uff0c\u4f46OCR\u4fe1\u606f\u5177\u4f53\u5728\u8bed\u8a00\u5904\u7406\u6d41\u7a0b\u7684\u54ea\u4e2a\u73af\u8282\u8fdb\u5165\u7cfb\u7edf\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u4e0d\u540c\u67b6\u6784\u7684VLMs\u4e2dOCR\u4fe1\u606f\u7684\u5904\u7406\u8def\u5f84\u548c\u673a\u5236\u3002", "method": "\u4f7f\u7528\u56e0\u679c\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u539f\u59cb\u56fe\u50cf\u4e0e\u6587\u672c\u4fee\u590d\u7248\u672c\u4e4b\u95f4\u7684\u6fc0\u6d3b\u5dee\u5f02\uff0c\u5206\u6790\u4e09\u79cd\u67b6\u6784\u5bb6\u65cf\uff08Qwen3-VL\u3001Phi-4\u3001InternVL3.5\uff09\u7684OCR\u5904\u7406\u673a\u5236\u3002\u91c7\u7528\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7814\u7a76OCR\u4fe1\u53f7\u7684\u7ef4\u5ea6\u7279\u6027\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "result": "1. \u4e0d\u540c\u67b6\u6784\u7684OCR\u74f6\u9888\u4f4d\u7f6e\u4e0d\u540c\uff1aDeepStack\u6a21\u578b\uff08Qwen\uff09\u5728\u4e2d\u95f4\u5c42\uff08\u7ea650%\uff09\u5bf9\u573a\u666f\u6587\u672c\u6700\u654f\u611f\uff1b\u5355\u9636\u6bb5\u6295\u5f71\u6a21\u578b\uff08Phi-4\u3001InternVL\uff09\u5728\u65e9\u671f\u5c42\uff086-25%\uff09\u6700\u654f\u611f\n2. OCR\u4fe1\u53f7\u5177\u6709\u4f4e\u7ef4\u7279\u6027\uff1a\u7b2c\u4e00\u4e3b\u6210\u5206\uff08PC1\uff09\u80fd\u89e3\u91ca72.9%\u7684\u65b9\u5dee\n3. PCA\u65b9\u5411\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u53ef\u8fc1\u79fb\uff0c\u8868\u660e\u5b58\u5728\u5171\u4eab\u7684\u6587\u672c\u5904\u7406\u901a\u8def\n4. \u5728\u6a21\u5757\u5316OCR\u7535\u8def\u67b6\u6784\u4e2d\uff08\u5982Qwen3-VL-4B\uff09\uff0c\u79fb\u9664OCR\u53cd\u800c\u80fd\u63d0\u5347\u8ba1\u6570\u6027\u80fd\u8fbe6.9\u4e2a\u767e\u5206\u70b9", "conclusion": "\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u91c7\u7528\u4e0d\u540c\u7684OCR\u4fe1\u606f\u5904\u7406\u7b56\u7565\uff0cOCR\u4fe1\u53f7\u5177\u6709\u4f4e\u7ef4\u548c\u53ef\u8fc1\u79fb\u7684\u7279\u6027\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u6a21\u5757\u5316\u67b6\u6784\u4e2d\uff0cOCR\u5904\u7406\u53ef\u80fd\u5e72\u6270\u5176\u4ed6\u89c6\u89c9\u5904\u7406\u4efb\u52a1\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u8bbe\u8ba1\u9700\u8981\u5728\u6587\u672c\u8bc6\u522b\u548c\u5176\u4ed6\u89c6\u89c9\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2602.23057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23057", "abs": "https://arxiv.org/abs/2602.23057", "authors": ["Jeongin Bae", "Baeseong Park", "Gunho Park", "Minsub Kim", "Joonhyung Lee", "Junhee Yoo", "Sunghyeon Woo", "Jiwon Ryu", "Se Jung Kwon", "Dongsoo Lee"], "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention", "comment": "Preprint. 14 pages, 11 figures", "summary": "Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.\n  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAffine-Scaled Attention\uff0c\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u548c\u504f\u7f6e\u9879\u6765\u653e\u677esoftmax\u4e25\u683c\u5f52\u4e00\u5316\u7ea6\u675f\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u884c\u4e3a\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684Transformer\u6ce8\u610f\u529b\u4f7f\u7528softmax\u5f52\u4e00\u5316\uff0c\u5f3a\u5236\u6ce8\u610f\u529b\u6743\u91cd\u5177\u6709\u5355\u4f4d\u603b\u548c\u5f52\u4e00\u5316\u3002\u8fd9\u79cd\u7ea6\u675f\u9650\u5236\u4e86\u6ce8\u610f\u529b\u5e45\u5ea6\u7684\u7075\u6d3b\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u8fc7\u4e8e\u96c6\u4e2d\u6216\u4e0d\u7a33\u5b9a\u3002\u73b0\u6709\u7684\u6539\u8fdb\u65b9\u6cd5\u5982\u6ce8\u610f\u529b\u6c47\u805a\u6216\u95e8\u63a7\u673a\u5236\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u6216\u95f4\u63a5\u7684\u6ce8\u610f\u529b\u91cd\u52a0\u6743\u63a7\u5236\u3002", "method": "\u63d0\u51faAffine-Scaled Attention\uff0c\u5728\u6807\u51c6\u6ce8\u610f\u529b\u57fa\u7840\u4e0a\u5f15\u5165\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u548c\u76f8\u5e94\u7684\u504f\u7f6e\u9879\uff0c\u5e94\u7528\u4e8esoftmax\u5f52\u4e00\u5316\u7684\u6ce8\u610f\u529b\u6743\u91cd\u3002\u8fd9\u79cd\u8bbe\u8ba1\u653e\u677e\u4e86\u4e25\u683c\u7684\u5f52\u4e00\u5316\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u503c\u8868\u793a\u7684\u805a\u5408\uff0c\u5141\u8bb8\u6a21\u578b\u4ee5\u53ef\u63a7\u65b9\u5f0f\u8c03\u6574\u6ce8\u610f\u529b\u7684\u76f8\u5bf9\u5206\u5e03\u548c\u5c3a\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u6807\u51c6softmax\u6ce8\u610f\u529b\u548c\u6ce8\u610f\u529b\u6c47\u805a\u57fa\u7ebf\u76f8\u6bd4\uff0cAffine-Scaled Attention\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u884c\u4e3a\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u9002\u5ea6\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u91cd\u52a0\u6743\u4e3a\u6539\u8fdbTransformer\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u65b9\u6cd5\u3002Affine-Scaled Attention\u901a\u8fc7\u653e\u677esoftmax\u7684\u4e25\u683c\u5f52\u4e00\u5316\u7ea6\u675f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u63a7\u5236\u6ce8\u610f\u529b\u5206\u5e03\u548c\u5c3a\u5ea6\u3002"}}
{"id": "2602.22759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22759", "abs": "https://arxiv.org/abs/2602.22759", "authors": ["Yuan-Chih Chen", "Chun-Shien Lu"], "title": "Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval", "comment": null, "summary": "Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u9690\u85cf\u4ee3\u7801\u6062\u590d\u6846\u67b6\uff0c\u652f\u6301\u4ece\u540e\u5904\u7406\u548c\u751f\u6210\u4e2d\u6c34\u5370\u4e24\u79cd\u8303\u5f0f\u8fdb\u884c\u56fe\u50cf\u5185\u5bb9\u68c0\u7d22\u4e0e\u6062\u590d\uff0c\u5728ImageNet-S\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u771f\u5b9e\u6027\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u800c\u9488\u5bf9\u7be1\u6539\u5185\u5bb9\u7684\u4e8b\u5b9e\u68c0\u7d22\u6062\u590d\u76f8\u5bf9\u7f3a\u4e4f\u63a2\u7d22\u3002\u9700\u8981\u5efa\u7acb\u8d85\u8d8a\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u901a\u7528\u56fe\u50cf\u6062\u590d\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u9690\u85cf\u4ee3\u7801\u6062\u590d\u6846\u67b6\uff0c\u5c06\u8bed\u4e49\u548c\u611f\u77e5\u4fe1\u606f\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u9690\u85cf\u4ee3\u7801\u8868\u793a\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\u8fdb\u884c\u7cbe\u70bc\uff0c\u5e76\u5229\u7528\u6761\u4ef6Transformer\u6a21\u5757\u589e\u5f3a\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728ImageNet-S\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u68c0\u7d22\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u591a\u79cd\u6c34\u5370\u7ba1\u9053\u7684\u5b8c\u5168\u517c\u5bb9\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d85\u8d8a\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u901a\u7528\u56fe\u50cf\u6062\u590d\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u4ece\u6c34\u5370\u4e2d\u68c0\u7d22\u548c\u6062\u590d\u7be1\u6539\u5185\u5bb9\u7684\u80fd\u529b\u3002"}}
{"id": "2602.22919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22919", "abs": "https://arxiv.org/abs/2602.22919", "authors": ["Haofan Wu", "Nay Aung", "Theodoros N. Arvanitis", "Joao A. C. Lima", "Steffen E. Petersen", "Le Zhang"], "title": "Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins", "comment": "10 pages, 8 figures. Submitted to IEEE Transactions on Medical Imaging (TMI). Code will be released after review", "summary": "A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.", "AI": {"tldr": "Chain of Flow (COF) \u662f\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7535\u56fe\u7684\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u6b21\u5fc3\u52a8\u5468\u671f\u91cd\u5efa\u5b8c\u6574\u76844D\u5fc3\u810f\u7ed3\u6784\u548c\u8fd0\u52a8\uff0c\u5c06\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u4ece\u7279\u5b9a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u8f6c\u53d8\u4e3a\u5b8c\u5168\u751f\u6210\u7684\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u5fc3\u810f\u3002", "motivation": "\u73b0\u6709\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u9884\u6d4b\uff0c\u800c\u975e\u6784\u5efa\u60a3\u8005\u7279\u5f02\u6027\u3001\u53ef\u64cd\u4f5c\u7684\u865a\u62df\u5fc3\u810f\u3002\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u9700\u8981\u91cd\u5efa\u4e2a\u4f53\u5316\u5fc3\u810f\u89e3\u5256\u7ed3\u6784\u548c\u751f\u7406\u529f\u80fd\uff0c\u4ece\u591a\u6a21\u6001\u4fe1\u53f7\u66f4\u65b0\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u4e0b\u6e38\u6a21\u62df\u4efb\u52a1\u3002", "method": "COF\u662f\u4e00\u4e2a\u57fa\u7840\u6027\u7684\u5fc3\u7535\u56fe\u9a71\u52a8\u751f\u6210\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6574\u5408\u7535\u5f71\u78c1\u5171\u632f\u6210\u50cf\u548c12\u5bfc\u8054\u5fc3\u7535\u56fe\uff0c\u5b66\u4e60\u5fc3\u810f\u51e0\u4f55\u7ed3\u6784\u3001\u7535\u751f\u7406\u548c\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u7edf\u4e00\u8868\u793a\uff0c\u80fd\u591f\u4ece\u5355\u4e2a\u5fc3\u52a8\u5468\u671f\u91cd\u5efa\u5b8c\u6574\u76844D\u5fc3\u810f\u7ed3\u6784\u548c\u8fd0\u52a8\u3002", "result": "\u5728\u591a\u6837\u5316\u961f\u5217\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cCOF\u80fd\u591f\u51c6\u786e\u6062\u590d\u5fc3\u810f\u89e3\u5256\u7ed3\u6784\u3001\u5404\u8154\u5ba4\u529f\u80fd\u548c\u52a8\u6001\u8fd0\u52a8\u6a21\u5f0f\u3002\u91cd\u5efa\u76844D\u5fc3\u810f\u8fdb\u4e00\u6b65\u652f\u6301\u4e0b\u6e38\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u4efb\u52a1\uff0c\u5982\u5bb9\u79ef\u6d4b\u91cf\u3001\u533a\u57df\u529f\u80fd\u5206\u6790\u548c\u865a\u62df\u7535\u5f71\u5408\u6210\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u4ece\u5fc3\u7535\u56fe\u5b9e\u73b0\u5b8c\u6574\u76844D\u5668\u5b98\u91cd\u5efa\uff0cCOF\u5c06\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u4ece\u72ed\u7a84\u7684\u9884\u6d4b\u6a21\u578b\u8f6c\u53d8\u4e3a\u5b8c\u5168\u751f\u6210\u7684\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u5fc3\u810f\uff0c\u4e3a\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.23088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23088", "abs": "https://arxiv.org/abs/2602.23088", "authors": ["Matthew Sutton", "Katrin Amunts", "Timo Dickscheid", "Christian Schiffer"], "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy", "comment": "8 pages, 3 figures, submitted for inclusion at a conference", "summary": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u7b7e\u4e2d\u4ecb\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u7b7e\u800c\u975e\u6210\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u6570\u636e\uff0c\u5c06\u7ec6\u80de\u6784\u7b51\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fde\u63a5\u5230\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u663e\u5fae\u955c\u56fe\u50cf\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "motivation": "\u5728\u7814\u7a76\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u83b7\u53d6\u6210\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u6570\u636e\u975e\u5e38\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u7ec6\u80de\u6784\u7b51\u5b66\u5206\u6790\u9886\u57df\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u7cbe\u7ec6\u914d\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u8d77\u6765\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u754c\u9762\u3002", "method": "\u4f7f\u7528\u6807\u7b7e\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u81ea\u52a8\u4ece\u76f8\u5173\u6587\u732e\u4e2d\u6316\u6398\u533a\u57df\u63cf\u8ff0\u4f5c\u4e3a\u5408\u6210\u6807\u9898\uff0c\u53cd\u6620\u5178\u578b\u7684\u7ec6\u80de\u6784\u7b51\u5b66\u5c5e\u6027\u3002\u7136\u540e\u5c06\u73b0\u6709\u7684\u7ec6\u80de\u6784\u7b51\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08CytoNet\uff09\u901a\u8fc7\u56fe\u50cf\u5230\u6587\u672c\u7684\u8bad\u7ec3\u76ee\u6807\u8fde\u63a5\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u572857\u4e2a\u8111\u533a\u4e0a\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u5408\u7406\u7684\u533a\u57df\u7ea7\u63cf\u8ff0\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u660e\u786e\u62d2\u7edd\u672a\u89c1\u533a\u57df\u5b9e\u73b0\u5f00\u653e\u96c6\u4f7f\u7528\u3002\u5728\u8303\u56f4\u5185\u8865\u4e01\u4e0a\u5339\u914d\u7ec6\u80de\u6784\u7b51\u5b66\u53c2\u8003\u6807\u7b7e\u7684\u51c6\u786e\u7387\u8fbe\u523090.6%\uff0c\u5728\u6807\u7b7e\u88ab\u63a9\u853d\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u63cf\u8ff0\u4ecd\u5177\u6709\u8db3\u591f\u533a\u5206\u5ea6\uff0c\u57288\u8def\u6d4b\u8bd5\u4e2d\u6062\u590d\u533a\u57df\u7684\u51c6\u786e\u7387\u8fbe\u523068.6%\u3002", "conclusion": "\u5f31\u6807\u7b7e\u4e2d\u4ecb\u914d\u5bf9\u8db3\u4ee5\u5c06\u73b0\u6709\u7684\u751f\u7269\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fde\u63a5\u5230\u8bed\u8a00\uff0c\u4e3a\u5728\u7cbe\u7ec6\u914d\u5bf9\u6ce8\u91ca\u7a00\u7f3a\u7684\u9886\u57df\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.23165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23165", "abs": "https://arxiv.org/abs/2602.23165", "authors": ["Yichen Peng", "Jyun-Ting Song", "Siyeol Jung", "Ruofan Liu", "Haiyang Liu", "Xuangeng Chu", "Ruicong Liu", "Erwin Wu", "Hideki Koike", "Kris Kitani"], "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation", "comment": "13 pages, 9 figures", "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.", "AI": {"tldr": "DyaDiT\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u80fd\u591f\u4ece\u53cc\u4eba\u5bf9\u8bdd\u97f3\u9891\u751f\u6210\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u4eba\u7c7b\u624b\u52bf\u52a8\u4f5c\uff0c\u8003\u8651\u4e86\u793e\u4ea4\u4e92\u52a8\u52a8\u6001\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u5355\u97f3\u9891\u5230\u5355\u52a8\u4f5c\u7684\u6620\u5c04\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5355\u4e00\u97f3\u9891\u6d41\u6620\u5c04\u5230\u5355\u4e00\u8bf4\u8bdd\u8005\u7684\u52a8\u4f5c\uff0c\u5ffd\u7565\u4e86\u793e\u4ea4\u4e0a\u4e0b\u6587\u548c\u5bf9\u8bdd\u53cc\u65b9\u4e4b\u95f4\u7684\u76f8\u4e92\u52a8\u6001\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u751f\u6210\u624b\u52bf\u7684\u81ea\u7136\u6027\u548c\u793e\u4ea4\u4e92\u52a8\u6027\u3002", "method": "DyaDiT\u91c7\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u8f93\u5165\u53cc\u4eba\u5bf9\u8bdd\u97f3\u9891\u548c\u53ef\u9009\u7684\u793e\u4ea4\u4e0a\u4e0b\u6587\u6807\u8bb0\uff0c\u878d\u5408\u53cc\u65b9\u4fe1\u606f\u6355\u6349\u4e92\u52a8\u52a8\u6001\uff0c\u4f7f\u7528\u52a8\u4f5c\u5b57\u5178\u7f16\u7801\u52a8\u4f5c\u5148\u9a8c\uff0c\u5e76\u53ef\u9009\u62e9\u6027\u5730\u5229\u7528\u5bf9\u8bdd\u4f19\u4f34\u7684\u624b\u52bf\u751f\u6210\u66f4\u5177\u54cd\u5e94\u6027\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u6807\u51c6\u52a8\u4f5c\u751f\u6210\u6307\u6807\u8bc4\u4f30\u548c\u5b9a\u91cf\u7528\u6237\u7814\u7a76\u4e2d\uff0cDyaDiT\u4e0d\u4ec5\u5728\u5ba2\u89c2\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u7528\u6237\u504f\u597d\u65b9\u9762\u83b7\u5f97\u5f3a\u70c8\u8ba4\u53ef\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u793e\u4ea4\u53cb\u597d\u7684\u52a8\u4f5c\u751f\u6210\u80fd\u529b\u3002", "conclusion": "DyaDiT\u901a\u8fc7\u8003\u8651\u793e\u4ea4\u4e0a\u4e0b\u6587\u548c\u5bf9\u8bdd\u53cc\u65b9\u7684\u4e92\u52a8\u52a8\u6001\uff0c\u80fd\u591f\u751f\u6210\u66f4\u52a0\u81ea\u7136\u3001\u793e\u4ea4\u53c2\u4e0e\u5ea6\u66f4\u9ad8\u7684\u5bf9\u8bdd\u624b\u52bf\uff0c\u4e3a\u6570\u5b57\u4eba\u7c7b\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23169", "abs": "https://arxiv.org/abs/2602.23169", "authors": ["Xiaole Tang", "Xiaoyi He", "Jiayi Xu", "Xiang Gu", "Jian Sun"], "title": "Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration", "comment": null, "summary": "Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.", "AI": {"tldr": "BaryIR\u662f\u4e00\u4e2a\u901a\u8fc7Wasserstein\u91cd\u5fc3\u7a7a\u95f4\u5bf9\u9f50\u591a\u6e90\u9000\u5316\u7279\u5f81\u7684\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5168\u5408\u4e00\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5bf9\u5206\u5e03\u5916\u9000\u5316\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5168\u5408\u4e00\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u867d\u7136\u80fd\u5904\u7406\u591a\u79cd\u9000\u5316\u7c7b\u578b\uff0c\u4f46\u5bf9\u5206\u5e03\u5916\u9000\u5316\u573a\u666f\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u591a\u6e90\u9000\u5316\u7279\u5f81\u5206\u5e03\u662f\u7531\u4e0d\u540c\u9000\u5316\u7279\u5b9a\u504f\u79fb\u4ece\u5e95\u5c42\u9000\u5316\u65e0\u5173\u5206\u5e03\u4e2d\u8bf1\u5bfc\u4ea7\u751f\u7684\uff0c\u56e0\u6b64\u6062\u590d\u8fd9\u79cd\u5171\u4eab\u5206\u5e03\u5bf9\u4e8e\u5b9e\u73b0\u8de8\u9000\u5316\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faBaryIR\u6846\u67b6\uff1a1\uff09\u5728Wasserstein\u91cd\u5fc3\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u591a\u6e90\u9000\u5316\u7279\u5f81\uff0c\u8be5\u7a7a\u95f4\u901a\u8fc7\u6700\u5c0f\u5316\u5230\u591a\u6e90\u9000\u5316\u5206\u5e03\u7684Wasserstein\u8ddd\u79bb\u5e73\u5747\u503c\u6765\u5efa\u6a21\u9000\u5316\u65e0\u5173\u5206\u5e03\uff1b2\uff09\u5f15\u5165\u6b8b\u5dee\u5b50\u7a7a\u95f4\uff0c\u5176\u5d4c\u5165\u76f8\u4e92\u5bf9\u6bd4\u540c\u65f6\u4fdd\u6301\u4e0eWB\u5d4c\u5165\u6b63\u4ea4\uff1b3\uff09\u663e\u5f0f\u89e3\u8026\u4e24\u4e2a\u6b63\u4ea4\u7a7a\u95f4\uff1aWB\u7a7a\u95f4\u7f16\u7801\u8de8\u9000\u5316\u5171\u4eab\u7684\u9000\u5316\u65e0\u5173\u4e0d\u53d8\u5185\u5bb9\uff0c\u6b8b\u5dee\u5b50\u7a7a\u95f4\u81ea\u9002\u5e94\u4fdd\u7559\u9000\u5316\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "BaryIR\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u5168\u5408\u4e00\u65b9\u6cd5\u7ade\u4e89\uff0c\u5728\u672a\u89c1\u9000\u5316\u7c7b\u578b\u548c\u7ea7\u522b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u6709\u9650\u9000\u5316\u7c7b\u578b\u4e0a\u8bad\u7ec3\u5e76\u5728\u6df7\u5408\u9000\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8bc4\u4f30\u65f6\uff0c\u4e5f\u80fd\u5b66\u4e60\u5230\u6cdb\u5316\u6027\u5f3a\u7684\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7Wasserstein\u91cd\u5fc3\u7a7a\u95f4\u5bf9\u9f50\u548c\u6b63\u4ea4\u7a7a\u95f4\u89e3\u8026\uff0cBaryIR\u6709\u6548\u7f13\u89e3\u4e86\u5bf9\u5206\u5e03\u5185\u9000\u5316\u7684\u8fc7\u62df\u5408\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u9000\u5316\u65e0\u5173\u5171\u4eab\u4e0d\u53d8\u6027\u7684\u81ea\u9002\u5e94\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u5408\u4e00\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.23177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23177", "abs": "https://arxiv.org/abs/2602.23177", "authors": ["Bin Zeng", "Johannes K\u00fcnzel", "Anna Hilsmann", "Peter Eisert"], "title": "Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms", "comment": "published at VISAPP 2026", "summary": "Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u5b9e\u65f6\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\uff0c\u5229\u7528\u5b89\u88c5\u5728\u5217\u8f66\u4e0a\u7684\u5355\u6444\u50cf\u5934\u626b\u63cf\u7ad9\u53f0\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u6a21\u578b\u548c\u865a\u62df\u8ba1\u6570\u5e26\u5b9e\u73b0\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u8ba1\u6570\u3002", "motivation": "\u94c1\u8def\u7ad9\u53f0\u5b9e\u65f6\u4eba\u7fa4\u8ba1\u6570\u5bf9\u5b89\u5168\u548c\u5bb9\u91cf\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8ddf\u8e2a\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u9759\u6001\u6444\u50cf\u5934\u6216\u5ffd\u7565\u8fd0\u52a8\u5efa\u6a21\u7684\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5728\u5217\u8f66\u8fdb\u7ad9\u65f6\u7684\u52a8\u6001\u6761\u4ef6\u4e0b\uff08\u5bc6\u96c6\u906e\u6321\u3001\u6444\u50cf\u5934\u8fd0\u52a8\u3001\u900f\u89c6\u7578\u53d8\uff09\u8ba1\u6570\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u7269\u7406\u7ea6\u675f\u8ddf\u8e2a\u6846\u67b6\uff0c\u5c06\u68c0\u6d4b\u3001\u5916\u89c2\u548c3D\u8fd0\u52a8\u63a8\u7406\u7edf\u4e00\u5230\u5b9e\u65f6\u6d41\u7a0b\u4e2d\u3002\u96c6\u6210\u8fc1\u79fb\u5b66\u4e60\u7684YOLOv11m\u68c0\u6d4b\u5668\u548cEfficientNet-B0\u5916\u89c2\u7f16\u7801\u5230DeepSORT\u4e2d\uff0c\u5f15\u5165\u7269\u7406\u7ea6\u675f\u5361\u5c14\u66fc\u6a21\u578b\uff08Phys-3D\uff09\uff0c\u901a\u8fc7\u9488\u5b54\u51e0\u4f55\u5f3a\u5236\u7269\u7406\u4e0a\u5408\u7406\u76843D\u8fd0\u52a8\u52a8\u529b\u5b66\u3002\u4e3a\u5e94\u5bf9\u906e\u6321\u4e0b\u7684\u8ba1\u6570\u8106\u5f31\u6027\uff0c\u5b9e\u73b0\u5e26\u6301\u4e45\u6027\u7684\u865a\u62df\u8ba1\u6570\u5e26\u3002", "result": "\u5728\u5e73\u53f0\u57fa\u51c6\u6d4b\u8bd5MOT-RailwayPlatformCrowdHead\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u8ba1\u6570\u8bef\u5dee\u964d\u4f4e\u52302.97%\uff0c\u5728\u8fd0\u52a8\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u7b2c\u4e00\u539f\u7406\u51e0\u4f55\u548c\u8fd0\u52a8\u5148\u9a8c\uff0c\u80fd\u591f\u5728\u5b89\u5168\u5173\u952e\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u4eba\u7fa4\u8ba1\u6570\uff0c\u6709\u52a9\u4e8e\u6709\u6548\u7684\u5217\u8f66\u8c03\u5ea6\u548c\u7ad9\u53f0\u5b89\u5168\u7ba1\u7406\u3002"}}
{"id": "2602.23191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23191", "abs": "https://arxiv.org/abs/2602.23191", "authors": ["Xinyuan Chen", "Yao Xu", "Shaowen Wang", "Pengjie Song", "Bowen Deng"], "title": "Uni-Animator: Towards Unified Visual Colorization", "comment": "10 pages, 8 figures. Submitted to CVPR 2026", "summary": "We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.", "AI": {"tldr": "Uni-Animator\u662f\u4e00\u4e2a\u57fa\u4e8eDiffusion Transformer\u7684\u7edf\u4e00\u56fe\u50cf\u548c\u89c6\u9891\u8349\u56fe\u7740\u8272\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u989c\u8272\u8f6c\u79fb\u4e0d\u7cbe\u786e\u3001\u7269\u7406\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8349\u56fe\u7740\u8272\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\uff0c\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1\uff09\u4f7f\u7528\u5355\u4e2a\u6216\u591a\u4e2a\u53c2\u8003\u65f6\u989c\u8272\u8f6c\u79fb\u4e0d\u7cbe\u786e\uff1b2\uff09\u9ad8\u9891\u7269\u7406\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\uff1b3\uff09\u5927\u8fd0\u52a8\u573a\u666f\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\uff0c\u5b58\u5728\u8fd0\u52a8\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u901a\u8fc7\u5b9e\u4f8b\u8865\u4e01\u5d4c\u5165\u5b9e\u73b0\u89c6\u89c9\u53c2\u8003\u589e\u5f3a\uff0c\u7cbe\u786e\u5bf9\u9f50\u548c\u878d\u5408\u53c2\u8003\u989c\u8272\u4fe1\u606f\uff1b2\uff09\u4f7f\u7528\u7269\u7406\u7279\u5f81\u8fdb\u884c\u7269\u7406\u7ec6\u8282\u5f3a\u5316\uff0c\u6709\u6548\u6355\u6349\u548c\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\uff1b3\uff09\u57fa\u4e8e\u8349\u56fe\u7684\u52a8\u6001RoPE\u7f16\u7801\uff0c\u81ea\u9002\u5e94\u5efa\u6a21\u8fd0\u52a8\u611f\u77e5\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUni-Animator\u5728\u56fe\u50cf\u548c\u89c6\u9891\u8349\u56fe\u7740\u8272\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e0e\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u89e3\u9501\u4e86\u7edf\u4e00\u7684\u8de8\u57df\u80fd\u529b\uff0c\u5177\u6709\u9ad8\u7ec6\u8282\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "Uni-Animator\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u8349\u56fe\u7740\u8272\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u7684\u7edf\u4e00\u5904\u7406\uff0c\u5728\u989c\u8272\u8f6c\u79fb\u7cbe\u5ea6\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.23217", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.23217", "abs": "https://arxiv.org/abs/2602.23217", "authors": ["Alaa El Ichi", "Khalide Jbilou"], "title": "Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks", "comment": null, "summary": "This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u7ef4\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u5e7f\u4e49\u7231\u56e0\u65af\u5766MLP\uff08GE-MLP\uff09\uff0c\u901a\u8fc7\u7231\u56e0\u65af\u5766\u79ef\u76f4\u63a5\u5728\u5f20\u91cf\u4e0a\u64cd\u4f5c\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u8868\u8ff0\u53d7\u9650\u4e8e\u57fa\u4e8e\u77e9\u9635\u7684\u601d\u7ef4\uff1a\u6807\u51c6\u67b6\u6784\u4f9d\u8d56\u77e9\u9635\u503c\u6743\u91cd\u548c\u5411\u91cf\u503c\u504f\u7f6e\uff0c\u9700\u8981\u8fdb\u884c\u7ed3\u6784\u6241\u5e73\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u7136\u53ef\u8868\u8fbe\u4efb\u52a1\u7684\u7a7a\u95f4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u907f\u514d\u4fe1\u606f\u635f\u5931\u3001\u66f4\u7075\u6d3b\u5730\u63a7\u5236\u7ef4\u5ea6\u4fdd\u6301\u6216\u6536\u7f29\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u7231\u56e0\u65af\u5766MLP\uff08GE-MLP\uff09\u7684\u591a\u7ef4\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u5f20\u91cf\u503c\u53c2\u6570\uff0c\u901a\u8fc7\u7231\u56e0\u65af\u5766\u79ef\u76f4\u63a5\u5728\u5f20\u91cf\u4e0a\u64cd\u4f5c\uff0c\u80fd\u591f\u663e\u5f0f\u63a7\u5236\u54ea\u4e9b\u7ef4\u5ea6\u88ab\u4fdd\u7559\u6216\u6536\u7f29\uff0c\u800c\u4e0d\u9700\u8981\u4fe1\u606f\u635f\u5931\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u63a8\u5bfc\u8bc1\u660e\uff0c\u5206\u7c7b\u3001\u5206\u5272\u548c\u68c0\u6d4b\u90fd\u662fMTL\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u4ec5\u5728\u5f62\u5f0f\u5316\u5b9a\u4e49\u7684\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u7ef4\u5ea6\u914d\u7f6e\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u8bc1\u660e\u8be5\u4efb\u52a1\u7a7a\u95f4\u4e25\u683c\u5927\u4e8e\u57fa\u4e8e\u77e9\u9635\u7684\u8868\u8ff0\u6240\u80fd\u539f\u751f\u8868\u8fbe\u7684\u7a7a\u95f4\uff0c\u652f\u6301\u65f6\u7a7a\u6216\u8de8\u6a21\u6001\u9884\u6d4b\u7b49\u9700\u8981\u7834\u574f\u6027\u6241\u5e73\u5316\u7684\u4efb\u52a1\u914d\u7f6e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u901a\u8fc7\u5f20\u91cf\u4ee3\u6570\u89c6\u89d2\u7406\u89e3\u3001\u6bd4\u8f83\u548c\u8bbe\u8ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u77e9\u9635\u601d\u7ef4\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u8868\u8fbe\u529b\u66f4\u5f3a\u7684\u4efb\u52a1\u8868\u8ff0\u3002"}}
{"id": "2602.23297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23297", "abs": "https://arxiv.org/abs/2602.23297", "authors": ["Yiqing Wang", "Chunming He", "Ming-Chen Lu", "Mercy Pawar", "Leslie Niziol", "Maria Woodward", "Sina Farsiu"], "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM", "comment": null, "summary": "Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.", "AI": {"tldr": "PRIMA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u7279\u5f81\u548c\u4e34\u5e8a\u5143\u6570\u636e\uff0c\u5229\u7528\u98ce\u9669-\u75be\u75c5\u76f8\u5173\u6027\u77e5\u8bc6\u548c\u56db\u79cd\u4e92\u8865\u635f\u5931\u51fd\u6570\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75be\u75c5\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u8bca\u65ad\u65b9\u6cd5\u901a\u5e38\u5c06\u4e34\u5e8a\u5143\u6570\u636e\u89c6\u4e3a\u5b64\u7acb\u6807\u7b7e\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e34\u5e8a\u63cf\u8ff0\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u89c6\u89c9\u8868\u73b0\u548c\u4e34\u5e8a\u5143\u6570\u636e\u7684\u6846\u67b6\u3002", "method": "1. \u901a\u8fc7RAG\u6784\u5efa\u98ce\u9669-\u75be\u75c5\u76f8\u5173\u6027\u4e13\u5bb6\u8bed\u6599\u5e93\uff0c\u4f18\u5316Clinical ModernBERT\u6587\u672c\u7f16\u7801\u5668\uff1b2. \u91c7\u7528DINOv3\u548c\u4f18\u5316BERT\u7684\u53cc\u7f16\u7801\u5668\u9884\u8bad\u7ec3\u7b56\u7565\uff1b3. \u8bbe\u8ba1\u56db\u79cd\u4e92\u8865\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u591a\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff1b4. \u4f7f\u7528Qwen-3\u878d\u5408\u5bf9\u9f50\u7279\u5f81\u8fdb\u884c\u75be\u75c5\u5206\u7c7b\u3002", "result": "PRIMA\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u534f\u8c03\u50cf\u7d20\u7ea7\u7279\u5f81\u548c\u62bd\u8c61\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u6216\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "PRIMA\u6846\u67b6\u6210\u529f\u5c06\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6574\u5408\u5230\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u98ce\u9669\u96c6\u6210\u56fe\u50cf-\u5143\u6570\u636e\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u8bca\u65ad\u4e2d\u89c6\u89c9\u7279\u5f81\u548c\u4e34\u5e8a\u5143\u6570\u636e\u7684\u6709\u6548\u878d\u5408\u3002"}}
{"id": "2602.23306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23306", "abs": "https://arxiv.org/abs/2602.23306", "authors": ["Yiran Guan", "Sifan Tu", "Dingkang Liang", "Linghao Zhu", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Yuliang Liu", "Xiang Bai"], "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding", "comment": "Accept by ICLR 2026", "summary": "Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.", "AI": {"tldr": "ThinkOmni\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u548c\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u5927\u63a8\u7406\u6a21\u578b\u6307\u5bfc\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\uff0c\u5c06\u6587\u672c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5230\u5168\u6a21\u6001\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u611f\u77e5\u591a\u79cd\u6a21\u6001\uff0c\u4f46\u7f3a\u4e4f\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u800c\u901a\u8fc7\u989d\u5916\u8bad\u7ec3\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u9762\u4e34\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\u3001\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faThinkOmni\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) LRM-as-a-Guide\uff1a\u5229\u7528\u73b0\u6210\u7684\u5927\u63a8\u7406\u6a21\u578b\u6307\u5bfc\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\uff1b2) Stepwise Contrastive Scaling\uff1a\u81ea\u9002\u5e94\u5e73\u8861\u611f\u77e5\u548c\u63a8\u7406\u4fe1\u53f7\uff0c\u65e0\u9700\u624b\u52a8\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cThinkOmni\u6301\u7eed\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4e3b\u8981\u7ed3\u679c\u5728MathVista\u4e0a\u8fbe\u523070.2\u5206\uff0c\u5728MMAU\u4e0a\u8fbe\u523075.5\u5206\u3002", "conclusion": "ThinkOmni\u4e3a\u5168\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u63a8\u7406\u80fd\u529b\u7684\u6cdb\u5316\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.23339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23339", "abs": "https://arxiv.org/abs/2602.23339", "authors": ["Tilemachos Aravanis", "Vladan Stojni\u0107", "Bill Psomas", "Nikos Komodakis", "Giorgos Tolias"], "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?", "comment": null, "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u9002\u914d\u5668\uff0c\u901a\u8fc7\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u652f\u6301\u7279\u5f81\u6765\u5b66\u4e60\u8f7b\u91cf\u7ea7\u7684\u6bcf\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u96f6\u6837\u672c\u4e0e\u5168\u76d1\u7763\u5206\u5272\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u5206\u5272\uff08OVS\uff09\u867d\u7136\u6269\u5c55\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u8bc6\u522b\u80fd\u529b\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u5168\u76d1\u7763\u65b9\u6cd5\uff0c\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u8bad\u7ec3VLM\u65f6\u4f7f\u7528\u7684\u7c97\u7c92\u5ea6\u56fe\u50cf\u7ea7\u76d1\u7763\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u3002", "method": "\u5f15\u5165\u5c11\u6837\u672c\u8bbe\u7f6e\uff0c\u901a\u8fc7\u50cf\u7d20\u6807\u6ce8\u56fe\u50cf\u7684\u652f\u6301\u96c6\u589e\u5f3a\u6587\u672c\u63d0\u793a\uff1b\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u9002\u914d\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u6bcf\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u652f\u6301\u7279\u5f81\uff0c\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u66f4\u5f3a\u534f\u540c\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u96f6\u6837\u672c\u4e0e\u76d1\u7763\u5206\u5272\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u652f\u6301\u6301\u7eed\u6269\u5c55\u7684\u652f\u6301\u96c6\uff0c\u5e76\u9002\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5982\u4e2a\u6027\u5316\u5206\u5272\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u652f\u6301\u7279\u5f81\u7684\u68c0\u7d22\u589e\u5f3a\u6d4b\u8bd5\u65f6\u9002\u914d\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u76d1\u7763\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
