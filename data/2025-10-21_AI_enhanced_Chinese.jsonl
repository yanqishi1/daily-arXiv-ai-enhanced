{"id": "2510.15972", "categories": ["cs.CL", "cs.AI", "81P68 (Primary), 68T50, 68T07 (Secondary)", "I.2.7; F.1.2"], "pdf": "https://arxiv.org/pdf/2510.15972", "abs": "https://arxiv.org/abs/2510.15972", "authors": ["Ling Sun", "Peter Sullivan", "Michael Martin", "Yun Zhou"], "title": "Quantum NLP models on Natural Language Inference", "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum\n  AI and NLP 2025 Conference", "summary": "Quantum natural language processing (QNLP) offers a novel approach to\nsemantic modeling by embedding compositional structure directly into quantum\ncircuits. This paper investigates the application of QNLP models to the task of\nNatural Language Inference (NLI), comparing quantum, hybrid, and classical\ntransformer-based models under a constrained few-shot setting. Using the lambeq\nlibrary and the DisCoCat framework, we construct parameterized quantum circuits\nfor sentence pairs and train them for both semantic relatedness and inference\nclassification. To assess efficiency, we introduce a novel\ninformation-theoretic metric, Information Gain per Parameter (IGPP), which\nquantifies learning dynamics independent of model size. Our results demonstrate\nthat quantum models achieve performance comparable to classical baselines while\noperating with dramatically fewer parameters. The Quantum-based models\noutperform randomly initialized transformers in inference and achieve lower\ntest error on relatedness tasks. Moreover, quantum models exhibit significantly\nhigher per-parameter learning efficiency (up to five orders of magnitude more\nthan classical counterparts), highlighting the promise of QNLP in low-resource,\nstructure-sensitive settings. To address circuit-level isolation and promote\nparameter sharing, we also propose a novel cluster-based architecture that\nimproves generalization by tying gate parameters to learned word clusters\nrather than individual tokens.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u91cf\u5b50\u3001\u6df7\u5408\u548c\u7ecf\u5178\u6a21\u578b\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u589e\u76ca\u53c2\u6570\u6bd4\u6307\u6807\u6765\u8bc4\u4f30\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u8bed\u4e49\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\uff0c\u91cf\u5b50\u6a21\u578b\u80fd\u5426\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "method": "\u4f7f\u7528lambeq\u5e93\u548cDisCoCat\u6846\u67b6\u6784\u5efa\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5904\u7406\u53e5\u5b50\u5bf9\uff0c\u8bad\u7ec3\u8bed\u4e49\u76f8\u5173\u6027\u548c\u63a8\u7406\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u7684\u67b6\u6784\u6765\u4fc3\u8fdb\u53c2\u6570\u5171\u4eab\u3002", "result": "\u91cf\u5b50\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4e0e\u7ecf\u5178\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u6570\u91cf\u5927\u5e45\u51cf\u5c11\uff1b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684transformer\u6a21\u578b\uff1b\u91cf\u5b50\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u6bd4\u7ecf\u5178\u6a21\u578b\u9ad8\u51fa\u6700\u591a\u4e94\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u4f4e\u8d44\u6e90\u3001\u7ed3\u6784\u654f\u611f\u7684\u573a\u666f\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u91cf\u5b50\u6a21\u578b\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408ChatGPT\u548cClaude\u4e24\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u5171\u8bc6\u65b9\u6cd5\u63d0\u5347\u80f8\u90e8X\u5149\u7247\u8bca\u65ad\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u63d0\u9ad8AI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u7684\u53ef\u4fe1\u5ea6\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e95%\u8f93\u51fa\u76f8\u4f3c\u6027\u9608\u503c\u7684\u5171\u8bc6\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u5408\u6210\u4e34\u5e8a\u7b14\u8bb0\u7684\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5355\u6a21\u6001\u8bbe\u7f6e\u4e0b\uff0cChatGPT\u548cClaude\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a62.8%\u548c76.9%\uff0c\u5171\u8bc6\u65b9\u6cd5\u63d0\u5347\u81f377.6%\u3002\u591a\u6a21\u6001\u8f93\u5165\u4e0b\uff0cChatGPT\u548cClaude\u5206\u522b\u8fbe\u523084%\u548c76%\uff0c\u5171\u8bc6\u51c6\u786e\u7387\u8fbe\u523091.3%\u3002", "conclusion": "\u57fa\u4e8e\u534f\u8bae\u7684\u878d\u5408\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u6574\u5408\u4e92\u8865\u6a21\u6001\u548c\u4f7f\u7528\u8f93\u51fa\u7ea7\u5171\u8bc6\u53ef\u663e\u8457\u63d0\u9ad8AI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u4ef7\u503c\u3002"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86CorrectBench\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u81ea\u6821\u6b63\u80fd\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u6548\u7387\uff0c\u800c\u7b80\u5355\u7684\u601d\u7ef4\u94fe\u57fa\u7ebf\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u81ea\u6821\u6b63\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u4ecd\u7136\u7f3a\u4e4f\uff0c\u4e14LLM\u662f\u5426\u771f\u80fd\u81ea\u6211\u6821\u6b63\u4ecd\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5f00\u53d1CorrectBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5185\u5728\u3001\u5916\u90e8\u548c\u5fae\u8c03\u4e09\u79cd\u81ea\u6821\u6b63\u7b56\u7565\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u6548\u679c\u3002", "result": "1) \u81ea\u6821\u6b63\u65b9\u6cd5\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff1b2) \u6df7\u5408\u4e0d\u540c\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u6539\u8fdb\u4f46\u964d\u4f4e\u6548\u7387\uff1b3) \u63a8\u7406LLM\u5728\u989d\u5916\u81ea\u6821\u6b63\u4e0b\u4f18\u5316\u6709\u9650\u4e14\u65f6\u95f4\u6210\u672c\u9ad8\uff1b4) \u7b80\u5355\u601d\u7ef4\u94fe\u57fa\u7ebf\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u81ea\u6821\u6b63\u6709\u6f5c\u529b\u589e\u5f3aLLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u9700\u5e73\u8861\u63a8\u7406\u80fd\u529b\u4e0e\u64cd\u4f5c\u6548\u7387\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u4f18\u5316\u8fd9\u4e00\u5e73\u8861\u3002"}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "EvolveR\u662f\u4e00\u4e2a\u8ba9LLM\u667a\u80fd\u4f53\u901a\u8fc7\u5b8c\u6574\u95ed\u73af\u7ecf\u9a8c\u751f\u547d\u5468\u671f\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u5305\u542b\u79bb\u7ebf\u81ea\u6211\u84b8\u998f\u548c\u5728\u7ebf\u4ea4\u4e92\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u7cfb\u7edf\u5b66\u4e60\u7684\u80fd\u529b\u3002\u73b0\u6709\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u5f25\u8865\u5916\u90e8\u77e5\u8bc6\u5dee\u8ddd\uff0c\u672a\u80fd\u89e3\u51b3\u66f4\u6839\u672c\u7684\u9650\u5236\uff1a\u65e0\u6cd5\u8fed\u4ee3\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3002", "method": "EvolveR\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff1a(1)\u79bb\u7ebf\u81ea\u6211\u84b8\u998f\uff1a\u5c06\u667a\u80fd\u4f53\u4ea4\u4e92\u8f68\u8ff9\u5408\u6210\u4e3a\u7ed3\u6784\u5316\u3001\u62bd\u8c61\u53ef\u91cd\u7528\u7684\u7b56\u7565\u539f\u5219\u5e93\uff1b(2)\u5728\u7ebf\u4ea4\u4e92\uff1a\u667a\u80fd\u4f53\u4e0e\u4efb\u52a1\u4ea4\u4e92\u5e76\u4e3b\u52a8\u68c0\u7d22\u84b8\u998f\u539f\u5219\u6307\u5bfc\u51b3\u7b56\uff0c\u79ef\u7d2f\u591a\u6837\u5316\u884c\u4e3a\u8f68\u8ff9\u3002\u91c7\u7528\u7b56\u7565\u5f3a\u5316\u673a\u5236\u8fed\u4ee3\u66f4\u65b0\u667a\u80fd\u4f53\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvolveR\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u4e0d\u4ec5\u4ece\u5916\u90e8\u6570\u636e\u5b66\u4e60\uff0c\u8fd8\u4ece\u81ea\u8eab\u884c\u4e3a\u540e\u679c\u5b66\u4e60\u7684\u5168\u9762\u84dd\u56fe\uff0c\u4e3a\u66f4\u81ea\u4e3b\u548c\u6301\u7eed\u6539\u8fdb\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.15963", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86ESCA\u6846\u67b6\u548cSGClip\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65f6\u7a7a\u7406\u89e3\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u573a\u666f\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u9ad8\u5c42\u89c6\u89c9-\u58f0\u97f3-\u6587\u672c\u5bf9\uff0c\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u89c6\u89c9\u5185\u5bb9\u4e0e\u6587\u672c\u8bed\u4e49\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u5316\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86SGClip\u6a21\u578b\uff0c\u57fa\u4e8eCLIP\u67b6\u6784\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7ba1\u9053\u572887K+\u5f00\u653e\u57df\u89c6\u9891\u4e0a\u8bad\u7ec3\uff0c\u5229\u7528\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u76d1\u7763\u548c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u652f\u6301\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "result": "SGClip\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0cESCA\u6846\u67b6\u663e\u8457\u63d0\u5347\u5f00\u6e90\u548c\u5546\u4e1aMLLM\u6027\u80fd\uff0c\u5728\u4e24\u4e2a\u5177\u8eab\u73af\u5883\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5927\u5e45\u51cf\u5c11\u611f\u77e5\u9519\u8bef\u3002", "conclusion": "ESCA\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u7a7a\u95f4-\u65f6\u95f4\u7406\u89e3\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5177\u8eab\u667a\u80fd\u80fd\u529b\uff0c\u4f7f\u5f00\u6e90\u6a21\u578b\u8d85\u8d8a\u4e13\u6709\u57fa\u7ebf\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cf\u5316\u4e86\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u9636\u6bb5\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u8bc4\u4f306\u4e2aLLM\u6a21\u578b\u57285\u79cd\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0CoT-few-shot\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u7cbe\u5ea6-\u53ec\u56de\u5e73\u8861\uff0c\u5e76\u63a8\u8350\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u7cfb\u7edf\u5206\u6790\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u732e\u7b5b\u9009\u81ea\u52a8\u5316\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u4e3a\u4efb\u52a1\u81ea\u9002\u5e94LLM\u90e8\u7f72\u63d0\u4f9b\u6bd4\u8f83\u57fa\u51c6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u8bc4\u4f306\u4e2aLLM\u6a21\u578b\uff08GPT-4o\u3001GPT-4o-mini\u3001DeepSeek-Chat-V3\u3001Gemini-2.5-Flash\u3001Claude-3.5-Haiku\u3001Llama-4-Maverick\uff09\u57285\u79cd\u63d0\u793a\u7c7b\u578b\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001CoT-\u5c11\u6837\u672c\u3001\u81ea\u6211\u53cd\u601d\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u6307\u6807\u3002", "result": "\u53d1\u73b0\u660e\u663e\u7684\u6a21\u578b-\u63d0\u793a\u4ea4\u4e92\u6548\u5e94\uff1aCoT-\u5c11\u6837\u672c\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u7cbe\u5ea6-\u53ec\u56de\u5e73\u8861\uff1b\u96f6\u6837\u672c\u5728\u9ad8\u7075\u654f\u5ea6\u7b5b\u9009\u4e2d\u6700\u5927\u5316\u53ec\u56de\u7387\uff1b\u81ea\u6211\u53cd\u601d\u56e0\u8fc7\u5ea6\u5305\u5bb9\u6027\u548c\u6a21\u578b\u95f4\u4e0d\u7a33\u5b9a\u6027\u8868\u73b0\u4e0d\u4f73\u3002GPT-4o\u548cDeepSeek\u6574\u4f53\u8868\u73b0\u7a33\u5065\uff0cGPT-4o-mini\u5728\u663e\u8457\u66f4\u4f4e\u6210\u672c\u4e0b\u8868\u73b0\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "\u63a8\u8350\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u4f4e\u6210\u672c\u6a21\u578b\u548c\u7ed3\u6784\u5316\u63d0\u793a\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\uff0c\u4ec5\u5c06\u8fb9\u754c\u6848\u4f8b\u5347\u7ea7\u5230\u9ad8\u5bb9\u91cf\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86LLM\u5728\u6587\u732e\u7b5b\u9009\u81ea\u52a8\u5316\u4e2d\u4e0d\u5747\u8861\u4f46\u6709\u524d\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.15991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15991", "abs": "https://arxiv.org/abs/2510.15991", "authors": ["Huiming Yang"], "title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "comment": "13 pages", "summary": "The sparse cross-modality detector offers more advantages than its\ncounterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of\nadaptability for downstream tasks and computational cost savings. However,\nexisting sparse detectors overlook the quality of token representation, leaving\nit with a sub-optimal foreground quality and limited performance. In this\npaper, we identify that the geometric structure preserved and the class\ndistribution are the key to improving the performance of the sparse detector,\nand propose a Sparse Selector (SS). The core module of SS is Ray-Aware\nSupervision (RAS), which preserves rich geometric information during the\ntraining stage, and Class-Balanced Supervision, which adaptively reweights the\nsalience of class semantics, ensuring that tokens associated with small objects\nare retained during token sampling. Thereby, outperforming other sparse\nmulti-modal detectors in the representation of tokens. Additionally, we design\nRay Positional Encoding (Ray PE) to address the distribution differences\nbetween the LiDAR modality and the image. Finally, we integrate the\naforementioned module into an end-to-end sparse multi-modality detector, dubbed\nCrossRay3D. Experiments show that, on the challenging nuScenes benchmark,\nCrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,\nwhile running 1.84 faster than other leading methods. Moreover, CrossRay3D\ndemonstrates strong robustness even in scenarios where LiDAR or camera data are\npartially or entirely missing.", "AI": {"tldr": "\u63d0\u51faCrossRay3D\u7a00\u758f\u591a\u6a21\u60013D\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7Ray-Aware Supervision\u548cClass-Balanced Supervision\u63d0\u5347token\u8868\u793a\u8d28\u91cf\uff0c\u5728nuScenes\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u68c0\u6d4b\u5668\u5ffd\u89c6token\u8868\u793a\u8d28\u91cf\uff0c\u5bfc\u81f4\u524d\u666f\u8d28\u91cf\u4e0d\u4f73\u548c\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u53d1\u73b0\u51e0\u4f55\u7ed3\u6784\u4fdd\u6301\u548c\u7c7b\u522b\u5206\u5e03\u662f\u63d0\u5347\u7a00\u758f\u68c0\u6d4b\u5668\u6027\u80fd\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faSparse Selector(SS)\uff0c\u6838\u5fc3\u6a21\u5757\u5305\u62ec\uff1aRay-Aware Supervision(RAS)\u4fdd\u6301\u51e0\u4f55\u4fe1\u606f\uff0cClass-Balanced Supervision\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u7c7b\u522b\u8bed\u4e49\uff0c\u4ee5\u53caRay Positional Encoding\u89e3\u51b3LiDAR\u548c\u56fe\u50cf\u6a21\u6001\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u4e0a\u8fbe\u523072.4 mAP\u548c74.7 NDS\u7684SOTA\u6027\u80fd\uff0c\u6bd4\u5176\u4ed6\u9886\u5148\u65b9\u6cd5\u5feb1.84\u500d\uff0c\u4e14\u5728LiDAR\u6216\u76f8\u673a\u6570\u636e\u90e8\u5206\u6216\u5b8c\u5168\u7f3a\u5931\u65f6\u4ecd\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "CrossRay3D\u901a\u8fc7\u6539\u8fdbtoken\u8868\u793a\u8d28\u91cf\uff0c\u5728\u7a00\u758f\u591a\u6a21\u60013D\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16096", "abs": "https://arxiv.org/abs/2510.16096", "authors": ["Tina Behnia", "Puneesh Deora", "Christos Thrampoulidis"], "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization", "comment": "28 pages, 15 figures", "summary": "Language models are pretrained on sequences that blend statistical\nregularities (making text fluent) with factual associations between specific\ntokens (knowledge of facts). While recent work suggests that the variability of\ntheir interaction, such as paraphrases of factual associations, critically\ndetermines generalization ability, we lack a systematic analysis of these\nimpacts. This paper introduces a flexible synthetic testbed that combines a\nstatistical stream of generic tokens with an abstract factual stream of\nsource-target token pairs, enabling fine-grained control over their\ninteraction. The design enables the independent control of diversity nature by\nmanipulating stream composition (contextual structure) and the diversity level\nby varying which statistical streams each fact appears in. Through controlled\nexperiments, we find that while higher contextual diversity delays\nin-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)\nfactual generalization depends critically on contextual structure. In some\ncases, OOD performance follows the same trend as ID, but in others, diversity\nbecomes essential for non-trivial factual recall. Even when low diversity\nprohibits factual recall, optimal diversity levels depend on training duration.\nBeyond factual recall failures, we identify structures where statistical\ngeneralization fails independently, and others where both capabilities degrade.\nThis shows how the interplay between contextual design and diversity level\nimpacts different generalization aspects. Further, through a series of\ncontrolled interventions on the model components, we trace the OOD failures to\ndistinct optimization bottlenecks, highlighting the importance of the embedding\nand unembedding layers. Our synthetic framework allows us to isolate effects\nthat would be confounded in large-scale studies, offering a controlled testbed\nfor future investigations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u4e0e\u4e8b\u5b9e\u5173\u8054\u4e4b\u95f4\u7684\u4ea4\u4e92\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u548c\u7ed3\u6784\u5bf9\u4e8b\u5b9e\u6cdb\u5316\u6709\u590d\u6742\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u4e86\u4e0d\u540c\u7684\u4f18\u5316\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u4e0e\u4e8b\u5b9e\u5173\u8054\u4ea4\u4e92\u5f71\u54cd\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u7279\u522b\u662f\u8fd9\u79cd\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c06\u901a\u7528\u6807\u8bb0\u7684\u7edf\u8ba1\u6d41\u4e0e\u6e90-\u76ee\u6807\u6807\u8bb0\u5bf9\u7684\u4e8b\u5b9e\u6d41\u7ed3\u5408\uff0c\u80fd\u591f\u7cbe\u7ec6\u63a7\u5236\u5b83\u4eec\u7684\u4ea4\u4e92\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u7ed3\u6784\u548c\u591a\u6837\u6027\u6c34\u5e73\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u66f4\u9ad8\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4f1a\u5ef6\u8fdf\u5206\u5e03\u5185\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u5206\u5e03\u5916\u4e8b\u5b9e\u6cdb\u5316\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u4e0a\u4e0b\u6587\u7ed3\u6784\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5206\u5e03\u5916\u6027\u80fd\u4e0e\u5206\u5e03\u5185\u8d8b\u52bf\u4e00\u81f4\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u591a\u6837\u6027\u5bf9\u975e\u5e73\u51e1\u4e8b\u5b9e\u56de\u5fc6\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u548c\u591a\u6837\u6027\u6c34\u5e73\u7684\u76f8\u4e92\u4f5c\u7528\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u578b\u7ec4\u4ef6\u5e72\u9884\u53d1\u73b0\u5206\u5e03\u5916\u5931\u8d25\u6e90\u4e8e\u4e0d\u540c\u7684\u4f18\u5316\u74f6\u9888\uff0c\u7279\u522b\u662f\u5d4c\u5165\u548c\u89e3\u5d4c\u5165\u5c42\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16017", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16017", "abs": "https://arxiv.org/abs/2510.16017", "authors": ["Ibrahim Sheikh Mohamed", "Abdullah Yahya Abdullah Omaisan"], "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects", "comment": null, "summary": "Infrastructure in smart cities is increasingly monitored by networks of\nclosed circuit television (CCTV) cameras. Roads, bridges and tunnels develop\ncracks, potholes, and fluid leaks that threaten public safety and require\ntimely repair. Manual inspection is costly and hazardous, and existing\nautomatic systems typically address individual defect types or provide\nunstructured outputs that cannot directly guide maintenance crews. This paper\nproposes a comprehensive pipeline that leverages street CCTV streams for multi\ndefect detection and segmentation using the YOLO family of object detectors and\npasses the detections to a vision language model (VLM) for scene aware\nsummarization. The VLM generates a structured action plan in JSON format that\nincludes incident descriptions, recommended tools, dimensions, repair plans,\nand urgent alerts. We review literature on pothole, crack and leak detection,\nhighlight recent advances in large vision language models such as QwenVL and\nLLaVA, and describe the design of our early prototype. Experimental evaluation\non public datasets and captured CCTV clips demonstrates that the system\naccurately identifies diverse defects and produces coherent summaries. We\nconclude by discussing challenges and directions for scaling the system to city\nwide deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eCCTV\u76d1\u63a7\u7684\u667a\u80fd\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528YOLO\u8fdb\u884c\u591a\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5316\u7ef4\u4fee\u8ba1\u5212\u3002", "motivation": "\u667a\u80fd\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u76d1\u63a7\u9700\u6c42\u589e\u957f\uff0c\u4eba\u5de5\u68c0\u6d4b\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u73b0\u6709\u81ea\u52a8\u7cfb\u7edf\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u7f3a\u9677\u6216\u8f93\u51fa\u975e\u7ed3\u6784\u5316\u7ed3\u679c\uff0c\u65e0\u6cd5\u76f4\u63a5\u6307\u5bfc\u7ef4\u4fee\u5de5\u4f5c\u3002", "method": "\u5229\u7528\u8857\u9053CCTV\u6d41\u8fdb\u884c\u591a\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\uff08\u4f7f\u7528YOLO\u7cfb\u5217\u76ee\u6807\u68c0\u6d4b\u5668\uff09\uff0c\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u9012\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u611f\u77e5\u603b\u7ed3\uff0c\u751f\u6210\u5305\u542b\u4e8b\u4ef6\u63cf\u8ff0\u3001\u63a8\u8350\u5de5\u5177\u3001\u5c3a\u5bf8\u3001\u7ef4\u4fee\u8ba1\u5212\u548c\u7d27\u6025\u8b66\u62a5\u7684JSON\u683c\u5f0f\u7ed3\u6784\u5316\u884c\u52a8\u8ba1\u5212\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u6355\u83b7\u7684CCTV\u7247\u6bb5\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u7f3a\u9677\u5e76\u751f\u6210\u8fde\u8d2f\u7684\u603b\u7ed3\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5c06\u7cfb\u7edf\u6269\u5c55\u5230\u57ce\u5e02\u8303\u56f4\u90e8\u7f72\u7684\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2510.16173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16173", "abs": "https://arxiv.org/abs/2510.16173", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "comment": null, "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As\nthese systems become embedded in everyday practices, understanding public trust\nin them also becomes essential for responsible adoption and governance. Prior\nwork on trust in AI has largely drawn from psychology and human-computer\ninteraction, but there is a lack of computational, large-scale, and\nlongitudinal approaches to measuring trust and distrust in GenAI and large\nlanguage models (LLMs). This paper presents the first computational study of\nTrust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)\nspanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a\nrepresentative sample were combined with classification models to scale\nanalysis. We find that Trust and Distrust are nearly balanced over time, with\nshifts around major model releases. Technical performance and usability\ndominate as dimensions, while personal experience is the most frequent reason\nshaping attitudes. Distinct patterns also emerge across trustors (e.g.,\nexperts, ethicists, general users). Our results provide a methodological\nframework for large-scale Trust analysis and insights into evolving public\nperceptions of GenAI.", "AI": {"tldr": "\u9996\u4e2a\u5173\u4e8e\u751f\u6210\u5f0fAI\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u7684\u8ba1\u7b97\u7814\u7a76\uff0c\u4f7f\u75282022-2025\u5e74Reddit\u6570\u636e\uff0c\u53d1\u73b0\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u57fa\u672c\u5e73\u8861\uff0c\u6280\u672f\u6027\u80fd\u548c\u53ef\u7528\u6027\u662f\u4e3b\u8981\u7ef4\u5ea6\uff0c\u4e2a\u4eba\u7ecf\u9a8c\u662f\u6001\u5ea6\u5f62\u6210\u7684\u6700\u5e38\u89c1\u539f\u56e0\u3002", "motivation": "\u751f\u6210\u5f0fAI\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u7eb5\u5411\u7684\u4fe1\u4efb\u6d4b\u91cf\u65b9\u6cd5\uff0c\u9700\u8981\u7406\u89e3\u516c\u4f17\u4fe1\u4efb\u4ee5\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u91c7\u7528\u548c\u6cbb\u7406\u3002", "method": "\u4f7f\u7528\u591a\u5e74\u5ea6Reddit\u6570\u636e\u96c6\uff0839\u4e2a\u5b50\u7248\u5757\uff0c197,618\u4e2a\u5e16\u5b50\uff09\uff0c\u7ed3\u5408\u4f17\u5305\u6807\u6ce8\u548c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u6269\u5c55\u5206\u6790\u3002", "result": "\u4fe1\u4efb\u4e0e\u4e0d\u4fe1\u4efb\u968f\u65f6\u95f4\u57fa\u672c\u5e73\u8861\uff0c\u4e3b\u8981\u6a21\u578b\u53d1\u5e03\u65f6\u51fa\u73b0\u6ce2\u52a8\uff1b\u6280\u672f\u6027\u80fd\u548c\u53ef\u7528\u6027\u662f\u6700\u91cd\u8981\u7684\u7ef4\u5ea6\uff1b\u4e2a\u4eba\u7ecf\u9a8c\u662f\u6001\u5ea6\u5f62\u6210\u7684\u6700\u5e38\u89c1\u539f\u56e0\uff1b\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\uff08\u4e13\u5bb6\u3001\u4f26\u7406\u5b66\u5bb6\u3001\u666e\u901a\u7528\u6237\uff09\u5448\u73b0\u4e0d\u540c\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u4fe1\u4efb\u5206\u6790\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u516c\u4f17\u5bf9\u751f\u6210\u5f0fAI\u4e0d\u65ad\u6f14\u53d8\u7684\u8ba4\u77e5\uff0c\u4e3a\u8d1f\u8d23\u4efbAI\u91c7\u7528\u548c\u6cbb\u7406\u63d0\u4f9b\u89c1\u89e3\u3002"}}
{"id": "2510.16036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16036", "abs": "https://arxiv.org/abs/2510.16036", "authors": ["Zewen Li", "Zitong Yu", "Qilang Ye", "Weicheng Xie", "Wei Zhuo", "Linlin Shen"], "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection", "comment": "Accepted by IEEE Transactions on Instrumentation and Measurement\n  (TIM)", "summary": "The robust causal capability of Multimodal Large Language Models (MLLMs) hold\nthe potential of detecting defective objects in Industrial Anomaly Detection\n(IAD). However, most traditional IAD methods lack the ability to provide\nmulti-turn human-machine dialogues and detailed descriptions, such as the color\nof objects, the shape of an anomaly, or specific types of anomalies. At the\nsame time, methods based on large pre-trained models have not fully stimulated\nthe ability of large models in anomaly detection tasks. In this paper, we\nexplore the combination of rich text semantics with both image-level and\npixel-level information from images and propose IAD-GPT, a novel paradigm based\non MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate\ndetailed anomaly prompts for specific objects. These specific prompts from the\nlarge language model (LLM) are used to activate the detection and segmentation\nfunctions of the pre-trained visual-language model (i.e., CLIP). To enhance the\nvisual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein\nimage features interact with normal and abnormal text prompts to dynamically\nselect enhancement pathways, which enables language models to focus on specific\naspects of visual data, enhancing their ability to accurately interpret and\nrespond to anomalies within images. Moreover, we design a Multi-Mask Fusion\nmodule to incorporate mask as expert knowledge, which enhances the LLM's\nperception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA\ndatasets demonstrate our state-of-the-art performance on self-supervised and\nfew-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA\ndatasets. The codes are available at\n\\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.", "AI": {"tldr": "\u63d0\u51fa\u4e86IAD-GPT\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5f02\u5e38\u63d0\u793a\u751f\u6210\u5668\u3001\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u5668\u548c\u591a\u63a9\u7801\u878d\u5408\u6a21\u5757\uff0c\u5728MVTec-AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u591a\u8f6e\u4eba\u673a\u5bf9\u8bdd\u548c\u8be6\u7ec6\u63cf\u8ff0\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u6fc0\u53d1\u5927\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u5f02\u5e38\u63d0\u793a\u751f\u6210\u5668\u751f\u6210\u8be6\u7ec6\u5f02\u5e38\u63d0\u793a\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u5668\u589e\u5f3a\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u8bbe\u8ba1\u591a\u63a9\u7801\u878d\u5408\u6a21\u5757\u5c06\u63a9\u7801\u4f5c\u4e3a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u7ed3\u5408\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u4fe1\u606f\u3002", "result": "\u5728MVTec-AD\u548cVisA\u6570\u636e\u96c6\u7684\u81ea\u76d1\u7763\u548c\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "IAD-GPT\u6210\u529f\u7ed3\u5408\u4e86\u4e30\u5bcc\u6587\u672c\u8bed\u4e49\u4e0e\u56fe\u50cf\u4fe1\u606f\uff0c\u4e3a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86EgMM-Corpus\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u57c3\u53ca\u6587\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b3000\u591a\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6313\u4e2a\u6982\u5ff5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57c3\u53ca\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u4e2d\u4e1c\u548c\u975e\u6d32\u5730\u533a\u7684\u591a\u6a21\u6001\u6587\u5316\u591a\u6837\u6027\u6570\u636e\u96c6\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6587\u5316\u6570\u636e\u96c6\u6765\u51cf\u5c11\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u3002", "method": "\u8bbe\u8ba1\u548c\u8fd0\u884c\u65b0\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u6536\u96c6\u4e86\u6db5\u76d6\u5730\u6807\u3001\u98df\u7269\u548c\u6c11\u95f4\u4f20\u8bf4\u7b49313\u4e2a\u6982\u5ff5\u76843000\u591a\u5f20\u56fe\u50cf\uff0c\u6bcf\u4e2a\u6761\u76ee\u90fd\u7ecf\u8fc7\u6587\u5316\u771f\u5b9e\u6027\u548c\u591a\u6a21\u6001\u4e00\u81f4\u6027\u7684\u624b\u52a8\u9a8c\u8bc1\u3002", "result": "CLIP\u6a21\u578b\u5728EgMM-Corpus\u4e0a\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\uff1aTop-1\u51c6\u786e\u7387\u4e3a21.2%\uff0cTop-5\u51c6\u786e\u7387\u4e3a36.4%\u3002", "conclusion": "\u7ed3\u679c\u51f8\u663e\u4e86\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6587\u5316\u504f\u89c1\uff0c\u8bc1\u660e\u4e86EgMM-Corpus\u4f5c\u4e3a\u5f00\u53d1\u6587\u5316\u611f\u77e5\u6a21\u578b\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16070", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.IV", "H.5.5; H.1.2; I.4.0"], "pdf": "https://arxiv.org/pdf/2510.16070", "abs": "https://arxiv.org/abs/2510.16070", "authors": ["Mahta Khoobi", "Marc Sebastian von der Stueck", "Felix Barajas Ordonez", "Anca-Maria Iancu", "Eric Corban", "Julia Nowak", "Aleksandar Kargaliev", "Valeria Perelygina", "Anna-Sophie Schott", "Daniel Pinto dos Santos", "Christiane Kuhl", "Daniel Truhn", "Sven Nebelung", "Robert Siepmann"], "title": "Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography", "comment": "Preprint version - Under second revision at Radiology (manuscript\n  RAD-25-1348)", "summary": "Structured reporting (SR) and artificial intelligence (AI) may transform how\nradiologists interact with imaging studies. This prospective study (July to\nDecember 2024) evaluated the impact of three reporting modes: free-text (FT),\nstructured reporting (SR), and AI-assisted structured reporting (AI-SR), on\nimage analysis behavior, diagnostic accuracy, efficiency, and user experience.\nFour novice and four non-novice readers (radiologists and medical students)\neach analyzed 35 bedside chest radiographs per session using a customized\nviewer and an eye-tracking system. Outcomes included diagnostic accuracy\n(compared with expert consensus using Cohen's $\\kappa$), reporting time per\nradiograph, eye-tracking metrics, and questionnaire-based user experience.\nStatistical analysis used generalized linear mixed models with Bonferroni\npost-hoc tests with a significance level of ($P \\le .01$). Diagnostic accuracy\nwas similar in FT ($\\kappa = 0.58$) and SR ($\\kappa = 0.60$) but higher in\nAI-SR ($\\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \\pm 38$\ns (FT) to $37 \\pm 18$ s (SR) and $25 \\pm 9$ s (AI-SR) ($P < .001$). Saccade\ncounts for the radiograph field ($205 \\pm 135$ (FT), $123 \\pm 88$ (SR), $97 \\pm\n58$ (AI-SR)) and total fixation duration for the report field ($11 \\pm 5$ s\n(FT), $5 \\pm 3$ s (SR), $4 \\pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <\n.001$ each). Novice readers shifted gaze towards the radiograph in SR, while\nnon-novice readers maintained their focus on the radiograph. AI-SR was the\npreferred mode. In conclusion, SR improves efficiency by guiding visual\nattention toward the image, and AI-prefilled SR further enhances diagnostic\naccuracy and user satisfaction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u653e\u5c04\u5b66\u62a5\u544a\u6a21\u5f0f\uff08\u81ea\u7531\u6587\u672c\u3001\u7ed3\u6784\u5316\u62a5\u544a\u3001AI\u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544a\uff09\u5bf9\u56fe\u50cf\u5206\u6790\u884c\u4e3a\u3001\u8bca\u65ad\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002AI\u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544a\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bc4\u4f30\u7ed3\u6784\u5316\u62a5\u544a\u548c\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u6539\u53d8\u653e\u5c04\u79d1\u533b\u751f\u4e0e\u5f71\u50cf\u7814\u7a76\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u6bd4\u8f83\u4e0d\u540c\u62a5\u544a\u6a21\u5f0f\u5bf9\u8bca\u65ad\u6027\u80fd\u548c\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u524d\u77bb\u6027\u7814\u7a76\uff082024\u5e747-12\u6708\uff09\uff0c8\u540d\u8bfb\u8005\uff084\u540d\u65b0\u624b\u548c4\u540d\u975e\u65b0\u624b\uff09\u4f7f\u7528\u5b9a\u5236\u67e5\u770b\u5668\u548c\u773c\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u5206\u679035\u5f20\u5e8a\u65c1\u80f8\u7247\u3002\u6bd4\u8f83\u4e09\u79cd\u62a5\u544a\u6a21\u5f0f\uff1a\u81ea\u7531\u6587\u672c\u3001\u7ed3\u6784\u5316\u62a5\u544a\u3001AI\u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544a\u3002", "result": "AI\u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544a\u7684\u8bca\u65ad\u51c6\u786e\u6027\u6700\u9ad8\uff08\u03ba=0.71\uff09\uff0c\u62a5\u544a\u65f6\u95f4\u6700\u77ed\uff0825\u00b19\u79d2\uff09\u3002\u7ed3\u6784\u5316\u62a5\u544a\u548cAI\u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544a\u51cf\u5c11\u4e86\u653e\u5c04\u5f71\u50cf\u533a\u57df\u7684\u626b\u89c6\u6b21\u6570\u548c\u62a5\u544a\u533a\u57df\u7684\u6ce8\u89c6\u65f6\u95f4\u3002\u65b0\u624b\u8bfb\u8005\u5728\u7ed3\u6784\u5316\u62a5\u544a\u4e2d\u5c06\u89c6\u7ebf\u8f6c\u5411\u5f71\u50cf\u3002", "conclusion": "\u7ed3\u6784\u5316\u62a5\u544a\u901a\u8fc7\u5f15\u5bfc\u89c6\u89c9\u6ce8\u610f\u529b\u671d\u5411\u56fe\u50cf\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u800cAI\u9884\u586b\u5145\u7684\u7ed3\u6784\u5316\u62a5\u544a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2510.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16227", "abs": "https://arxiv.org/abs/2510.16227", "authors": ["Jennifer Hu", "Ethan Gotlieb Wilcox", "Siyuan Song", "Kyle Mahowald", "Roger P. Levy"], "title": "What Can String Probability Tell Us About Grammaticality?", "comment": null, "summary": "What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u6cd5\u7684\u5b66\u4e60\u60c5\u51b5\uff0c\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u9a8c\u8bc1\u4e86\u5b57\u7b26\u4e32\u6982\u7387\u4e0e\u8bed\u6cd5\u77e5\u8bc6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f7f\u752828\u4e07\u53e5\u5bf9\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b66\u4e60\u4e86\u8bed\u6cd5\u77e5\u8bc6\uff0c\u4ee5\u53ca\u5b57\u7b26\u4e32\u6982\u7387\u80fd\u5426\u53cd\u6620\u6a21\u578b\u7684\u8bed\u6cd5\u80fd\u529b\uff0c\u8fd9\u5bf9\u8bed\u8a00\u5b66\u7406\u8bba\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u8bed\u6599\u5e93\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u7b80\u5355\u5047\u8bbe\uff0c\u5efa\u7acb\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u5e76\u901a\u8fc7280K\u82f1\u8bed\u548c\u4e2d\u6587\u53e5\u5b50\u5bf9\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5305\u62ec\u6700\u5c0f\u5bf9\u5b57\u7b26\u4e32\u6982\u7387\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u9a8c\u8bc1\u4e86\u4e09\u4e2a\u9884\u6d4b\uff1a(1)\u6700\u5c0f\u5bf9\u5b57\u7b26\u4e32\u6982\u7387\u76f8\u5173\u6027\uff1b(2)\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u6700\u5c0f\u5bf9\u4e2d\u5dee\u5f02\u7684\u76f8\u5173\u6027\uff1b(3)\u8bed\u6cd5\u548c\u975e\u8bed\u6cd5\u5b57\u7b26\u4e32\u5728\u6982\u7387\u7a7a\u95f4\u4e2d\u5206\u79bb\u5ea6\u5dee\u3002", "conclusion": "\u4e3a\u4f7f\u7528\u6982\u7387\u6765\u4e86\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u77e5\u8bc6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u8bed\u8a00\u6a21\u578b\u8bed\u6cd5\u8bc4\u4f30\u5de5\u4f5c\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.16072", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16072", "abs": "https://arxiv.org/abs/2510.16072", "authors": ["Farjana Yesmin"], "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation", "comment": "18 pages", "summary": "Machine learning models trained on imbalanced datasets often exhibit\nintersectional biases-systematic errors arising from the interaction of\nmultiple attributes such as object class and environmental conditions. This\npaper presents a data-driven framework for analyzing and mitigating such biases\nin image classification. We introduce the Intersectional Fairness Evaluation\nFramework (IFEF), which combines quantitative fairness metrics with\ninterpretability tools to systematically identify bias patterns in model\npredictions. Building on this analysis, we propose Bias-Weighted Augmentation\n(BWA), a novel data augmentation strategy that adapts transformation\nintensities based on subgroup distribution statistics. Experiments on the Open\nImages V7 dataset with five object classes demonstrate that BWA improves\naccuracy for underrepresented class-environment intersections by up to 24\npercentage points while reducing fairness metric disparities by 35%.\nStatistical analysis across multiple independent runs confirms the significance\nof improvements (p < 0.05). Our methodology provides a replicable approach for\nanalyzing and addressing intersectional biases in image classification systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u6765\u5206\u6790\u548c\u7f13\u89e3\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u4ea4\u53c9\u504f\u89c1\uff0c\u5305\u62ec\u4ea4\u53c9\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6(IFEF)\u548c\u57fa\u4e8e\u504f\u89c1\u7684\u52a0\u6743\u589e\u5f3a(BWA)\u65b9\u6cd5\uff0c\u5728Open Images V7\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u8868\u6027\u4e0d\u8db3\u7c7b\u522b\u7684\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4e86\u516c\u5e73\u6027\u6307\u6807\u5dee\u5f02\u3002", "motivation": "\u5728\u975e\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7ecf\u5e38\u8868\u73b0\u51fa\u4ea4\u53c9\u504f\u89c1\u2014\u2014\u7531\u5bf9\u8c61\u7c7b\u522b\u548c\u73af\u5883\u6761\u4ef6\u7b49\u591a\u4e2a\u5c5e\u6027\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u7684\u7cfb\u7edf\u6027\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e86\u4ea4\u53c9\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6(IFEF)\u7ed3\u5408\u5b9a\u91cf\u516c\u5e73\u6027\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u6765\u8bc6\u522b\u504f\u89c1\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u504f\u89c1\u52a0\u6743\u589e\u5f3a(BWA)\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5b50\u7ec4\u5206\u5e03\u7edf\u8ba1\u81ea\u9002\u5e94\u8c03\u6574\u53d8\u6362\u5f3a\u5ea6\u3002", "result": "\u5728Open Images V7\u6570\u636e\u96c6\u4e0a\uff0cBWA\u5c06\u4ee3\u8868\u6027\u4e0d\u8db3\u7c7b\u522b-\u73af\u5883\u4ea4\u53c9\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u6700\u591a24\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5c06\u516c\u5e73\u6027\u6307\u6807\u5dee\u5f02\u51cf\u5c11\u4e8635%\u3002\u7edf\u8ba1\u5206\u6790\u786e\u8ba4\u4e86\u6539\u8fdb\u7684\u663e\u8457\u6027(p < 0.05)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u6790\u548c\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\u4e2d\u7684\u4ea4\u53c9\u504f\u89c1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u7684\u65b9\u6cd5\uff08\u591a\u5143\u5316\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\uff09\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5143\u5bf9\u9f50\u80fd\u529b\uff0c\u4ec5\u752850\u4e2a\u6807\u6ce8\u6837\u672c\u5c31\u80fd\u5728\u591a\u4e2a\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5bf9\u793e\u4f1a\u5f71\u54cd\u65e5\u76ca\u589e\u5927\uff0c\u9700\u8981\u786e\u4fdd\u5b83\u4eec\u80fd\u591f\u5bf9\u9f50\u591a\u6837\u5316\u7684\u89c2\u70b9\u5e76\u53cd\u6620\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u5f53\u524d\u4e3b\u6d41\u8bad\u7ec3\u8303\u5f0f\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u53ea\u6709\u4e00\u4e2a\u6700\u4f18\u7b54\u6848\uff0c\u5bfc\u81f4\u54cd\u5e94\u6cdb\u5316\u4e14\u5bf9\u9f50\u6548\u679c\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u591a\u5143\u5316\u89e3\u7801 - \u751f\u6210\u591a\u6837\u5316\u7684\u54cd\u5e94\uff1b2) \u6a21\u578b\u5f15\u5bfc - \u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\uff08\u4ec550\u4e2a\u6807\u6ce8\u6837\u672c\uff09\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u591a\u5143\u89c2\u70b9\u3002", "result": "\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\u4e0a\u8868\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7b49\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u964d\u4f4e\u4e86\u5047\u9633\u6027\u7387\uff0c\u5728GlobalOpinionQA\u4e0a\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5206\u5e03\u5bf9\u9f50\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u9002\u5e94\u8003\u8651\u7ec6\u5fae\u89c2\u70b9\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u548c\u4ee3\u8868\u6027\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.16088", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16088", "abs": "https://arxiv.org/abs/2510.16088", "authors": ["Zia Badar"], "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch", "comment": null, "summary": "Quantization of neural networks provides benefits of inference in less\ncompute and memory requirements. Previous work in quantization lack two\nimportant aspects which this work provides. First almost all previous work in\nquantization used a non-differentiable approach and for learning; the\nderivative is usually set manually in backpropogation which make the learning\nability of algorithm questionable, our approach is not just differentiable, we\nalso provide proof of convergence of our approach to the optimal neural\nnetwork. Second previous work in shift/logrithmic quantization either have\navoided activation quantization along with weight quantization or achieved less\naccuracy. Learning logrithmic quantize values of form $2^n$ requires the\nquantization function can scale to more than 1 bit quantization which is\nanother benifit of our quantization that it provides $n$ bits quantization as\nwell. Our approach when tested with image classification task using imagenet\ndataset, resnet18 and weight quantization only achieves less than 1 percent\naccuracy compared to full precision accuracy while taking only 15 epochs to\ntrain using shift bit quantization and achieves comparable to SOTA approaches\naccuracy in both weight and activation quantization using shift bit\nquantization in 15 training epochs with slightly higher(only higher cpu\ninstructions) inference cost compared to 1 bit quantization(without logrithmic\nquantization) and not requiring any higher precision multiplication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u4e14\u6536\u655b\u7684\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u65b9\u6cd5\uff0c\u652f\u6301n\u4f4d\u91cf\u5316\uff0c\u5728ImageNet\u4e0a\u4f7f\u7528ResNet18\u5b9e\u73b0\u6743\u91cd\u91cf\u5316\u4ec5\u635f\u5931\u4e0d\u52301%\u51c6\u786e\u7387\uff0c\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u5747\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u4e0d\u53ef\u5fae\u5206\u3001\u9700\u8981\u624b\u52a8\u8bbe\u7f6e\u5bfc\u6570\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u9ad8\u7cbe\u5ea6\u91cf\u5316\uff0c\u907f\u514d\u4ec5\u652f\u63011\u4f4d\u91cf\u5316\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6536\u655b\u6027\u8bc1\u660e\uff0c\u652f\u63012^n\u5f62\u5f0f\u7684\u5bf9\u6570\u91cf\u5316\u503c\uff0c\u5b9e\u73b0n\u4f4d\u91cf\u5316\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0cResNet18\u4ec5\u6743\u91cd\u91cf\u5316\u76f8\u6bd4\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e1%\uff0c\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u5747\u8fbe\u5230SOTA\u51c6\u786e\u7387\uff0c\u4ec5\u970015\u4e2a\u8bad\u7ec3\u5468\u671f\u3002", "conclusion": "\u63d0\u51fa\u7684\u91cf\u5316\u65b9\u6cd5\u5177\u6709\u53ef\u5fae\u5206\u6027\u548c\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u652f\u6301\u591a\u6bd4\u7279\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2510.16282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16282", "abs": "https://arxiv.org/abs/2510.16282", "authors": ["Zhaoxuan Tan", "Zixuan Zhang", "Haoyang Wen", "Zheng Li", "Rongzhi Zhang", "Pei Chen", "Fengran Mo", "Zheyuan Liu", "Qingkai Zeng", "Qingyu Yin", "Meng Jiang"], "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork", "comment": null, "summary": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86Profile-to-PEFT\u6846\u67b6\uff0c\u4f7f\u7528\u8d85\u7f51\u7edc\u5c06\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u6620\u5c04\u5230\u9002\u914d\u5668\u53c2\u6570\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u7528\u6237\u5355\u72ec\u8bad\u7ec3\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u7684LLM\u4e2a\u6027\u5316\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u7528\u6237\u8bad\u7ec3\u5355\u72ec\u7684\u9002\u914d\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u5b9e\u65f6\u66f4\u65b0\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u4e2a\u6027\u5316\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u8d85\u7f51\u7edc\uff0c\u5c06\u7f16\u7801\u540e\u7684\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u76f4\u63a5\u6620\u5c04\u5230\u5b8c\u6574\u7684\u9002\u914d\u5668\u53c2\u6570\uff08\u5982LoRA\uff09\uff0c\u90e8\u7f72\u65f6\u65e0\u9700\u5bf9\u6bcf\u4e2a\u7528\u6237\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u4e2a\u6027\u5316\u548cOne-PEFT-Per-User\u65b9\u6cd5\uff0c\u540c\u65f6\u90e8\u7f72\u65f6\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u663e\u8457\u51cf\u5c11\uff0c\u5bf9\u5206\u5e03\u5916\u7528\u6237\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Profile-to-PEFT\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684LLM\u4e2a\u6027\u5316\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.16115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16115", "abs": "https://arxiv.org/abs/2510.16115", "authors": ["Jianhan Lin", "Yuchu Qin", "Shuai Gao", "Yikang Rui", "Jie Liu", "Yanjie Lv"], "title": "StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection", "comment": null, "summary": "Well-maintained road networks are crucial for achieving Sustainable\nDevelopment Goal (SDG) 11. Road surface damage not only threatens traffic\nsafety but also hinders sustainable urban development. Accurate detection,\nhowever, remains challenging due to the diverse shapes of damages, the\ndifficulty of capturing slender cracks with high aspect ratios, and the high\nerror rates in small-scale damage recognition. To address these issues, we\npropose StripRFNet, a novel deep neural network comprising three modules: (1) a\nShape Perception Module (SPM) that enhances shape discrimination via large\nseparable kernel attention (LSKA) in multi-scale feature aggregation; (2) a\nStrip Receptive Field Module (SRFM) that employs large strip convolutions and\npooling to capture features of slender cracks; and (3) a Small-Scale\nEnhancement Module (SSEM) that leverages a high-resolution P2 feature map, a\ndedicated detection head, and dynamic upsampling to improve small-object\ndetection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses\nexisting methods. On the Chinese subset, it improves F1-score, mAP50, and\nmAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,\nrespectively. On the full dataset, it achieves the highest F1-score of 80.33%\ncompared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while\nmaintaining competitive inference speed. These results demonstrate that\nStripRFNet achieves state-of-the-art accuracy and real-time efficiency,\noffering a promising tool for intelligent road maintenance and sustainable\ninfrastructure management.", "AI": {"tldr": "\u63d0\u51fa\u4e86StripRFNet\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5f62\u72b6\u611f\u77e5\u3001\u6761\u72b6\u611f\u53d7\u91ce\u548c\u5c0f\u5c3a\u5ea6\u589e\u5f3a\u4e09\u4e2a\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u9053\u8def\u635f\u4f24\u68c0\u6d4b\u4e2d\u5f62\u72b6\u591a\u6837\u3001\u7ec6\u957f\u88c2\u7f1d\u96be\u8bc6\u522b\u548c\u5c0f\u76ee\u6807\u68c0\u6d4b\u8bef\u5dee\u9ad8\u7684\u95ee\u9898\uff0c\u5728RDD2022\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u9053\u8def\u8868\u9762\u635f\u4f24\u5a01\u80c1\u4ea4\u901a\u5b89\u5168\u5e76\u963b\u788d\u53ef\u6301\u7eed\u57ce\u5e02\u53d1\u5c55\uff0c\u4f46\u51c6\u786e\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff1a\u635f\u4f24\u5f62\u72b6\u591a\u6837\u3001\u7ec6\u957f\u88c2\u7f1d\u96be\u4ee5\u6355\u6349\u3001\u5c0f\u5c3a\u5ea6\u635f\u4f24\u8bc6\u522b\u9519\u8bef\u7387\u9ad8\u3002", "method": "StripRFNet\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1\uff09\u5f62\u72b6\u611f\u77e5\u6a21\u5757\uff08SPM\uff09\u901a\u8fc7\u5927\u5c3a\u5ea6\u53ef\u5206\u79bb\u6838\u6ce8\u610f\u529b\u589e\u5f3a\u5f62\u72b6\u5224\u522b\uff1b2\uff09\u6761\u72b6\u611f\u53d7\u91ce\u6a21\u5757\uff08SRFM\uff09\u4f7f\u7528\u5927\u6761\u72b6\u5377\u79ef\u548c\u6c60\u5316\u6355\u6349\u7ec6\u957f\u88c2\u7f1d\u7279\u5f81\uff1b3\uff09\u5c0f\u5c3a\u5ea6\u589e\u5f3a\u6a21\u5757\uff08SSEM\uff09\u5229\u7528\u9ad8\u5206\u8fa8\u7387P2\u7279\u5f81\u56fe\u3001\u4e13\u7528\u68c0\u6d4b\u5934\u548c\u52a8\u6001\u4e0a\u91c7\u6837\u6539\u5584\u5c0f\u76ee\u6807\u68c0\u6d4b\u3002", "result": "\u5728RDD2022\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e2d\u56fd\u5b50\u96c6\u7684F1\u5206\u6570\u3001mAP50\u548cmAP50:95\u5206\u522b\u6bd4\u57fa\u7ebf\u63d0\u9ad84.4\u30012.9\u548c3.4\u4e2a\u767e\u5206\u70b9\uff1b\u5b8c\u6574\u6570\u636e\u96c6\u4e0a\u8fbe\u523080.33%\u7684\u6700\u9ad8F1\u5206\u6570\uff0c\u4f18\u4e8eCRDDC'2022\u53c2\u4e0e\u8005\u548cORDDC'2024\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "StripRFNet\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u9053\u8def\u7ef4\u62a4\u548c\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2510.16340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16340", "abs": "https://arxiv.org/abs/2510.16340", "authors": ["Pratham Singla", "Shivank Garg", "Ayush Singh", "Ishan Garg", "Ketan Suhaas Saichandran"], "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models", "comment": null, "summary": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u540e\u8bad\u7ec3LLM\u5bf9\u81ea\u8eab\u5b66\u4e60\u5185\u5bb9\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u73b0RL\u8bad\u7ec3\u6a21\u578b\u6bd4SFT\u6a21\u578b\u66f4\u4e86\u89e3\u6240\u5b66\u884c\u4e3a\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u63a8\u7406\u8f68\u8ff9\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u8f83\u5f31\u3002", "motivation": "\u63a2\u7a76LLM\u662f\u5426\u610f\u8bc6\u5230\u81ea\u5df1\"\u5b66\u4e60\"\u548c\"\u601d\u8003\"\u7684\u5185\u5bb9\uff0c\u5b9a\u4e49\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u8ba4\u77e5\u3001\u7b56\u7565\u8de8\u9886\u57df\u6cdb\u5316\u3001\u63a8\u7406\u4e0e\u8f93\u51fa\u7684\u5bf9\u9f50\u3002", "method": "\u5728\u591a\u4e2a\u9700\u8981\u5b66\u4e60\u4e0d\u540c\u7b56\u7565\u7684\u4efb\u52a1\u4e0a\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5bf9\u6bd4SFT\u3001DPO\u548cGRPO\u4e09\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u7684\u6a21\u578b\u8868\u73b0\u3002", "result": "RL\u8bad\u7ec3\u6a21\u578b\u6bd4SFT\u6a21\u578b\u66f4\u4e86\u89e3\u6240\u5b66\u884c\u4e3a\uff0c\u5728\u7ed3\u6784\u76f8\u4f3c\u7684\u65b0\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u63a8\u7406\u8f68\u8ff9\u4e0e\u6700\u7ec8\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u8f83\u5f31\uff0cGRPO\u6a21\u578b\u5c24\u5176\u660e\u663e\u3002", "conclusion": "\u540e\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u5f71\u54cdLLM\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0cRL\u65b9\u6cd5\u589e\u5f3a\u7b56\u7565\u8ba4\u77e5\u548c\u6cdb\u5316\uff0c\u4f46\u53ef\u80fd\u635f\u5bb3\u63a8\u7406\u900f\u660e\u5ea6\u3002"}}
{"id": "2510.16118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16118", "abs": "https://arxiv.org/abs/2510.16118", "authors": ["Nishad Sahu", "Shounak Sural", "Aditya Satish Patil", "Ragunathan", "Rajkumar"], "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025\n  Workshops", "summary": "Reliable perception is fundamental for safety critical decision making in\nautonomous driving. Yet, vision based object detector neural networks remain\nvulnerable to uncertainty arising from issues such as data bias and\ndistributional shifts. In this paper, we introduce ObjectTransforms, a\ntechnique for quantifying and reducing uncertainty in vision based object\ndetection through object specific transformations at both training and\ninference times. At training time, ObjectTransforms perform color space\nperturbations on individual objects, improving robustness to lighting and color\nvariations. ObjectTransforms also uses diffusion models to generate realistic,\ndiverse pedestrian instances. At inference time, object perturbations are\napplied to detected objects and the variance of detection scores are used to\nquantify predictive uncertainty in real time. This uncertainty signal is then\nused to filter out false positives and also recover false negatives, improving\nthe overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K\ndataset demonstrate that our method yields notable accuracy improvements and\nuncertainty reduction across all object classes during training, while\npredicting desirably higher uncertainty values for false positives as compared\nto true positives during inference. Our results highlight the potential of\nObjectTransforms as a lightweight yet effective mechanism for reducing and\nquantifying uncertainty in vision-based perception during training and\ninference respectively.", "AI": {"tldr": "ObjectTransforms\u662f\u4e00\u79cd\u901a\u8fc7\u5bf9\u8c61\u7279\u5b9a\u53d8\u6362\u6765\u91cf\u5316\u548c\u51cf\u5c11\u57fa\u4e8e\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u6280\u672f\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5206\u522b\u63d0\u5347\u9c81\u68d2\u6027\u548c\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u53ef\u9760\u7684\u611f\u77e5\u5bf9\u5b89\u5168\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u4e8e\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u53d7\u5230\u6570\u636e\u504f\u5dee\u548c\u5206\u5e03\u504f\u79fb\u7b49\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "method": "\u5728\u8bad\u7ec3\u65f6\u5bf9\u5355\u4e2a\u5bf9\u8c61\u8fdb\u884c\u8272\u5f69\u7a7a\u95f4\u6270\u52a8\uff0c\u5e76\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u884c\u4eba\u5b9e\u4f8b\uff1b\u5728\u63a8\u7406\u65f6\u5bf9\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u5e94\u7528\u6270\u52a8\uff0c\u5229\u7528\u68c0\u6d4b\u5206\u6570\u7684\u65b9\u5dee\u5b9e\u65f6\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728NuImages 10K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u5bf9\u8c61\u7c7b\u522b\u4e0a\u90fd\u5e26\u6765\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\uff0c\u5bf9\u5047\u9633\u6027\u9884\u6d4b\u4e86\u6bd4\u771f\u9633\u6027\u66f4\u9ad8\u7684\u4e0d\u786e\u5b9a\u6027\u503c\u3002", "conclusion": "ObjectTransforms\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5206\u522b\u5177\u6709\u51cf\u5c11\u548c\u91cf\u5316\u57fa\u4e8e\u89c6\u89c9\u7684\u611f\u77e5\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16359", "abs": "https://arxiv.org/abs/2510.16359", "authors": ["Utsav Dhanuka", "Soham Poddar", "Saptarshi Ghosh"], "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets", "comment": "14 pages, 1 figure, work done as a part of B.Tech project at IIT\n  Kharagpur", "summary": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9488\u5bf9\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u7684\u53cd\u9a73\u8bba\u70b9\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u53cd\u9a73\u751f\u6210\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u5bf9\u53cd\u75ab\u82d7\u63a8\u6587\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u4ee5\u5b9e\u73b0\u66f4\u60c5\u5883\u611f\u77e5\u7684\u53cd\u9a73\u3002", "motivation": "\u5728\u793e\u4ea4\u5a92\u4f53\u5f71\u54cd\u516c\u5171\u536b\u751f\u7684\u65f6\u4ee3\uff0c\u6253\u51fb\u75ab\u82d7\u6000\u7591\u8bba\u548c\u9519\u8bef\u4fe1\u606f\u6210\u4e3a\u5173\u952e\u793e\u4f1a\u76ee\u6807\u3002\u867d\u7136\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5b9e\u65f6\u751f\u6210\u9488\u5bf9\u6027\u53cd\u9a73\u8bba\u70b9\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u5b9e\u9a8c\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316\u53cd\u9a73\u8bba\u70b9\u751f\u6210\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u53cd\u75ab\u82d7\u63a8\u6587\u5206\u7c7b\u4e3a\u75ab\u82d7\u6548\u529b\u3001\u526f\u4f5c\u7528\u3001\u653f\u6cbb\u5f71\u54cd\u7b49\u591a\u6807\u7b7e\u7c7b\u522b\uff0c\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u7684\u53cd\u9a73\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u5224\u65ad\u3001LLM\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u7684\u8bc4\u4f30\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u5177\u6709\u5f3a\u4e00\u81f4\u6027\uff0c\u6574\u5408\u6807\u7b7e\u63cf\u8ff0\u548c\u7ed3\u6784\u5316\u5fae\u8c03\u80fd\u589e\u5f3a\u53cd\u9a73\u8bba\u70b9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u7f13\u89e3\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2510.16134", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16134", "abs": "https://arxiv.org/abs/2510.16134", "authors": ["Chen Kong", "James Fort", "Aria Kang", "Jonathan Wittmer", "Simon Green", "Tianwei Shen", "Yipu Zhao", "Cheng Peng", "Gustavo Solaira", "Andrew Berkovich", "Nikhil Raina", "Vijay Baiyya", "Evgeniy Oleinik", "Eric Huang", "Fan Zhang", "Julian Straub", "Mark Schwesinger", "Luis Pesqueira", "Xiaqing Pan", "Jakob Julian Engel", "Carl Ren", "Mingfei Yan", "Richard Newcombe"], "title": "Aria Gen 2 Pilot Dataset", "comment": null, "summary": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset\ncaptured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely\naccess, A2PD is released incrementally with ongoing dataset enhancements. The\ninitial release features Dia'ane, our primary subject, who records her daily\nactivities alongside friends, each equipped with Aria Gen 2 glasses. It\nencompasses five primary scenarios: cleaning, cooking, eating, playing, and\noutdoor walking. In each of the scenarios, we provide comprehensive raw sensor\ndata and output data from various machine perception algorithms. These data\nillustrate the device's ability to perceive the wearer, the surrounding\nenvironment, and interactions between the wearer and the environment, while\nmaintaining robust performance across diverse users and conditions. The A2PD is\npublicly available at projectaria.com, with open-source tools and usage\nexamples provided in Project Aria Tools.", "AI": {"tldr": "A2PD\u662f\u4e00\u4e2a\u4f7f\u7528Aria Gen 2\u773c\u955c\u91c7\u96c6\u7684\u81ea\u6211\u4e2d\u5fc3\u591a\u6a21\u6001\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5305\u542b\u6e05\u6d01\u3001\u70f9\u996a\u3001\u8fdb\u98df\u3001\u73a9\u800d\u548c\u6237\u5916\u6b65\u884c\u4e94\u4e2a\u4e3b\u8981\u573a\u666f\u7684\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u548c\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u8f93\u51fa\u3002", "motivation": "\u4e3a\u4e86\u53ca\u65f6\u63d0\u4f9b\u5148\u8fdb\u7684\u81ea\u6211\u4e2d\u5fc3\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u652f\u6301\u7814\u7a76\u8bbe\u5907\u611f\u77e5\u4f69\u6234\u8005\u3001\u5468\u56f4\u73af\u5883\u4ee5\u53ca\u4eba\u673a\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528Aria Gen 2\u773c\u955c\u91c7\u96c6\u4e3b\u8981\u5bf9\u8c61Dia'ane\u53ca\u5176\u670b\u53cb\u7684\u65e5\u5e38\u6d3b\u52a8\u6570\u636e\uff0c\u6db5\u76d6\u4e94\u4e2a\u4e3b\u8981\u573a\u666f\uff0c\u63d0\u4f9b\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u548c\u591a\u79cd\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u7684\u8f93\u51fa\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u8bbe\u5907\u5728\u4e0d\u540c\u7528\u6237\u548c\u6761\u4ef6\u4e0b\u90fd\u80fd\u4fdd\u6301\u7a33\u5065\u6027\u80fd\uff0c\u80fd\u591f\u611f\u77e5\u4f69\u6234\u8005\u3001\u73af\u5883\u548c\u4ea4\u4e92\u3002\u6570\u636e\u96c6\u5728projectaria.com\u516c\u5f00\u53ef\u7528\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5de5\u5177\u548c\u4f7f\u7528\u793a\u4f8b\u3002", "conclusion": "A2PD\u662f\u4e00\u4e2a\u589e\u91cf\u53d1\u5e03\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u81ea\u6211\u4e2d\u5fc3\u591a\u6a21\u6001\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u652f\u6301\u8bbe\u5907\u5728\u5404\u79cd\u573a\u666f\u4e0b\u611f\u77e5\u80fd\u529b\u7684\u5f00\u53d1\u4e0e\u9a8c\u8bc1\u3002"}}
{"id": "2510.16363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16363", "abs": "https://arxiv.org/abs/2510.16363", "authors": ["Nilmadhab Das", "Vishal Vaibhav", "Yash Sunil Choudhary", "V. Vijaya Saradhi", "Ashish Anand"], "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction", "comment": "Accepted version. To appear in IJCNN 2025", "summary": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AASP\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u7ed3\u6784\u9884\u6d4b\u65b9\u6cd5\u8054\u5408\u5efa\u6a21\u8bba\u8bc1\u6316\u6398\u4e2d\u7684\u8bba\u8bc1\u7ec4\u4ef6\u548c\u8bba\u8bc1\u5173\u7cfb\uff0c\u5728\u4e09\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u8bba\u8bc1\u6316\u6398\u4efb\u52a1\u4e2d\u5efa\u6a21\u8bba\u8bc1\u7ec4\u4ef6\u548c\u8bba\u8bc1\u5173\u7cfb\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u8303\u5f0f\u6241\u5e73\u5316\u5904\u7406\u8bba\u8bc1\u7ed3\u6784\uff0c\u672c\u6587\u65e8\u5728\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u8054\u5408\u5efa\u6a21\u8fd9\u4e9b\u5173\u952e\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52\u8bba\u8bc1\u7ed3\u6784\u9884\u6d4b(AASP)\u6846\u67b6\uff0c\u5c06\u8bba\u8bc1\u7ed3\u6784\u5efa\u6a21\u4e3a\u9884\u5b9a\u4e49\u7ea6\u675f\u52a8\u4f5c\u96c6\uff0c\u4f7f\u7528\u6761\u4ef6\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u9010\u6b65\u6784\u5efa\u8bba\u8bc1\u7ed3\u6784\uff0c\u6709\u6548\u6355\u6349\u8bba\u8bc1\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u8bba\u8bc1\u6316\u6398\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAASP\u5728\u4e24\u4e2a\u57fa\u51c6\u4e0a\u6240\u6709\u4efb\u52a1\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728\u53e6\u4e00\u4e2a\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u5f3a\u52b2\u3002", "conclusion": "AASP\u6846\u67b6\u901a\u8fc7\u81ea\u56de\u5f52\u7ed3\u6784\u9884\u6d4b\u6709\u6548\u89e3\u51b3\u4e86\u8bba\u8bc1\u6316\u6398\u4e2d\u7684\u590d\u6742\u4f9d\u8d56\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.16136", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16136", "abs": "https://arxiv.org/abs/2510.16136", "authors": ["Sayan Deb Sarkar", "Sinisa Stekovic", "Vincent Lepetit", "Iro Armeni"], "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer", "comment": "NeurIPS 2025. Project Page: https://sayands.github.io/guideflow3d/", "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u5f15\u5bfc\u539f\u7406\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u51e0\u4f55\u5dee\u5f02\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u5c06\u5916\u89c2\u4ece\u56fe\u50cf\u6216\u6587\u672c\u8f6c\u79fb\u52303D\u8d44\u4ea7\u4e0a\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6dfb\u52a0\u53ef\u5fae\u5206\u635f\u5931\u51fd\u6570\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8f93\u5165\u548c\u5916\u89c2\u5bf9\u8c61\u7684\u51e0\u4f55\u5f62\u72b6\u663e\u8457\u4e0d\u540c\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u76f4\u63a5\u5e94\u75283D\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u4ea7\u751f\u5438\u5f15\u4eba\u7684\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6574\u6d41\u6d41\u6a21\u578b\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5468\u671f\u6027\u6dfb\u52a0\u5f15\u5bfc\uff0c\u5305\u62ec\u90e8\u4ef6\u611f\u77e5\u635f\u5931\u548c\u81ea\u76f8\u4f3c\u6027\u635f\u5931\u7b49\u53ef\u5fae\u5206\u635f\u5931\u51fd\u6570\u3002", "result": "\u6210\u529f\u5c06\u7eb9\u7406\u548c\u51e0\u4f55\u7ec6\u8282\u8f6c\u79fb\u52303D\u8d44\u4ea7\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f7f\u7528\u57fa\u4e8eGPT\u7684\u7cfb\u7edf\u8fdb\u884c\u5ba2\u89c2\u6392\u540d\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u4e0d\u540c\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\u548c\u5f15\u5bfc\u51fd\u6570\uff0c\u5728\u6e38\u620f\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.16373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16373", "abs": "https://arxiv.org/abs/2510.16373", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira"], "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u7279\u5b9a\u5c42\u7684\u6fc0\u6d3b\u5411\u91cf\u6765\u63d0\u5347LLM\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8ba1\u7b97\u5bc6\u96c6\u578b\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u5e94\u7528\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u8fd9\u6837\u7684\u654f\u611f\u9ad8\u5f71\u54cd\u9886\u57df\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5bf9\u7279\u5b9a\u5c42\u7684\u6fc0\u6d3b\u5e94\u7528\u7ebf\u6027\u53d8\u6362\uff0c\u5229\u7528\u8f6c\u5411\u5411\u91cf\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6539\u8fdb\uff1a\u8bc6\u522bReddit\u5e16\u5b50\u662f\u5426\u6709\u52a9\u4e8e\u68c0\u6d4b\u6291\u90c1\u75c7\u72b6\u7684\u76f8\u5173\u6027\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7528\u6237Reddit\u5386\u53f2\u5b8c\u6210\u6807\u51c6\u5316\u6291\u90c1\u7b5b\u67e5\u95ee\u5377\u7684\u4efb\u52a1\u3002", "conclusion": "\u8f6c\u5411\u673a\u5236\u4f5c\u4e3a\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5de5\u5177\uff0c\u5728LLMs\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u9002\u5e94\u65b9\u9762\u5177\u6709\u672a\u5f00\u53d1\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16145", "abs": "https://arxiv.org/abs/2510.16145", "authors": ["Ahmad Arrabi", "Jay hwasung Jung", "J Le", "A Nguyen", "J Reed", "E Stahl", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy", "comment": null, "summary": "Thrombectomy is one of the most effective treatments for ischemic stroke, but\nit is resource and personnel-intensive. We propose employing deep learning to\nautomate critical aspects of thrombectomy, thereby enhancing efficiency and\nsafety. In this work, we introduce a self-supervised framework that classifies\nvarious skeletal landmarks using a regression-based pretext task. Our\nexperiments demonstrate that our model outperforms existing methods in both\nregression and classification tasks. Notably, our results indicate that the\npositional pretext task significantly enhances downstream classification\nperformance. Future work will focus on extending this framework toward fully\nautonomous C-arm control, aiming to optimize trajectories from the pelvis to\nthe head during stroke thrombectomy procedures. All code used is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5316\u8840\u6813\u5207\u9664\u672f\u5173\u952e\u73af\u8282\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u56de\u5f52\u7684\u524d\u7f6e\u4efb\u52a1\u5206\u7c7b\u9aa8\u9abc\u6807\u5fd7\u70b9\uff0c\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd", "motivation": "\u8840\u6813\u5207\u9664\u672f\u662f\u7f3a\u8840\u6027\u4e2d\u98ce\u6700\u6709\u6548\u7684\u6cbb\u7597\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4f46\u8d44\u6e90\u5bc6\u96c6\u4e14\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u4eba\u5458\u3002\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5316\u5173\u952e\u73af\u8282\u53ef\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027", "method": "\u5f15\u5165\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u56de\u5f52\u7684\u524d\u7f6e\u4efb\u52a1\u5206\u7c7b\u5404\u79cd\u9aa8\u9abc\u6807\u5fd7\u70b9", "result": "\u6a21\u578b\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f4d\u7f6e\u524d\u7f6e\u4efb\u52a1\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b8c\u5168\u81ea\u4e3b\u7684C\u81c2\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u5c06\u4e13\u6ce8\u4e8e\u4f18\u5316\u4ece\u4e2d\u9aa8\u76c6\u5230\u5934\u90e8\u7684\u8f68\u8ff9\uff0c\u7528\u4e8e\u4e2d\u98ce\u8840\u6813\u5207\u9664\u672f"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "MoReBench\u662f\u4e00\u4e2a\u5305\u542b1000\u4e2a\u9053\u5fb7\u573a\u666f\u548c23000\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u5b66\u3001\u4ee3\u7801\u548c\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u5728\u9053\u5fb7\u63a8\u7406\u8bc4\u4f30\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u51b3\u7b56\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u9700\u8981\u7406\u89e3AI\u7684\u51b3\u7b56\u8fc7\u7a0b\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u7ec8\u7ed3\u679c\u3002\u9053\u5fb7\u56f0\u5883\u662f\u8bc4\u4f30\u8fc7\u7a0b\u63a8\u7406\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u4e3a\u5141\u8bb8\u591a\u79cd\u5408\u7406\u7684\u7ed3\u8bba\u3002", "method": "\u521b\u5efaMoReBench\u57fa\u51c6\uff0c\u5305\u542b\u9053\u5fb7\u573a\u666f\u548c\u4e13\u5bb6\u5236\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6db5\u76d6\u8bc6\u522b\u9053\u5fb7\u8003\u91cf\u3001\u6743\u8861\u5229\u5f0a\u3001\u7ed9\u51fa\u53ef\u884c\u5efa\u8bae\u7b49\u65b9\u9762\u3002\u540c\u65f6\u521b\u5efaMoReBench-Theory\u6d4b\u8bd5AI\u5728\u4e94\u79cd\u4e3b\u8981\u89c4\u8303\u4f26\u7406\u5b66\u6846\u67b6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6269\u5c55\u5b9a\u5f8b\u548c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u9884\u6d4b\u6a21\u578b\u5728\u9053\u5fb7\u63a8\u7406\u4e0a\u7684\u80fd\u529b\u3002\u6a21\u578b\u5bf9\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\uff08\u5982\u8fb9\u6c81\u529f\u5229\u4e3b\u4e49\u548c\u5eb7\u5fb7\u4e49\u52a1\u8bba\uff09\u8868\u73b0\u51fa\u504f\u597d\uff0c\u8fd9\u53ef\u80fd\u662f\u6d41\u884c\u8bad\u7ec3\u8303\u5f0f\u7684\u526f\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u57fa\u51c6\u63a8\u52a8\u4e86\u4ee5\u8fc7\u7a0b\u4e3a\u91cd\u70b9\u7684\u63a8\u7406\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u900f\u660e\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2510.16146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16146", "abs": "https://arxiv.org/abs/2510.16146", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Vi Vu", "Ba-Thinh Lam", "Phat Huynh", "Tianyang Wang", "Xingjian Li", "Ulas Bagci", "Min Xu"], "title": "DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization", "comment": "The paper is under review at CMIG", "summary": "The limited availability of annotated data in medical imaging makes\nsemi-supervised learning increasingly appealing for its ability to learn from\nimperfect supervision. Recently, teacher-student frameworks have gained\npopularity for their training benefits and robust performance. However, jointly\noptimizing the entire network can hinder convergence and stability, especially\nin challenging scenarios. To address this for medical image segmentation, we\npropose DuetMatch, a novel dual-branch semi-supervised framework with\nasynchronous optimization, where each branch optimizes either the encoder or\ndecoder while keeping the other frozen. To improve consistency under noisy\nconditions, we introduce Decoupled Dropout Perturbation, enforcing\nregularization across branches. We also design Pair-wise CutMix Cross-Guidance\nto enhance model diversity by exchanging pseudo-labels through augmented input\npairs. To mitigate confirmation bias from noisy pseudo-labels, we propose\nConsistency Matching, refining labels using stable predictions from frozen\nteacher models. Extensive experiments on benchmark brain MRI segmentation\ndatasets, including ISLES2022 and BraTS, show that DuetMatch consistently\noutperforms state-of-the-art methods, demonstrating its effectiveness and\nrobustness across diverse semi-supervised segmentation scenarios.", "AI": {"tldr": "DuetMatch\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u91c7\u7528\u5f02\u6b65\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u89e3\u8026\u7684dropout\u6270\u52a8\u548c\u914d\u5bf9CutMix\u4ea4\u53c9\u5f15\u5bfc\u6765\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u548c\u591a\u6837\u6027\uff0c\u5728\u8111MRI\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u6709\u9650\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u73b0\u6709\u7684\u5e08\u751f\u6846\u67b6\u5728\u8054\u5408\u4f18\u5316\u6574\u4e2a\u7f51\u7edc\u65f6\u5b58\u5728\u6536\u655b\u56f0\u96be\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u5f02\u6b65\u4f18\u5316\u6846\u67b6\uff0c\u6bcf\u4e2a\u5206\u652f\u5206\u522b\u4f18\u5316\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\uff1b\u5f15\u5165\u89e3\u8026dropout\u6270\u52a8\u589e\u5f3a\u6b63\u5219\u5316\uff1b\u8bbe\u8ba1\u914d\u5bf9CutMix\u4ea4\u53c9\u5f15\u5bfc\u901a\u8fc7\u589e\u5f3a\u8f93\u5165\u5bf9\u4ea4\u6362\u4f2a\u6807\u7b7e\uff1b\u63d0\u51fa\u4e00\u81f4\u6027\u5339\u914d\u5229\u7528\u51bb\u7ed3\u6559\u5e08\u6a21\u578b\u7684\u7a33\u5b9a\u9884\u6d4b\u6765\u7cbe\u70bc\u6807\u7b7e\u3002", "result": "\u5728ISLES2022\u548cBraTS\u7b49\u8111MRI\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDuetMatch\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u534a\u76d1\u7763\u5206\u5272\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "DuetMatch\u901a\u8fc7\u5f02\u6b65\u4f18\u5316\u3001\u89e3\u8026\u6270\u52a8\u548c\u4e00\u81f4\u6027\u5339\u914d\u7b49\u521b\u65b0\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6536\u655b\u7a33\u5b9a\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.16381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16381", "abs": "https://arxiv.org/abs/2510.16381", "authors": ["David Peer", "Sebastian Stabinger"], "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet\ntheir deployment in high-stakes domains is hindered by inherent limitations in\ntrustworthiness, including hallucinations, instability, and a lack of\ntransparency. To address these challenges, we introduce a generic\nneuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The\ncore of our approach lies in decoupling tasks into two distinct phases: Offline\nknowledge ingestion and online task processing. During knowledge ingestion, an\nLLM translates an informal problem specification into a formal, symbolic\nknowledge base. This formal representation is crucial as it can be verified and\nrefined by human experts, ensuring its correctness and alignment with domain\nrequirements. In the subsequent task processing phase, each incoming input is\nencoded into the same formal language. A symbolic decision engine then utilizes\nthis encoded input in conjunction with the formal knowledge base to derive a\nreliable result. Through an extensive evaluation on a complex reasoning task,\nwe demonstrate that a concrete implementation of ATA is competitive with\nstate-of-the-art end-to-end reasoning models in a fully automated setup while\nmaintaining trustworthiness. Crucially, with a human-verified and corrected\nknowledge base, our approach significantly outperforms even larger models,\nwhile exhibiting perfect determinism, enhanced stability against input\nperturbations, and inherent immunity to prompt injection attacks. By generating\ndecisions grounded in symbolic reasoning, ATA offers a practical and\ncontrollable architecture for building the next generation of transparent,\nauditable, and reliable autonomous agents.", "AI": {"tldr": "\u63d0\u51faATA\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u5206\u79bb\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u79bb\u7ebf\u77e5\u8bc6\u63d0\u53d6\u548c\u5728\u7ebf\u4efb\u52a1\u5904\u7406\uff0c\u63d0\u5347LLM\u7684\u53ef\u4fe1\u5ea6", "motivation": "\u89e3\u51b3LLM\u5728\u5173\u952e\u9886\u57df\u90e8\u7f72\u65f6\u7684\u5e7b\u89c9\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7b49\u53ef\u4fe1\u5ea6\u95ee\u9898", "method": "\u5c06\u4efb\u52a1\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u79bb\u7ebf\u9636\u6bb5LLM\u5c06\u95ee\u9898\u89c4\u8303\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u77e5\u8bc6\u5e93\uff0c\u5728\u7ebf\u9636\u6bb5\u7b26\u53f7\u51b3\u7b56\u5f15\u64ce\u57fa\u4e8e\u5f62\u5f0f\u5316\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406", "result": "\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cATA\u4e0e\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u63a8\u7406\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u5177\u6709\u5b8c\u7f8e\u786e\u5b9a\u6027\u3001\u589e\u5f3a\u7684\u7a33\u5b9a\u6027\u548c\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u80fd\u529b", "conclusion": "ATA\u4e3a\u6784\u5efa\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u9760\u7684\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u63a7\u7684\u67b6\u6784"}}
{"id": "2510.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16160", "abs": "https://arxiv.org/abs/2510.16160", "authors": ["Ahmad Arrabi", "Jay Hwasung Jung", "Jax Luo", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "Automated C-Arm Positioning via Conformal Landmark Localization", "comment": null, "summary": "Accurate and reliable C-arm positioning is essential for fluoroscopy-guided\ninterventions. However, clinical workflows rely on manual alignment that\nincreases radiation exposure and procedural delays. In this work, we present a\npipeline that autonomously navigates the C-arm to predefined anatomical\nlandmarks utilizing X-ray images. Given an input X-ray image from an arbitrary\nstarting location on the operating table, the model predicts a 3D displacement\nvector toward each target landmark along the body. To ensure reliable\ndeployment, we capture both aleatoric and epistemic uncertainties in the\nmodel's predictions and further calibrate them using conformal prediction. The\nderived prediction regions are interpreted as 3D confidence regions around the\npredicted landmark locations. The training framework combines a probabilistic\nloss with skeletal pose regularization to encourage anatomically plausible\noutputs. We validate our approach on a synthetic X-ray dataset generated from\nDeepDRR. Results show not only strong localization accuracy across multiple\narchitectures but also well-calibrated prediction bounds. These findings\nhighlight the pipeline's potential as a component in safe and reliable\nautonomous C-arm systems. Code is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance_APAH", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u5bfc\u822aC\u81c2\u5230\u9884\u5b9a\u4e49\u89e3\u5256\u6807\u5fd7\u7684\u7ba1\u9053\uff0c\u5229\u7528X\u5c04\u7ebf\u56fe\u50cf\u9884\u6d4b3D\u4f4d\u79fb\u5411\u91cf\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u4fdd\u5f62\u9884\u6d4b\u786e\u4fdd\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4f9d\u8d56\u624b\u52a8\u5bf9\u9f50C\u81c2\uff0c\u8fd9\u4f1a\u589e\u52a0\u8f90\u5c04\u66b4\u9732\u548c\u624b\u672f\u5ef6\u8fdf\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528X\u5c04\u7ebf\u56fe\u50cf\u9884\u6d4b3D\u4f4d\u79fb\u5411\u91cf\uff0c\u7ed3\u5408\u6982\u7387\u635f\u5931\u548c\u9aa8\u9abc\u59ff\u6001\u6b63\u5219\u5316\uff0c\u91c7\u7528\u4fdd\u5f62\u9884\u6d4b\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u751f\u62103D\u7f6e\u4fe1\u533a\u57df\u3002", "result": "\u5728DeepDRR\u751f\u6210\u7684\u5408\u6210X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u793a\u5f3a\u5927\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u826f\u597d\u6821\u51c6\u7684\u9884\u6d4b\u8fb9\u754c\u3002", "conclusion": "\u8be5\u7ba1\u9053\u6709\u6f5c\u529b\u6210\u4e3a\u5b89\u5168\u53ef\u9760\u81ea\u4e3bC\u81c2\u7cfb\u7edf\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u80fd\u591f\u51cf\u5c11\u8f90\u5c04\u66b4\u9732\u548c\u624b\u672f\u65f6\u95f4\u3002"}}
{"id": "2510.16387", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16387", "abs": "https://arxiv.org/abs/2510.16387", "authors": ["Fu-An Chao", "Bi-Cheng Yan", "Berlin Chen"], "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment", "comment": null, "summary": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86Whisper\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u7b2c\u4e8c\u8bed\u8a00\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u63d0\u53d6\u5176\u9690\u85cf\u8868\u793a\u4e2d\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u5373\u53ef\u5728GEPT\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5206\u6790Whisper\u4ea7\u751f\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u6316\u6398\u5176\u6f5c\u5728\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u7b2c\u4e8c\u8bed\u8a00\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4eceWhisper\u7684\u4e2d\u95f4\u548c\u6700\u7ec8\u8f93\u51fa\u4e2d\u63d0\u53d6\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u5e76\u5f15\u5165\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u8f85\u52a9\u76f8\u5173\u7ebf\u7d22\u3002", "result": "\u5728GEPT\u56fe\u7247\u63cf\u8ff0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u5305\u62ec\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u5185\u7684\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\uff0c\u901a\u8fc7\u6dfb\u52a0\u8f85\u52a9\u4fe1\u606f\u83b7\u5f97\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0cWhisper\u6a21\u578b\u672c\u8d28\u4e0a\u7f16\u7801\u4e86\u53e3\u8bed\u719f\u7ec3\u5ea6\u7684\u5e8f\u6570\u6a21\u5f0f\u548c\u8bed\u4e49\u65b9\u9762\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u53e3\u8bed\u8bc4\u4f30\u548c\u53e3\u8bed\u7406\u89e3\u4efb\u52a1\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16179", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16179", "abs": "https://arxiv.org/abs/2510.16179", "authors": ["Xavier Giro-i-Nieto", "Nefeli Andreou", "Anqi Liang", "Manel Baradad", "Francesc Moreno-Noguer", "Aleix Martinez"], "title": "Cost Savings from Automatic Quality Assessment of Generated Images", "comment": null, "summary": "Deep generative models have shown impressive progress in recent years, making\nit possible to produce high quality images with a simple text prompt or a\nreference image. However, state of the art technology does not yet meet the\nquality standards offered by traditional photographic methods. For this reason,\nproduction pipelines that use generated images often include a manual stage of\nimage quality assessment (IQA). This process is slow and expensive, especially\nbecause of the low yield of automatically generated images that pass the\nquality bar. The IQA workload can be reduced by introducing an automatic\npre-filtering stage, that will increase the overall quality of the images sent\nto review and, therefore, reduce the average cost required to obtain a high\nquality image. We present a formula that estimates the cost savings depending\non the precision and pass yield of a generic IQA engine. This formula is\napplied in a use case of background inpainting, showcasing a significant cost\nsaving of 51.61% obtained with a simple AutoML solution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u516c\u5f0f\u6765\u4f30\u8ba1\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u5f15\u64ce\u7684\u6210\u672c\u8282\u7701\uff0c\u5e76\u5728\u80cc\u666f\u4fee\u590d\u7528\u4f8b\u4e2d\u5c55\u793a\u4e8651.61%\u7684\u6210\u672c\u8282\u7701", "motivation": "\u5f53\u524d\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4ecd\u8fbe\u4e0d\u5230\u4f20\u7edf\u6444\u5f71\u6807\u51c6\uff0c\u624b\u52a8\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u8fc7\u7a0b\u7f13\u6162\u4e14\u6602\u8d35\uff0c\u9700\u8981\u81ea\u52a8\u9884\u8fc7\u6ee4\u6765\u964d\u4f4e\u6210\u672c", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u516c\u5f0f\u6765\u4f30\u8ba1\u57fa\u4e8e\u7cbe\u5ea6\u548c\u901a\u8fc7\u7387\u7684\u901a\u7528IQA\u5f15\u64ce\u7684\u6210\u672c\u8282\u7701\uff0c\u5e76\u5728\u80cc\u666f\u4fee\u590d\u7528\u4f8b\u4e2d\u4f7f\u7528\u7b80\u5355\u7684AutoML\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u9a8c\u8bc1", "result": "\u5728\u80cc\u666f\u4fee\u590d\u7528\u4f8b\u4e2d\u5b9e\u73b0\u4e8651.61%\u7684\u663e\u8457\u6210\u672c\u8282\u7701", "conclusion": "\u81ea\u52a8\u56fe\u50cf\u8d28\u91cf\u9884\u8fc7\u6ee4\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u83b7\u53d6\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u5e73\u5747\u6210\u672c\uff0c\u63d0\u51fa\u7684\u516c\u5f0f\u4e3a\u8bc4\u4f30IQA\u5f15\u64ce\u7684\u6210\u672c\u6548\u76ca\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177"}}
{"id": "2510.16439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16439", "abs": "https://arxiv.org/abs/2510.16439", "authors": ["Syed Rifat Raiyan", "Md Farhan Ishmam", "Abdullah Al Imran", "Mohammad Ali Moni"], "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution", "comment": null, "summary": "Large language models (LLMs) owe much of their stellar performance to\nexpansive input contexts, yet such verbosity inflates monetary costs, carbon\nfootprint, and inference-time latency. Much of this overhead manifests from the\nredundant low-utility tokens present in typical prompts, as only a fraction of\ntokens typically carries the majority of the semantic weight. We address this\ninefficiency by introducing FrugalPrompt, a novel prompt compression framework\nfor LLMs, which retains only the most semantically significant tokens.\nLeveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,\nwe assign salience scores to every token in an input sequence, rank them to\npreserve the top-k% tokens in their original order, and obtain a sparse\nfrugalized prompt. We evaluate the approach across four NLP tasks: Sentiment\nAnalysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a\nsuite of frontier LLMs. For the first three tasks, a 20% prompt reduction\nincurs only a marginal loss in task performance, demonstrating that\ncontemporary LLMs can reconstruct elided context from high-salience cues. In\ncontrast, performance on mathematical reasoning deteriorates sharply,\nreflecting a stronger dependence on complete token continuity. Further analysis\nwith bottom-k% and random-k% tokens reveals asymmetric performance patterns\nthat may suggest potential task contamination effects, wherein models may\nresort to shallow memorized patterns from pretraining exposure for conventional\nNLP tasks. We posit that our work contributes to a more nuanced understanding\nof LLM behavior in performance-efficiency trade-offs, and delineate the\nboundary between tasks tolerant to contextual sparsity and those requiring\nexhaustive context. Our source code and models are available at:\nhttps://github.com/Starscream-11813/Frugal-ICL", "AI": {"tldr": "FrugalPrompt\u662f\u4e00\u4e2a\u65b0\u9896\u7684LLM\u63d0\u793a\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u6700\u5177\u8bed\u4e49\u91cd\u8981\u6027\u7684token\u6765\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\uff0c\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e2d\u5b9e\u73b020%\u538b\u7f29\u7387\u65f6\u4ec5\u5e26\u6765\u8f7b\u5fae\u6027\u80fd\u635f\u5931\uff0c\u4f46\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3LLM\u4e2d\u5197\u957f\u8f93\u5165\u5e26\u6765\u7684\u6210\u672c\u3001\u78b3\u8db3\u8ff9\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u56e0\u4e3a\u5178\u578b\u63d0\u793a\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u7684\u4f4e\u6548\u7528token\uff0c\u53ea\u6709\u5c11\u6570token\u627f\u8f7d\u4e3b\u8981\u8bed\u4e49\u6743\u91cd\u3002", "method": "\u4f7f\u7528GlobEnc\u548cDecompX\u4e24\u79cdtoken\u5f52\u56e0\u65b9\u6cd5\u4e3a\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2atoken\u5206\u914d\u663e\u8457\u6027\u5206\u6570\uff0c\u6309\u539f\u59cb\u987a\u5e8f\u4fdd\u7559\u524dk%\u7684token\uff0c\u83b7\u5f97\u7a00\u758f\u7684\u538b\u7f29\u63d0\u793a\u3002", "result": "\u5728\u524d\u4e09\u4e2a\u4efb\u52a1\uff08\u60c5\u611f\u5206\u6790\u3001\u5e38\u8bc6\u95ee\u7b54\u3001\u6458\u8981\uff09\u4e2d\uff0c20%\u7684\u63d0\u793a\u538b\u7f29\u4ec5\u5e26\u6765\u8fb9\u9645\u6027\u80fd\u635f\u5931\uff1b\u4f46\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6027\u80fd\u6025\u5267\u6076\u5316\uff1b\u4f7f\u7528\u5e95\u90e8k%\u548c\u968f\u673ak%token\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u5bf9\u79f0\u6027\u80fd\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u66f4\u7ec6\u81f4\u5730\u7406\u89e3LLM\u5728\u6027\u80fd-\u6548\u7387\u6743\u8861\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u754c\u5b9a\u4e86\u5bb9\u5fcd\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u7684\u4efb\u52a1\u4e0e\u9700\u8981\u5b8c\u6574\u4e0a\u4e0b\u6587\u7684\u4efb\u52a1\u4e4b\u95f4\u7684\u8fb9\u754c\u3002"}}
{"id": "2510.16196", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16196", "abs": "https://arxiv.org/abs/2510.16196", "authors": ["Zheng Huang", "Enpei Zhang", "Yinghao Cai", "Weikang Qiu", "Carl Yang", "Elynn Chen", "Xiang Zhang", "Rex Ying", "Dawei Zhou", "Yujun Yan"], "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI", "comment": null, "summary": "Understanding how the brain encodes visual information is a central challenge\nin neuroscience and machine learning. A promising approach is to reconstruct\nvisual stimuli, essentially images, from functional Magnetic Resonance Imaging\n(fMRI) signals. This involves two stages: transforming fMRI signals into a\nlatent space and then using a pretrained generative model to reconstruct\nimages. The reconstruction quality depends on how similar the latent space is\nto the structure of neural activity and how well the generative model produces\nimages from that space. Yet, it remains unclear which type of latent space best\nsupports this transformation and how it should be organized to represent visual\nstimuli effectively. We present two key findings. First, fMRI signals are more\nsimilar to the text space of a language model than to either a vision based\nspace or a joint text image space. Second, text representations and the\ngenerative model should be adapted to capture the compositional nature of\nvisual stimuli, including objects, their detailed attributes, and\nrelationships. Building on these insights, we propose PRISM, a model that\nProjects fMRI sIgnals into a Structured text space as an interMediate\nrepresentation for visual stimuli reconstruction. It includes an object centric\ndiffusion module that generates images by composing individual objects to\nreduce object detection errors, and an attribute relationship search module\nthat automatically identifies key attributes and relationships that best align\nwith the neural activity. Extensive experiments on real world datasets\ndemonstrate that our framework outperforms existing methods, achieving up to an\n8% reduction in perceptual loss. These results highlight the importance of\nusing structured text as the intermediate space to bridge fMRI signals and\nimage reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faPRISM\u6a21\u578b\uff0c\u901a\u8fc7\u5c06fMRI\u4fe1\u53f7\u6295\u5f71\u5230\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6765\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\uff0c\u5229\u7528\u5bf9\u8c61\u4e2d\u5fc3\u6269\u6563\u6a21\u5757\u548c\u5c5e\u6027\u5173\u7cfb\u641c\u7d22\u6a21\u5757\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u5982\u4f55\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u662f\u795e\u7ecf\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u54ea\u79cd\u6f5c\u5728\u7a7a\u95f4\u6700\u9002\u5408\u5c06fMRI\u4fe1\u53f7\u8f6c\u6362\u4e3a\u89c6\u89c9\u523a\u6fc0\uff0c\u4ee5\u53ca\u5982\u4f55\u7ec4\u7ec7\u8be5\u7a7a\u95f4\u4ee5\u6709\u6548\u8868\u793a\u89c6\u89c9\u523a\u6fc0\u3002", "method": "\u63d0\u51faPRISM\u6a21\u578b\uff1a1\uff09\u5c06fMRI\u4fe1\u53f7\u6295\u5f71\u5230\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\uff1b2\uff09\u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3\u6269\u6563\u6a21\u5757\u901a\u8fc7\u7ec4\u5408\u5355\u4e2a\u5bf9\u8c61\u751f\u6210\u56fe\u50cf\u4ee5\u51cf\u5c11\u5bf9\u8c61\u68c0\u6d4b\u9519\u8bef\uff1b3\uff09\u5c5e\u6027\u5173\u7cfb\u641c\u7d22\u6a21\u5757\u81ea\u52a8\u8bc6\u522b\u4e0e\u795e\u7ecf\u6d3b\u52a8\u6700\u5339\u914d\u7684\u5173\u952e\u5c5e\u6027\u548c\u5173\u7cfb\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u611f\u77e5\u635f\u5931\u964d\u4f4e\u9ad8\u8fbe8%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u7ed3\u6784\u5316\u6587\u672c\u4f5c\u4e3a\u4e2d\u95f4\u7a7a\u95f4\u6765\u6865\u63a5fMRI\u4fe1\u53f7\u548c\u56fe\u50cf\u91cd\u5efa\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0cfMRI\u4fe1\u53f7\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u7a7a\u95f4\u66f4\u76f8\u4f3c\uff0c\u800c\u975e\u57fa\u4e8e\u89c6\u89c9\u7684\u7a7a\u95f4\u6216\u8054\u5408\u6587\u672c\u56fe\u50cf\u7a7a\u95f4\u3002"}}
{"id": "2510.16449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16449", "abs": "https://arxiv.org/abs/2510.16449", "authors": ["Bin Yu", "Xinming Wang", "Shijie Lian", "Haotian Li", "Changti Wu", "Ruina Hu", "Bailing Wang", "Yuliang Wei", "Kai Chen"], "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model", "comment": "13 pages, 6 figures. Project website:\n  https://zgca-ai4edu.github.io/TrajSelector", "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.", "AI": {"tldr": "TrajSelector\u662f\u4e00\u4e2a\u9ad8\u6548\u7684Best-of-N\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u8bc4\u5206\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5916\u90e8TTS\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u9ad8\u548c\u672a\u5145\u5206\u5229\u7528LLM\u5185\u5728\u6f5c\u5728\u8868\u793a\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u9009\u62e9\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\uff08\u4ec50.6B\u53c2\u6570\uff09\u8bc4\u4f30\u6b65\u9aa4\u7ea7\u8f68\u8ff9\u8d28\u91cf\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u8bc4\u5206\uff0c\u65e0\u9700\u5927\u91cf\u6b65\u9aa4\u7ea7\u6807\u6ce8\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrajSelector\u5728Best-of-32\u8bbe\u7f6e\u4e0b\u6bd4\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u9ad84.61%\uff0c\u6bd4\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u9ad84.31%-12.21%\uff0c\u4e14\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "TrajSelector\u901a\u8fc7\u6709\u6548\u5229\u7528LLM\u5185\u5728\u8868\u793a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u63a8\u7406\u8f68\u8ff9\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2510.16207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16207", "abs": "https://arxiv.org/abs/2510.16207", "authors": ["Mateus Pinto da Silva", "Sabrina P. L. P. Correa", "Hugo N. Oliveira", "Ian M. Nunes", "Jefersson A. dos Santos"], "title": "Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions", "comment": "5 pages, 1 figure", "summary": "Mapping agriculture in tropical areas through remote sensing presents unique\nchallenges, including the lack of high-quality annotated data, the elevated\ncosts of labeling, data variability, and regional generalisation. This paper\nadvocates a Data-Centric Artificial Intelligence (DCAI) perspective and\npipeline, emphasizing data quality and curation as key drivers for model\nrobustness and scalability. It reviews and prioritizes techniques such as\nconfident learning, core-set selection, data augmentation, and active learning.\nThe paper highlights the readiness and suitability of 25 distinct strategies in\nlarge-scale agricultural mapping pipelines. The tropical context is of high\ninterest, since high cloudiness, diverse crop calendars, and limited datasets\nlimit traditional model-centric approaches. This tutorial outlines practical\nsolutions as a data-centric approach for curating and training AI models better\nsuited to the dynamic realities of tropical agriculture. Finally, we propose a\npractical pipeline using the 9 most mature and straightforward methods that can\nbe applied to a large-scale tropical agricultural mapping project.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u91c7\u7528\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u6765\u89e3\u51b3\u70ed\u5e26\u5730\u533a\u519c\u4e1a\u9065\u611f\u5236\u56fe\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63a8\u8350\u4e8625\u79cd\u6570\u636e\u7b56\u7565\uff0c\u5176\u4e2d9\u79cd\u6700\u6210\u719f\u7684\u65b9\u6cd5\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u70ed\u5e26\u519c\u4e1a\u5236\u56fe\u9879\u76ee\u3002", "motivation": "\u70ed\u5e26\u5730\u533a\u519c\u4e1a\u9065\u611f\u5236\u56fe\u9762\u4e34\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u6570\u636e\u53d8\u5f02\u6027\u548c\u533a\u57df\u6cdb\u5316\u7b49\u72ec\u7279\u6311\u6218\uff0c\u4f20\u7edf\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u53d7\u5230\u9650\u5236\u3002", "method": "\u91c7\u7528\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u89c6\u89d2\u548c\u6d41\u7a0b\uff0c\u91cd\u70b9\u5e94\u7528\u81ea\u4fe1\u5b66\u4e60\u3001\u6838\u5fc3\u96c6\u9009\u62e9\u3001\u6570\u636e\u589e\u5f3a\u548c\u4e3b\u52a8\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5f3a\u8c03\u6570\u636e\u8d28\u91cf\u548c\u7ba1\u7406\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "result": "\u8bc6\u522b\u5e76\u4f18\u5148\u6392\u5e8f\u4e8625\u79cd\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u519c\u4e1a\u5236\u56fe\u6d41\u7a0b\u7684\u6570\u636e\u7b56\u7565\uff0c\u5176\u4e2d9\u79cd\u6700\u6210\u719f\u4e14\u76f4\u63a5\u7684\u65b9\u6cd5\u88ab\u63a8\u8350\u7528\u4e8e\u5b9e\u9645\u9879\u76ee\u3002", "conclusion": "\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e3a\u70ed\u5e26\u519c\u4e1a\u52a8\u6001\u73b0\u5b9e\u63d0\u4f9b\u4e86\u66f4\u5408\u9002\u7684AI\u6a21\u578b\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u6570\u636e\u7ba1\u7406\u548c\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.16455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16455", "abs": "https://arxiv.org/abs/2510.16455", "authors": ["Deyi Ji", "Yuekui Yang", "Haiyang Wu", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning", "comment": "ACL 2025 (Oral, Industry Track)", "summary": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.", "AI": {"tldr": "RAVEN\u662f\u4e00\u4e2a\u96c6\u6210\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u5206\u5c42\u5956\u52b1\u673a\u5236\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u8fdd\u89c4\u7c7b\u522b\u51c6\u786e\u6027\u548c\u65f6\u95f4\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u65b9\u6cd5\u5728\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u3001\u566a\u58f0\u6807\u6ce8\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u6574\u5408\u7cbe\u786e\u548c\u7c97\u7565\u6807\u6ce8\u6570\u636e\uff0c\u4f7f\u7528GRPO\u7b97\u6cd5\u5f00\u53d1\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u673a\u5236\u786e\u4fdd\u65f6\u95f4\u5b9a\u4f4d\u548c\u7c7b\u522b\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RAVEN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5728\u7ebf\u5e7f\u544a\u670d\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7f13\u89e3\u4e86\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2510.16209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16209", "abs": "https://arxiv.org/abs/2510.16209", "authors": ["Nyle Siddiqui", "Rohit Gupta", "Sirnam Swetha", "Mubarak Shah"], "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales", "comment": null, "summary": "State space models (SSMs) have emerged as a competitive alternative to\ntransformers in various tasks. Their linear complexity and hidden-state\nrecurrence make them particularly attractive for modeling long sequences,\nwhereas attention becomes quadratically expensive. However, current training\nmethods for video understanding are tailored towards transformers and fail to\nfully leverage the unique attributes of SSMs. For example, video models are\noften trained at a fixed resolution and video length to balance the quadratic\nscaling of attention cost against performance. Consequently, these models\nsuffer from degraded performance when evaluated on videos with spatial and\ntemporal resolutions unseen during training; a property we call spatio-temporal\ninflexibility. In the context of action recognition, this severely limits a\nmodel's ability to retain performance across both short- and long-form videos.\nTherefore, we propose a flexible training method that leverages and improves\nthe inherent adaptability of SSMs. Our method samples videos at varying\ntemporal and spatial resolutions during training and dynamically interpolates\nmodel weights to accommodate any spatio-temporal scale. This instills our SSM,\nwhich we call StretchySnake, with spatio-temporal flexibility and enables it to\nseamlessly handle videos ranging from short, fine-grained clips to long,\ncomplex activities. We introduce and compare five different variants of\nflexible training, and identify the most effective strategy for video SSMs. On\nshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,\nStretchySnake outperforms transformer and SSM baselines alike by up to 28%,\nwith strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,\nour method provides a simple drop-in training recipe that makes video SSMs more\nrobust, resolution-agnostic, and efficient across diverse action recognition\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStretchySnake\u7684\u7075\u6d3b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65f6\u7a7a\u5206\u8fa8\u7387\u6765\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u672a\u89c1\u65f6\u7a7a\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9\u89c6\u9891\u7406\u89e3\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9transformer\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u72ec\u7279\u4f18\u52bf\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u56fa\u5b9a\u5206\u8fa8\u7387\u548c\u89c6\u9891\u957f\u5ea6\u4e0b\u8bad\u7ec3\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u9762\u5bf9\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u65f6\u7a7a\u4e0d\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u91c7\u6837\u4e0d\u540c\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u89c6\u9891\uff0c\u5e76\u52a8\u6001\u63d2\u503c\u6a21\u578b\u6743\u91cd\u4ee5\u9002\u5e94\u4efb\u610f\u65f6\u7a7a\u5c3a\u5ea6\u3002\u6bd4\u8f83\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u7075\u6d3b\u8bad\u7ec3\u53d8\u4f53\uff0c\u786e\u5b9a\u4e86\u6700\u9002\u5408\u89c6\u9891\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u7b56\u7565\u3002", "result": "\u5728\u77ed\u52a8\u4f5c\uff08UCF-101\u3001HMDB-51\uff09\u548c\u957f\u52a8\u4f5c\uff08COIN\u3001Breakfast\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStretchySnake\u6bd4transformer\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe28%\uff0c\u5728\u7ec6\u7c92\u5ea6\u52a8\u4f5c\uff08SSV2\u3001Diving-48\uff09\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5373\u63d2\u5373\u7528\u8bad\u7ec3\u65b9\u6848\uff0c\u4f7f\u89c6\u9891\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u5404\u79cd\u52a8\u4f5c\u8bc6\u522b\u573a\u666f\u4e2d\u66f4\u52a0\u9c81\u68d2\u3001\u5206\u8fa8\u7387\u65e0\u5173\u4e14\u9ad8\u6548\u3002"}}
{"id": "2510.16458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16458", "abs": "https://arxiv.org/abs/2510.16458", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Benjamin Roth", "Barbara Plank"], "title": "Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations", "comment": null, "summary": "Natural Language Inference datasets often exhibit human label variation. To\nbetter understand these variations, explanation-based approaches analyze the\nunderlying reasoning behind annotators' decisions. One such approach is the\nLiTEx taxonomy, which categorizes free-text explanations in English into\nreasoning types. However, previous work applying such taxonomies has focused on\nwithin-label variation: cases where annotators agree on the final NLI label but\nprovide different explanations. In contrast, this paper broadens the scope by\nexamining how annotators may diverge not only in the reasoning type but also in\nthe labeling step. We use explanations as a lens to decompose the reasoning\nprocess underlying NLI annotation and to analyze individual differences. We\napply LiTEx to two NLI English datasets and align annotation variation from\nmultiple aspects: NLI label agreement, explanation similarity, and taxonomy\nagreement, with an additional compounding factor of annotators' selection bias.\nWe observe instances where annotators disagree on the label but provide highly\nsimilar explanations, suggesting that surface-level disagreement may mask\nunderlying agreement in interpretation. Moreover, our analysis reveals\nindividual preferences in explanation strategies and label choices. These\nfindings highlight that agreement in reasoning types better reflects the\nsemantic similarity of free-text explanations than label agreement alone. Our\nfindings underscore the richness of reasoning-based explanations and the need\nfor caution in treating labels as ground truth.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u89e3\u91ca\u5206\u6790\u6269\u5c55\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u6807\u6ce8\u53d8\u5f02\u7684\u7406\u89e3\uff0c\u4e0d\u4ec5\u5173\u6ce8\u6807\u6ce8\u4e00\u81f4\u4f46\u89e3\u91ca\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u8fd8\u7814\u7a76\u4e86\u6807\u6ce8\u548c\u89e3\u91ca\u90fd\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u53d1\u73b0\u8868\u9762\u6807\u6ce8\u5206\u6b67\u53ef\u80fd\u63a9\u76d6\u6df1\u5c42\u7684\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6807\u6ce8\u4e00\u81f4\u4f46\u89e3\u91ca\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u800c\u672c\u6587\u65e8\u5728\u66f4\u5168\u9762\u5730\u7406\u89e3NLI\u6570\u636e\u96c6\u4e2d\u7684\u4eba\u7c7b\u6807\u6ce8\u53d8\u5f02\uff0c\u901a\u8fc7\u89e3\u91ca\u5206\u6790\u6765\u63ed\u793a\u6807\u6ce8\u8005\u63a8\u7406\u8fc7\u7a0b\u7684\u5dee\u5f02\u3002", "method": "\u4f7f\u7528LiTEx\u5206\u7c7b\u6cd5\u5206\u6790\u4e24\u4e2a\u82f1\u6587NLI\u6570\u636e\u96c6\uff0c\u4ece\u591a\u4e2a\u7ef4\u5ea6\u5bf9\u9f50\u6807\u6ce8\u53d8\u5f02\uff1aNLI\u6807\u7b7e\u4e00\u81f4\u6027\u3001\u89e3\u91ca\u76f8\u4f3c\u6027\u548c\u5206\u7c7b\u6cd5\u4e00\u81f4\u6027\uff0c\u5e76\u8003\u8651\u6807\u6ce8\u8005\u7684\u9009\u62e9\u504f\u89c1\u3002", "result": "\u53d1\u73b0\u6807\u6ce8\u8005\u53ef\u80fd\u5728\u6807\u7b7e\u4e0a\u5b58\u5728\u5206\u6b67\u4f46\u63d0\u4f9b\u9ad8\u5ea6\u76f8\u4f3c\u7684\u89e3\u91ca\uff0c\u8868\u660e\u8868\u9762\u5206\u6b67\u53ef\u80fd\u63a9\u76d6\u89e3\u91ca\u5c42\u9762\u7684\u4e00\u81f4\u6027\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u6807\u6ce8\u8005\u5728\u89e3\u91ca\u7b56\u7565\u548c\u6807\u7b7e\u9009\u62e9\u4e0a\u7684\u4e2a\u4f53\u504f\u597d\u3002", "conclusion": "\u63a8\u7406\u7c7b\u578b\u7684\u4e00\u81f4\u6027\u6bd4\u5355\u7eaf\u7684\u6807\u7b7e\u4e00\u81f4\u6027\u66f4\u80fd\u53cd\u6620\u81ea\u7531\u6587\u672c\u89e3\u91ca\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u89e3\u91ca\u7684\u4e30\u5bcc\u6027\uff0c\u4ee5\u53ca\u5c06\u6807\u7b7e\u89c6\u4e3a\u7edd\u5bf9\u771f\u5b9e\u65f6\u9700\u8981\u8c28\u614e\u5bf9\u5f85\u3002"}}
{"id": "2510.16220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16220", "abs": "https://arxiv.org/abs/2510.16220", "authors": ["Djamel Eddine Boukhari"], "title": "VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction", "comment": null, "summary": "Facial Beauty Prediction (FBP) is a complex and challenging computer vision\ntask, aiming to model the subjective and intricate nature of human aesthetic\nperception. While deep learning models, particularly Convolutional Neural\nNetworks (CNNs), have made significant strides, they often struggle to capture\nthe global, holistic facial features that are critical to human judgment.\nVision Transformers (ViT) address this by effectively modeling long-range\nspatial relationships, but their quadratic complexity can be a bottleneck. This\npaper introduces a novel, heterogeneous ensemble architecture,\n\\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths\nof a Vision Transformer and a Mamba-based Vision model, a recent advancement in\nState-Space Models (SSMs). The ViT backbone excels at capturing global facial\nstructure and symmetry, while the Mamba backbone efficiently models long-range\ndependencies with linear complexity, focusing on sequential features and\ntextures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our\nproposed VM-BeautyNet achieves state-of-the-art performance, with a\n\\textbf{Pearson Correlation (PC) of 0.9212}, a \\textbf{Mean Absolute Error\n(MAE) of 0.2085}, and a \\textbf{Root Mean Square Error (RMSE) of 0.2698}.\nFurthermore, through Grad-CAM visualizations, we provide interpretability\nanalysis that confirms the complementary feature extraction of the two\nbackbones, offering new insights into the model's decision-making process and\npresenting a powerful new architectural paradigm for computational aesthetics.", "AI": {"tldr": "\u63d0\u51faVM-BeautyNet\uff0c\u4e00\u79cd\u878d\u5408Vision Transformer\u548cMamba\u6a21\u578b\u7684\u5f02\u6784\u96c6\u6210\u67b6\u6784\uff0c\u7528\u4e8e\u9762\u90e8\u7f8e\u989c\u9884\u6d4b\uff0c\u5728SCUT-FBP5500\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CNN\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5168\u5c40\u9762\u90e8\u7279\u5f81\uff0cViT\u80fd\u5efa\u6a21\u957f\u8ddd\u79bb\u7a7a\u95f4\u5173\u7cfb\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u79cd\u6a21\u578b\u7684\u4f18\u52bf\u6765\u89e3\u51b3\u9762\u90e8\u7f8e\u989c\u9884\u6d4b\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5f02\u6784\u96c6\u6210\u67b6\u6784\uff0cViT\u4e3b\u5e72\u6355\u6349\u5168\u5c40\u9762\u90e8\u7ed3\u6784\u548c\u5bf9\u79f0\u6027\uff0cMamba\u4e3b\u5e72\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u5173\u6ce8\u5e8f\u5217\u7279\u5f81\u548c\u7eb9\u7406\u3002", "result": "\u5728SCUT-FBP5500\u6570\u636e\u96c6\u4e0a\u53d6\u5f97PC=0.9212\u3001MAE=0.2085\u3001RMSE=0.2698\u7684SOTA\u6027\u80fd\uff0c\u5e76\u901a\u8fc7Grad-CAM\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u4e24\u4e3b\u5e72\u7684\u4e92\u8865\u7279\u5f81\u63d0\u53d6\u3002", "conclusion": "VM-BeautyNet\u4e3a\u8ba1\u7b97\u7f8e\u5b66\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u67b6\u6784\u8303\u5f0f\uff0c\u901a\u8fc7\u878d\u5408ViT\u548cMamba\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9762\u90e8\u7f8e\u989c\u9884\u6d4b\u4e2d\u7684\u5168\u5c40\u7279\u5f81\u6355\u6349\u95ee\u9898\u3002"}}
{"id": "2510.16492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16492", "abs": "https://arxiv.org/abs/2510.16492", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "comment": "Reliable ML and Regulatable ML workshops, Neurips 2025", "summary": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\"\u9000\u51fa\"\u4f5c\u4e3aLLM\u4ee3\u7406\u7684\u5b89\u5168\u673a\u5236\uff0c\u8ba9\u4ee3\u7406\u5728\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa\uff0c\u5728ToolEmu\u6846\u67b6\u4e0b\u8bc4\u4f3012\u4e2a\u5148\u8fdbLLM\uff0c\u8bc1\u660e\u9000\u51fa\u6307\u4ee4\u80fd\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u5e2e\u52a9\u6027\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5176\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u591a\u8f6e\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u7cca\u6027\u4f1a\u7d2f\u79ef\uff0c\u5bfc\u81f4\u8d85\u51fa\u4f20\u7edf\u6587\u672c\u751f\u6210\u5931\u8d25\u7684\u4e25\u91cd\u98ce\u9669\u3002", "method": "\u5229\u7528ToolEmu\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f3012\u4e2a\u6700\u5148\u8fdbLLM\u7684\u9000\u51fa\u884c\u4e3a\uff0c\u901a\u8fc7\u6dfb\u52a0\u663e\u5f0f\u9000\u51fa\u6307\u4ee4\u6765\u6d4b\u8bd5\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u5e2e\u52a9\u6027\u6743\u8861\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5b89\u5168\u6027\u5e73\u5747\u63d0\u5347+0.39\uff080-3\u5206\u5236\uff09\uff0c\u4e13\u6709\u6a21\u578b\u63d0\u5347+0.64\uff0c\u800c\u5e2e\u52a9\u6027\u4ec5\u5e73\u5747\u4e0b\u964d-0.03\uff0c\u8bc1\u660e\u9000\u51fa\u673a\u5236\u5177\u6709\u6781\u4f73\u7684\u5b89\u5168-\u5e2e\u52a9\u6027\u6743\u8861\u3002", "conclusion": "\u7b80\u5355\u6dfb\u52a0\u663e\u5f0f\u9000\u51fa\u6307\u4ee4\u662f\u4e00\u79cd\u9ad8\u5ea6\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\uff0c\u53ef\u7acb\u5373\u90e8\u7f72\u5230\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u5e76\u786e\u7acb\u9000\u51fa\u4f5c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81ea\u4e3b\u4ee3\u7406\u7684\u6709\u6548\u7b2c\u4e00\u9053\u9632\u7ebf\u3002"}}
{"id": "2510.16235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16235", "abs": "https://arxiv.org/abs/2510.16235", "authors": ["Vishal Manikanden", "Aniketh Bandlamudi", "Daniel Haehn"], "title": "Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection", "comment": null, "summary": "Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head\nand neck cancer. Due to the subtle nature of its early stages, deep and hidden\nareas of development, and slow growth, OCSCC often goes undetected, leading to\npreventable deaths. However, properly trained Convolutional Neural Networks\n(CNNs), with their precise image segmentation techniques and ability to apply\nkernel matrices to modify the RGB values of images for accurate image pattern\nrecognition, would be an effective means for early detection of OCSCC. Pairing\nthis neural network with image capturing and processing hardware would allow\nincreased efficacy in OCSCC detection. The aim of our project is to develop a\nConvolutional Neural Network trained to recognize OCSCC, as well as to design a\nphysical hardware system to capture and process detailed images, in order to\ndetermine the image quality required for accurate predictions. A CNN was\ntrained on 4293 training images consisting of benign and malignant tumors, as\nwell as negative samples, and was evaluated for its precision, recall, and Mean\nAverage Precision (mAP) in its predictions of OCSCC. A testing dataset of\nrandomly assorted images of cancerous, non-cancerous, and negative images was\nchosen, and each image was altered to represent 5 common resolutions. This test\ndata set was thoroughly analyzed by the CNN and predictions were scored on the\nbasis of accuracy. The designed enhancement hardware was used to capture\ndetailed images, and its impact was scored. An application was developed to\nfacilitate the testing process and bring open access to the CNN. Images of\nincreasing resolution resulted in higher-accuracy predictions on a logarithmic\nscale, demonstrating the diminishing returns of higher pixel counts.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u65e9\u671f\u68c0\u6d4b\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u50cf\u91c7\u96c6\u786c\u4ef6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5206\u8fa8\u7387\u56fe\u50cf\u9a8c\u8bc1\u4e86\u56fe\u50cf\u8d28\u91cf\u5bf9\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u7531\u4e8e\u65e9\u671f\u75c7\u72b6\u4e0d\u660e\u663e\u3001\u751f\u957f\u4f4d\u7f6e\u9690\u853d\u4e14\u751f\u957f\u7f13\u6162\uff0c\u5e38\u5e38\u96be\u4ee5\u88ab\u53ca\u65f6\u53d1\u73b0\uff0c\u5bfc\u81f4\u53ef\u9884\u9632\u7684\u6b7b\u4ea1\u3002\u5229\u7528CNN\u7684\u7cbe\u786e\u56fe\u50cf\u5206\u5272\u6280\u672f\u548c\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u53ef\u4ee5\u6709\u6548\u8fdb\u884c\u65e9\u671f\u68c0\u6d4b\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5305\u542b4293\u5f20\u8bad\u7ec3\u56fe\u50cf\u7684CNN\uff0c\u5305\u62ec\u826f\u6027\u548c\u6076\u6027\u80bf\u7624\u4ee5\u53ca\u9634\u6027\u6837\u672c\u3002\u8bbe\u8ba1\u4e86\u56fe\u50cf\u91c7\u96c6\u548c\u5904\u7406\u786c\u4ef6\u7cfb\u7edf\uff0c\u6d4b\u8bd5\u4e865\u79cd\u5e38\u89c1\u5206\u8fa8\u7387\u4e0b\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u968f\u7740\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u63d0\u9ad8\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u5448\u5bf9\u6570\u589e\u957f\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u50cf\u7d20\u6570\u5e26\u6765\u7684\u6536\u76ca\u9012\u51cf\u6548\u5e94\u3002\u8bbe\u8ba1\u7684\u589e\u5f3a\u786c\u4ef6\u80fd\u591f\u6355\u83b7\u8be6\u7ec6\u56fe\u50cf\u5e76\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "CNN\u7ed3\u5408\u4e13\u7528\u56fe\u50cf\u91c7\u96c6\u786c\u4ef6\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u5bf9\u68c0\u6d4b\u51c6\u786e\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u5b58\u5728\u6536\u76ca\u9012\u51cf\u73b0\u8c61\u3002"}}
{"id": "2510.16499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16499", "abs": "https://arxiv.org/abs/2510.16499", "authors": ["Michelle Yuan", "Khushbu Pahwa", "Shuaichen Chang", "Mustafa Kaba", "Jiarong Jiang", "Xiaofei Ma", "Yi Zhang", "Monica Sunkara"], "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection", "comment": "Accepted to NeurIPS 2025 Conference", "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u95ee\u9898\u7684\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6d4b\u8bd5\u548c\u5b9e\u65f6\u6548\u7528\u5efa\u6a21\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u4f18\u5316\u9009\u62e9\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8bed\u4e49\u68c0\u7d22\uff0c\u5b58\u5728\u80fd\u529b\u63cf\u8ff0\u4e0d\u5b8c\u6574\u3001\u68c0\u7d22\u65b9\u6cd5\u5c40\u9650\u7b49\u95ee\u9898\uff0c\u7ec4\u4ef6\u9009\u62e9\u672a\u5145\u5206\u8003\u8651\u80fd\u529b\u3001\u6210\u672c\u548c\u5b9e\u65f6\u6548\u7528\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u53d7\u80cc\u5305\u95ee\u9898\u542f\u53d1\uff0c\u8ba9\u7ec4\u5408\u5668\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u3001\u9009\u62e9\u548c\u7ec4\u88c5\u6700\u4f18\u667a\u80fd\u4f53\u7ec4\u4ef6\u96c6\uff0c\u7efc\u5408\u8003\u8651\u6027\u80fd\u3001\u9884\u7b97\u7ea6\u675f\u548c\u517c\u5bb9\u6027\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u7684\u7ec4\u5408\u5668\u59cb\u7ec8\u4f4d\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe31.6%\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6210\u529f\u7387\u4ece37%\u63d0\u5347\u81f387%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u9886\u57df\u548c\u9884\u7b97\u7ea6\u675f\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6210\u529f\u7387\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2510.16258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16258", "abs": "https://arxiv.org/abs/2510.16258", "authors": ["Claire McLean", "Makenzie Meendering", "Tristan Swartz", "Orri Gabbay", "Alexandra Olsen", "Rachel Jacobs", "Nicholas Rosen", "Philippe de Bree", "Tony Garcia", "Gadsden Merrill", "Jake Sandakly", "Julia Buffalini", "Neham Jain", "Steven Krenn", "Moneish Kumar", "Dejan Markovic", "Evonne Ng", "Fabian Prada", "Andrew Saba", "Siwei Zhang", "Vasu Agrawal", "Tim Godisart", "Alexander Richard", "Michael Zollhoefer"], "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset", "comment": null, "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.", "AI": {"tldr": "Embody 3D\u662f\u4e00\u4e2a\u5305\u542b500\u5c0f\u65f63D\u8fd0\u52a8\u6570\u636e\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6765\u81ea439\u540d\u53c2\u4e0e\u8005\uff0c\u5305\u542b\u8d85\u8fc75400\u4e07\u5e27\u8ffd\u8e2a\u76843D\u8fd0\u52a8\u6570\u636e\uff0c\u6db5\u76d6\u5355\u4eba\u548c\u591a\u4eba\u884c\u4e3a\u3002", "motivation": "\u521b\u5efa\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u3001\u884c\u4e3a\u7406\u89e3\u548c\u793e\u4ea4\u4e92\u52a8\u7814\u7a76\u3002", "method": "\u5728\u591a\u76f8\u673a\u91c7\u96c6\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u5305\u62ec\u8ffd\u8e2a\u4eba\u4f53\u8fd0\u52a8\u3001\u624b\u90e8\u8ffd\u8e2a\u3001\u8eab\u4f53\u5f62\u72b6\u3001\u6587\u672c\u6807\u6ce8\u548c\u5355\u72ec\u7684\u97f3\u9891\u8f68\u9053\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b500\u5c0f\u65f63D\u8fd0\u52a8\u6570\u636e\u7684Embody 3D\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u8fd0\u52a8\u7c7b\u578b\u548c\u793e\u4ea4\u573a\u666f\u3002", "conclusion": "Embody 3D\u6570\u636e\u96c6\u4e3a\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u548c\u793e\u4ea4\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u652f\u6301\u591a\u79cd\u7814\u7a76\u5e94\u7528\u3002"}}
{"id": "2510.16549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16549", "abs": "https://arxiv.org/abs/2510.16549", "authors": ["Haoxuan Zhang", "Ruochi Li", "Sarthak Shrestha", "Shree Harshini Mamidala", "Revanth Putta", "Arka Krishan Aggarwal", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation", "comment": null, "summary": "Peer review serves as the gatekeeper of science, yet the surge in submissions\nand widespread adoption of large language models (LLMs) in scholarly evaluation\npresent unprecedented challenges. Recent work has focused on using LLMs to\nimprove review efficiency or generate insightful review content. However,\nunchecked deficient reviews from both human experts and AI systems threaten to\nsystematically undermine the peer review ecosystem and compromise academic\nintegrity. To address this critical issue, we introduce ReviewGuard, an\nautomated system for detecting and categorizing deficient reviews. ReviewGuard\nemploys a comprehensive four-stage LLM-driven framework that: (1) collects ICLR\nand NeurIPS papers with their corresponding reviews from OpenReview; (2)\nannotates review types using GPT-4.1 with human validation; (3) addresses class\nimbalance and data scarcity through LLM-driven synthetic data augmentation,\nproducing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438\nsynthetic reviews; and (4) fine-tunes both encoder-based models and open source\nLLMs. We perform comprehensive feature analysis of the structure and quality of\nthe review text. Compared to sufficient reviews, deficient reviews demonstrate\nlower rating scores, higher self-reported confidence, reduced structural\ncomplexity, and a higher proportion of negative sentiment. AI-generated text\ndetection reveals that, since ChatGPT's emergence, AI-generated reviews have\nincreased dramatically. In the evaluation of deficient review detection models,\nmixed training with synthetic and real review data provides substantial\nenhancements to recall and F1 scores on the binary task. This study presents\nthe first LLM-driven system for detecting deficient peer reviews, providing\nevidence to inform AI governance in peer review while offering valuable\ninsights into human-AI collaboration to maintain academic integrity.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReviewGuard\u7cfb\u7edf\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7684\u56db\u9636\u6bb5\u6846\u67b6\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u6709\u7f3a\u9677\u7684\u540c\u884c\u8bc4\u5ba1\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u5fae\u8c03\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u540c\u884c\u8bc4\u5ba1\u4f5c\u4e3a\u79d1\u5b66\u7684\u5b88\u95e8\u4eba\u9762\u4e34\u6311\u6218\uff1a\u63d0\u4ea4\u91cf\u6fc0\u589e\u548cLLM\u5728\u5b66\u672f\u8bc4\u4f30\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bfc\u81f4\u6709\u7f3a\u9677\u7684\u8bc4\u5ba1\u53ef\u80fd\u7cfb\u7edf\u6027\u5730\u7834\u574f\u540c\u884c\u8bc4\u5ba1\u751f\u6001\u7cfb\u7edf\u548c\u5b66\u672f\u8bda\u4fe1\u3002", "method": "\u56db\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff1a1)\u4eceOpenReview\u6536\u96c6ICLR\u548cNeurIPS\u8bba\u6587\u53ca\u8bc4\u5ba1\uff1b2)\u4f7f\u7528GPT-4.1\u6807\u6ce8\u8bc4\u5ba1\u7c7b\u578b\u5e76\u4eba\u5de5\u9a8c\u8bc1\uff1b3)\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1b4)\u5fae\u8c03\u7f16\u7801\u5668\u6a21\u578b\u548c\u5f00\u6e90LLM\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b6,634\u7bc7\u8bba\u6587\u300124,657\u6761\u771f\u5b9e\u8bc4\u5ba1\u548c46,438\u6761\u5408\u6210\u8bc4\u5ba1\u7684\u8bed\u6599\u5e93\u3002\u6709\u7f3a\u9677\u8bc4\u5ba1\u663e\u793a\u66f4\u4f4e\u7684\u8bc4\u5206\u3001\u66f4\u9ad8\u7684\u81ea\u62a5\u4fe1\u5fc3\u3001\u66f4\u7b80\u5355\u7684\u7ed3\u6784\u548c\u66f4\u591a\u8d1f\u9762\u60c5\u7eea\u3002AI\u751f\u6210\u8bc4\u5ba1\u81eaChatGPT\u51fa\u73b0\u540e\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u6709\u7f3a\u9677\u540c\u884c\u8bc4\u5ba1\u7684LLM\u9a71\u52a8\u7cfb\u7edf\uff0c\u4e3a\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684AI\u6cbb\u7406\u63d0\u4f9b\u8bc1\u636e\uff0c\u5e76\u4e3a\u7ef4\u62a4\u5b66\u672f\u8bda\u4fe1\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2510.16272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16272", "abs": "https://arxiv.org/abs/2510.16272", "authors": ["Baicheng Li", "Zike Yan", "Dong Wu", "Hongbin Zha"], "title": "Proactive Scene Decomposition and Reconstruction", "comment": null, "summary": "Human behaviors are the major causes of scene dynamics and inherently contain\nrich cues regarding the dynamics. This paper formalizes a new task of proactive\nscene decomposition and reconstruction, an online approach that leverages\nhuman-object interactions to iteratively disassemble and reconstruct the\nenvironment. By observing these intentional interactions, we can dynamically\nrefine the decomposition and reconstruction process, addressing inherent\nambiguities in static object-level reconstruction. The proposed system\neffectively integrates multiple tasks in dynamic environments such as accurate\ncamera and object pose estimation, instance decomposition, and online map\nupdating, capitalizing on cues from human-object interactions in egocentric\nlive streams for a flexible, progressive alternative to conventional\nobject-level reconstruction methods. Aided by the Gaussian splatting technique,\naccurate and consistent dynamic scene modeling is achieved with photorealistic\nand efficient rendering. The efficacy is validated in multiple real-world\nscenarios with promising advantages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba-\u7269\u4ea4\u4e92\u7684\u4e3b\u52a8\u573a\u666f\u5206\u89e3\u4e0e\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c2\u5bdf\u4eba\u7c7b\u884c\u4e3a\u52a8\u6001\u4f18\u5316\u573a\u666f\u5206\u89e3\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u7269\u4f53\u7ea7\u91cd\u5efa\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u884c\u4e3a\u662f\u573a\u666f\u52a8\u6001\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u52a8\u6001\u7ebf\u7d22\u3002\u4f20\u7edf\u9759\u6001\u7269\u4f53\u7ea7\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u9700\u8981\u5229\u7528\u4eba\u7c7b\u610f\u56fe\u6027\u4ea4\u4e92\u6765\u52a8\u6001\u4f18\u5316\u91cd\u5efa\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u89c2\u5bdf\u4eba-\u7269\u4ea4\u4e92\u5728\u7ebf\u8fed\u4ee3\u5206\u89e3\u548c\u91cd\u5efa\u73af\u5883\uff0c\u6574\u5408\u76f8\u673a\u548c\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3001\u5b9e\u4f8b\u5206\u89e3\u3001\u5728\u7ebf\u5730\u56fe\u66f4\u65b0\u7b49\u591a\u4e2a\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u4e00\u81f4\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\uff0c\u5177\u6709\u903c\u771f\u9ad8\u6548\u7684\u6e32\u67d3\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f20\u7edf\u7269\u4f53\u7ea7\u91cd\u5efa\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u6e10\u8fdb\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5229\u7528\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f4\u64ad\u4e2d\u7684\u4eba-\u7269\u4ea4\u4e92\u7ebf\u7d22\u8fdb\u884c\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3002"}}
{"id": "2510.16565", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16565", "abs": "https://arxiv.org/abs/2510.16565", "authors": ["Seungho Cho", "Changgeon Ko", "Eui Jun Hwang", "Junmyeong Lee", "Huije Lee", "Jong C. Park"], "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models", "comment": "Accepted to CIKM 2025 Workshop on Human Centric AI", "summary": "Large language models (LLMs) are increasingly used across diverse cultural\ncontexts, making accurate cultural understanding essential. Prior evaluations\nhave mostly focused on output-level performance, obscuring the factors that\ndrive differences in responses, while studies using circuit analysis have\ncovered few languages and rarely focused on culture. In this work, we trace\nLLMs' internal cultural understanding mechanisms by measuring activation path\noverlaps when answering semantically equivalent questions under two conditions:\nvarying the target country while fixing the question language, and varying the\nquestion language while fixing the country. We also use same-language country\npairs to disentangle language from cultural aspects. Results show that internal\npaths overlap more for same-language, cross-country questions than for\ncross-language, same-country questions, indicating strong language-specific\npatterns. Notably, the South Korea-North Korea pair exhibits low overlap and\nhigh variability, showing that linguistic similarity does not guarantee aligned\ninternal representation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790LLM\u5728\u56de\u7b54\u6587\u5316\u76f8\u5173\u95ee\u9898\u65f6\u5185\u90e8\u6fc0\u6d3b\u8def\u5f84\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u5bf9\u6a21\u578b\u5185\u90e8\u6587\u5316\u7406\u89e3\u673a\u5236\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u53d1\u73b0\u8bed\u8a00\u7279\u5f02\u6027\u6a21\u5f0f\u5f3a\u4e8e\u6587\u5316\u7279\u5f02\u6027\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740LLM\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u51c6\u786e\u7684\u6587\u5316\u7406\u89e3\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\uff0c\u96be\u4ee5\u63ed\u793a\u54cd\u5e94\u5dee\u5f02\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u4e14\u7535\u8def\u5206\u6790\u7814\u7a76\u8986\u76d6\u8bed\u8a00\u5c11\uff0c\u5f88\u5c11\u5173\u6ce8\u6587\u5316\u3002", "method": "\u901a\u8fc7\u6d4b\u91cfLLM\u5728\u56de\u7b54\u8bed\u4e49\u7b49\u6548\u95ee\u9898\u65f6\u5185\u90e8\u6fc0\u6d3b\u8def\u5f84\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u6bd4\u8f83\u4e24\u79cd\u6761\u4ef6\uff1a\u56fa\u5b9a\u95ee\u9898\u8bed\u8a00\u6539\u53d8\u76ee\u6807\u56fd\u5bb6\uff0c\u4ee5\u53ca\u56fa\u5b9a\u56fd\u5bb6\u6539\u53d8\u95ee\u9898\u8bed\u8a00\u3002\u4f7f\u7528\u540c\u8bed\u8a00\u56fd\u5bb6\u5bf9\u6765\u5206\u79bb\u8bed\u8a00\u548c\u6587\u5316\u56e0\u7d20\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u540c\u8bed\u8a00\u8de8\u56fd\u5bb6\u95ee\u9898\u7684\u5185\u90e8\u8def\u5f84\u91cd\u53e0\u5ea6\u9ad8\u4e8e\u8de8\u8bed\u8a00\u540c\u56fd\u5bb6\u95ee\u9898\uff0c\u8868\u660e\u5b58\u5728\u5f3a\u70c8\u7684\u8bed\u8a00\u7279\u5f02\u6027\u6a21\u5f0f\u3002\u7279\u522b\u662f\u97e9\u56fd-\u671d\u9c9c\u5bf9\u7684\u4f4e\u91cd\u53e0\u5ea6\u548c\u9ad8\u53d8\u5f02\u6027\u8868\u660e\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u80fd\u4fdd\u8bc1\u5185\u90e8\u8868\u5f81\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "LLM\u7684\u6587\u5316\u7406\u89e3\u673a\u5236\u53d7\u8bed\u8a00\u5f71\u54cd\u663e\u8457\uff0c\u8bed\u8a00\u7279\u5f02\u6027\u6a21\u5f0f\u5f3a\u4e8e\u6587\u5316\u7279\u5f02\u6027\u6a21\u5f0f\uff0c\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u7b49\u4e8e\u5185\u90e8\u8868\u5f81\u7684\u5bf9\u9f50\u3002"}}
{"id": "2510.16290", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16290", "abs": "https://arxiv.org/abs/2510.16290", "authors": ["Yue Zheng", "Xiufang Shi", "Jiming Chen", "Yuanchao Shu"], "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models", "comment": null, "summary": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.", "AI": {"tldr": "Cerberus\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u4e24\u7ea7\u7ea7\u8054\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u901f\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u5177\u6709\u4f18\u5f02\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u5de8\u5927\u4e14\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e24\u7ea7\u7ea7\u8054\u7cfb\u7edf\uff1a\u79bb\u7ebf\u5b66\u4e60\u6b63\u5e38\u884c\u4e3a\u89c4\u5219\uff0c\u5728\u7ebf\u63a8\u7406\u65f6\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u548c\u7ec6\u7c92\u5ea6VLM\u63a8\u7406\u3002\u5173\u952e\u521b\u65b0\u5305\u62ec\u8fd0\u52a8\u63a9\u7801\u63d0\u793a\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u504f\u5dee\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCerberus\u5728NVIDIA L40S GPU\u4e0a\u5e73\u5747\u8fbe\u523057.68 fps\uff0c\u901f\u5ea6\u63d0\u5347151.79\u500d\uff0c\u51c6\u786e\u7387\u8fbe\u523097.2%\uff0c\u4e0e\u6700\u5148\u8fdb\u7684VLM-based VAD\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Cerberus\u4e3a\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.16567", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16567", "abs": "https://arxiv.org/abs/2510.16567", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Manuel Giollo", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "Hallucination Benchmark for Speech Foundation Models", "comment": "Under Review", "summary": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent\nand coherent transcriptions produced by neural ASR models that are completely\nunrelated to the underlying acoustic input (i.e., the speech signal). While\nsimilar to conventional decoding errors in potentially compromising the\nusability of transcriptions for downstream applications, hallucinations can be\nmore detrimental due to their preservation of syntactically and semantically\nplausible structure. This apparent coherence can mislead subsequent processing\nstages and introduce serious risks, particularly in critical domains such as\nhealthcare and law. Conventional evaluation metrics are primarily centered on\nerror-based metrics and fail to distinguish between phonetic inaccuracies and\nhallucinations. Consequently, there is a critical need for new evaluation\nframeworks that can effectively identify and assess models with a heightened\npropensity for generating hallucinated content. To this end, we introduce\nSHALLOW, the first benchmark framework that systematically categorizes and\nquantifies hallucination phenomena in ASR along four complementary axes:\nlexical, phonetic, morphological, and semantic. We define targeted metrics\nwithin each category to produce interpretable profiles of model behavior.\nThrough evaluation across various architectures and speech domains, we have\nfound that SHALLOW metrics correlate strongly with word error rate (WER) when\nrecognition quality is high (i.e., low WER). Still, this correlation weakens\nsubstantially as WER increases. SHALLOW, therefore, captures fine-grained error\npatterns that WER fails to distinguish under degraded and challenging\nconditions. Our framework supports specific diagnosis of model weaknesses and\nprovides feedback for model improvement beyond what aggregate error rates can\noffer.", "AI": {"tldr": "SHALLOW\u662f\u9996\u4e2a\u7cfb\u7edf\u5206\u7c7b\u548c\u91cf\u5316ASR\u7cfb\u7edf\u4e2d\u5e7b\u89c9\u73b0\u8c61\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b\u8bcd\u6c47\u3001\u8bed\u97f3\u3001\u5f62\u6001\u548c\u8bed\u4e49\u56db\u4e2a\u7ef4\u5ea6\uff0c\u80fd\u6355\u6349WER\u65e0\u6cd5\u533a\u5206\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "ASR\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u4f1a\u4ea7\u751f\u4e0e\u8bed\u97f3\u4fe1\u53f7\u5b8c\u5168\u65e0\u5173\u4f46\u8bed\u6cd5\u8bed\u4e49\u5408\u7406\u7684\u8f6c\u5f55\uff0c\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u5173\u952e\u9886\u57df\u5e26\u6765\u4e25\u91cd\u98ce\u9669\u3002\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u533a\u5206\u8bed\u97f3\u4e0d\u51c6\u786e\u548c\u5e7b\u89c9\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faSHALLOW\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u5c06ASR\u5e7b\u89c9\u5206\u4e3a\u56db\u4e2a\u4e92\u8865\u7ef4\u5ea6\uff1a\u8bcd\u6c47\u3001\u8bed\u97f3\u3001\u5f62\u6001\u548c\u8bed\u4e49\uff0c\u5e76\u5728\u6bcf\u4e2a\u7c7b\u522b\u4e2d\u5b9a\u4e49\u9488\u5bf9\u6027\u6307\u6807\u6765\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u884c\u4e3a\u5206\u6790\u3002", "result": "\u5728\u4e0d\u540c\u67b6\u6784\u548c\u8bed\u97f3\u9886\u57df\u7684\u8bc4\u4f30\u663e\u793a\uff0cSHALLOW\u6307\u6807\u5728\u9ad8\u8bc6\u522b\u8d28\u91cf\uff08\u4f4eWER\uff09\u65f6\u4e0eWER\u5f3a\u76f8\u5173\uff0c\u4f46\u968f\u7740WER\u589e\u52a0\u76f8\u5173\u6027\u663e\u8457\u51cf\u5f31\uff0c\u8868\u660eSHALLOW\u80fd\u6355\u6349WER\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u65e0\u6cd5\u533a\u5206\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "SHALLOW\u6846\u67b6\u652f\u6301\u5bf9\u6a21\u578b\u5f31\u70b9\u7684\u5177\u4f53\u8bca\u65ad\uff0c\u5e76\u63d0\u4f9b\u8d85\u8d8a\u805a\u5408\u9519\u8bef\u7387\u7684\u6a21\u578b\u6539\u8fdb\u53cd\u9988\uff0c\u4e3aASR\u7cfb\u7edf\u7684\u5e7b\u89c9\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.16295", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16295", "abs": "https://arxiv.org/abs/2510.16295", "authors": ["Ryoto Miyamoto", "Xin Fan", "Fuyuko Kido", "Tsuneo Matsumoto", "Hayato Yamana"], "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models", "comment": null, "summary": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.", "AI": {"tldr": "OpenLVLM-MIA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65f6\u5b58\u5728\u7684\u6839\u672c\u6027\u6311\u6218\uff0c\u6307\u51fa\u5148\u524d\u7814\u7a76\u7684\u9ad8\u6210\u529f\u7387\u4e3b\u8981\u6e90\u4e8e\u6570\u636e\u96c6\u6784\u5efa\u4e2d\u7684\u5206\u5e03\u504f\u5dee\u800c\u975e\u771f\u5b9e\u7684\u6210\u5458\u72b6\u6001\u8bc6\u522b\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u7814\u7a76\u62a5\u544a\u4e86\u9ad8\u6210\u529f\u7387\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u53ef\u80fd\u6e90\u4e8e\u6570\u636e\u96c6\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u504f\u5dee\uff0c\u800c\u975e\u771f\u6b63\u7684\u6210\u5458\u72b6\u6001\u8bc6\u522b\u80fd\u529b\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u65e0\u504f\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u51c6\u786e\u8bc4\u4f30MIA\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5305\u542b6,000\u5f20\u56fe\u50cf\u7684\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\u7684\u5206\u5e03\u7ecf\u8fc7\u4ed4\u7ec6\u5e73\u8861\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u8bad\u7ec3\u9636\u6bb5\u63d0\u4f9b\u771f\u5b9e\u6210\u5458\u6807\u7b7e\uff0c\u786e\u4fdd\u8bc4\u4f30\u73af\u5883\u7684\u65e0\u504f\u6027\u3002", "result": "\u5728OpenLVLM-MIA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u65e0\u504f\u6761\u4ef6\u4e0b\uff0c\u6700\u5148\u8fdb\u7684MIA\u65b9\u6cd5\u7684\u6027\u80fd\u6536\u655b\u5230\u968f\u673a\u731c\u6d4b\u6c34\u5e73\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\u3002", "conclusion": "OpenLVLM-MIA\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u900f\u660e\u4e14\u65e0\u504f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6f84\u6e05\u4e86\u5f53\u524dLVLM\u4e0aMIA\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.16573", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16573", "abs": "https://arxiv.org/abs/2510.16573", "authors": ["Muhammad Ammar", "Hadiya Murad Hadi", "Usman Majeed Butt"], "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu", "comment": null, "summary": "Large Language Models (LLMs) are now capable of generating text that closely\nresembles human writing, making them powerful tools for content creation, but\nthis growing ability has also made it harder to tell whether a piece of text\nwas written by a human or by a machine. This challenge becomes even more\nserious for languages like Urdu, where there are very few tools available to\ndetect AI-generated text. To address this gap, we propose a novel AI-generated\ntext detection framework tailored for the Urdu language. A balanced dataset\ncomprising 1,800 humans authored, and 1,800 AI generated texts, sourced from\nmodels such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed\nlinguistic and statistical analysis was conducted, focusing on features such as\ncharacter and word counts, vocabulary richness (Type Token Ratio), and N-gram\npatterns, with significance evaluated through t-tests and MannWhitney U tests.\nThree state-of-the-art multilingual transformer models such as\nmdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were\nfine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest\nperformance, with an F1-score 91.29 and accuracy of 91.26% on the test set.\nThis research advances efforts in contesting misinformation and academic\nmisconduct in Urdu-speaking communities and contributes to the broader\ndevelopment of NLP tools for low resource languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u8bed\u8a00transformer\u6a21\u578b\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cmDeBERTa-v3-base\u6a21\u578b\u53d6\u5f97\u4e8691.26%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4fAI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5de5\u5177\uff0c\u968f\u7740LLM\u751f\u6210\u6587\u672c\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u533a\u5206\u4eba\u7c7b\u4e0e\u673a\u5668\u751f\u6210\u6587\u672c\u53d8\u5f97\u56f0\u96be\uff0c\u8fd9\u5bf9\u6297\u51fb\u9519\u8bef\u4fe1\u606f\u548c\u5b66\u672f\u4e0d\u7aef\u884c\u4e3a\u6784\u6210\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b1800\u7bc7\u4eba\u7c7b\u64b0\u5199\u548c1800\u7bc7AI\u751f\u6210\u6587\u672c\u7684\u5e73\u8861\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8bed\u8a00\u5b66\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u4f7f\u7528\u5b57\u7b26\u6570\u3001\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u3001N-gram\u6a21\u5f0f\u7b49\u7279\u5f81\uff0c\u5e76\u5fae\u8c03\u4e09\u79cd\u591a\u8bed\u8a00transformer\u6a21\u578b\u3002", "result": "mDeBERTa-v3-base\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u657091.29\uff0c\u51c6\u786e\u738791.26%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u4e4c\u5c14\u90fd\u8bed\u793e\u533a\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\u548c\u5b66\u672f\u4e0d\u7aef\u7684\u52aa\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u5de5\u5177\u5f00\u53d1\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.16319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16319", "abs": "https://arxiv.org/abs/2510.16319", "authors": ["Rui Yang", "Huining Li", "Yiyi Long", "Xiaojun Wu", "Shengfeng He"], "title": "Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation", "comment": "ICCV 2025", "summary": "Generating sketches guided by reference styles requires precise transfer of\nstroke attributes, such as line thickness, deformation, and texture sparsity,\nwhile preserving semantic structure and content fidelity. To this end, we\npropose Stroke2Sketch, a novel training-free framework that introduces\ncross-image stroke attention, a mechanism embedded within self-attention layers\nto establish fine-grained semantic correspondences and enable accurate stroke\nattribute transfer. This allows our method to adaptively integrate reference\nstroke characteristics into content images while maintaining structural\nintegrity. Additionally, we develop adaptive contrast enhancement and\nsemantic-focused attention to reinforce content preservation and foreground\nemphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches\nthat closely resemble handcrafted results, outperforming existing methods in\nexpressive stroke control and semantic coherence. Codes are available at\nhttps://github.com/rane7/Stroke2Sketch.", "AI": {"tldr": "Stroke2Sketch\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u56fe\u50cf\u7b14\u753b\u6ce8\u610f\u673a\u5236\u5b9e\u73b0\u53c2\u8003\u98ce\u683c\u5230\u5185\u5bb9\u56fe\u50cf\u7684\u7cbe\u786e\u7b14\u753b\u5c5e\u6027\u8fc1\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "motivation": "\u751f\u6210\u5177\u6709\u53c2\u8003\u98ce\u683c\u5f15\u5bfc\u7684\u8349\u56fe\u9700\u8981\u7cbe\u786e\u4f20\u8f93\u7b14\u753b\u5c5e\u6027\uff08\u5982\u7ebf\u6761\u7c97\u7ec6\u3001\u53d8\u5f62\u548c\u7eb9\u7406\u7a00\u758f\u5ea6\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u548c\u5185\u5bb9\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u8de8\u56fe\u50cf\u7b14\u753b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5d4c\u5165\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u5efa\u7acb\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff1b\u5f00\u53d1\u81ea\u9002\u5e94\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u548c\u8bed\u4e49\u805a\u7126\u6ce8\u610f\u529b\u6765\u5f3a\u5316\u5185\u5bb9\u4fdd\u6301\u548c\u524d\u666f\u5f3a\u8c03\u3002", "result": "\u6709\u6548\u5408\u6210\u98ce\u683c\u5fe0\u5b9e\u4e14\u63a5\u8fd1\u624b\u5de5\u7ed8\u5236\u7ed3\u679c\u7684\u8349\u56fe\uff0c\u5728\u8868\u8fbe\u6027\u7b14\u753b\u63a7\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Stroke2Sketch\u6846\u67b6\u80fd\u591f\u81ea\u9002\u5e94\u5730\u6574\u5408\u53c2\u8003\u7b14\u753b\u7279\u5f81\u5230\u5185\u5bb9\u56fe\u50cf\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5728\u7b14\u753b\u5c5e\u6027\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16604", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16604", "abs": "https://arxiv.org/abs/2510.16604", "authors": ["Francisco Jose Cortes Delgado", "Eduardo Martinez Gracia", "Rafael Valencia Garcia"], "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach", "comment": "6 pages, 3 figures. Submitted to SEPLN 2023 Conference", "summary": "Recent advances in natural language processing with large neural models have\nopened new possibilities for syntactic analysis based on machine learning. This\nwork explores a novel approach to phrase-structure analysis by fine-tuning\nlarge language models (LLMs) to translate an input sentence into its\ncorresponding syntactic structure. The main objective is to extend the\ncapabilities of MiSintaxis, a tool designed for teaching Spanish syntax.\nSeveral models from the Hugging Face repository were fine-tuned using training\ndata generated from the AnCora-ES corpus, and their performance was evaluated\nusing the F1 score. The results demonstrate high accuracy in phrase-structure\nanalysis and highlight the potential of this methodology.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03LLMs\u5c06\u53e5\u5b50\u7ffb\u8bd1\u6210\u5bf9\u5e94\u7684\u53e5\u6cd5\u7ed3\u6784\uff0c\u7528\u4e8e\u6269\u5c55\u897f\u73ed\u7259\u8bed\u6cd5\u6559\u5b66\u5de5\u5177MiSintaxis\u7684\u529f\u80fd\u3002", "motivation": "\u5229\u7528\u5927\u578b\u795e\u7ecf\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u8fdb\u5c55\uff0c\u63a2\u7d22\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53e5\u6cd5\u5206\u6790\u65b0\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u6269\u5c55\u897f\u73ed\u7259\u8bed\u6cd5\u6559\u5b66\u5de5\u5177MiSintaxis\u7684\u80fd\u529b\u3002", "method": "\u4eceHugging Face\u4ed3\u5e93\u9009\u62e9\u591a\u4e2a\u6a21\u578b\uff0c\u4f7f\u7528AnCora-ES\u8bed\u6599\u5e93\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u8ba9LLMs\u5b66\u4e60\u5c06\u8f93\u5165\u53e5\u5b50\u7ffb\u8bd1\u6210\u5bf9\u5e94\u7684\u53e5\u6cd5\u7ed3\u6784\u3002", "result": "\u4f7f\u7528F1\u5206\u6570\u8bc4\u4f30\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5728\u77ed\u8bed\u7ed3\u6784\u5206\u6790\u65b9\u9762\u8fbe\u5230\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u53e5\u6cd5\u5206\u6790\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u6f5c\u529b\uff0c\u7a81\u51fa\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.16320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16320", "abs": "https://arxiv.org/abs/2510.16320", "authors": ["Wenhao Wang", "Longqi Cai", "Taihong Xiao", "Yuxiao Wang", "Ming-Hsuan Yang"], "title": "Scaling Laws for Deepfake Detection", "comment": null, "summary": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u53d1\u73b0\u68c0\u6d4b\u9519\u8bef\u7387\u968f\u771f\u5b9e\u56fe\u50cf\u57df\u6570\u91cf\u548c\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u6570\u91cf\u7684\u589e\u52a0\u5448\u5e42\u5f8b\u8870\u51cf\uff0c\u7c7b\u4f3c\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\u7684\u9700\u6c42\uff0c\u9700\u8981\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u5206\u6790\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "method": "\u6784\u5efa\u4e86ScaleDF\u6570\u636e\u96c6\uff0c\u5305\u542b580\u4e07\u5f20\u6765\u81ea51\u4e2a\u4e0d\u540c\u57df\u7684\u771f\u5b9e\u56fe\u50cf\u548c880\u4e07\u5f20\u7531102\u79cd\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u751f\u6210\u7684\u5047\u56fe\u50cf\uff0c\u7528\u4e8e\u5206\u6790\u6a21\u578b\u6027\u80fd\u4e0e\u771f\u5b9e\u57df\u6570\u91cf\u3001\u4f2a\u9020\u65b9\u6cd5\u548c\u8bad\u7ec3\u56fe\u50cf\u6570\u91cf\u7684\u5173\u7cfb\u3002", "result": "\u89c2\u5bdf\u5230\u5e73\u5747\u68c0\u6d4b\u9519\u8bef\u7387\u968f\u771f\u5b9e\u57df\u6570\u91cf\u6216\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u6570\u91cf\u7684\u589e\u52a0\u5448\u53ef\u9884\u6d4b\u7684\u5e42\u5f8b\u8870\u51cf\uff0c\u53ef\u4ee5\u9884\u6d4b\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u6240\u9700\u7684\u989d\u5916\u8d44\u6e90\u3002", "conclusion": "\u7f29\u653e\u5b9a\u5f8b\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6570\u636e\u4e2d\u5fc3\u7684\u5e94\u5bf9\u7b56\u7565\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u548c\u6570\u636e\u589e\u5f3a\u5728\u7f29\u653e\u4e2d\u7684\u4f5c\u7528\u4ee5\u53ca\u7f29\u653e\u672c\u8eab\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16645", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16645", "abs": "https://arxiv.org/abs/2510.16645", "authors": ["Zhixuan He", "Yue Feng"], "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.", "AI": {"tldr": "DiMo\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u4e13\u95e8\u5316LLM\u4ee3\u7406\u7684\u6a21\u62df\u8fa9\u8bba\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u8fa9\u8bba\u57fa\u7ebf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u5347\u6027\u80fd\u53c8\u80fd\u63d0\u4f9b\u660e\u786e\u63a8\u7406\u94fe\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4e13\u95e8\u5316LLM\u4ee3\u7406\uff0c\u6bcf\u4e2a\u4ee3\u7406\u4ee3\u8868\u4e0d\u540c\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fa9\u8bba\u6765\u6311\u6218\u548c\u5b8c\u5584\u521d\u59cb\u56de\u7b54\u3002", "result": "\u5728\u7edf\u4e00\u5f00\u6e90\u8bbe\u7f6e\u4e0b\uff0cDiMo\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u63d0\u5347\u6700\u5927\u3002", "conclusion": "DiMo\u662f\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u7684Web\u539f\u751f\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u4ea7\u751f\u8bed\u4e49\u7c7b\u578b\u5316\u3001URL\u6ce8\u91ca\u7684\u8bc1\u636e\u94fe\uff0c\u652f\u6301\u4e0b\u6e38\u7cfb\u7edf\u68c0\u67e5\u548c\u91cd\u7528\u3002"}}
{"id": "2510.16325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16325", "abs": "https://arxiv.org/abs/2510.16325", "authors": ["Yuyao Zhang", "Yu-Wing Tai"], "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention", "comment": "22 pages", "summary": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.", "AI": {"tldr": "Scale-DiT\u662f\u4e00\u4e2a\u65b0\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u8d85\u9ad8\u6e05\u56fe\u50cf\u7684\u9ad8\u6548\u751f\u6210\uff0c\u80fd\u5728\u4e0d\u4f9d\u8d564K\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fbe\u52304K\u5206\u8fa8\u7387\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u53d7\u9650\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u6027\u548c\u7f3a\u4e4f\u539f\u751f4K\u8bad\u7ec3\u6570\u636e\uff0c\u65e0\u6cd5\u751f\u6210\u8d85\u9ad8\u6e05\u56fe\u50cf\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u8fde\u8d2f\u6027\u53c8\u80fd\u5408\u6210\u7cbe\u7ec6\u7eb9\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u6f5c\u5728\u7a7a\u95f4\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5c40\u90e8\u7a97\u53e3\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u6f5c\u5728\u7a7a\u95f4\u6ce8\u5165\u5168\u5c40\u8bed\u4e49\u3002\u901a\u8fc7LoRA\u9002\u914d\u5668\u8fde\u63a5\u5168\u5c40\u548c\u5c40\u90e8\u8def\u5f84\uff0c\u5e76\u91c7\u7528Hilbert\u66f2\u7ebf\u91cd\u6392\u548c\u878d\u5408\u5185\u6838\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "result": "Scale-DiT\u76f8\u6bd4\u5bc6\u96c6\u6ce8\u610f\u529b\u57fa\u7ebf\u5b9e\u73b0\u4e862\u500d\u4ee5\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u53ef\u9760\u5730\u6269\u5c55\u52304K\u5206\u8fa8\u7387\uff0c\u5728\u5b9a\u91cf\u6307\u6807\uff08FID\u3001IS\u3001CLIP Score\uff09\u548c\u5b9a\u6027\u6bd4\u8f83\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5168\u5c40\u8fde\u8d2f\u6027\u548c\u66f4\u6e05\u6670\u7684\u5c40\u90e8\u7ec6\u8282\u3002", "conclusion": "\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u914d\u5408\u5f15\u5bfc\u6027\u4f4e\u5206\u8fa8\u7387\u951a\u70b9\u662f\u4e00\u79cd\u63a8\u8fdb\u8d85\u9ad8\u6e05\u56fe\u50cf\u751f\u6210\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u989d\u5916\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76844K\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2510.16670", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16670", "abs": "https://arxiv.org/abs/2510.16670", "authors": ["Yiyang Liu", "James C. Liang", "Heng Fan", "Wenhao Yang", "Yiming Cui", "Xiaotian Han", "Lifu Huang", "Dongfang Liu", "Qifan Wang", "Cheng Han"], "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "comment": "NeurIPS 2025", "summary": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)\napproach to facilitate Large Language Model (LLM) adaptation to downstream\ntasks by conditioning generation with task-aware guidance. Despite its\nsuccesses, current prompt-based learning methods heavily rely on laborious grid\nsearching for optimal prompt length and typically require considerable number\nof prompts, introducing additional computational burden. Worse yet, our pioneer\nfindings indicate that the task-aware prompt design is inherently limited by\nits absence of instance-aware information, leading to a subtle attention\ninterplay with the input sequence. In contrast, simply incorporating\ninstance-aware information as a part of the guidance can enhance the\nprompt-tuned model performance without additional fine-tuning. Moreover, we\nfind an interesting phenomenon, namely \"attention anchor\", that incorporating\ninstance-aware tokens at the earliest position of the sequence can successfully\npreserve strong attention to critical structural information and exhibit more\nactive attention interaction with all input tokens. In light of our\nobservation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and\neffective solution that leverages off-the-shelf, informative instance semantics\ninto prompt-based learning. Our approach innovatively integrates both\ninstance-aware and task-aware information in a nearly parameter-free manner\n(i.e., one single capsule prompt). Empirical results demonstrate that our\nmethod can exhibit superior performance across various language tasks (e.g.,\n84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while\nenjoying high parameter efficiency (e.g., 0.003\\% of model parameters on\nLlama3.2-1B).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Capsule Prompt-Tuning (CaPT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5b9e\u4f8b\u611f\u77e5\u4fe1\u606f\u4e0e\u4efb\u52a1\u611f\u77e5\u4fe1\u606f\u7ed3\u5408\u5230\u5355\u4e2a\u80f6\u56ca\u63d0\u793a\u4e2d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5bf9\u63d0\u793a\u957f\u5ea6\u548c\u6570\u91cf\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u63d0\u793a\u8c03\u4f18\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u7f51\u683c\u641c\u7d22\u5bfb\u627e\u6700\u4f18\u63d0\u793a\u957f\u5ea6\uff0c\u9700\u8981\u5927\u91cf\u63d0\u793a\uff0c\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u4e14\u7f3a\u4e4f\u5b9e\u4f8b\u611f\u77e5\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0e\u8f93\u5165\u5e8f\u5217\u7684\u6ce8\u610f\u529b\u4ea4\u4e92\u53d7\u9650\u3002", "method": "\u63d0\u51faCapsule Prompt-Tuning (CaPT)\uff0c\u5229\u7528\u73b0\u6210\u7684\u4fe1\u606f\u6027\u5b9e\u4f8b\u8bed\u4e49\uff0c\u4ee5\u8fd1\u4e4e\u53c2\u6570\u514d\u8d39\u7684\u65b9\u5f0f\uff08\u5355\u4e2a\u80f6\u56ca\u63d0\u793a\uff09\u96c6\u6210\u5b9e\u4f8b\u611f\u77e5\u548c\u4efb\u52a1\u611f\u77e5\u4fe1\u606f\uff0c\u4f5c\u4e3a\"\u6ce8\u610f\u529b\u951a\"\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u79cd\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff08\u5982T5-Large\u4e0a\u5e73\u5747\u51c6\u786e\u738784.03%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53c2\u6570\u6548\u7387\uff08\u5982Llama3.2-1B\u4e0a\u4ec5\u9700\u6a21\u578b\u53c2\u6570\u76840.003%\uff09\u3002", "conclusion": "CaPT\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u611f\u77e5\u4fe1\u606f\u4f5c\u4e3a\u6ce8\u610f\u529b\u951a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63d0\u793a\u8c03\u4f18\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.16326", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16326", "abs": "https://arxiv.org/abs/2510.16326", "authors": ["Yi Wei", "Shunpu Tang", "Liang Zhao", "Qiangian Yang"], "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution", "comment": null, "summary": "Recent advances in diffusion models have driven remarkable progress in image\ngeneration. However, the generation process remains computationally intensive,\nand users often need to iteratively refine prompts to achieve the desired\nresults, further increasing latency and placing a heavy burden on cloud\nresources. To address this challenge, we propose DiffusionX, a cloud-edge\ncollaborative framework for efficient multi-round, prompt-based generation. In\nthis system, a lightweight on-device diffusion model interacts with users by\nrapidly producing preview images, while a high-capacity cloud model performs\nfinal refinements after the prompt is finalized. We further introduce a noise\nlevel predictor that dynamically balances the computation load, optimizing the\ntrade-off between latency and cloud workload. Experiments show that DiffusionX\nreduces average generation time by 15.8% compared with Stable Diffusion v1.5,\nwhile maintaining comparable image quality. Moreover, it is only 0.9% slower\nthan Tiny-SD with significantly improved image quality, thereby demonstrating\nefficiency and scalability with minimal overhead.", "AI": {"tldr": "DiffusionX\u662f\u4e00\u4e2a\u4e91\u8fb9\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u5907\u7aef\u6269\u6563\u6a21\u578b\u5feb\u901f\u751f\u6210\u9884\u89c8\u56fe\u50cf\uff0c\u9ad8\u6027\u80fd\u4e91\u7aef\u6a21\u578b\u8fdb\u884c\u6700\u7ec8\u4f18\u5316\uff0c\u51cf\u5c11\u751f\u6210\u65f6\u95f415.8%\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u7528\u6237\u9700\u8981\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u8bcd\uff0c\u589e\u52a0\u4e86\u5ef6\u8fdf\u548c\u4e91\u8d44\u6e90\u8d1f\u62c5\u3002", "method": "\u63d0\u51fa\u4e91\u8fb9\u534f\u4f5c\u6846\u67b6\uff0c\u8bbe\u5907\u7aef\u8f7b\u91cf\u6a21\u578b\u751f\u6210\u9884\u89c8\uff0c\u4e91\u7aef\u9ad8\u6027\u80fd\u6a21\u578b\u8fdb\u884c\u6700\u7ec8\u4f18\u5316\uff0c\u5f15\u5165\u566a\u58f0\u6c34\u5e73\u9884\u6d4b\u5668\u52a8\u6001\u5e73\u8861\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u76f8\u6bd4Stable Diffusion v1.5\u51cf\u5c11\u5e73\u5747\u751f\u6210\u65f6\u95f415.8%\uff0c\u56fe\u50cf\u8d28\u91cf\u76f8\u5f53\uff1b\u6bd4Tiny-SD\u4ec5\u61620.9%\u4f46\u56fe\u50cf\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DiffusionX\u5728\u6700\u5c0f\u5f00\u9500\u4e0b\u5c55\u793a\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002"}}
{"id": "2510.16685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16685", "abs": "https://arxiv.org/abs/2510.16685", "authors": ["Damin Zhang", "Julia Rayz"], "title": "Temporal Understanding under Deictic Frame of Reference", "comment": "Under review", "summary": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.", "AI": {"tldr": "\u63d0\u51fa\u4e86TUuD\u6846\u67b6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u53c2\u8003\u70b9\u52a8\u6001\u53d8\u5316\u65f6\u5bf9\u65f6\u95f4-\u4e8b\u4ef6\u548c\u4e8b\u4ef6-\u4e8b\u4ef6\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0LLMs\u5728\u65f6\u95f4\u8ba4\u77e5\u4e0a\u8868\u73b0\u51fa\u90e8\u5206\u7c7b\u4eba\u7279\u5f81\uff0c\u4f46\u5bf9\u53c2\u8003\u6846\u67b6\u53d8\u5316\u548c\u65f6\u95f4\u8ddd\u79bb\u654f\u611f\u3002", "motivation": "\u867d\u7136LLMs\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5bf9\u65f6\u95f4\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u4eba\u7c7b\u901a\u8fc7\u7a7a\u95f4\u9690\u55bb\u6765\u6982\u5ff5\u5316\u65f6\u95f4\u4f53\u9a8c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u5728\u52a8\u6001\u53d8\u5316\u7684\"\u73b0\u5728\"\u53c2\u8003\u70b9\u4e0b\u7406\u89e3\u65f6\u95f4\u5173\u7cfb\u3002", "method": "\u5f15\u5165TUuD\u6846\u67b6\uff0c\u8ba9LLMs\u8bc4\u4f30\u5f53\u524d\u65f6\u523b\u4e0e\u76ee\u6807\u4e8b\u4ef6\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff080.00-1.00\uff09\uff0c\u91cf\u5316\u4e24\u4e2a\u65f6\u95f4\u70b9\u4e4b\u95f4\u7684\u611f\u77e5\u65f6\u95f4\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u7814\u7a76\u6a21\u578b\u5728\u65f6\u95f4\u53c2\u8003\u70b9\u52a8\u6001\u53d8\u5316\u65f6\u7684\u8868\u73b0\u3002", "result": "\u56db\u4e2a\u8bc4\u4f30\u7684LLMs\u663e\u793a\u51fa\u5bf9\u65f6\u95f4\u53c2\u8003\u6846\u67b6\u7684\u53ef\u6d4b\u91cf\u9002\u5e94\uff0c\u76f8\u4f3c\u5ea6\u8bc4\u5206\u5728\u5f53\u524d\u65f6\u523b\u9644\u8fd1\u8fbe\u5230\u5cf0\u503c\uff0c\u5e76\u5411\u8fc7\u53bb\u548c\u672a\u6765\u4e8b\u4ef6\u9012\u51cf\u3002\u4f46\u8fd9\u79cd\u9002\u5e94\u5728\u8fd1\u671f\u8bed\u5883\u4e4b\u5916\u4f1a\u51cf\u5f31\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u90e8\u5206\u7c7b\u4eba\u7684\u65f6\u95f4\u8ba4\u77e5\u80fd\u529b\uff0c\u4f46\u5176\u65f6\u95f4\u63a8\u7406\u4ecd\u7136\u5bf9\u53c2\u8003\u6846\u67b6\u53d8\u5316\u548c\u65f6\u95f4\u8ddd\u79bb\u654f\u611f\uff0c\u8868\u660e\u5176\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u4ecd\u6709\u5c40\u9650\u6027\u3002"}}
{"id": "2510.16332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16332", "abs": "https://arxiv.org/abs/2510.16332", "authors": ["Haiyue Sun", "Qingdong He", "Jinlong Peng", "Peng Tang", "Jiangning Zhang", "Junwei Zhu", "Xiaobin Hu", "Shuicheng Yan"], "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement", "comment": null, "summary": "Autoregressive Model (AR) has shown remarkable success in conditional image\ngeneration. However, these approaches for multiple reference generation\nstruggle with decoupling different reference identities. In this work, we\npropose the TokenAR framework, specifically focused on a simple but effective\ntoken-level enhancement mechanism to address reference identity confusion\nproblem. Such token-level enhancement consists of three parts, 1). Token Index\nEmbedding clusters the tokens index for better representing the same reference\nimages; 2). Instruct Token Injection plays as a role of extra visual feature\ncontainer to inject detailed and complementary priors for reference tokens; 3).\nThe identity-token disentanglement strategy (ITD) explicitly guides the token\nrepresentations toward independently representing the features of each\nidentity.This token-enhancement framework significantly augments the\ncapabilities of existing AR based methods in conditional image generation,\nenabling good identity consistency while preserving high quality background\nreconstruction. Driven by the goal of high-quality and high-diversity in\nmulti-subject generation, we introduce the InstructAR Dataset, the first\nopen-source, large-scale, multi-reference input, open domain image generation\ndataset that includes 28K training pairs, each example has two reference\nsubjects, a relative prompt and a background with mask annotation, curated for\nmultiple reference image generation training and evaluating. Comprehensive\nexperiments validate that our approach surpasses current state-of-the-art\nmodels in multiple reference image generation task. The implementation code and\ndatasets will be made publicly. Codes are available, see\nhttps://github.com/lyrig/TokenAR", "AI": {"tldr": "\u63d0\u51fa\u4e86TokenAR\u6846\u67b6\uff0c\u901a\u8fc7token\u7ea7\u589e\u5f3a\u673a\u5236\u89e3\u51b3\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u6df7\u6dc6\u95ee\u9898\uff0c\u5305\u542btoken\u7d22\u5f15\u5d4c\u5165\u3001\u6307\u5bfctoken\u6ce8\u5165\u548c\u8eab\u4efdtoken\u89e3\u7f20\u7b56\u7565\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u53c2\u8003\u751f\u6210\u65f6\u96be\u4ee5\u89e3\u8026\u4e0d\u540c\u53c2\u8003\u8eab\u4efd\uff0c\u5b58\u5728\u8eab\u4efd\u6df7\u6dc6\u95ee\u9898\u3002", "method": "TokenAR\u6846\u67b6\u5305\u542b\u4e09\u4e2atoken\u7ea7\u589e\u5f3a\u7ec4\u4ef6\uff1aToken Index Embedding\u805a\u7c7b\u76f8\u540c\u53c2\u8003\u56fe\u50cf\u7684token\u7d22\u5f15\uff1bInstruct Token Injection\u4f5c\u4e3a\u989d\u5916\u89c6\u89c9\u7279\u5f81\u5bb9\u5668\u6ce8\u5165\u8be6\u7ec6\u5148\u9a8c\uff1b\u8eab\u4efdtoken\u89e3\u7f20\u7b56\u7565(ITD)\u5f15\u5bfctoken\u72ec\u7acb\u8868\u793a\u6bcf\u4e2a\u8eab\u4efd\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u73b0\u6709AR\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u80cc\u666f\u91cd\u5efa\u7684\u540c\u65f6\u5b9e\u73b0\u826f\u597d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u5728\u591a\u4e2a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "TokenAR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u6df7\u6dc6\u95ee\u9898\uff0c\u901a\u8fc7token\u7ea7\u589e\u5f3a\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u591a\u6837\u6027\u7684\u591a\u4e3b\u4f53\u751f\u6210\u3002"}}
{"id": "2510.16686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16686", "abs": "https://arxiv.org/abs/2510.16686", "authors": ["Wenhang Shi", "Shuqing Bian", "Yiren Chen", "Xinyi Zhang", "Zhe Zhao", "Pengfei Hu", "Wei Lu", "Xiaoyong Du"], "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding", "comment": null, "summary": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to\nderive final answers, benefit LLMs in both inference and training.\nIncorporating rationales, either by generating them before answering during\ninference, or by placing them before or after the original answers during\ntraining - significantly improves model performance on mathematical, symbolic\nand commonsense reasoning tasks. However, most work focuses on the role of\nrationales in these reasoning tasks, overlooking their potential impact on\nother important tasks like natural language understanding (NLU) tasks. In this\nwork, we raise the question: Can rationales similarly benefit NLU tasks? To\nconduct a systematic exploration, we construct NLURC, a comprehensive and\nhigh-quality NLU dataset collection with rationales, and develop various\nrationale-augmented methods. Through exploring the applicability of these\nmethods on NLU tasks using the dataset, we uncover several potentially\nsurprising findings: (1) CoT inference shifts from hindering NLU performance to\nsurpassing direct label prediction as model size grows, indicating a positive\ncorrelation. (2) Most rationale-augmented training methods perform worse than\nlabel-only training, with one specially designed method consistently achieving\nimprovements. (3) LLMs trained with rationales achieve significant performance\ngains on unseen NLU tasks, rivaling models ten times their size, while\ndelivering interpretability on par with commercial LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0cCoT\u63a8\u7406\u4ece\u963b\u788d\u53d8\u4e3a\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u7279\u5b9a\u8bbe\u8ba1\u7684\u8bad\u7ec3\u65b9\u6cd5\u80fd\u5e26\u6765\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u601d\u7ef4\u94fe\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5ffd\u89c6\u4e86\u5176\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u63a2\u7d22\u601d\u7ef4\u94fe\u5bf9NLU\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u601d\u7ef4\u94fe\u7684NLURC\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u591a\u79cd\u601d\u7ef4\u94fe\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u5728NLU\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u4e0eCoT\u6548\u679c\u6b63\u76f8\u5173\uff1b\u591a\u6570\u601d\u7ef4\u94fe\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u6548\u679c\u4e0d\u5982\u4ec5\u6807\u7b7e\u8bad\u7ec3\uff0c\u4f46\u6709\u4e00\u79cd\u7279\u6b8a\u8bbe\u8ba1\u7684\u65b9\u6cd5\u80fd\u6301\u7eed\u6539\u8fdb\uff1b\u4f7f\u7528\u601d\u7ef4\u94fe\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1NLU\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u5ab2\u7f8e\u5927\u5341\u500d\u6a21\u578b\u3002", "conclusion": "\u601d\u7ef4\u94fe\u5728NLU\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e3aNLU\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.16333", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16333", "abs": "https://arxiv.org/abs/2510.16333", "authors": ["Junha Song", "Sangdoo Yun", "Dongyoon Han", "Jaegul Choo", "Byeongho Heo"], "title": "RL makes MLLMs see better than SFT", "comment": null, "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60(RL)\u6bd4\u76d1\u7763\u5fae\u8c03(SFT)\u5728\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7f16\u7801\u5668\u8bad\u7ec3\u4e2d\u4ea7\u751f\u66f4\u5f3a\u3001\u66f4\u7cbe\u786e\u7684\u89c6\u89c9\u8868\u793a\uff0c\u63d0\u51fa\u4e86PIVOT\u65b9\u6cd5\uff0c\u80fd\u4ee5\u4e0d\u52301%\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u666e\u904d\u5047\u8bbe\u6027\u80fd\u4e3b\u8981\u7ee7\u627f\u81eaLLM\u9aa8\u5e72\u7f51\u7edc\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u4eceSFT\u8f6c\u5411RL\u8bad\u7ec3\u8303\u5f0f\u540e\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u5982\u4f55\u88ab\u91cd\u5851\u7684\u5206\u6790\u3002", "method": "\u901a\u8fc7ImageNet\u5206\u7c7b\u3001\u5206\u5272\u548c\u68af\u5ea6\u53ef\u89c6\u5316\u7b49\u591a\u6837\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86Preference-Instructed Vision Optimization (PIVOT)\u65b9\u6cd5\u3002", "result": "RL\u8bad\u7ec3\u4ea7\u751f\u66f4\u5f3a\u4e14\u7cbe\u786e\u5b9a\u4f4d\u7684\u89c6\u89c9\u8868\u793a\uff0cPIVOT\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u4e0d\u5230\u6807\u51c6\u89c6\u89c9\u9884\u8bad\u7ec3\u76841%\u3002", "conclusion": "RL\u8bad\u7ec3\u8303\u5f0f\u80fd\u6709\u6548\u91cd\u5851\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u80fd\u529b\uff0cPIVOT\u65b9\u6cd5\u4e3a\u63a8\u8fdbMLLM\u89c6\u89c9\u9aa8\u5e72\u63d0\u4f9b\u4e86\u9ad8\u6548\u8def\u5f84\u3002"}}
{"id": "2510.16708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16708", "abs": "https://arxiv.org/abs/2510.16708", "authors": ["Kailai Yang", "Yan Leng", "Xin Zhang", "Tianlin Zhang", "Paul Thompson", "Bernard Keavney", "Maciej Tomaszewski", "Sophia Ananiadou"], "title": "Natural Language Processing Applications in Cardiology: A Narrative Review", "comment": null, "summary": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u56de\u987e\u4e862014-2025\u5e74\u95f4\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u5fc3\u810f\u75c5\u5b66\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\uff0c\u5206\u6790\u4e86265\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u4eceNLP\u8303\u5f0f\u3001\u5fc3\u810f\u75c5\u5b66\u4efb\u52a1\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6765\u6e90\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u65e5\u76ca\u6d41\u884c\uff0c\u5bf9\u5168\u7403\u5065\u5eb7\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002\u5fc3\u810f\u75c5\u76f8\u5173\u6761\u4ef6\u590d\u6742\u591a\u6837\uff0c\u76f8\u5173\u4fe1\u606f\u5206\u6563\u5728\u60a3\u8005\u53d9\u8ff0\u3001\u533b\u7597\u8bb0\u5f55\u548c\u79d1\u5b66\u6587\u732e\u7b49\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u672c\u6570\u636e\u4e2d\uff0c\u9700\u8981NLP\u6280\u672f\u6765\u5206\u6790\u8fd9\u4e9b\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002", "method": "\u67e5\u8be2\u4e86\u516d\u4e2a\u6587\u732e\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u4e25\u683c\u7b5b\u9009\u8fc7\u7a0b\u786e\u5b9a\u4e86265\u7bc7\u76f8\u5173\u6587\u7ae0\uff0c\u4eceNLP\u8303\u5f0f\u7c7b\u578b\u3001\u5fc3\u810f\u75c5\u5b66\u4efb\u52a1\u7c7b\u578b\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\u7c7b\u578b\u548c\u6570\u636e\u6765\u6e90\u7c7b\u578b\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u8fdb\u884c\u4e86\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u7ef4\u5ea6\u5185\u5b58\u5728\u76f8\u5f53\u5927\u7684\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86NLP\u7814\u7a76\u5728\u8be5\u9886\u57df\u7684\u5e7f\u5ea6\u3002\u65f6\u95f4\u5206\u6790\u63ed\u793a\u4e86\u6240\u6db5\u76d6\u7684\u8fc7\u53bb\u5341\u5e74\u4e2dNLP\u65b9\u6cd5\u7684\u6f14\u53d8\u548c\u53d8\u5316\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u5bf9\u5fc3\u810f\u75c5\u5b66\u9886\u57dfNLP\u7814\u7a76\u6700\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5c55\u793a\u4e86NLP\u6280\u672f\u5728\u5fc3\u810f\u75c5\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u9884\u9632\u65b9\u9762\u7684\u9769\u547d\u6027\u6f5c\u529b\u3002"}}
{"id": "2510.16335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16335", "abs": "https://arxiv.org/abs/2510.16335", "authors": ["Bo Peng", "Jie Lu", "Guangquan Zhang", "Zhen Fang"], "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering", "comment": "revised and extended version of ICCV2025", "summary": "This paper investigates the recently emerged problem of Language-assisted\nImage Clustering (LaIC), where textual semantics are leveraged to improve the\ndiscriminability of visual representations to facilitate image clustering. Due\nto the unavailability of true class names, one of core challenges of LaIC lies\nin how to filter positive nouns, i.e., those semantically close to the images\nof interest, from unlabeled wild corpus data. Existing filtering strategies are\npredominantly based on the off-the-shelf feature space learned by CLIP;\nhowever, despite being intuitive, these strategies lack a rigorous theoretical\nfoundation. To fill this gap, we propose a novel gradient-based framework,\ntermed as GradNorm, which is theoretically guaranteed and shows strong\nempirical performance. In particular, we measure the positiveness of each noun\nbased on the magnitude of gradients back-propagated from the cross-entropy\nbetween the predicted target distribution and the softmax output.\nTheoretically, we provide a rigorous error bound to quantify the separability\nof positive nouns by GradNorm and prove that GradNorm naturally subsumes\nexisting filtering strategies as extremely special cases of itself.\nEmpirically, extensive experiments show that GradNorm achieves the\nstate-of-the-art clustering performance on various benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86GradNorm\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5927\u5c0f\u8861\u91cf\u540d\u8bcd\u4e0e\u56fe\u50cf\u8bed\u4e49\u7684\u63a5\u8fd1\u7a0b\u5ea6\uff0c\u7528\u4e8e\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\u4e2d\u7684\u6b63\u540d\u8bcd\u7b5b\u9009\uff0c\u7406\u8bba\u4e0a\u6709\u4e25\u683c\u8bef\u5dee\u754c\u9650\u4e14\u5305\u542b\u73b0\u6709\u65b9\u6cd5\u4e3a\u7279\u4f8b\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\u4e2d\u5982\u4f55\u4ece\u672a\u6807\u6ce8\u8bed\u6599\u4e2d\u7b5b\u9009\u4e0e\u76ee\u6807\u56fe\u50cf\u8bed\u4e49\u76f8\u8fd1\u7684\u6b63\u540d\u8bcd\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8eCLIP\u7279\u5f81\u7a7a\u95f4\u7684\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684GradNorm\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u76ee\u6807\u5206\u5e03\u4e0esoftmax\u8f93\u51fa\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u5927\u5c0f\u6765\u8861\u91cf\u540d\u8bcd\u7684\u8bed\u4e49\u63a5\u8fd1\u7a0b\u5ea6\u3002", "result": "\u7406\u8bba\u8bc1\u660eGradNorm\u5177\u6709\u4e25\u683c\u7684\u8bef\u5dee\u754c\u9650\uff0c\u80fd\u5c06\u73b0\u6709\u8fc7\u6ee4\u7b56\u7565\u4f5c\u4e3a\u81ea\u8eab\u7279\u4f8b\uff0c\u5b9e\u9a8c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u6027\u80fd\u3002", "conclusion": "GradNorm\u4e3a\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6b63\u540d\u8bcd\u7b5b\u9009\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u7b56\u7565\u5e76\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6548\u679c\u3002"}}
{"id": "2510.16712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16712", "abs": "https://arxiv.org/abs/2510.16712", "authors": ["Shivam Ratnakar", "Sanjay Raghavendra"], "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models", "comment": null, "summary": "Integration of Large Language Models with search/retrieval engines has become\nubiquitous, yet these systems harbor a critical vulnerability that undermines\ntheir reliability. We present the first systematic investigation of \"chameleon\nbehavior\" in LLMs: their alarming tendency to shift stances when presented with\ncontradictory questions in multi-turn conversations (especially in\nsearch-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising\n17,770 carefully crafted question-answer pairs across 1,180 multi-turn\nconversations spanning 12 controversial domains, we expose fundamental flaws in\nstate-of-the-art systems. We introduce two theoretically grounded metrics: the\nChameleon Score (0-1) that quantifies stance instability, and Source Re-use\nRate (0-1) that measures knowledge diversity. Our rigorous evaluation of\nLlama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent\nfailures: all models exhibit severe chameleon behavior (scores 0.391-0.511),\nwith GPT-4o-mini showing the worst performance. Crucially, small\nacross-temperature variance (less than 0.004) suggests the effect is not a\nsampling artifact. Our analysis uncovers the mechanism: strong correlations\nbetween source re-use rate and confidence (r=0.627) and stance changes\n(r=0.429) are statistically significant (p less than 0.05), indicating that\nlimited knowledge diversity makes models pathologically deferential to query\nframing. These findings highlight the need for comprehensive consistency\nevaluation before deploying LLMs in healthcare, legal, and financial systems\nwhere maintaining coherent positions across interactions is critical for\nreliable decision support.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u4e2d\u7684\"\u53d8\u8272\u9f99\u884c\u4e3a\"\uff1a\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9762\u5bf9\u77db\u76fe\u95ee\u9898\u65f6\u7acb\u573a\u4e0d\u7a33\u5b9a\u7684\u503e\u5411\u3002\u901a\u8fc7Chameleon\u57fa\u51c6\u6570\u636e\u96c6\u8bc4\u4f30\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u90fd\u5b58\u5728\u4e25\u91cd\u7684\u7acb\u573a\u4e0d\u7a33\u5b9a\u6027\uff0c\u8fd9\u79cd\u6548\u5e94\u4e0e\u77e5\u8bc6\u591a\u6837\u6027\u4e0d\u8db3\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u96c6\u6210\u68c0\u7d22\u5f15\u64ce\u7684LLM\u7cfb\u7edf\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u5176\u53ef\u9760\u6027\u53d7\u5230\u8d28\u7591\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9762\u5bf9\u77db\u76fe\u95ee\u9898\u65f6\u7684\u7acb\u573a\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d\u7b49\u9700\u8981\u4e00\u81f4\u51b3\u7b56\u652f\u6301\u7684\u9886\u57df\u3002", "method": "\u6784\u5efa\u5305\u542b17,770\u4e2a\u95ee\u7b54\u5bf9\u7684Chameleon\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d612\u4e2a\u4e89\u8bae\u9886\u57df\u76841,180\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u3002\u63d0\u51fa\u4e24\u4e2a\u7406\u8bba\u57fa\u7840\u7684\u6307\u6807\uff1a\u91cf\u5316\u7acb\u573a\u4e0d\u7a33\u5b9a\u6027\u7684\u53d8\u8272\u9f99\u5206\u6570(0-1)\u548c\u8861\u91cf\u77e5\u8bc6\u591a\u6837\u6027\u7684\u6e90\u91cd\u7528\u7387(0-1)\u3002", "result": "\u5bf9Llama-4-Maverick\u3001GPT-4o-mini\u548cGemini-2.5-Flash\u7684\u8bc4\u4f30\u663e\u793a\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e25\u91cd\u7684\u53d8\u8272\u9f99\u884c\u4e3a(\u5206\u65700.391-0.511)\uff0cGPT-4o-mini\u8868\u73b0\u6700\u5dee\u3002\u6e90\u91cd\u7528\u7387\u4e0e\u7f6e\u4fe1\u5ea6(r=0.627)\u548c\u7acb\u573a\u53d8\u5316(r=0.429)\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u7684\u77e5\u8bc6\u591a\u6837\u6027\u6709\u9650\u5bfc\u81f4\u5176\u75c5\u6001\u5730\u4f9d\u8d56\u67e5\u8be2\u6846\u67b6\uff0c\u5728\u5173\u952e\u5e94\u7528\u9886\u57df\u90e8\u7f72\u524d\u9700\u8981\u8fdb\u884c\u5168\u9762\u7684\u7a33\u5b9a\u6027\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u8de8\u4ea4\u4e92\u65f6\u4fdd\u6301\u4e00\u81f4\u7684\u7acb\u573a\u3002"}}
{"id": "2510.16370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16370", "abs": "https://arxiv.org/abs/2510.16370", "authors": ["Pulin Li", "Guocheng Wu", "Li Yin", "Yuxin Zheng", "Wei Zhang", "Yanjie Zhou"], "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization", "comment": "https://github.com/wu33learn/MIRAD", "summary": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86MIRAD\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u793e\u4ea4\u5236\u9020\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u8bbe\u8ba1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u5b9a\u5236\u4ea7\u54c1\u3001\u516d\u4e2a\u5730\u7406\u5206\u6563\u5236\u9020\u8282\u70b9\u7684\u6570\u636e\u4ee5\u53ca\u663e\u8457\u7684\u6210\u50cf\u5f02\u8d28\u6027\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u793e\u4ea4\u5236\u9020\u6a21\u5f0f\u4e0b\u7684\u8d28\u91cf\u63a7\u5236\u548c\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u4ea7\u54c1\u9ad8\u5ea6\u5b9a\u5236\u5316\u3001\u751f\u4ea7\u788e\u7247\u5316\u5c0f\u6279\u91cf\u3001\u5206\u5e03\u5f0f\u7ad9\u70b9\u6210\u50cf\u73af\u5883\u5dee\u5f02\u5927\uff0c\u9700\u8981\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5b9a\u5236\u7b97\u6cd5\u7684\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u6784\u5efaMIRAD\u6570\u636e\u96c6\uff0c\u6355\u6349\u793e\u4ea4\u5236\u9020\u9886\u57df\u7684\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u591a\u6837\u5316\u5b9a\u5236\u4ea7\u54c1\u7684\u5927\u7c7b\u5185\u53d8\u5f02\u3001\u516d\u4e2a\u5730\u7406\u5206\u6563\u5236\u9020\u8282\u70b9\u7684\u6570\u636e\u6536\u96c6\u3001\u663e\u8457\u7684\u6210\u50cf\u5f02\u8d28\u6027\uff08\u5149\u7167\u3001\u80cc\u666f\u3001\u8fd0\u52a8\u6761\u4ef6\u53d8\u5316\uff09\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff08\u5355\u7c7b\u3001\u591a\u7c7b\u548c\u96f6\u6837\u672c\u65b9\u6cd5\uff09\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4e0e\u4f20\u7edf\u57fa\u51c6\u76f8\u6bd4\uff0c\u6240\u6709\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u663e\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2a\u6027\u5316\u751f\u4ea7\u4e2d\u7f3a\u9677\u68c0\u6d4b\u7684\u672a\u89e3\u51b3\u590d\u6742\u6027\u3002", "conclusion": "MIRAD\u901a\u8fc7\u8fde\u63a5\u5de5\u4e1a\u9700\u6c42\u548c\u5b66\u672f\u7814\u7a76\uff0c\u4e3a\u5f00\u53d1\u5de5\u4e1a5.0\u6240\u9700\u7684\u7a33\u5065\u8d28\u91cf\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u73b0\u5b9e\u57fa\u7840\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.16713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16713", "abs": "https://arxiv.org/abs/2510.16713", "authors": ["Sriharsh Bhyravajjula", "Melanie Walsh", "Anna Preus", "Maria Antoniak"], "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs", "comment": null, "summary": "Whitespace is a critical component of poetic form, reflecting both adherence\nto standardized forms and rebellion against those forms. Each poem's whitespace\ndistribution reflects the artistic choices of the poet and is an integral\nsemantic and spatial feature of the poem. Yet, despite the popularity of poetry\nas both a long-standing art form and as a generation task for large language\nmodels (LLMs), whitespace has not received sufficient attention from the NLP\ncommunity. Using a corpus of 19k English-language published poems from Poetry\nFoundation, we investigate how 4k poets have used whitespace in their works. We\nrelease a subset of 2.8k public-domain poems with preserved formatting to\nfacilitate further research in this area. We compare whitespace usage in the\npublished poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems\nposted in an online community. We also explore whitespace usage across time\nperiods, poetic forms, and data sources. Additionally, we find that different\ntext processing methods can result in significantly different representations\nof whitespace in poetry data, motivating us to use these poems and whitespace\npatterns to discuss implications for the processing strategies used to assemble\npretraining datasets for LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bd7\u6b4c\u4e2d\u7a7a\u683c\u7684\u4f7f\u7528\u6a21\u5f0f\uff0c\u6bd4\u8f83\u4e86\u4eba\u7c7b\u521b\u4f5c\u8bd7\u6b4c\u3001LLM\u751f\u6210\u8bd7\u6b4c\u548c\u672a\u53d1\u8868\u8bd7\u6b4c\u5728\u7a7a\u683c\u4f7f\u7528\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u6587\u672c\u5904\u7406\u65b9\u6cd5\u5bf9\u7a7a\u683c\u8868\u793a\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u8bd7\u6b4c\u662f\u957f\u671f\u5b58\u5728\u7684\u827a\u672f\u5f62\u5f0f\uff0c\u4e5f\u662fLLM\u7684\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u7a7a\u683c\u4f5c\u4e3a\u8bd7\u6b4c\u5f62\u5f0f\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5728NLP\u793e\u533a\u4e2d\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002", "method": "\u4f7f\u7528\u6765\u81eaPoetry Foundation\u768419k\u9996\u82f1\u6587\u51fa\u7248\u8bd7\u6b4c\u8bed\u6599\u5e93\uff0c\u5206\u67904k\u4f4d\u8bd7\u4eba\u7684\u7a7a\u683c\u4f7f\u7528\u60c5\u51b5\uff0c\u5e76\u6bd4\u8f83\u4e86LLM\u751f\u6210\u8bd7\u6b4c\u548c\u5728\u7ebf\u793e\u533a\u672a\u53d1\u8868\u8bd7\u6b4c\u7684\u7a7a\u683c\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6587\u672c\u5904\u7406\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8bd7\u6b4c\u6570\u636e\u4e2d\u7a7a\u683c\u8868\u793a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u4eba\u7c7b\u521b\u4f5c\u8bd7\u6b4c\u4e0eLLM\u751f\u6210\u8bd7\u6b4c\u5728\u7a7a\u683c\u4f7f\u7528\u4e0a\u7684\u4e0d\u540c\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u7a7a\u683c\u5728\u8bd7\u6b4c\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u6784\u5efaLLM\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5904\u7406\u7b56\u7565\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.16371", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16371", "abs": "https://arxiv.org/abs/2510.16371", "authors": ["Mohammad Javad Ahmadi", "Iman Gandomi", "Parisa Abdi", "Seyed-Farzad Mohammadi", "Amirhossein Taslimi", "Mehdi Khodaparast", "Hassan Hashemi", "Mahdi Tavakoli", "Hamid D. Taghirad"], "title": "Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis", "comment": "20 pages, 11 figures, 11 tables. Data descriptor for the Cataract-LMM\n  benchmark dataset. Source code and dataset are available", "summary": "The development of computer-assisted surgery systems depends on large-scale,\nannotated datasets. Current resources for cataract surgery often lack the\ndiversity and annotation depth needed to train generalizable deep-learning\nmodels. To address this gap, we present a dataset of 3,000 phacoemulsification\ncataract surgery videos from two surgical centers, performed by surgeons with a\nrange of experience levels. This resource is enriched with four annotation\nlayers: temporal surgical phases, instance segmentation of instruments and\nanatomical structures, instrument-tissue interaction tracking, and quantitative\nskill scores based on the established competency rubrics like the ICO-OSCAR.\nThe technical quality of the dataset is supported by a series of benchmarking\nexperiments for key surgical AI tasks, including workflow recognition, scene\nsegmentation, and automated skill assessment. Furthermore, we establish a\ndomain adaptation baseline for the phase recognition task by training a model\non a subset of surgical centers and evaluating its performance on a held-out\ncenter. The dataset and annotations are available in Google Form\n(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b3000\u4e2a\u767d\u5185\u969c\u624b\u672f\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u5177\u6709\u56db\u4e2a\u6807\u6ce8\u5c42\uff1a\u624b\u672f\u9636\u6bb5\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u5668\u68b0-\u7ec4\u7ec7\u4ea4\u4e92\u8ffd\u8e2a\u548c\u6280\u80fd\u8bc4\u5206\uff0c\u4e3a\u624b\u672fAI\u4efb\u52a1\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524d\u767d\u5185\u969c\u624b\u672f\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u6df1\u5ea6\u6807\u6ce8\uff0c\u96be\u4ee5\u8bad\u7ec3\u6cdb\u5316\u6027\u5f3a\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u6536\u96c6\u6765\u81ea\u4e24\u4e2a\u624b\u672f\u4e2d\u5fc3\u76843000\u4e2a\u767d\u5185\u969c\u624b\u672f\u89c6\u9891\uff0c\u5305\u542b\u56db\u4e2a\u6807\u6ce8\u5c42\uff1a\u65f6\u95f4\u624b\u672f\u9636\u6bb5\u3001\u5668\u68b0\u548c\u89e3\u5256\u7ed3\u6784\u5b9e\u4f8b\u5206\u5272\u3001\u5668\u68b0-\u7ec4\u7ec7\u4ea4\u4e92\u8ffd\u8e2a\u3001\u57fa\u4e8eICO-OSCAR\u6807\u51c6\u7684\u6280\u80fd\u8bc4\u5206\u3002", "result": "\u901a\u8fc7\u57fa\u51c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u5728\u624b\u672f\u5de5\u4f5c\u6d41\u8bc6\u522b\u3001\u573a\u666f\u5206\u5272\u548c\u81ea\u52a8\u6280\u80fd\u8bc4\u4f30\u7b49\u5173\u952eAI\u4efb\u52a1\u4e2d\u7684\u6280\u672f\u8d28\u91cf\uff0c\u5e76\u5efa\u7acb\u4e86\u9886\u57df\u81ea\u9002\u5e94\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u767d\u5185\u969c\u624b\u672fAI\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\uff0c\u652f\u6301\u6a21\u578b\u6cdb\u5316\u6027\u548c\u8de8\u4e2d\u5fc3\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2510.16727", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16727", "abs": "https://arxiv.org/abs/2510.16727", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Angkul Puniya", "Sohom Pal"], "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models", "comment": null, "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Beacon\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6d4b\u91cfLLM\u5728\u771f\u5b9e\u6027\u548c\u987a\u4ece\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53d1\u73b0\u8c04\u5a9a\u504f\u89c1\u968f\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u800c\u52a0\u5267\uff0c\u5e76\u63d0\u51fa\u4e86\u5e72\u9884\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5956\u52b1\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6df7\u6dc6\u4e86\u5e2e\u52a9\u6027\u548c\u793c\u8c8c\u670d\u4ece\uff0c\u5f62\u6210\u4e86\u5728\u771f\u5b9e\u6027\u548c\u8c04\u5a9a\u987a\u4ece\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u6743\u8861\uff0c\u8fd9\u79cd\u504f\u89c1\u88ab\u79f0\u4e3a\u8c04\u5a9a\u504f\u89c1\u3002", "method": "\u5f15\u5165Beacon\u5355\u8f6e\u5f3a\u5236\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\uff0c\u72ec\u7acb\u4e8e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u9694\u79bb\u8fd9\u79cd\u504f\u89c1\uff1b\u8bc4\u4f30\u4e8612\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\uff1b\u63d0\u51fa\u4e86\u63d0\u793a\u7ea7\u548c\u6fc0\u6d3b\u7ea7\u5e72\u9884\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8c04\u5a9a\u504f\u89c1\u5206\u89e3\u4e3a\u7a33\u5b9a\u7684\u8bed\u8a00\u548c\u60c5\u611f\u5b50\u504f\u89c1\uff0c\u6bcf\u79cd\u504f\u89c1\u90fd\u968f\u6a21\u578b\u5bb9\u91cf\u6269\u5c55\u800c\u589e\u5f3a\uff1b\u5e72\u9884\u65b9\u6cd5\u80fd\u591f\u5728\u76f8\u53cd\u65b9\u5411\u4e0a\u8c03\u8282\u8fd9\u4e9b\u504f\u89c1\u3002", "conclusion": "Beacon\u5c06\u8c04\u5a9a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u6d4b\u91cf\u7684\u89c4\u8303\u9519\u8bef\u6cdb\u5316\u5f62\u5f0f\uff0c\u4e3a\u7814\u7a76\u548c\u7f13\u89e3\u5927\u89c4\u6a21\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u5bf9\u9f50\u6f02\u79fb\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002"}}
{"id": "2510.16375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16375", "abs": "https://arxiv.org/abs/2510.16375", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance", "comment": "Under review", "summary": "Road potholes pose significant safety hazards and maintenance challenges,\nparticularly on India's diverse and under-maintained road networks. This paper\npresents iWatchRoadv2, a fully automated end-to-end platform for real-time\npothole detection, GPS-based geotagging, and dynamic road health visualization\nusing OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000\ndashcam frames capturing diverse Indian road conditions, weather patterns, and\nlighting scenarios, which we used to fine-tune the Ultralytics YOLO model for\naccurate pothole detection. The system synchronizes OCR-extracted video\ntimestamps with external GPS logs to precisely geolocate each detected pothole,\nenriching detections with comprehensive metadata, including road segment\nattribution and contractor information managed through an optimized backend\ndatabase. iWatchRoadv2 introduces intelligent governance features that enable\nauthorities to link road segments with contract metadata through a secure login\ninterface. The system automatically sends alerts to contractors and officials\nwhen road health deteriorates, supporting automated accountability and warranty\nenforcement. The intuitive web interface delivers actionable analytics to\nstakeholders and the public, facilitating evidence-driven repair planning,\nbudget allocation, and quality assessment. Our cost-effective and scalable\nsolution streamlines frame processing and storage while supporting seamless\npublic engagement for urban and rural deployments. By automating the complete\npothole monitoring lifecycle, from detection to repair verification,\niWatchRoadv2 enables data-driven smart city management, transparent governance,\nand sustainable improvements in road infrastructure maintenance. The platform\nand live demonstration are accessible at\nhttps://smlab.niser.ac.in/project/iwatchroad.", "AI": {"tldr": "iWatchRoadv2\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u5751\u6d1e\u68c0\u6d4b\u3001GPS\u5730\u7406\u6807\u8bb0\u548c\u52a8\u6001\u9053\u8def\u5065\u5eb7\u53ef\u89c6\u5316\u7684\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u5e73\u53f0\uff0c\u4e13\u95e8\u9488\u5bf9\u5370\u5ea6\u591a\u6837\u5316\u7684\u9053\u8def\u7f51\u7edc\u8bbe\u8ba1\u3002", "motivation": "\u5370\u5ea6\u9053\u8def\u7f51\u7edc\u591a\u6837\u5316\u4e14\u7ef4\u62a4\u4e0d\u8db3\uff0c\u9053\u8def\u5751\u6d1e\u6784\u6210\u91cd\u5927\u5b89\u5168\u9690\u60a3\u548c\u7ef4\u62a4\u6311\u6218\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u3002", "method": "\u4f7f\u7528\u81ea\u6807\u6ce8\u76847000\u591a\u5f20\u5370\u5ea6\u9053\u8def\u4eea\u8868\u76d8\u6444\u50cf\u5934\u56fe\u50cf\u6570\u636e\u96c6\u5fae\u8c03Ultralytics YOLO\u6a21\u578b\u8fdb\u884c\u5751\u6d1e\u68c0\u6d4b\uff0c\u7ed3\u5408OCR\u63d0\u53d6\u7684\u65f6\u95f4\u6233\u548c\u5916\u90e8GPS\u65e5\u5fd7\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\uff0c\u901a\u8fc7\u4f18\u5316\u540e\u7aef\u6570\u636e\u5e93\u7ba1\u7406\u9053\u8def\u6bb5\u5c5e\u6027\u548c\u627f\u5305\u5546\u4fe1\u606f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u667a\u80fd\u6cbb\u7406\u529f\u80fd\uff0c\u80fd\u591f\u81ea\u52a8\u5411\u627f\u5305\u5546\u548c\u5b98\u5458\u53d1\u9001\u8b66\u62a5\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u5206\u6790\u6570\u636e\u3002", "conclusion": "iWatchRoadv2\u901a\u8fc7\u81ea\u52a8\u5316\u5b8c\u6574\u7684\u5751\u6d1e\u76d1\u6d4b\u751f\u547d\u5468\u671f\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u9a71\u52a8\u7684\u667a\u6167\u57ce\u5e02\u7ba1\u7406\u3001\u900f\u660e\u6cbb\u7406\u548c\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u7684\u53ef\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2510.16761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16761", "abs": "https://arxiv.org/abs/2510.16761", "authors": ["Yikai Zhang", "Ye Rong", "Siyu Yuan", "Jiangjie Chen", "Jian Xie", "Yanghua Xiao"], "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games", "comment": null, "summary": "Existing language agents often encounter difficulties in dynamic adversarial\ngames due to poor strategic reasoning. To mitigate this limitation, a promising\napproach is to allow agents to learn from game interactions automatically,\nwithout relying on costly expert-labeled data. Unlike static environments where\nagents receive fixed feedback or rewards, selecting appropriate opponents in\ndynamic adversarial games can significantly impact learning performance.\nHowever, the discussion of opponents in adversarial environments remains an\narea under exploration. In this paper, we propose a Step-level poliCy\nOptimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we\nconduct a detailed analysis of opponent selection by setting opponents at\ndifferent levels and find that self-play is the most effective way to improve\nstrategic reasoning in such adversarial environments. Utilizing SCO-PAL with\nself-play, we increase the average win rate against four opponents by\napproximately 30% compared to baselines and achieve a 54.76% win rate against\nGPT-4 in six adversarial games.", "AI": {"tldr": "\u63d0\u51fa\u4e86SCO-PAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u63d0\u5347\u8bed\u8a00\u4ee3\u7406\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u7b56\u7565\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u80dc\u7387\u63d0\u5347\u7ea630%\uff0c\u5bf9\u6297GPT-4\u8fbe\u523054.76%\u80dc\u7387\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u5728\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u7b56\u7565\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u65e0\u9700\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u81ea\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u800c\u5bf9\u624b\u9009\u62e9\u5bf9\u5b66\u4e60\u6548\u679c\u6709\u91cd\u8981\u5f71\u54cd\u4f46\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faSCO-PAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e38\u620f\u4ea4\u4e92\u81ea\u52a8\u5b66\u4e60\uff0c\u5206\u6790\u4e0d\u540c\u7ea7\u522b\u5bf9\u624b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u535a\u5f08\u662f\u6700\u6709\u6548\u7684\u5bf9\u624b\u9009\u62e9\u7b56\u7565\u3002", "result": "\u4f7f\u7528SCO-PAL\u4e0e\u81ea\u535a\u5f08\uff0c\u57286\u4e2a\u5bf9\u6297\u6e38\u620f\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u80dc\u7387\u63d0\u5347\u7ea630%\uff0c\u5bf9\u6297GPT-4\u8fbe\u523054.76%\u80dc\u7387\u3002", "conclusion": "\u81ea\u535a\u5f08\u662f\u63d0\u5347\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u4e2d\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u6700\u6709\u6548\u65b9\u5f0f\uff0cSCO-PAL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u4ee3\u7406\u7684\u5bf9\u6297\u6027\u80fd\u3002"}}
{"id": "2510.16377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16377", "abs": "https://arxiv.org/abs/2510.16377", "authors": ["Tianhang Cheng", "Albert J. Zhai", "Evan Z. Chen", "Rui Zhou", "Yawen Deng", "Zitong Li", "Kejie Zhao", "Janice Shiu", "Qianyu Zhao", "Yide Xu", "Xinlei Wang", "Yuan Shen", "Sheng Wang", "Lisa Ainsworth", "Kaiyu Guan", "Shenlong Wang"], "title": "Demeter: A Parametric Model of Crop Plant Morphology from the Real World", "comment": "ICCV 2025", "summary": "Learning 3D parametric shape models of objects has gained popularity in\nvision and graphics and has showed broad utility in 3D reconstruction,\ngeneration, understanding, and simulation. While powerful models exist for\nhumans and animals, equally expressive approaches for modeling plants are\nlacking. In this work, we present Demeter, a data-driven parametric model that\nencodes key factors of a plant morphology, including topology, shape,\narticulation, and deformation into a compact learned representation. Unlike\nprevious parametric models, Demeter handles varying shape topology across\nvarious species and models three sources of shape variation: articulation,\nsubcomponent shape variation, and non-rigid deformation. To advance crop plant\nmodeling, we collected a large-scale, ground-truthed dataset from a soybean\nfarm as a testbed. Experiments show that Demeter effectively synthesizes\nshapes, reconstructs structures, and simulates biophysical processes. Code and\ndata is available at https://tianhang-cheng.github.io/Demeter/.", "AI": {"tldr": "Demeter\u662f\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u53c2\u6570\u5316\u690d\u7269\u5f62\u6001\u6a21\u578b\uff0c\u80fd\u591f\u7f16\u7801\u690d\u7269\u7684\u62d3\u6251\u7ed3\u6784\u3001\u5f62\u72b6\u3001\u5173\u8282\u548c\u53d8\u5f62\uff0c\u652f\u6301\u4e0d\u540c\u7269\u79cd\u7684\u62d3\u6251\u53d8\u5316\uff0c\u5e76\u5efa\u6a21\u4e09\u79cd\u5f62\u72b6\u53d8\u5316\u6765\u6e90\u3002", "motivation": "\u5f53\u524d\u867d\u7136\u5b58\u5728\u5f3a\u5927\u7684\u4eba\u7c7b\u548c\u52a8\u72693D\u53c2\u6570\u5316\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u540c\u6837\u8868\u8fbe\u529b\u7684\u690d\u7269\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86Demeter\u53c2\u6570\u5316\u6a21\u578b\uff0c\u7f16\u7801\u690d\u7269\u5f62\u6001\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5904\u7406\u4e0d\u540c\u7269\u79cd\u7684\u62d3\u6251\u53d8\u5316\uff0c\u5e76\u5efa\u6a21\u5173\u8282\u3001\u5b50\u7ec4\u4ef6\u5f62\u72b6\u53d8\u5316\u548c\u975e\u521a\u6027\u53d8\u5f62\u4e09\u79cd\u5f62\u72b6\u53d8\u5316\u6765\u6e90\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5927\u8c46\u519c\u573a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cDemeter\u80fd\u6709\u6548\u5408\u6210\u5f62\u72b6\u3001\u91cd\u5efa\u7ed3\u6784\u548c\u6a21\u62df\u751f\u7269\u7269\u7406\u8fc7\u7a0b\u3002", "conclusion": "Demeter\u4e3a\u690d\u7269\u5efa\u6a21\u63d0\u4f9b\u4e86\u8868\u8fbe\u529b\u5f3a\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5728\u4f5c\u7269\u690d\u7269\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16783", "abs": "https://arxiv.org/abs/2510.16783", "authors": ["Sheikh Jubair", "Arwa Omayrah", "Amal Alshammari", "Alhanoof Althnian", "Abdulhamed Alothaimen", "Norah A. Alzahrani", "Shahad D. Alzaidi", "Nora Al-Twairesh", "Abdulmohsen Al-Thubaity"], "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding", "comment": "1 figure, 15 tables, 10 main pages", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsophisticated capabilities, including the ability to process and comprehend\nextended contexts. These emergent capabilities necessitate rigorous evaluation\nmethods to effectively assess their performance in long-context understanding.\nIn this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation\nbenchmark designed to evaluate long-context understanding in English and\nArabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval\nintroduces four novel and challenging tasks: multi-document question answering,\nbilingual question answering, claim verification within a paragraph, and\nmultiple-choice questions based on long contexts. These tasks are designed to\nassess LLMs' abilities in deep reasoning, document comprehension, information\ntracing, and bilingual information extraction and understanding. The benchmark\nincludes datasets in both Arabic and English for each task, allowing for a\ncomparative analysis of their performance across different text genres.\nEvaluations were conducted on both open-weight and closed LLMs, with results\nindicating that LC-Eval presents significant challenges. Even high-performing\nmodels, such as GPT-4o, struggled with certain tasks, highlighting the\ncomplexity and rigor of the benchmark.", "AI": {"tldr": "LC-Eval\u662f\u4e00\u4e2a\u53cc\u8bed\u591a\u4efb\u52a1\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u6db5\u76d64k\u5230128k+token\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u968f\u7740LLM\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6709\u6548\u8bc4\u4f30\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u4e2a\u65b0\u9896\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff1a\u591a\u6587\u6863\u95ee\u7b54\u3001\u53cc\u8bed\u95ee\u7b54\u3001\u6bb5\u843d\u5185\u58f0\u660e\u9a8c\u8bc1\u548c\u57fa\u4e8e\u957f\u4e0a\u4e0b\u6587\u7684\u591a\u9009\u9898\uff0c\u5e76\u5728\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u4e2d\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90LLM\u7684\u8bc4\u4f30\u8868\u660e\uff0cLC-Eval\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u56f0\u96be\u3002", "conclusion": "LC-Eval\u57fa\u51c6\u7684\u590d\u6742\u6027\u548c\u4e25\u683c\u6027\u51f8\u663e\u4e86\u5f53\u524dLLM\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.16396", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16396", "abs": "https://arxiv.org/abs/2510.16396", "authors": ["Yeh Keng Hao", "Hsu Tzu Wei", "Sun Min"], "title": "SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation", "comment": "Accepted to AICCC 2025", "summary": "With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411AR/VR\u8fb9\u7f18\u8bbe\u5907\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u5377\u79ef\u3001SPLite\u89e3\u7801\u5668\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u5728Raspberry Pi 5\u4e0a\u5b9e\u73b02.98\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740AR/VR\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u5b9e\u65f6\u63a8\u7406\u3001\u4f4e\u529f\u8017\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u57fa\u4e8eResNet-18\u9aa8\u5e72\u7f51\u7edc\u5e94\u7528\u7a00\u758f\u5377\u79ef\uff0c\u63d0\u51faSPLite\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6765\u4f18\u5316\u6027\u80fd\u3002", "result": "\u7aef\u5230\u7aef\u6548\u7387\u63d0\u534742%\uff0c\u89e3\u7801\u5e27\u7387\u5728Raspberry Pi 5\u4e0a\u63d0\u53473.1\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\uff0c\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\uff08PA-MPJPE\u4ece9.0mm\u4ec5\u589e\u81f39.1mm\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8eAR/VR\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u65f6\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u3002"}}
{"id": "2510.16797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16797", "abs": "https://arxiv.org/abs/2510.16797", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "comment": null, "summary": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.", "AI": {"tldr": "\u63d0\u51faMOSAIC\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u57df\u9002\u5e94\u65b9\u6cd5\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u6709\u6548\u5c06\u901a\u7528\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u9002\u914d\u5230\u4e13\u4e1a\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u901a\u7528\u9886\u57df\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u9002\u5e94\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u4fdd\u6301\u539f\u6709\u6a21\u578b\u7684\u8bed\u4e49\u5224\u522b\u80fd\u529b\u540c\u65f6\u5b66\u4e60\u9886\u57df\u76f8\u5173\u77e5\u8bc6\u3002", "method": "\u591a\u9636\u6bb5\u57df\u9002\u5e94\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u9002\u5e94\u5b9e\u73b0\u9886\u57df\u76f8\u5173\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u9886\u57df\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0cNDCG@10\u6307\u6807\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe13.4%\u3002", "conclusion": "\u5e73\u8861\u7684\u8054\u5408\u76d1\u7763\u548c\u5206\u9636\u6bb5\u9002\u5e94\u7b56\u7565\u5bf9\u4e8e\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u7684\u57df\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0cMOSAIC\u6846\u67b6\u5728\u6b64\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.16410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16410", "abs": "https://arxiv.org/abs/2510.16410", "authors": ["Changyue Shi", "Minghao Chen", "Yiping Mao", "Chuxiao Yang", "Xinyuan Hu", "Jiajun Ding", "Zhou Yu"], "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting", "comment": null, "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.", "AI": {"tldr": "REALM\u662f\u4e00\u4e2a\u521b\u65b0\u7684MLLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u5b9e\u73b0\u57fa\u4e8e\u63a8\u7406\u7684\u5f00\u653e\u4e16\u754c3D\u5206\u5272\uff0c\u65e0\u9700\u5927\u91cf3D\u7279\u5b9a\u540e\u8bad\u7ec3\u3002", "motivation": "\u5f25\u5408\u590d\u6742\u4eba\u7c7b\u6307\u4ee4\u4e0e\u7cbe\u786e3D\u5bf9\u8c61\u5b9a\u4f4d\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u73b0\u67093D\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u89e3\u91ca\u6a21\u7cca\u7684\u57fa\u4e8e\u63a8\u7406\u7684\u6307\u4ee4\uff0c\u800c2D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u64c5\u957f\u63a8\u7406\u4f46\u7f3a\u4e4f\u5185\u57283D\u7a7a\u95f4\u7406\u89e3\u3002", "method": "\u76f4\u63a5\u57283D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u4e0a\u8fdb\u884c\u5206\u5272\uff0c\u5229\u7528\u5176\u6e32\u67d3\u903c\u771f\u65b0\u89c6\u56fe\u7684\u80fd\u529b\uff1b\u63d0\u51fa\u5168\u5c40\u5230\u5c40\u90e8\u7a7a\u95f4\u5b9a\u4f4d\u7b56\u7565\uff0c\u5148\u4f7f\u7528\u591a\u4e2a\u5168\u5c40\u89c6\u56fe\u8fdb\u884c\u7c97\u7c92\u5ea6\u5b9a\u4f4d\uff0c\u518d\u5408\u6210\u8fd1\u8ddd\u79bb\u65b0\u89c6\u56fe\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u5206\u5272\u3002", "result": "\u5728LERF\u30013D-OVS\u548c\u65b0\u5f15\u5165\u7684REALM3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREALM\u5728\u89e3\u91ca\u663e\u5f0f\u548c\u9690\u5f0f\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u652f\u6301\u5bf9\u8c61\u79fb\u9664\u3001\u66ff\u6362\u548c\u98ce\u683c\u8f6c\u6362\u7b493D\u4ea4\u4e92\u4efb\u52a1\u3002", "conclusion": "REALM\u6846\u67b6\u57283D\u5bf9\u8c61\u5206\u5272\u548c\u4ea4\u4e92\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u590d\u6742\u6307\u4ee4\u9a71\u52a8\u76843D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16815", "abs": "https://arxiv.org/abs/2510.16815", "authors": ["Hans Hergen Lehmann", "Jae Hee Lee", "Steven Schockaert", "Stefan Wermter"], "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities", "comment": "33 pages, 20 figures. Submitted ACL ARR 2025 October (under review)", "summary": "Large Language Models (LLMs) are increasingly used for knowledge-based\nreasoning tasks, yet understanding when they rely on genuine knowledge versus\nsuperficial heuristics remains challenging. We investigate this question\nthrough entity comparison tasks by asking models to compare entities along\nnumerical attributes (e.g., ``Which river is longer, the Danube or the\nNile?''), which offer clear ground truth for systematic analysis. Despite\nhaving sufficient numerical knowledge to answer correctly, LLMs frequently make\npredictions that contradict this knowledge. We identify three heuristic biases\nthat strongly influence model predictions: entity popularity, mention order,\nand semantic co-occurrence. For smaller models, a simple logistic regression\nusing only these surface cues predicts model choices more accurately than the\nmodel's own numerical predictions, suggesting heuristics largely override\nprincipled reasoning. Crucially, we find that larger models (32B parameters)\nselectively rely on numerical knowledge when it is more reliable, while smaller\nmodels (7--8B parameters) show no such discrimination, which explains why\nlarger models outperform smaller ones even when the smaller models possess more\naccurate knowledge. Chain-of-thought prompting steers all models towards using\nthe numerical features across all model sizes.", "AI": {"tldr": "LLMs\u5728\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\u4e2d\u7ecf\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u89c1\u800c\u975e\u771f\u5b9e\u77e5\u8bc6\uff0c\u5373\u4f7f\u5177\u5907\u6b63\u786e\u7b54\u6848\u6240\u9700\u7684\u77e5\u8bc6\u3002\u7814\u7a76\u53d1\u73b0\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u63d0\u53ca\u987a\u5e8f\u548c\u8bed\u4e49\u5171\u73b0\u4e09\u79cd\u542f\u53d1\u5f0f\u504f\u89c1\u4e25\u91cd\u5f71\u54cd\u9884\u6d4b\u3002\u5927\u6a21\u578b\u80fd\u9009\u62e9\u6027\u4f9d\u8d56\u66f4\u53ef\u9760\u7684\u77e5\u8bc6\uff0c\u800c\u5c0f\u6a21\u578b\u5219\u65e0\u6cd5\u533a\u5206\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4f55\u5927\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u5f15\u5bfc\u6240\u6709\u6a21\u578b\u4f7f\u7528\u6570\u503c\u7279\u5f81\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u4f55\u65f6\u4f9d\u8d56\u771f\u5b9e\u77e5\u8bc6\u800c\u975e\u8868\u9762\u542f\u53d1\u5f0f\uff0c\u901a\u8fc7\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\uff08\u5982\u6cb3\u6d41\u957f\u5ea6\u6bd4\u8f83\uff09\u6765\u7cfb\u7edf\u5206\u6790\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u4efb\u52a1\u6709\u660e\u786e\u7684\u4e8b\u5b9e\u6807\u51c6\u3002", "method": "\u4f7f\u7528\u5b9e\u4f53\u6bd4\u8f83\u4efb\u52a1\uff0c\u5206\u6790LLMs\u5728\u6570\u503c\u5c5e\u6027\u6bd4\u8f83\u4e2d\u7684\u8868\u73b0\u3002\u8bc6\u522b\u4e09\u79cd\u542f\u53d1\u5f0f\u504f\u89c1\uff1a\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u63d0\u53ca\u987a\u5e8f\u548c\u8bed\u4e49\u5171\u73b0\u3002\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff087-8B vs 32B\u53c2\u6570\uff09\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5e76\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u6548\u679c\u3002", "result": "\u5373\u4f7f\u5177\u5907\u6b63\u786e\u7b54\u6848\u6240\u9700\u7684\u77e5\u8bc6\uff0cLLMs\u4ecd\u7ecf\u5e38\u505a\u51fa\u4e0e\u77e5\u8bc6\u76f8\u77db\u76fe\u7684\u9884\u6d4b\u3002\u5c0f\u6a21\u578b\u4e2d\uff0c\u4ec5\u4f7f\u7528\u8868\u9762\u7ebf\u7d22\u7684\u903b\u8f91\u56de\u5f52\u6bd4\u6a21\u578b\u81ea\u8eab\u6570\u503c\u9884\u6d4b\u66f4\u51c6\u786e\uff0c\u8868\u660e\u542f\u53d1\u5f0f\u4e3b\u5bfc\u4e86\u539f\u5219\u6027\u63a8\u7406\u3002\u5927\u6a21\u578b\u80fd\u9009\u62e9\u6027\u4f9d\u8d56\u66f4\u53ef\u9760\u7684\u77e5\u8bc6\uff0c\u800c\u5c0f\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u5f15\u5bfc\u6240\u6709\u6a21\u578b\u4f7f\u7528\u6570\u503c\u7279\u5f81\u3002", "conclusion": "LLMs\u5728\u77e5\u8bc6\u63a8\u7406\u4e2d\u4e25\u91cd\u4f9d\u8d56\u542f\u53d1\u5f0f\u504f\u89c1\u800c\u975e\u771f\u5b9e\u77e5\u8bc6\uff0c\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u5176\u9009\u62e9\u6027\u4f9d\u8d56\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u662f\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.16416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16416", "abs": "https://arxiv.org/abs/2510.16416", "authors": ["Xiaojun Guo", "Runyu Zhou", "Yifei Wang", "Qi Zhang", "Chenheng Zhang", "Stefanie Jegelka", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Yisen Wang"], "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "comment": null, "summary": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86SSL4RL\u6846\u67b6\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u6269\u5c55\u548c\u53ef\u9760\u5956\u52b1\u673a\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6587\u672c\u6377\u5f84\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u5145\u5206\u5229\u7528\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u5956\u52b1\u673a\u5236\u800c\u96be\u4ee5\u5e94\u7528\u4e8eVLMs\u3002", "method": "\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff08\u5982\u56fe\u50cf\u65cb\u8f6c\u9884\u6d4b\u3001\u63a9\u7801\u8865\u4e01\u91cd\u5efa\uff09\u91cd\u65b0\u6784\u5efa\u4e3a\u5bc6\u96c6\u3001\u81ea\u52a8\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u504f\u597d\u6570\u636e\u6216\u4e0d\u53ef\u9760\u7684AI\u8bc4\u4f30\u5668\u3002", "result": "SSL4RL\u663e\u8457\u63d0\u5347\u4e86\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u5b9e\u9a8c\u786e\u5b9a\u4e86\u4efb\u52a1\u96be\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\u548c\u8bed\u4e49\u5bf9\u9f50\u7b49\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "SSL4RL\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u6709\u6548\u7684\u8303\u5f0f\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u81ea\u76d1\u7763\u76ee\u6807\u6765\u5bf9\u9f50\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5e76\u5728\u56fe\u5b66\u4e60\u9886\u57df\u4e5f\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.16819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16819", "abs": "https://arxiv.org/abs/2510.16819", "authors": ["Shantanu Agarwal", "Joel Barry", "Steven Fincke", "Scott Miller"], "title": "Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank", "comment": null, "summary": "Authorship attribution (AA) is the task of identifying the most likely author\nof a query document from a predefined set of candidate authors. We introduce a\ntwo-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.\nUnlike the field of information retrieval (IR), where retrieve-and-rerank is a\nde facto strategy, cross-genre AA systems must avoid relying on topical cues\nand instead learn to identify author-specific linguistic patterns that are\nindependent of the text's subject matter (genre/domain/topic). Consequently,\nfor the reranker, we demonstrate that training strategies commonly used in IR\nare fundamentally misaligned with cross-genre AA, leading to suboptimal\nbehavior. To address this, we introduce a targeted data curation strategy that\nenables the reranker to effectively learn author-discriminative signals. Using\nour LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of\n22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on\nHIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u68c0\u7d22-\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u7528\u4e8e\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\uff0c\u5728HIATUS\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u9700\u8981\u8bc6\u522b\u4e0e\u4e3b\u9898\u65e0\u5173\u7684\u4f5c\u8005\u7279\u5b9a\u8bed\u8a00\u6a21\u5f0f\uff0c\u800c\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e3b\u9898\u7ebf\u7d22\uff0c\u4e0d\u9002\u5408\u8be5\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u68c0\u7d22-\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u9488\u5bf9\u6027\u7684\u6570\u636e\u7b56\u5c55\u7b56\u7565\uff0c\u4f7f\u91cd\u6392\u5e8f\u5668\u80fd\u6709\u6548\u5b66\u4e60\u4f5c\u8005\u533a\u5206\u6027\u4fe1\u53f7\u3002", "result": "\u5728HIATUS\u7684HRS1\u548cHRS2\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u57fa\u51c6\u4e0a\uff0c\u5206\u522b\u53d6\u5f9722.3\u548c34.4\u7edd\u5bf9Success@8\u70b9\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684LLM\u68c0\u7d22-\u91cd\u6392\u5e8f\u7ba1\u9053\u5728\u8de8\u4f53\u88c1\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u4e13\u95e8\u8bad\u7ec3\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16438", "abs": "https://arxiv.org/abs/2510.16438", "authors": ["Aidyn Ubingazhibov", "R\u00e9mi Pautrat", "Iago Su\u00e1rez", "Shaohui Liu", "Marc Pollefeys", "Viktor Larsson"], "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching", "comment": "Accepted at ICCVW 2025", "summary": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick.", "AI": {"tldr": "LightGlueStick\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u70b9\u548c\u7ebf\u6bb5\u5339\u914d\u5668\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u7ebf\u6d88\u606f\u4f20\u9012\u673a\u5236\u663e\u5f0f\u66b4\u9732\u7ebf\u6bb5\u8fde\u901a\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u70b9\u548c\u7ebf\u5339\u914d\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u800cGlueStick\u867d\u7136\u80fd\u540c\u65f6\u5904\u7406\u70b9\u548c\u7ebf\uff0c\u4f46\u5176\u590d\u6742\u7684GNN\u67b6\u6784\u963b\u788d\u4e86\u5b9e\u65f6\u5e94\u7528\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u7ebf\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u663e\u5f0f\u66b4\u9732\u7ebf\u6bb5\u8fde\u901a\u6027\uff0c\u5b9e\u73b0\u8282\u70b9\u95f4\u9ad8\u6548\u901a\u4fe1\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u5339\u914d\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "LightGlueStick\u901a\u8fc7\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u7ebf\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u5339\u914d\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2510.16829", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16829", "abs": "https://arxiv.org/abs/2510.16829", "authors": ["Navreet Kaur", "Hoda Ayad", "Hayoung Jung", "Shravika Mittal", "Munmun De Choudhury", "Tanushree Mitra"], "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation", "comment": null, "summary": "Language model users often embed personal and social context in their\nquestions. The asker's role -- implicit in how the question is framed --\ncreates specific needs for an appropriate response. However, most evaluations,\nwhile capturing the model's capability to respond, often ignore who is asking.\nThis gap is especially critical in stigmatized domains such as opioid use\ndisorder (OUD), where accounting for users' contexts is essential to provide\naccessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for\nUser-centric Question Simulation), a framework for simulating role-based\nquestions. Drawing on role theory and posts from an online OUD recovery\ncommunity (r/OpiatesRecovery), we first build a taxonomy of asker roles --\npatients, caregivers, practitioners. Next, we use it to simulate 15,321\nquestions that embed each role's goals, behaviors, and experiences. Our\nevaluations show that these questions are both highly believable and comparable\nto real-world data. When used to evaluate five LLMs, for the same question but\ndiffering roles, we find systematic differences: vulnerable roles, such as\npatients and caregivers, elicit more supportive responses (+17%) and reduced\nknowledge content (-19%) in comparison to practitioners. Our work demonstrates\nhow implicitly signaling a user's role shapes model responses, and provides a\nmethodology for role-informed evaluation of conversational AI.", "AI": {"tldr": "CoRUS\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u95ee\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5728\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u7b49\u654f\u611f\u9886\u57df\u4e2d\uff0c\u63d0\u95ee\u8005\u7684\u89d2\u8272\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u56de\u590d\u7684\u5185\u5bb9\u548c\u8bed\u6c14\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5927\u591a\u5ffd\u89c6\u63d0\u95ee\u8005\u89d2\u8272\uff0c\u4f46\u5728\u654f\u611f\u9886\u57df\u5982\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u4e2d\uff0c\u8003\u8651\u7528\u6237\u80cc\u666f\u5bf9\u63d0\u4f9b\u65e0\u504f\u89c1\u3001\u53ef\u8bbf\u95ee\u7684\u56de\u7b54\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u89d2\u8272\u7406\u8bba\u548c\u5728\u7ebf\u5eb7\u590d\u793e\u533a\u6570\u636e\u6784\u5efa\u63d0\u95ee\u8005\u89d2\u8272\u5206\u7c7b\uff08\u60a3\u8005\u3001\u7167\u62a4\u8005\u3001\u4ece\u4e1a\u8005\uff09\uff0c\u5e76\u6a21\u62df\u751f\u621015,321\u4e2a\u89d2\u8272\u5316\u95ee\u9898\u7528\u4e8e\u8bc4\u4f30\u4e94\u4e2a\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u89d2\u8272\u6a21\u62df\u95ee\u9898\u53ef\u4fe1\u5ea6\u9ad8\u4e14\u4e0e\u73b0\u5b9e\u6570\u636e\u76f8\u5f53\u3002\u8106\u5f31\u89d2\u8272\uff08\u60a3\u8005\u3001\u7167\u62a4\u8005\uff09\u76f8\u6bd4\u4ece\u4e1a\u8005\u83b7\u5f97\u66f4\u591a\u652f\u6301\u6027\u56de\u590d\uff08+17%\uff09\u4f46\u77e5\u8bc6\u5185\u5bb9\u51cf\u5c11\uff08-19%\uff09\u3002", "conclusion": "\u7528\u6237\u89d2\u8272\u7684\u9690\u6027\u4fe1\u53f7\u4f1a\u7cfb\u7edf\u6027\u5730\u5f71\u54cd\u6a21\u578b\u56de\u590d\uff0cCoRUS\u4e3a\u5bf9\u8bddAI\u7684\u89d2\u8272\u77e5\u60c5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2510.16442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16442", "abs": "https://arxiv.org/abs/2510.16442", "authors": ["Haoran Sun", "Chen Cai", "Huiping Zhuang", "Kong Aik Lee", "Lap-Pui Chau", "Yi Wang"], "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning", "comment": null, "summary": "The rapid development of deepfake video technology has not only facilitated\nartistic creation but also made it easier to spread misinformation. Traditional\ndeepfake video detection (DVD) methods face issues such as a lack of\ntransparency in their principles and insufficient generalization capabilities\nto cope with evolving forgery techniques. This highlights an urgent need for\ndetectors that can identify forged content and provide verifiable reasoning\nexplanations. This paper proposes the explainable deepfake video detection\n(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model\n(MLLM) reasoning framework, which provides traceable reasoning processes\nalongside accurate detection results and trustworthy explanations. Our approach\nfirst incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)\nto extract and fuse global and local cross-frame deepfake features, providing\nrich spatio-temporal semantic information input for MLLM reasoning. Second, we\nconstruct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which\nintroduces facial feature data as hard constraints during the reasoning process\nto achieve pixel-level spatio-temporal video localization, suppress\nhallucinated outputs, and enhance the reliability of the chain of thought. In\naddition, we build an Explainable Reasoning FF++ benchmark dataset\n(ER-FF++set), leveraging structured data to annotate videos and ensure quality\ncontrol, thereby supporting dual supervision for reasoning and detection.\nExtensive experiments demonstrate that EDVD-LLaMA achieves outstanding\nperformance and robustness in terms of detection accuracy, explainability, and\nits ability to handle cross-forgery methods and cross-dataset scenarios.\nCompared to previous DVD methods, it provides a more explainable and superior\nsolution. The source code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53ef\u89e3\u91ca\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u4efb\u52a1\uff0c\u8bbe\u8ba1EDVD-LLaMA\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u7a7a\u7ec6\u7c92\u5ea6\u4fe1\u606f\u6807\u8bb0\u5316\u548c\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u673a\u5236\uff0c\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u53ef\u4fe1\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u80fd\u591f\u8bc6\u522b\u4f2a\u9020\u5185\u5bb9\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u63a8\u7406\u89e3\u91ca\u7684\u68c0\u6d4b\u5668\u3002", "method": "1. \u65f6\u7a7a\u7ec6\u7c92\u5ea6\u4fe1\u606f\u6807\u8bb0\u5316(ST-SIT)\u63d0\u53d6\u878d\u5408\u5168\u5c40\u5c40\u90e8\u8de8\u5e27\u7279\u5f81\uff1b2. \u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u601d\u7ef4\u94fe(Fg-MCoT)\u5f15\u5165\u9762\u90e8\u7279\u5f81\u4f5c\u4e3a\u786c\u7ea6\u675f\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u65f6\u7a7a\u5b9a\u4f4d\uff1b3. \u6784\u5efaER-FF++\u57fa\u51c6\u6570\u636e\u96c6\u652f\u6301\u53cc\u76d1\u7763\u3002", "result": "EDVD-LLaMA\u5728\u68c0\u6d4b\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u3001\u8de8\u4f2a\u9020\u65b9\u6cd5\u548c\u8de8\u6570\u636e\u96c6\u573a\u666f\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u8ffd\u6eaf\u63a8\u7406\u8fc7\u7a0b\u548c\u53ef\u4fe1\u89e3\u91ca\u7684\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2510.16844", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16844", "abs": "https://arxiv.org/abs/2510.16844", "authors": ["Jiajie Jin", "Yuyao Zhang", "Yimeng Xu", "Hongjin Qian", "Yutao Zhu", "Zhicheng Dou"], "title": "FinSight: Towards Real-World Financial Deep Research", "comment": "Working in progress", "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.", "AI": {"tldr": "FinSight\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7CAVM\u67b6\u6784\u3001\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u548c\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u8d22\u52a1\u62a5\u544a\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u751f\u6210\u4e13\u4e1a\u8d22\u52a1\u62a5\u544a\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u4e14\u667a\u529b\u8981\u6c42\u9ad8\u7684\u8fc7\u7a0b\uff0c\u73b0\u6709AI\u7cfb\u7edf\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u3001\u591a\u6a21\u6001\u8d22\u52a1\u62a5\u544a\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6838\u5fc3\u662fCAVM\u67b6\u6784\u7edf\u4e00\u5916\u90e8\u6570\u636e\u3001\u5de5\u5177\u548c\u667a\u80fd\u4f53\uff1b\u63d0\u51fa\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u9010\u6b65\u4f18\u5316\u539f\u59cb\u89c6\u89c9\u8f93\u51fa\uff1b\u4f7f\u7528\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\u5c06\u7b80\u6d01\u7684\u5206\u6790\u94fe\u6269\u5c55\u4e3a\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u62a5\u544a\u3002", "result": "\u5728\u5404\u79cd\u516c\u53f8\u548c\u884c\u4e1a\u7ea7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFinSight\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5305\u62ec\u9886\u5148\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u3002", "conclusion": "FinSight\u5c55\u793a\u4e86\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8d28\u91cf\u62a5\u544a\u7684\u6709\u6548\u8def\u5f84\uff0c\u901a\u8fc7\u5176\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u673a\u5236\u5b9e\u73b0\u4e86\u4e13\u4e1a\u7ea7\u8d22\u52a1\u62a5\u544a\u7684\u81ea\u52a8\u5316\u751f\u6210\u3002"}}
{"id": "2510.16444", "categories": ["cs.CV", "cs.MM", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16444", "abs": "https://arxiv.org/abs/2510.16444", "authors": ["Kunyu Peng", "Di Wen", "Jia Fu", "Jiamin Wu", "Kailun Yang", "Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Yuqian Fu", "Danda Pani Paudel", "Luc Van Gool", "Rainer Stiefelhagen"], "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba", "comment": "Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and\n  code are released at https://github.com/KPeng9510/refAVA2", "summary": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize\nfine-grained, atomic-level actions of a specific person of interest conditioned\non natural language descriptions. Distinct from conventional action recognition\nand detection tasks, RAVAR emphasizes precise language-guided action\nunderstanding, which is particularly critical for interactive human action\nanalysis in complex multi-person scenarios. In this work, we extend our\npreviously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million\nframes and >75.1k annotated persons in total. We benchmark this dataset using\nbaselines from multiple related domains, including atomic action localization,\nvideo question answering, and text-video retrieval, as well as our earlier\nmodel, RefAtomNet. Although RefAtomNet surpasses other baselines by\nincorporating agent attention to highlight salient features, its ability to\nalign and retrieve cross-modal information remains limited, leading to\nsuboptimal performance in localizing the target person and predicting\nfine-grained actions. To overcome the aforementioned limitations, we introduce\nRefAtomNet++, a novel framework that advances cross-modal token aggregation\nthrough a multi-hierarchical semantic-aligned cross-attention mechanism\ncombined with multi-trajectory Mamba modeling at the partial-keyword,\nscene-attribute, and holistic-sentence levels. In particular, scanning\ntrajectories are constructed by dynamically selecting the nearest visual\nspatial tokens at each timestep for both partial-keyword and scene-attribute\nlevels. Moreover, we design a multi-hierarchical semantic-aligned\ncross-attention strategy, enabling more effective aggregation of spatial and\ntemporal tokens across different semantic hierarchies. Experiments show that\nRefAtomNet++ establishes new state-of-the-art results. The dataset and code are\nreleased at https://github.com/KPeng9510/refAVA2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RefAtomNet++\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6307\u79f0\u539f\u5b50\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u591a\u8f68\u8ff9Mamba\u5efa\u6a21\uff0c\u5728RefAVA++\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RefAtomNet\u6a21\u578b\u5728\u8de8\u6a21\u6001\u4fe1\u606f\u5bf9\u9f50\u548c\u68c0\u7d22\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u5728\u5b9a\u4f4d\u76ee\u6807\u4eba\u7269\u548c\u9884\u6d4b\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u3002", "method": "\u63d0\u51faRefAtomNet++\u6846\u67b6\uff0c\u91c7\u7528\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u90e8\u5206\u5173\u952e\u8bcd\u3001\u573a\u666f\u5c5e\u6027\u548c\u6574\u4f53\u53e5\u5b50\u4e09\u4e2a\u5c42\u6b21\u7684\u591a\u8f68\u8ff9Mamba\u5efa\u6a21\uff0c\u52a8\u6001\u9009\u62e9\u6700\u8fd1\u89c6\u89c9\u7a7a\u95f4token\u6784\u5efa\u626b\u63cf\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRefAtomNet++\u5728RefAVA++\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u5305\u62ecRefAtomNet\u5728\u5185\u7684\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RefAtomNet++\u901a\u8fc7\u6539\u8fdb\u7684\u8de8\u6a21\u6001token\u805a\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u79f0\u539f\u5b50\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u591a\u4eba\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u5f0f\u4eba\u7c7b\u52a8\u4f5c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16851", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.16851", "abs": "https://arxiv.org/abs/2510.16851", "authors": ["Zhengqi Pei", "Qingming Huang", "Shuhui Wang"], "title": "Neuronal Group Communication for Efficient Neural representation", "comment": "28 pages, 2 figures", "summary": "The ever-increasing scale of modern neural networks has brought unprecedented\nperformance alongside daunting challenges in efficiency and interpretability.\nThis paper addresses the core question of how to build large neural systems\nthat learn efficient, modular, and interpretable representations. We propose\nNeuronal Group Communication (NGC), a theory-driven framework that reimagines a\nneural network as a dynamical system of interacting neuronal groups rather than\na monolithic collection of neural weights. Instead of treating each weight as\nan independent trainable parameter, NGC treats weights as transient\ninteractions between embedding-like neuronal states, with neural computation\nunfolding through iterative communication among groups of neurons. This\nlow-rank, modular representation yields compact models: groups of neurons\nexchange low-dimensional signals, enabling intra-group specialization and\ninter-group information sharing while dramatically reducing redundant\nparameters. By drawing on dynamical systems theory, we introduce a neuronal\nstability metric (analogous to Lyapunov stability) that quantifies the\ncontraction of neuron activations toward stable patterns during sequence\nprocessing. Using this metric, we reveal that emergent reasoning capabilities\ncorrespond to an external driving force or ``potential'', which nudges the\nneural dynamics away from trivial trajectories while preserving stability.\nEmpirically, we instantiate NGC in large language models (LLMs) and demonstrate\nimproved performance on complex reasoning benchmarks under moderate\ncompression. NGC consistently outperforms standard low-rank approximations and\ncross-layer basis-sharing methods at comparable compression rates. We conclude\nby discussing the broader implications of NGC, including how structured\nneuronal group dynamics might relate to generalization in high-dimensional\nlearning systems.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7fa4\u901a\u4fe1(NGC)\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u91cd\u6784\u4e3a\u795e\u7ecf\u7fa4\u4ea4\u4e92\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f4e\u7ef4\u4fe1\u53f7\u4ea4\u6362\u5b9e\u73b0\u53c2\u6570\u538b\u7f29\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u4f4e\u79e9\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u6269\u5927\u5e26\u6765\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u6784\u5efa\u9ad8\u6548\u3001\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5927\u89c4\u6a21\u795e\u7ecf\u8868\u793a\u7cfb\u7edf\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u795e\u7ecf\u7fa4\u4ea4\u4e92\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u6743\u91cd\u4f5c\u4e3a\u795e\u7ecf\u72b6\u6001\u95f4\u7684\u77ac\u65f6\u4ea4\u4e92\uff0c\u901a\u8fc7\u795e\u7ecf\u7fa4\u95f4\u7684\u8fed\u4ee3\u901a\u4fe1\u8fdb\u884c\u8ba1\u7b97\uff0c\u5f15\u5165\u795e\u7ecf\u7a33\u5b9a\u6027\u5ea6\u91cf\u6765\u91cf\u5316\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u7a33\u5b9a\u6a21\u5f0f\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0NGC\uff0c\u5728\u9002\u5ea6\u538b\u7f29\u4e0b\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5728\u76f8\u540c\u538b\u7f29\u7387\u4e0b\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u4f4e\u79e9\u8fd1\u4f3c\u548c\u8de8\u5c42\u57fa\u5171\u4eab\u65b9\u6cd5\u3002", "conclusion": "NGC\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u795e\u7ecf\u7fa4\u52a8\u6001\u53ef\u80fd\u5173\u8054\u9ad8\u7ef4\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.16445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16445", "abs": "https://arxiv.org/abs/2510.16445", "authors": ["Chien Thai", "Mai Xuan Trang", "Huong Ninh", "Hoang Hiep Ly", "Anh Son Le"], "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance", "comment": "Neurocomputing", "summary": "Detecting rotated objects accurately and efficiently is a significant\nchallenge in computer vision, particularly in applications such as aerial\nimagery, remote sensing, and autonomous driving. Although traditional object\ndetection frameworks are effective for axis-aligned objects, they often\nunderperform in scenarios involving rotated objects due to their limitations in\ncapturing orientation variations. This paper introduces an improved loss\nfunction aimed at enhancing detection accuracy and robustness by leveraging the\nGaussian bounding box representation and Bhattacharyya distance. In addition,\nwe advocate for the use of an anisotropic Gaussian representation to address\nthe issues associated with isotropic variance in square-like objects. Our\nproposed method addresses these challenges by incorporating a\nrotation-invariant loss function that effectively captures the geometric\nproperties of rotated objects. We integrate this proposed loss function into\nstate-of-the-art deep learning-based rotated object detection detectors, and\nextensive experiments demonstrated significant improvements in mean Average\nPrecision metrics compared to existing methods. The results highlight the\npotential of our approach to establish new benchmark in rotated object\ndetection, with implications for a wide range of applications requiring precise\nand reliable object localization irrespective of orientation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u9ad8\u65af\u8fb9\u754c\u6846\u8868\u793a\u548cBhattacharyya\u8ddd\u79bb\u6765\u63d0\u5347\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5404\u5411\u5f02\u6027\u9ad8\u65af\u8868\u793a\u89e3\u51b3\u65b9\u5f62\u7269\u4f53\u7684\u5404\u5411\u540c\u6027\u65b9\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u5728\u5904\u7406\u65cb\u8f6c\u76ee\u6807\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u822a\u7a7a\u5f71\u50cf\u3001\u9065\u611f\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\uff0c\u96be\u4ee5\u6709\u6548\u6355\u6349\u65b9\u5411\u53d8\u5316\u3002", "method": "\u5f15\u5165\u65cb\u8f6c\u4e0d\u53d8\u6027\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u9ad8\u65af\u8fb9\u754c\u6846\u8868\u793a\u548cBhattacharyya\u8ddd\u79bb\uff0c\u91c7\u7528\u5404\u5411\u5f02\u6027\u9ad8\u65af\u8868\u793a\u6765\u5904\u7406\u65b9\u5f62\u7269\u4f53\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u5668\u4e2d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u5e73\u5747\u7cbe\u5ea6\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u4e3a\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u5efa\u7acb\u65b0\u7684\u57fa\u51c6\uff0c\u5bf9\u9700\u8981\u7cbe\u786e\u53ef\u9760\u76ee\u6807\u5b9a\u4f4d\u7684\u5404\u79cd\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.16924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16924", "abs": "https://arxiv.org/abs/2510.16924", "authors": ["Zhihui Yang", "Yupei Wang", "Kaijie Mo", "Zhe Zhao", "Renfen Hu"], "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?", "comment": "Accepted to EMNLP 2025 (Findings). This version corrects a redundant\n  sentence in the Results section that appeared in the camera-ready version", "summary": "Despite significant progress in multimodal language models (LMs), it remains\nunclear whether visual grounding enhances their understanding of embodied\nknowledge compared to text-only models. To address this question, we propose a\nnovel embodied knowledge understanding benchmark based on the perceptual theory\nfrom psychology, encompassing visual, auditory, tactile, gustatory, olfactory\nexternal senses, and interoception. The benchmark assesses the models'\nperceptual abilities across different sensory modalities through vector\ncomparison and question-answering tasks with over 1,700 questions. By comparing\n30 state-of-the-art LMs, we surprisingly find that vision-language models\n(VLMs) do not outperform text-only models in either task. Moreover, the models\nperform significantly worse in the visual dimension compared to other sensory\ndimensions. Further analysis reveals that the vector representations are easily\ninfluenced by word form and frequency, and the models struggle to answer\nquestions involving spatial perception and reasoning. Our findings underscore\nthe need for more effective integration of embodied knowledge in LMs to enhance\ntheir understanding of the physical world.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8630\u4e2a\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u611f\u5b98\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f53\u73b0\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u5e76\u4e0d\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u4e0d\u6e05\u695a\u89c6\u89c9\u57fa\u7840\u662f\u5426\u6bd4\u7eaf\u6587\u672c\u6a21\u578b\u66f4\u597d\u5730\u589e\u5f3a\u4e86\u5bf9\u4f53\u73b0\u77e5\u8bc6\u7684\u7406\u89e3\u3002", "method": "\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u6784\u5efa\u4e86\u5305\u542b\u89c6\u89c9\u3001\u542c\u89c9\u3001\u89e6\u89c9\u3001\u5473\u89c9\u3001\u55c5\u89c9\u548c\u5185\u90e8\u611f\u89c9\u7684\u4f53\u73b0\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u901a\u8fc7\u5411\u91cf\u6bd4\u8f83\u548c\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u57281700\u591a\u4e2a\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u90fd\u4e0d\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\uff1b\u6a21\u578b\u5728\u89c6\u89c9\u7ef4\u5ea6\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u5176\u4ed6\u611f\u5b98\u7ef4\u5ea6\uff1b\u5411\u91cf\u8868\u793a\u6613\u53d7\u8bcd\u5f62\u548c\u9891\u7387\u5f71\u54cd\uff1b\u6a21\u578b\u5728\u6d89\u53ca\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u56f0\u96be\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u66f4\u6709\u6548\u5730\u5c06\u4f53\u73b0\u77e5\u8bc6\u6574\u5408\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5176\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.16446", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16446", "abs": "https://arxiv.org/abs/2510.16446", "authors": ["Jaekyun Park", "Hye Won Chung"], "title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion", "comment": "NeurIPS 2025", "summary": "In the era of large-scale foundation models, fully fine-tuning pretrained\nnetworks for each downstream task is often prohibitively resource-intensive.\nPrompt tuning offers a lightweight alternative by introducing tunable prompts\nwhile keeping the backbone frozen. However, existing visual prompt tuning\nmethods often fail to specialize the prompts or enrich the representation\nspace--especially when applied to self-supervised backbones. We show that these\nlimitations become especially pronounced in challenging tasks and data-scarce\nsettings, where effective adaptation is most critical. In this work, we\nintroduce VIPAMIN, a visual prompt initialization strategy that enhances\nadaptation of self-supervised models by (1) aligning prompts with semantically\ninformative regions in the embedding space, and (2) injecting novel\nrepresentational directions beyond the pretrained subspace. Despite its\nsimplicity--requiring only a single forward pass and lightweight\noperations--VIPAMIN consistently improves performance across diverse tasks and\ndataset sizes, setting a new state of the art in visual prompt tuning. Our code\nis available at https://github.com/iamjaekyun/vipamin.", "AI": {"tldr": "VIPAMIN\u662f\u4e00\u79cd\u89c6\u89c9\u63d0\u793a\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u63d0\u793a\u4e0e\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u533a\u57df\u5bf9\u9f50\uff0c\u5e76\u6ce8\u5165\u8d85\u51fa\u9884\u8bad\u7ec3\u5b50\u7a7a\u95f4\u7684\u65b0\u8868\u793a\u65b9\u5411\uff0c\u6765\u589e\u5f3a\u81ea\u76d1\u7763\u6a21\u578b\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\uff0c\u5b8c\u5168\u5fae\u8c03\u9884\u8bad\u7ec3\u7f51\u7edc\u5bf9\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u6765\u8bf4\u8d44\u6e90\u6d88\u8017\u5de8\u5927\u3002\u73b0\u6709\u7684\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u81ea\u76d1\u7763\u9aa8\u5e72\u7f51\u7edc\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u4e13\u95e8\u5316\u63d0\u793a\u6216\u4e30\u5bcc\u8868\u793a\u7a7a\u95f4\uff0c\u8fd9\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u548c\u6570\u636e\u7a00\u7f3a\u8bbe\u7f6e\u4e2d\u5c24\u4e3a\u660e\u663e\u3002", "method": "VIPAMIN\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u6b65\u9aa4\u589e\u5f3a\u81ea\u76d1\u7763\u6a21\u578b\u7684\u9002\u5e94\uff1a(1)\u5c06\u63d0\u793a\u4e0e\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u533a\u57df\u5bf9\u9f50\uff1b(2)\u6ce8\u5165\u8d85\u51fa\u9884\u8bad\u7ec3\u5b50\u7a7a\u95f4\u7684\u65b0\u8868\u793a\u65b9\u5411\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5355\u6b21\u524d\u5411\u4f20\u64ad\u548c\u8f7b\u91cf\u7ea7\u64cd\u4f5c\u3002", "result": "VIPAMIN\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u5927\u5c0f\u4e0a\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u5728\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u9886\u57df\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "VIPAMIN\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89c6\u89c9\u63d0\u793a\u521d\u59cb\u5316\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u589e\u5f3a\u81ea\u76d1\u7763\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u548c\u6570\u636e\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2510.16928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16928", "abs": "https://arxiv.org/abs/2510.16928", "authors": ["Emily Chang", "Niyati Bafna"], "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models", "comment": null, "summary": "Existing benchmarks for large language models (LLMs) are largely restricted\nto high- or mid-resource languages, and often evaluate performance on\nhigher-order tasks in reasoning and generation. However, plenty of evidence\npoints to the fact that LLMs lack basic linguistic competence in the vast\nmajority of the world's 3800+ written languages. We introduce ChiKhaPo,\nconsisting of 8 subtasks of varying difficulty designed to evaluate the lexical\ncomprehension and generation abilities of generative models. ChiKhaPo draws on\nexisting lexicons, monolingual data, and bitext, and provides coverage for\n2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of\nlanguage coverage. We further show that 6 SOTA models struggle on our\nbenchmark, and discuss the factors contributing to performance scores,\nincluding language family, language resourcedness, task, and comprehension\nversus generation directions. With ChiKhaPo, we hope to enable and encourage\nthe massively multilingual benchmarking of LLMs.", "AI": {"tldr": "ChiKhaPo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u96be\u5ea6\u4e0d\u540c\u7684\u5b50\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u57282700\u591a\u79cd\u8bed\u8a00\u4e2d\u7684\u8bcd\u6c47\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c40\u9650\u4e8e\u9ad8\u8d44\u6e90\u6216\u4e2d\u8d44\u6e90\u8bed\u8a00\uff0c\u4e14\u591a\u8bc4\u4f30\u63a8\u7406\u548c\u751f\u6210\u7b49\u9ad8\u9636\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5927\u91cf\u8bc1\u636e\u8868\u660eLLMs\u5728\u5168\u74033800\u591a\u79cd\u4e66\u9762\u8bed\u8a00\u4e2d\u7f3a\u4e4f\u57fa\u672c\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u8986\u76d6\u66f4\u5e7f\u6cdb\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5229\u7528\u73b0\u6709\u8bcd\u5178\u3001\u5355\u8bed\u6570\u636e\u548c\u53cc\u8bed\u5e73\u884c\u8bed\u6599\u5e93\u6784\u5efaChiKhaPo\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u5b50\u4efb\u52a1\uff0c\u6db5\u76d62700\u591a\u79cd\u8bed\u8a00\uff0c\u7279\u522b\u5173\u6ce8\u8bcd\u6c47\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u4e866\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728ChiKhaPo\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u5206\u6790\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u5305\u62ec\u8bed\u8a00\u5bb6\u65cf\u3001\u8bed\u8a00\u8d44\u6e90\u4e30\u5bcc\u5ea6\u3001\u4efb\u52a1\u7c7b\u578b\u4ee5\u53ca\u7406\u89e3\u4e0e\u751f\u6210\u65b9\u5411\u3002", "conclusion": "ChiKhaPo\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u5927\u89c4\u6a21\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u6709\u671b\u63a8\u52a8LLMs\u5728\u66f4\u591a\u8bed\u8a00\u4e0a\u7684\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2510.16450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16450", "abs": "https://arxiv.org/abs/2510.16450", "authors": ["Shan Xiong", "Jiabao Chen", "Ye Wang", "Jialin Peng"], "title": "Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy", "comment": null, "summary": "Annotation-efficient segmentation of the numerous mitochondria instances from\nvarious electron microscopy (EM) images is highly valuable for biological and\nneuroscience research. Although unsupervised domain adaptation (UDA) methods\ncan help mitigate domain shifts and reduce the high costs of annotating each\ndomain, they typically have relatively low performance in practical\napplications. Thus, we investigate weakly supervised domain adaptation (WDA)\nthat utilizes additional sparse point labels on the target domain, which\nrequire minimal annotation effort and minimal expert knowledge. To take full\nuse of the incomplete and imprecise point annotations, we introduce a multitask\nlearning framework that jointly conducts segmentation and center detection with\na novel cross-teaching mechanism and class-focused cross-domain contrastive\nlearning. While leveraging unlabeled image regions is essential, we introduce\nsegmentation self-training with a novel instance-aware pseudo-label (IPL)\nselection strategy. Unlike existing methods that typically rely on pixel-wise\npseudo-label filtering, the IPL semantically selects reliable and diverse\npseudo-labels with the help of the detection task. Comprehensive validations\nand comparisons on challenging datasets demonstrate that our method outperforms\nexisting UDA and WDA methods, significantly narrowing the performance gap with\nthe supervised upper bound. Furthermore, under the UDA setting, our method also\nachieves substantial improvements over other UDA techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u70b9\u6807\u6ce8\u5b9e\u73b0\u7ebf\u7c92\u4f53\u5206\u5272\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u5206\u5272\u548c\u4e2d\u5fc3\u68c0\u6d4b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7ebf\u7c92\u4f53\u5206\u5272\u7684\u6807\u6ce8\u6210\u672c\u9ad8\u95ee\u9898\uff0c\u4f20\u7edf\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u884c\u5206\u5272\u548c\u4e2d\u5fc3\u68c0\u6d4b\uff0c\u91c7\u7528\u4ea4\u53c9\u6559\u5b66\u673a\u5236\u548c\u7c7b\u805a\u7126\u8de8\u57df\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5b9e\u4f8b\u611f\u77e5\u7684\u4f2a\u6807\u7b7e\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u548c\u5f31\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5229\u7528\u7a00\u758f\u70b9\u6807\u6ce8\u548c\u4f2a\u6807\u7b7e\u9009\u62e9\u7b56\u7565\uff0c\u5728\u7ebf\u7c92\u4f53\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u8de8\u57df\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2510.16932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16932", "abs": "https://arxiv.org/abs/2510.16932", "authors": ["Emily Xiao", "Yixiao Zeng", "Ada Chen", "Chin-Jou Li", "Amanda Bertsch", "Graham Neubig"], "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs", "comment": null, "summary": "A popular method to adapt large language models (LLMs) to new tasks is\nin-context learning (ICL), which is effective but incurs high inference costs\nas context length grows. In this paper we propose a method to perform\ninstruction induction, where we take training examples and reduce them to a\ncompact but descriptive prompt that can achieve performance comparable to ICL\nover the full training set. Specifically, we propose PROMPT-MII, a\nreinforcement learning (RL) based framework to meta-learn an instruction\ninduction model that can generate compact instructions on the fly for an\narbitrary new dataset. We train on over 3,000 diverse classification datasets\nfrom the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves\ndownstream model quality by 4-9 F1 points (10-20% relative), matching ICL\nperformance while requiring 3-13x fewer tokens.", "AI": {"tldr": "\u63d0\u51faPROMPT-MII\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5143\u5b66\u4e60\u6307\u4ee4\u5f52\u7eb3\u6a21\u578b\uff0c\u751f\u6210\u7d27\u51d1\u6307\u4ee4\u66ff\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u867d\u7136\u6709\u6548\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u57283000\u591a\u4e2a\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u751f\u6210\u7d27\u51d1\u6307\u4ee4\u66ff\u4ee3\u5b8c\u6574\u8bad\u7ec3\u96c6\u3002", "result": "\u572890\u4e2a\u672a\u89c1\u4efb\u52a1\u4e0a\uff0c\u4e0b\u6e38\u6a21\u578b\u8d28\u91cf\u63d0\u53474-9 F1\u70b9\uff08\u76f8\u5bf9\u63d0\u534710-20%\uff09\uff0c\u5339\u914d\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u540c\u65f6\u51cf\u5c113-13\u500dtoken\u4f7f\u7528\u3002", "conclusion": "PROMPT-MII\u80fd\u591f\u6709\u6548\u66ff\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2510.16457", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16457", "abs": "https://arxiv.org/abs/2510.16457", "authors": ["Peiran Xu", "Xicheng Gong", "Yadong MU"], "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation", "comment": "ICCV 2025", "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language\nNavigation (VLN). Existing methods often make decisions based on historical\ninformation, overlooking the future implications and long-term outcomes of the\nactions. In contrast, we aim to develop a foresighted agent. Specifically, we\ndraw upon Q-learning to train a Q-model using large-scale unlabeled trajectory\ndata, in order to learn the general knowledge regarding the layout and object\nrelations within indoor scenes. This model can generate a Q-feature, analogous\nto the Q-value in traditional Q-network, for each candidate action, which\ndescribes the potential future information that may be observed after taking\nthe specific action. Subsequently, a cross-modal future encoder integrates the\ntask-agnostic Q-feature with navigation instructions to produce a set of action\nscores reflecting future prospects. These scores, when combined with the\noriginal scores based on history, facilitate an A*-style searching strategy to\neffectively explore the regions that are more likely to lead to the\ndestination. Extensive experiments conducted on widely used goal-oriented VLN\ndatasets validate the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u76ee\u6807\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7Q\u5b66\u4e60\u8bad\u7ec3Q\u6a21\u578b\u6765\u5b66\u4e60\u5ba4\u5185\u573a\u666f\u5e03\u5c40\u548c\u7269\u4f53\u5173\u7cfb\uff0c\u751f\u6210\u63cf\u8ff0\u6f5c\u5728\u672a\u6765\u4fe1\u606f\u7684Q\u7279\u5f81\uff0c\u7ed3\u5408\u5bfc\u822a\u6307\u4ee4\u8fdb\u884c\u524d\u77bb\u6027\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5386\u53f2\u4fe1\u606f\u505a\u51b3\u7b56\uff0c\u5ffd\u7565\u4e86\u884c\u52a8\u7684\u957f\u671f\u5f71\u54cd\u548c\u672a\u6765\u7ed3\u679c\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u524d\u77bb\u6027\u7684\u667a\u80fd\u4f53\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u8f68\u8ff9\u6570\u636e\u8bad\u7ec3Q\u6a21\u578b\u5b66\u4e60\u573a\u666f\u77e5\u8bc6\uff0c\u751f\u6210Q\u7279\u5f81\u63cf\u8ff0\u884c\u52a8\u540e\u7684\u6f5c\u5728\u672a\u6765\u4fe1\u606f\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u672a\u6765\u7f16\u7801\u5668\u7ed3\u5408\u5bfc\u822a\u6307\u4ee4\u751f\u6210\u53cd\u6620\u672a\u6765\u524d\u666f\u7684\u884c\u52a8\u5206\u6570\uff0c\u4e0e\u57fa\u4e8e\u5386\u53f2\u7684\u539f\u59cb\u5206\u6570\u7ed3\u5408\u5b9e\u73b0A*\u5f0f\u641c\u7d22\u7b56\u7565\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u76ee\u6807\u5bfc\u5411VLN\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u4fe1\u606f\u548c\u672a\u6765\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u76ee\u6807\u5bfc\u5411\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u3002"}}
{"id": "2510.16985", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16985", "abs": "https://arxiv.org/abs/2510.16985", "authors": ["Akif Islam", "Mohd Ruhul Ameen"], "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection", "comment": "Accepted to IEEE COMPAS 2025. 6 pages, 3 figures, 6 tables", "summary": "Bengali social media platforms have witnessed a sharp increase in hate\nspeech, disproportionately affecting women and adolescents. While datasets such\nas BD-SHS provide a basis for structured evaluation, most prior approaches rely\non either computationally costly full-model fine-tuning or proprietary APIs.\nThis paper presents the first application of Parameter-Efficient Fine-Tuning\n(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three\ninstruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and\nMistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated\ncomments. Each model was adapted by training fewer than 1% of its parameters,\nenabling experiments on a single consumer-grade GPU. The results show that\nLlama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at\n88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical\nand replicable strategy for Bengali and related low-resource languages.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u5e94\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u4f7f\u7528LoRA\u548cQLoRA\u6280\u672f\u5728\u4e09\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5728BD-SHS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad892.23%\u7684F1\u5206\u6570\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u793e\u4ea4\u5a92\u4f53\u4e2d\u4ec7\u6068\u8a00\u8bba\u6025\u5267\u589e\u52a0\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5987\u5973\u548c\u9752\u5c11\u5e74\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u5168\u6a21\u578b\u5fae\u8c03\uff0c\u8981\u4e48\u4f7f\u7528\u4e13\u6709API\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LoRA\u548cQLoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5728Gemma-3-4B\u3001Llama-3.2-3B\u548cMistral-7B\u4e09\u4e2a\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ec5\u8bad\u7ec3\u5c11\u4e8e1%\u7684\u53c2\u6570\uff0c\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u5b8c\u6210\u5b9e\u9a8c\u3002", "result": "Llama-3.2-3B\u83b7\u5f97\u6700\u9ad8F1\u5206\u657092.23%\uff0cMistral-7B\u4e3a88.94%\uff0cGemma-3-4B\u4e3a80.25%\uff0c\u8bc1\u660ePEFT\u65b9\u6cd5\u5728\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PEFT\u88ab\u786e\u7acb\u4e3a\u5b5f\u52a0\u62c9\u8bed\u53ca\u76f8\u5173\u4f4e\u8d44\u6e90\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7b56\u7565\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16463", "abs": "https://arxiv.org/abs/2510.16463", "authors": ["Haocheng Tang", "Ruoke Yan", "Xinhui Yin", "Qi Zhang", "Xinfeng Zhang", "Siwei Ma", "Wen Gao", "Chuanmin Jia"], "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars", "comment": "ACM International Conference on Multimedia 2025", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,\nphotorealistic rendering of dynamic 3D scenes, showing strong potential in\nimmersive communication. However, in digital human encoding and transmission,\nthe compression methods based on general 3DGS representations are limited by\nthe lack of human priors, resulting in suboptimal bitrate efficiency and\nreconstruction quality at the decoder side, which hinders their application in\nstreamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical\nGaussian Compression framework designed for efficient transmission and\nhigh-quality rendering of dynamic avatars. Our method disentangles the Gaussian\nrepresentation into a structural layer, which maps poses to Gaussians via a\nStyleUNet-based generator, and a motion layer, which leverages the SMPL-X model\nto represent temporal pose variations compactly and semantically. This\nhierarchical design supports layer-wise compression, progressive decoding, and\ncontrollable rendering from diverse pose inputs such as video sequences or\ntext. Since people are most concerned with facial realism, we incorporate a\nfacial attention mechanism during StyleUNet training to preserve identity and\nexpression details under low-bitrate constraints. Experimental results\ndemonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar\nrendering, while significantly outperforming prior methods in both visual\nquality and compression efficiency.", "AI": {"tldr": "HGC-Avatar\u662f\u4e00\u4e2a\u7528\u4e8e\u52a8\u60013D\u865a\u62df\u4eba\u9ad8\u6548\u4f20\u8f93\u548c\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u5206\u5c42\u9ad8\u65af\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5c42\u548c\u8fd0\u52a8\u5c42\u5206\u79bb\u8868\u793a\uff0c\u652f\u6301\u5206\u5c42\u538b\u7f29\u548c\u6e10\u8fdb\u89e3\u7801\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u538b\u7f29\u65b9\u6cd5\u7f3a\u4e4f\u4eba\u4f53\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6bd4\u7279\u7387\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5728\u53ef\u6d41\u5f0f3D\u865a\u62df\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u9ad8\u65af\u8868\u793a\u89e3\u8026\u4e3a\u7ed3\u6784\u5c42\uff08\u901a\u8fc7StyleUNet\u751f\u6210\u5668\u5c06\u59ff\u6001\u6620\u5c04\u5230\u9ad8\u65af\uff09\u548c\u8fd0\u52a8\u5c42\uff08\u5229\u7528SMPL-X\u6a21\u578b\u7d27\u51d1\u8868\u793a\u65f6\u5e8f\u59ff\u6001\u53d8\u5316\uff09\uff0c\u5e76\u5f15\u5165\u9762\u90e8\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aHGC-Avatar\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u538b\u7f29\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u53ef\u6d41\u5f0f\u7684\u5feb\u901f3D\u865a\u62df\u4eba\u6e32\u67d3\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "HGC-Avatar\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u548c\u4eba\u4f53\u5148\u9a8c\u77e5\u8bc6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u60013D\u865a\u62df\u4eba\u4f20\u8f93\u4e2d\u7684\u538b\u7f29\u6548\u7387\u548c\u6e32\u67d3\u8d28\u91cf\u95ee\u9898\u3002"}}
{"id": "2510.16987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16987", "abs": "https://arxiv.org/abs/2510.16987", "authors": ["Amit Moryossef", "Clara Meister", "Pavel Stepachev", "Desmond Elliott"], "title": "Back to Bytes: Revisiting Tokenization Through UTF-8", "comment": null, "summary": "We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text\nexactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding\n(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,\n2021; Pagnoni et al., 2025), our implementation never introduces out-of-range\nIDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior\n(e.g., padding, boundaries, conversation structure, attention segments, tool\ncalling, \"thinking\" spans, etc.) is encoded using C0 control bytes - just as\nASCII was originally designed to embed control information alongside printable\ntext. These design principles yield practical benefits: (1) faster tokenization\n(14x) and significantly lower host-device transfer (8x less than int64); (2)\nsimple, shareable 256*d embedding tables that can be aligned across models; and\n(3) a training-time enhancement via bit-biased embeddings, which exposes\nper-byte bit structure and can be added to the embedding table post-training,\nremoving inference costs. Our HuggingFace-compatible implementation improves\nlanguage modeling convergence.", "AI": {"tldr": "UTF8Tokenizer\u662f\u4e00\u4e2a\u6781\u7b80\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u5668\uff0c\u76f4\u63a5\u5c06\u6587\u672c\u6620\u5c04\u5230UTF-8\u7f16\u7801\u5bf9\u5e94\u7684\u5b57\u8282ID\uff0c\u4f7f\u7528C0\u63a7\u5236\u5b57\u8282\u7f16\u7801\u7279\u6b8a\u884c\u4e3a\uff0c\u63d0\u4f9b\u66f4\u5feb\u7684\u5206\u8bcd\u901f\u5ea6\u3001\u66f4\u5c0f\u7684\u4f20\u8f93\u5f00\u9500\u548c\u53ef\u5171\u4eab\u7684\u5d4c\u5165\u8868\u3002", "motivation": "\u73b0\u6709\u7684\u5b57\u8282\u7ea7\u5206\u8bcd\u65b9\u6cd5\u5b58\u5728\u8d85\u51fa\u8303\u56f4\u7684ID\u6216\u9700\u8981\u8f85\u52a9\u6807\u8bb0\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u4e2a\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u5206\u8bcd\u5668\uff0c\u5229\u7528ASCII\u539f\u59cb\u8bbe\u8ba1\u7406\u5ff5\u5c06\u63a7\u5236\u4fe1\u606f\u4e0e\u53ef\u6253\u5370\u6587\u672c\u4e00\u8d77\u7f16\u7801\u3002", "method": "\u91c7\u7528\u5b57\u8282\u7ea7\u5206\u8bcd\uff0c\u5c06\u6587\u672c\u7cbe\u786e\u6620\u5c04\u5230UTF-8\u7f16\u7801\u7684\u5b57\u8282ID\uff0c\u6240\u6709\u7279\u6b8a\u884c\u4e3a\uff08\u5982\u586b\u5145\u3001\u8fb9\u754c\u3001\u5bf9\u8bdd\u7ed3\u6784\u7b49\uff09\u90fd\u4f7f\u7528C0\u63a7\u5236\u5b57\u8282\u7f16\u7801\uff0c\u65e0\u9700\u989d\u5916\u6807\u8bb0\u3002", "result": "\u5b9e\u73b0\u4e8614\u500d\u7684\u5206\u8bcd\u52a0\u901f\u548c8\u500d\u4e8eint64\u7684\u4e3b\u673a-\u8bbe\u5907\u4f20\u8f93\u51cf\u5c11\uff0c\u63d0\u4f9b\u4e86\u53ef\u5171\u4eab\u7684256*d\u5d4c\u5165\u8868\uff0c\u5e76\u901a\u8fc7\u4f4d\u504f\u7f6e\u5d4c\u5165\u5728\u8bad\u7ec3\u65f6\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "UTF8Tokenizer\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7b80\u6d01\u7684\u5206\u8bcd\u65b9\u6848\uff0c\u6539\u8fdb\u4e86\u8bed\u8a00\u5efa\u6a21\u7684\u6536\u655b\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16505", "abs": "https://arxiv.org/abs/2510.16505", "authors": ["Lukas Selch", "Yufang Hou", "M. Jehanzeb Mirza", "Sivan Doveh", "James Glass", "Rogerio Feris", "Wei Lin"], "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies", "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.", "AI": {"tldr": "PRISMM-Bench\u662f\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5ba1\u7a3f\u4eba\u6807\u8bb0\u7684\u79d1\u5b66\u8bba\u6587\u4e0d\u4e00\u81f4\u6027\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5305\u542b262\u4e2a\u4e0d\u4e00\u81f4\u6848\u4f8b\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u8bc6\u522b\u3001\u7ea0\u6b63\u548c\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5ffd\u89c6\u4e86\u79d1\u5b66\u8bba\u6587\u4e2d\u8de8\u6587\u672c\u3001\u56fe\u8868\u3001\u65b9\u7a0b\u7684\u591a\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u79d1\u5b66\u7814\u7a76\u7684\u6e05\u6670\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\uff08\u5ba1\u7a3f\u6316\u6398\u3001LLM\u8f85\u52a9\u8fc7\u6ee4\u3001\u4eba\u5de5\u9a8c\u8bc1\uff09\u6784\u5efa\u57fa\u51c6\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u4efb\u52a1\uff08\u4e0d\u4e00\u81f4\u8bc6\u522b\u3001\u7ea0\u6b63\u3001\u914d\u5bf9\u5339\u914d\uff09\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u5316JSON\u7b54\u6848\u8868\u793a\u4ee5\u51cf\u5c11\u8bed\u8a00\u504f\u89c1\u3002", "result": "\u5bf921\u4e2a\u9886\u5148LMM\u7684\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u6781\u4f4e\uff0826.1-54.2%\uff09\uff0c\u5305\u62ec\u5927\u578b\u5f00\u6e90\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u79d1\u5b66\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u79d1\u5b66\u52a9\u624b\u3002"}}
{"id": "2510.17001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17001", "abs": "https://arxiv.org/abs/2510.17001", "authors": ["Yuval Reif", "Guy Kaplan", "Roy Schwartz"], "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic", "comment": null, "summary": "Large language models (LLMs) were shown to encode word form variations, such\nas \"walk\"->\"walked\", as linear directions in embedding space. However, standard\ntokenization algorithms treat these variations as distinct tokens -- filling\nthe size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\",\n\"Walk\"), at the expense of less frequent words and multilingual coverage. We\nshow that many of these variations can be captured by transformation vectors --\nadditive offsets that yield the appropriate word's representation when applied\nto the base form word embedding -- in both the input and output spaces.\nBuilding on this, we propose a compact reshaping of the vocabulary: rather than\nassigning unique tokens to each surface form, we compose them from shared base\nform and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We\napply our approach to multiple LLMs and across five languages, removing up to\n10% of vocabulary entries -- thereby freeing space to allocate new, more\ndiverse tokens. Importantly, we do so while also expanding vocabulary coverage\nto out-of-vocabulary words, with minimal impact on downstream performance, and\nwithout modifying model weights. Our findings motivate a foundational\nrethinking of vocabulary design, moving from string enumeration to a\ncompositional vocabulary that leverages the underlying structure of language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bcd\u6c47\u8868\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bcd\u5f62\u53d8\u5316\u8868\u793a\u4e3a\u8f6c\u6362\u5411\u91cf\u800c\u975e\u72ec\u7acbtoken\uff0c\u53ef\u51cf\u5c1110%\u8bcd\u6c47\u8868\u5927\u5c0f\uff0c\u540c\u65f6\u6269\u5c55\u8bcd\u6c47\u8986\u76d6\u8303\u56f4\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u5206\u8bcd\u7b97\u6cd5\u5c06\u8bcd\u5f62\u53d8\u5316\u89c6\u4e3a\u72ec\u7acbtoken\uff0c\u5bfc\u81f4\u8bcd\u6c47\u8868\u88ab\u8868\u9762\u5f62\u5f0f\u53d8\u4f53\u586b\u6ee1\uff0c\u727a\u7272\u4e86\u4f4e\u9891\u8bcd\u548c\u591a\u8bed\u8a00\u8986\u76d6\u3002\u9700\u8981\u66f4\u7d27\u51d1\u7684\u8bcd\u6c47\u8868\u8bbe\u8ba1\u6765\u5229\u7528\u8bed\u8a00\u7684\u5185\u5728\u7ed3\u6784\u3002", "method": "\u5c06\u8bcd\u5f62\u53d8\u5316\u5efa\u6a21\u4e3a\u8f6c\u6362\u5411\u91cf\uff08\u52a0\u6027\u504f\u79fb\uff09\uff0c\u5728\u8f93\u5165\u548c\u8f93\u51fa\u7a7a\u95f4\u4e2d\u6784\u5efa\u7ec4\u5408\u5f0f\u8bcd\u6c47\u8868\uff0c\u4f7f\u7528\u5171\u4eab\u7684\u57fa\u7840\u5f62\u5f0f\u548c\u8f6c\u6362\u5411\u91cf\u6765\u7ec4\u5408\u8868\u9762\u5f62\u5f0f\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u4e94\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\uff0c\u53ef\u79fb\u9664\u9ad8\u8fbe10%\u7684\u8bcd\u6c47\u6761\u76ee\uff0c\u6269\u5c55\u8bcd\u6c47\u8986\u76d6\u8303\u56f4\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002", "conclusion": "\u8bcd\u6c47\u8868\u8bbe\u8ba1\u5e94\u4ece\u5b57\u7b26\u4e32\u679a\u4e3e\u8f6c\u5411\u7ec4\u5408\u5f0f\u8bcd\u6c47\u8868\uff0c\u5145\u5206\u5229\u7528\u8bed\u8a00\u7684\u5185\u5728\u7ed3\u6784\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bcd\u6c47\u8868\u793a\u3002"}}
{"id": "2510.16508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16508", "abs": "https://arxiv.org/abs/2510.16508", "authors": ["Franko \u0160iki\u0107", "Sven Lon\u010dari\u0107"], "title": "OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks", "comment": null, "summary": "Out-of-stock (OOS) detection is a very important retail verification process\nthat aims to infer the unavailability of products in their designated areas on\nthe shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based\nmethod that advances OOS detection through auxiliary learning. In particular,\nwe extend a well-established YOLOv8 object detection architecture with\nadditional convolutional branches to simultaneously detect OOS, segment\nproducts, and estimate scene depth. While OOS detection and product\nsegmentation branches are trained using ground truth data, the depth estimation\nbranch is trained using pseudo-labeled annotations produced by the\nstate-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,\nsince the aforementioned pseudo-labeled depth estimates display relative depth,\nwe propose an appropriate depth normalization procedure that stabilizes the\ntraining process. The experimental results show that the proposed method\nsurpassed the performance of the SOTA OOS detection methods by 1.8% of the mean\naverage precision (mAP). In addition, ablation studies confirm the\neffectiveness of auxiliary learning and the proposed depth normalization\nprocedure, with the former increasing mAP by 3.7% and the latter by 4.2%.", "AI": {"tldr": "\u63d0\u51faOOS-DSD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u5b66\u4e60\u6539\u8fdb\u7f3a\u8d27\u68c0\u6d4b\uff0c\u5728YOLOv8\u57fa\u7840\u4e0a\u589e\u52a0\u4ea7\u54c1\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u5206\u652f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd51.8% mAP\u3002", "motivation": "\u7f3a\u8d27\u68c0\u6d4b\u662f\u96f6\u552e\u9a8c\u8bc1\u7684\u91cd\u8981\u8fc7\u7a0b\uff0c\u9700\u8981\u51c6\u786e\u63a8\u65ad\u8d27\u67b6\u4e0a\u4ea7\u54c1\u7684\u4e0d\u53ef\u7528\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u6269\u5c55YOLOv8\u67b6\u6784\uff0c\u589e\u52a0\u5377\u79ef\u5206\u652f\u540c\u65f6\u8fdb\u884c\u7f3a\u8d27\u68c0\u6d4b\u3001\u4ea7\u54c1\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u3002\u6df1\u5ea6\u5206\u652f\u4f7f\u7528Depth Anything V2\u7684\u4f2a\u6807\u7b7e\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u6df1\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u6027\u80fd\u63d0\u53471.8% mAP\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u8f85\u52a9\u5b66\u4e60\u63d0\u53473.7% mAP\uff0c\u6df1\u5ea6\u5f52\u4e00\u5316\u63d0\u53474.2% mAP\u3002", "conclusion": "OOS-DSD\u901a\u8fc7\u591a\u4efb\u52a1\u8f85\u52a9\u5b66\u4e60\u548c\u6df1\u5ea6\u5f52\u4e00\u5316\u6709\u6548\u63d0\u5347\u4e86\u7f3a\u8d27\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8f85\u52a9\u5b66\u4e60\u5728\u96f6\u552e\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.17006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17006", "abs": "https://arxiv.org/abs/2510.17006", "authors": ["Masahiro Kaneko", "Zeerak Talat", "Timothy Baldwin"], "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization", "comment": null, "summary": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u5bf9\u6297\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u4f7f\u7528Past-Direction Gradient Damping\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5728\u4e09\u4e2aLLM\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u65e0\u6cd5\u4e3b\u52a8\u7834\u574f\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\u7684\u52a8\u6001\u8bd5\u9519\u5faa\u73af\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u66f4\u65b0\u9632\u5fa1\u7b56\u7565\u7684\u4e3b\u52a8\u9632\u5fa1\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u4f18\u5316\u63d0\u793a\uff0c\u786e\u4fdd\u65e0\u5bb3\u4efb\u52a1\u7684\u9002\u5f53\u54cd\u5e94\u540c\u65f6\u660e\u786e\u62d2\u7edd\u6709\u5bb3\u63d0\u793a\uff0c\u5f15\u5165Past-Direction Gradient Damping\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e09\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4e94\u79cd\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u6297\u4e94\u79cd\u8fed\u4ee3\u8d8a\u72f1\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u65e0\u5bb3\u4efb\u52a1\u7684\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u9632\u5fa1\u6846\u67b6\u6709\u6548\u5bf9\u6297\u8fed\u4ee3\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65e0\u5bb3\u4efb\u52a1\u7684\u826f\u597d\u54cd\u5e94\u80fd\u529b\uff0c\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u9632\u5fa1\u601d\u8def\u3002"}}
{"id": "2510.16514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16514", "abs": "https://arxiv.org/abs/2510.16514", "authors": ["Duygu Sap", "Martin Lotz", "Connor Mattinson"], "title": "Image Categorization and Search via a GAT Autoencoder and Representative Models", "comment": "10 pages, 22 figures, Under review", "summary": "We propose a method for image categorization and retrieval that leverages\ngraphs and a graph attention network (GAT)-based autoencoder. Our approach is\nrepresentative-centric, that is, we execute the categorization and retrieval\nprocess via the representative models we construct for the images and image\ncategories. We utilize a graph where nodes represent images (or their\nrepresentatives) and edges capture similarity relationships. GAT highlights\nimportant features and relationships between images, enabling the autoencoder\nto construct context-aware latent representations that capture the key features\nof each image relative to its neighbors. We obtain category representatives\nfrom these embeddings and categorize a query image by comparing its\nrepresentative to the category representatives. We then retrieve the most\nsimilar image to the query image within its identified category. We demonstrate\nthe effectiveness of our representative-centric approach through experiments\nwith both the GAT autoencoders and standard feature-based techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u81ea\u7f16\u7801\u5668\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u56fe\u50cf\u548c\u7c7b\u522b\u7684\u4ee3\u8868\u6027\u6a21\u578b\u6765\u8fdb\u884c\u5206\u7c7b\u548c\u68c0\u7d22\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u4ee3\u8868\u6027\u6a21\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u7ed3\u6784\u6355\u6349\u56fe\u50cf\u95f4\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7a81\u51fa\u91cd\u8981\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u56fe\u7ed3\u6784\u8868\u793a\u56fe\u50cf\u53ca\u5176\u76f8\u4f3c\u5173\u7cfb\uff0c\u5e94\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u81ea\u7f16\u7801\u5668\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6f5c\u5728\u8868\u793a\uff0c\u4ece\u4e2d\u63d0\u53d6\u7c7b\u522b\u4ee3\u8868\uff0c\u901a\u8fc7\u6bd4\u8f83\u67e5\u8be2\u56fe\u50cf\u4ee3\u8868\u4e0e\u7c7b\u522b\u4ee3\u8868\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5728\u6240\u5c5e\u7c7b\u522b\u4e2d\u68c0\u7d22\u6700\u76f8\u4f3c\u56fe\u50cf\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u4ee3\u8868\u6027\u4e2d\u5fc3\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u4e86\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u81ea\u7f16\u7801\u5668\u4e0e\u6807\u51c6\u57fa\u4e8e\u7279\u5f81\u7684\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u4ee3\u8868\u6027\u4e2d\u5fc3\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u80fd\u591f\u6784\u5efa\u6355\u6349\u5173\u952e\u7279\u5f81\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u3002"}}
{"id": "2510.17013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17013", "abs": "https://arxiv.org/abs/2510.17013", "authors": ["Lanni Bu", "Lauren Levin", "Amir Zeldes"], "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking", "comment": null, "summary": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiscoTrack\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9488\u5bf912\u79cd\u8bed\u8a00\u7684\u8bed\u7bc7\u7406\u89e3\u4efb\u52a1\uff0c\u5305\u62ec\u663e\u7740\u6027\u8bc6\u522b\u3001\u5b9e\u4f53\u8ffd\u8e2a\u3001\u8bed\u7bc7\u5173\u7cfb\u548c\u6865\u63a5\u63a8\u7406\uff0c\u8bc4\u4f30\u663e\u793a\u8fd9\u4e9b\u4efb\u52a1\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u663e\u6027\u4fe1\u606f\u63d0\u53d6\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8bed\u7bc7\u8ddf\u8e2a\u4e2d\u9690\u542b\u4fe1\u606f\u548c\u8bed\u7528\u63a8\u7406\u7684\u591a\u8bed\u8a00\u6311\u6218\u6027\u57fa\u51c6\u3002", "method": "\u5f00\u53d1DiscoTrack\u57fa\u51c6\uff0c\u6db5\u76d612\u79cd\u8bed\u8a00\u548c\u56db\u4e2a\u8bed\u7bc7\u7406\u89e3\u5c42\u6b21\uff1a\u663e\u7740\u6027\u8bc6\u522b\u3001\u5b9e\u4f53\u8ffd\u8e2a\u3001\u8bed\u7bc7\u5173\u7cfb\u548c\u6865\u63a5\u63a8\u7406\u3002", "result": "\u8bc4\u4f30\u8868\u660e\u8fd9\u4e9b\u8bed\u7bc7\u7406\u89e3\u4efb\u52a1\u5373\u4f7f\u5bf9\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "DiscoTrack\u586b\u8865\u4e86\u591a\u8bed\u8a00\u8bed\u7bc7\u7406\u89e3\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u5728\u590d\u6742\u8bed\u7bc7\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.16540", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16540", "abs": "https://arxiv.org/abs/2510.16540", "authors": ["Jihoon Kwon", "Kyle Min", "Jy-yong Sohn"], "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "comment": "Accepted at NeurIPS 2025 (poster). This is the camera-ready version", "summary": "Despite recent advances, vision-language models trained with standard\ncontrastive objectives still struggle with compositional reasoning -- the\nability to understand structured relationships between visual and linguistic\nelements. This shortcoming is largely due to the tendency of the text encoder\nto focus on individual words rather than their relations, a limitation\nreinforced by contrastive training that primarily aligns words with visual\nobjects. In this paper, we introduce REconstruction and Alignment of text\nDescriptions (READ), a fine-tuning method designed to enhance compositional\nreasoning by adding two auxiliary objectives to the contrastive learning: (1) a\ntoken-level reconstruction objective, where a frozen pre-trained decoder\nreconstructs alternative captions based on the embedding of the original\ncaption; and (2) a sentence-level alignment objective, which explicitly aligns\nparaphrased sentences in the embedding space. We show that READ-CLIP, a model\nderived by applying the READ method to the pre-trained CLIP model, achieves the\nstate-of-the-art performance across five major compositional reasoning\nbenchmarks, outperforming the strongest conventional fine-tuning baseline by up\nto 4.1%. Furthermore, applying the READ to existing CLIP variants (including\nNegCLIP and FSC-CLIP) also improves performance on these benchmarks.\nQuantitative and qualitative analyses reveal that our proposed objectives --\nreconstruction and alignment -- offer complementary benefits: the former\nencourages the encoder to capture relationships between words within a caption,\nwhile the latter ensures consistent representations for paraphrases expressed\nwith different wording.", "AI": {"tldr": "READ\u662f\u4e00\u79cd\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0token\u7ea7\u91cd\u5efa\u548c\u53e5\u5b50\u7ea7\u5bf9\u9f50\u4e24\u4e2a\u8f85\u52a9\u76ee\u6807\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u5bf9\u6bd4\u8bad\u7ec3\u7684\u76ee\u6807\u51fd\u6570\u4f7f\u6587\u672c\u7f16\u7801\u5668\u503e\u5411\u4e8e\u5173\u6ce8\u5355\u4e2a\u5355\u8bcd\u800c\u975e\u5176\u5173\u7cfb\uff0c\u5bfc\u81f4\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "READ\u65b9\u6cd5\u5728\u5bf9\u6bd4\u5b66\u4e60\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e24\u4e2a\u8f85\u52a9\u76ee\u6807\uff1a(1) token\u7ea7\u91cd\u5efa\u76ee\u6807\uff1a\u4f7f\u7528\u51bb\u7ed3\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u57fa\u4e8e\u539f\u59cb\u6807\u9898\u5d4c\u5165\u91cd\u5efa\u66ff\u4ee3\u6807\u9898\uff1b(2) \u53e5\u5b50\u7ea7\u5bf9\u9f50\u76ee\u6807\uff1a\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u663e\u5f0f\u5bf9\u9f50\u540c\u4e49\u53e5\u3002", "result": "READ-CLIP\u5728\u4e94\u4e2a\u4e3b\u8981\u7ec4\u5408\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u6700\u5f3a\u4f20\u7edf\u5fae\u8c03\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe4.1%\u3002\u5e94\u7528\u4e8e\u73b0\u6709CLIP\u53d8\u4f53\u4e5f\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u91cd\u5efa\u548c\u5bf9\u9f50\u76ee\u6807\u63d0\u4f9b\u4e92\u8865\u4f18\u52bf\uff1a\u91cd\u5efa\u9f13\u52b1\u7f16\u7801\u5668\u6355\u6349\u6807\u9898\u5185\u5355\u8bcd\u95f4\u5173\u7cfb\uff0c\u5bf9\u9f50\u786e\u4fdd\u4e0d\u540c\u8868\u8fbe\u65b9\u5f0f\u7684\u540c\u4e49\u53e5\u5177\u6709\u4e00\u81f4\u8868\u793a\u3002"}}
{"id": "2510.17017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17017", "abs": "https://arxiv.org/abs/2510.17017", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "comment": "Code: https://github.com/ZQS1943/SafeSearch", "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked ``How can I track\nsomeone's location without their consent?'', a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u6bd4\u57fa\u7840LLM\u66f4\u5bb9\u6613\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff0c\u63d0\u51faSafeSearch\u65b9\u6cd5\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u540c\u65f6\u4f18\u5316\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u5458\u4e3b\u8981\u5173\u6ce8\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u5176\u5b89\u5168\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\u641c\u7d22\u4ee3\u7406\u5728\u5904\u7406\u6709\u5bb3\u95ee\u9898\u65f6\u66f4\u5bb9\u6613\u964d\u4f4e\u62d2\u7edd\u9608\u503c\uff0c\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\u3002", "method": "\u63d0\u51faSafeSearch\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u6700\u7ec8\u8f93\u51fa\u7684\u5b89\u5168/\u5b9e\u7528\u6027\u5956\u52b1\u548c\u67e5\u8be2\u7ea7\u522b\u7684\u60e9\u7f5a/\u5956\u52b1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSafeSearch\u5728\u4e09\u4e2a\u7ea2\u961f\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u4ee3\u7406\u6709\u5bb3\u6027\u8d85\u8fc770%\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6709\u7528\u7684\u54cd\u5e94\uff0c\u95ee\u7b54\u6027\u80fd\u4e0e\u4ec5\u4f18\u5316\u5b9e\u7528\u6027\u7684\u4ee3\u7406\u76f8\u5f53\u3002", "conclusion": "\u67e5\u8be2\u7ea7\u522b\u7684\u5956\u52b1\u673a\u5236\u80fd\u6709\u6548\u8054\u5408\u63d0\u5347\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u591a\u76ee\u6807\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16541", "abs": "https://arxiv.org/abs/2510.16541", "authors": ["Binyuan Huang", "Yongdong Luo", "Xianda Guo", "Xiawu Zheng", "Zheng Zhu", "Jiahui Pan", "Chengju Zhou"], "title": "Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition", "comment": null, "summary": "Deep learning-based gait recognition has achieved great success in various\napplications. The key to accurate gait recognition lies in considering the\nunique and diverse behavior patterns in different motion regions, especially\nwhen covariates affect visual appearance. However, existing methods typically\nuse predefined regions for temporal modeling, with fixed or equivalent temporal\nscales assigned to different types of regions, which makes it difficult to\nmodel motion regions that change dynamically over time and adapt to their\nspecific patterns. To tackle this problem, we introduce a Region-aware Dynamic\nAggregation and Excitation framework (GaitRDAE) that automatically searches for\nmotion regions, assigns adaptive temporal scales and applies corresponding\nattention. Specifically, the framework includes two core modules: the\nRegion-aware Dynamic Aggregation (RDA) module, which dynamically searches the\noptimal temporal receptive field for each region, and the Region-aware Dynamic\nExcitation (RDE) module, which emphasizes the learning of motion regions\ncontaining more stable behavior patterns while suppressing attention to static\nregions that are more susceptible to covariates. Experimental results show that\nGaitRDAE achieves state-of-the-art performance on several benchmark datasets.", "AI": {"tldr": "\u63d0\u51faGaitRDAE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u8fd0\u52a8\u533a\u57df\u5e76\u5206\u914d\u81ea\u9002\u5e94\u65f6\u95f4\u5c3a\u5ea6\u6765\u6539\u8fdb\u6b65\u6001\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u65f6\u95f4\u5c3a\u5ea6\u96be\u4ee5\u5efa\u6a21\u52a8\u6001\u53d8\u5316\u8fd0\u52a8\u533a\u57df\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u4f7f\u7528\u9884\u5b9a\u4e49\u533a\u57df\u548c\u56fa\u5b9a\u65f6\u95f4\u5c3a\u5ea6\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\uff0c\u96be\u4ee5\u5904\u7406\u968f\u65f6\u95f4\u52a8\u6001\u53d8\u5316\u7684\u8fd0\u52a8\u533a\u57df\uff0c\u4e5f\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u533a\u57df\u7684\u7279\u5b9a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u533a\u57df\u611f\u77e5\u52a8\u6001\u805a\u5408\u4e0e\u6fc0\u52b1\u6846\u67b6(GaitRDAE)\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aRDA\u6a21\u5757\u52a8\u6001\u641c\u7d22\u6bcf\u4e2a\u533a\u57df\u7684\u6700\u4f73\u65f6\u95f4\u611f\u53d7\u91ce\uff0cRDE\u6a21\u5757\u5f3a\u8c03\u5b66\u4e60\u5305\u542b\u7a33\u5b9a\u884c\u4e3a\u6a21\u5f0f\u7684\u8fd0\u52a8\u533a\u57df\uff0c\u6291\u5236\u5bf9\u6613\u53d7\u534f\u53d8\u91cf\u5f71\u54cd\u7684\u9759\u6001\u533a\u57df\u7684\u5173\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGaitRDAE\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GaitRDAE\u901a\u8fc7\u52a8\u6001\u533a\u57df\u641c\u7d22\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5c3a\u5ea6\u5206\u914d\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6b65\u6001\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u52a8\u6001\u53d8\u5316\u8fd0\u52a8\u533a\u57df\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17018", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17018", "abs": "https://arxiv.org/abs/2510.17018", "authors": ["Noor Islam S. Mohammad"], "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification", "comment": null, "summary": "Toxic comment detection remains a challenging task, where transformer-based\nmodels (e.g., BERT) incur high computational costs and degrade on minority\ntoxicity classes, while classical ensembles lack semantic adaptability. We\npropose xLSTM, a parameter-efficient and theoretically grounded framework that\nunifies cosine-similarity gating, adaptive feature prioritization, and\nprincipled class rebalancing. A learnable reference vector {v} in {R}^d\nmodulates contextual embeddings via cosine similarity, amplifying toxic cues\nand attenuating benign signals to yield stronger gradients under severe class\nimbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)\nthrough a projection layer, a character-level BiLSTM for morphological cues,\nembedding-space SMOTE for minority augmentation, and adaptive focal loss with\ndynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains\n96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%\non identity_hate categories, with 15 times fewer parameters and 50ms inference\nlatency. Cosine gating contributes a +4.8% F1 gain in ablations. The results\nestablish a new efficiency adaptability frontier, demonstrating that\nlightweight, theoretically informed architectures can surpass large pretrained\nmodels on imbalanced, domain-specific NLP tasks.", "AI": {"tldr": "\u63d0\u51faxLSTM\u6846\u67b6\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u95e8\u63a7\u3001\u81ea\u9002\u5e94\u7279\u5f81\u4f18\u5148\u548c\u7c7b\u522b\u91cd\u5e73\u8861\uff0c\u5728\u6bd2\u6027\u8bc4\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8d85\u8d8aBERT\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u63a8\u7406\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5728\u5c11\u6570\u6bd2\u6027\u7c7b\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u53c2\u8003\u5411\u91cf\u8fdb\u884c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u95e8\u63a7\uff0c\u6574\u5408\u591a\u6e90\u5d4c\u5165\uff08GloVe\u3001FastText\u3001BERT CLS\uff09\uff0c\u5305\u542b\u5b57\u7b26\u7ea7BiLSTM\u3001\u5d4c\u5165\u7a7a\u95f4SMOTE\u548c\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u3002", "result": "\u5728Jigsaw\u6bd2\u6027\u8bc4\u8bba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523096.0%\u51c6\u786e\u7387\u548c0.88\u5b8fF1\uff0c\u5728\u5a01\u80c1\u548c\u8eab\u4efd\u4ec7\u6068\u7c7b\u522b\u4e0a\u5206\u522b\u6bd4BERT\u63d0\u534733%\u548c28%\uff0c\u53c2\u6570\u51cf\u5c1115\u500d\uff0c\u63a8\u7406\u5ef6\u8fdf50ms\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u7406\u8bba\u9a71\u52a8\u7684\u67b6\u6784\u53ef\u4ee5\u5728\u4e0d\u5e73\u8861\u3001\u7279\u5b9a\u9886\u57df\u7684NLP\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6548\u7387\u9002\u5e94\u6027\u524d\u6cbf\u3002"}}
{"id": "2510.16556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16556", "abs": "https://arxiv.org/abs/2510.16556", "authors": ["Guangyu Lin", "Li Lin", "Christina P. Walker", "Daniel S. Schiff", "Shu Hu"], "title": "Fit for Purpose? Deepfake Detection in the Real World", "comment": null, "summary": "The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u4e8b\u4ef6\u7684\u7cfb\u7edf\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u68c0\u6d4b\u5668\u666e\u904d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5177\u653f\u6cbb\u80cc\u666f\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "AI\u751f\u6210\u5185\u5bb9\u7684\u5feb\u901f\u6269\u6563\u589e\u52a0\u4e86\u865a\u5047\u4fe1\u606f\u98ce\u9669\uff0c\u7279\u522b\u662f\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u4f1a\u7834\u574f\u771f\u76f8\u548c\u524a\u5f31\u5bf9\u653f\u6cbb\u673a\u6784\u7684\u4fe1\u4efb\u3002\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u5927\u591a\u5728\u5b9e\u9a8c\u5ba4\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f20\u64ad\u7684\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u3002", "method": "\u57fa\u4e8e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u4e8b\u4ef6\u6570\u636e\u5e93\u6784\u5efa\u7cfb\u7edf\u57fa\u51c6\uff0c\u5bf9\u5b66\u672f\u754c\u3001\u653f\u5e9c\u548c\u5de5\u4e1a\u754c\u7684\u6700\u5148\u8fdb\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u5b66\u672f\u754c\u548c\u653f\u5e9c\u5f00\u53d1\u7684\u68c0\u6d4b\u5668\u8868\u73b0\u76f8\u5bf9\u8f83\u5dee\uff0c\u4ed8\u8d39\u68c0\u6d4b\u5de5\u5177\u6027\u80fd\u76f8\u5bf9\u8f83\u9ad8\uff0c\u4f46\u6240\u6709\u8bc4\u4f30\u7684\u68c0\u6d4b\u5668\u90fd\u96be\u4ee5\u6709\u6548\u6cdb\u5316\u5230\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u4e14\u5bb9\u6613\u53d7\u5230\u7b80\u5355\u64cd\u4f5c\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u9886\u57df\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u5177\u6709\u653f\u6cbb\u80cc\u666f\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee5\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u66f4\u597d\u5730\u4fdd\u62a4\u516c\u4f17\u3002"}}
{"id": "2510.17028", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17028", "abs": "https://arxiv.org/abs/2510.17028", "authors": ["Kyle Cox", "Jiawei Xu", "Yikun Han", "Rong Xu", "Tianhao Li", "Chi-Yang Hsu", "Tianlong Chen", "Walter Gerych", "Ying Ding"], "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models", "comment": null, "summary": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u654f\u611f\u6027\u73b0\u8c61\uff0c\u63d0\u51fa\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u91c7\u6837\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6307\u6807\u6765\u91cf\u5316\u63d0\u793a\u654f\u611f\u6027\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u4e49\u76f8\u540c\u4f46\u8868\u8ff0\u4e0d\u540c\u7684\u63d0\u793a\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7b54\u6848\u5206\u5e03\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u5176\u5bf9\u63d0\u793a\u542b\u4e49\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06\u63d0\u793a\u654f\u611f\u6027\u5efa\u6a21\u4e3a\u6cdb\u5316\u8bef\u5dee\uff0c\u901a\u8fc7\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u7684\u91ca\u4e49\u6270\u52a8\u91c7\u6837\u6765\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u8fde\u7eed\u6027\u7684\u65b0\u6307\u6807\u6765\u5206\u89e3\u9ed1\u76d2LLM\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u8fdb\u884c\u91c7\u6837\u53ef\u4ee5\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u65b0\u63d0\u51fa\u7684\u5206\u89e3\u6307\u6807\u80fd\u591f\u6709\u6548\u91cf\u5316\u63d0\u793a\u654f\u611f\u6027\u5bf9LLM\u4e0d\u786e\u5b9a\u6027\u7684\u8d21\u732e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6539\u5584\u63d0\u793a\u654f\u611f\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u67d0\u4e9bLLM\u672a\u80fd\u5bf9\u5176\u8f93\u5165\u542b\u4e49\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.16596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16596", "abs": "https://arxiv.org/abs/2510.16596", "authors": ["Yiyang Huang", "Liang Shi", "Yitian Zhang", "Yi Xu", "Yun Fu"], "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense", "comment": null, "summary": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased.", "AI": {"tldr": "SHIELD\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u89c6\u89c9token\u3001\u5f15\u5165\u566a\u58f0token\u548c\u5e94\u7528\u5bf9\u6297\u653b\u51fb\u6765\u7f13\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u4e0d\u51c6\u786e\u7684\u7269\u4f53\u63cf\u8ff0\u3002\u4e0e\u4e4b\u524d\u5173\u6ce8LLM\u7ec4\u4ef6\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u672c\u6587\u9996\u6b21\u5c06\u5e7b\u89c9\u95ee\u9898\u8ffd\u6eaf\u5230\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u8bc6\u522b\u51fa\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7edf\u8ba1\u504f\u5dee\u3001\u56fa\u6709\u504f\u5dee\u548c\u8106\u5f31\u6027", "method": "\u63d0\u51faSHIELD\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u7b56\u7565\uff1a1) \u91cd\u65b0\u52a0\u6743\u89c6\u89c9token\u4ee5\u51cf\u5c11\u7edf\u8ba1\u504f\u5dee\uff1b2) \u5f15\u5165\u566a\u58f0\u884d\u751ftoken\u6765\u5bf9\u6297\u56fa\u6709\u504f\u5dee\uff1b3) \u5e94\u7528\u5bf9\u6297\u653b\u51fb\u7ed3\u5408\u5bf9\u6bd4\u89e3\u7801\u6765\u89e3\u51b3\u8106\u5f31\u6027", "result": "\u5b9e\u9a8c\u8868\u660eSHIELD\u80fd\u6709\u6548\u7f13\u89e3\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548cLVLM\u5bb6\u65cf\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u5728\u901a\u7528LVLM\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "SHIELD\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u6846\u67b6\uff0c\u80fd\u6709\u6548\u89e3\u51b3LVLM\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u76f4\u63a5\u9488\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u7684\u504f\u5dee\u548c\u8106\u5f31\u6027\u6765\u5b9e\u73b0\u6539\u8fdb"}}
{"id": "2510.17062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17062", "abs": "https://arxiv.org/abs/2510.17062", "authors": ["Guoqing Luo", "Iffat Maab", "Lili Mou", "Junichi Yamagishi"], "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation", "comment": null, "summary": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u4f1a\u52a0\u5267\u793e\u4f1a\u504f\u89c1\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u523b\u677f\u5370\u8c61\u91cd\u590d\u548c\u4e0d\u76f8\u5173\u4fe1\u606f\u6ce8\u5165\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u63d0\u793a\u65b9\u6cd5\u6765\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5185\u90e8\u7ed3\u6784\u5316\u601d\u8003\u8fc7\u7a0b\u4e2d\u4f1a\u52a0\u5267\u793e\u4f1a\u504f\u89c1\uff0c\u5bfc\u81f4\u6709\u504f\u89c1\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u4f46\u8fd9\u79cd\u73b0\u8c61\u7684\u5e95\u5c42\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u601d\u8003\u8fc7\u7a0b\u4e2d\u7684\u504f\u89c1\u52a0\u5267\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u8f7b\u91cf\u7ea7\u7f13\u89e3\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u81ea\u6211\u5ba1\u67e5\u521d\u59cb\u63a8\u7406\u3002", "result": "\u5728\u95ee\u7b54\uff08BBQ\u548cStereoSet\uff09\u548c\u5f00\u653e\u751f\u6210\uff08BOLD\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u504f\u89c1\u52a0\u5267\u7684\u673a\u5236\uff0c\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u7f13\u89e3\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\uff0c\u4e3a\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.16598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16598", "abs": "https://arxiv.org/abs/2510.16598", "authors": ["Jiaying Zhu", "Yurui Zhu", "Xin Lu", "Wenrui Yan", "Dong Li", "Kunlin Liu", "Xueyang Fu", "Zheng-Jun Zha"], "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs", "comment": "22 pages, 8 figures", "summary": "Multimodal Large Language Models (MLLMs) encounter significant computational\nand memory bottlenecks from the massive number of visual tokens generated by\nhigh-resolution images or multi-image inputs. Previous token compression\ntechniques are often constrained by heuristic rules that risk discarding\ncritical information. They may suffer from biases, such as attention sinks,\nthat lead to sharp performance drops under aggressive compression ratios. To\naddress these limitations, we reformulate token compression as a lightweight\nplug-and-play framework that reformulates token compression into an end-to-end\nlearnable decision process. To be specific, we propose VisionSelector, a scorer\nmodule decoupled from the MLLM backbone that incorporates a differentiable\nTop-K mechanism and a curriculum annealing strategy to bridge the\ntraining-inference gap, enabling efficient and adaptive token selection various\narbitrary compression rates. Remarkably lightweight with only 12.85M trainable\nparameters, VisionSelector demonstrates generalization across various\ncompression rates and adaptively identifying critical tokens. This leads to\nsuperior performance across all compression budgets, evidenced by preserving\n100% accuracy on MME with 30% retention budget, outperforming prior methods by\n12.14% at 10% retention budget, and doubling prefill speed. Our code is\navailable at https://github.com/JulietChoo/VisionSelector .", "AI": {"tldr": "VisionSelector\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u51b3\u7b56\u8fc7\u7a0b\u89e3\u51b3MLLMs\u4e2d\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u95ee\u9898\uff0c\u4f7f\u7528\u53ef\u5fae\u5206Top-K\u673a\u5236\u548c\u8bfe\u7a0b\u9000\u706b\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u591a\u56fe\u50cf\u8f93\u5165\u65f6\u9762\u4e34\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u73b0\u6709\u538b\u7f29\u6280\u672f\u53d7\u9650\u4e8e\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u53ef\u80fd\u4e22\u5931\u5173\u952e\u4fe1\u606f\u6216\u5b58\u5728\u6ce8\u610f\u529b\u504f\u5dee\u3002", "method": "\u63d0\u51faVisionSelector\u6846\u67b6\uff0c\u5305\u542b\u4e0eMLLM\u4e3b\u5e72\u89e3\u8026\u7684\u8bc4\u5206\u5668\u6a21\u5757\uff0c\u91c7\u7528\u53ef\u5fae\u5206Top-K\u673a\u5236\u548c\u8bfe\u7a0b\u9000\u706b\u7b56\u7565\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u4ee4\u724c\u9009\u62e9\u8fc7\u7a0b\u3002", "result": "\u4ec5\u970012.85M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728MME\u57fa\u51c6\u4e0a\u4fdd\u6301100%\u51c6\u786e\u7387\uff0830%\u4fdd\u7559\u9884\u7b97\uff09\uff0c\u572810%\u4fdd\u7559\u9884\u7b97\u4e0b\u6bd4\u4e4b\u524d\u65b9\u6cd5\u63d0\u534712.14%\uff0c\u9884\u586b\u5145\u901f\u5ea6\u7ffb\u500d\u3002", "conclusion": "VisionSelector\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u4ee4\u724c\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5404\u79cd\u538b\u7f29\u7387\u4e0b\u90fd\u80fd\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17109", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17109", "abs": "https://arxiv.org/abs/2510.17109", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "title": "Verification-Aware Planning for Multi-Agent Systems", "comment": "Submission for ARR Oct", "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.", "AI": {"tldr": "VeriMAP\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u5e76\u7f16\u7801\u9a8c\u8bc1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u89c4\u5212\u3001\u534f\u8c03\u548c\u9a8c\u8bc1\u6311\u6218\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u89c4\u5212\u3001\u534f\u8c03\u548c\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u6267\u884c\u5931\u8d25\u5f80\u5f80\u6e90\u4e8e\u4efb\u52a1\u89e3\u91ca\u3001\u8f93\u51fa\u683c\u5f0f\u6216\u4ea4\u63a5\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u5fae\u504f\u5dee\uff0c\u800c\u975e\u63a8\u7406\u9519\u8bef\u3002", "method": "VeriMAP\u89c4\u5212\u5668\u5206\u89e3\u4efb\u52a1\uff0c\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5c06\u89c4\u5212\u5668\u5b9a\u4e49\u7684\u901a\u8fc7\u6807\u51c6\u7f16\u7801\u4e3aPython\u548c\u81ea\u7136\u8bed\u8a00\u7684\u5b50\u4efb\u52a1\u9a8c\u8bc1\u51fd\u6570\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cVeriMAP\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u4f7f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u534f\u8c03\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6807\u7b7e\u6216\u6ce8\u91ca\u3002"}}
{"id": "2510.16611", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16611", "abs": "https://arxiv.org/abs/2510.16611", "authors": ["Melika Filvantorkaman", "Maral Filvan Torkaman"], "title": "A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications", "comment": "20 pages, 4 figures", "summary": "Medical imaging plays a vital role in modern diagnostics; however,\ninterpreting high-resolution radiological data remains time-consuming and\nsusceptible to variability among clinicians. Traditional image processing\ntechniques often lack the precision, robustness, and speed required for\nreal-time clinical use. To overcome these limitations, this paper introduces a\ndeep learning framework for real-time medical image analysis designed to\nenhance diagnostic accuracy and computational efficiency across multiple\nimaging modalities, including X-ray, CT, and MRI. The proposed system\nintegrates advanced neural network architectures such as U-Net, EfficientNet,\nand Transformer-based models with real-time optimization strategies including\nmodel pruning, quantization, and GPU acceleration. The framework enables\nflexible deployment on edge devices, local servers, and cloud infrastructures,\nensuring seamless interoperability with clinical systems such as PACS and EHR.\nExperimental evaluations on public benchmark datasets demonstrate\nstate-of-the-art performance, achieving classification accuracies above 92%,\nsegmentation Dice scores exceeding 91%, and inference times below 80\nmilliseconds. Furthermore, visual explanation tools such as Grad-CAM and\nsegmentation overlays enhance transparency and clinical interpretability. These\nresults indicate that the proposed framework can substantially accelerate\ndiagnostic workflows, reduce clinician workload, and support trustworthy AI\nintegration in time-critical healthcare environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408U-Net\u3001EfficientNet\u548cTransformer\u7b49\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u578b\u526a\u679d\u3001\u91cf\u5316\u548cGPU\u52a0\u901f\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u56fe\u50cf\u5904\u7406\u6280\u672f\u7f3a\u4e4f\u5b9e\u65f6\u4e34\u5e8a\u4f7f\u7528\u6240\u9700\u7684\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u901f\u5ea6\uff0c\u4e14\u9ad8\u5206\u8fa8\u7387\u653e\u5c04\u5b66\u6570\u636e\u89e3\u8bfb\u8017\u65f6\u4e14\u5b58\u5728\u4e34\u5e8a\u533b\u751f\u95f4\u53d8\u5f02\u6027\u3002", "method": "\u96c6\u6210U-Net\u3001EfficientNet\u548cTransformer\u7b49\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u578b\u526a\u679d\u3001\u91cf\u5316\u548cGPU\u52a0\u901f\u7b49\u5b9e\u65f6\u4f18\u5316\u7b56\u7565\uff0c\u652f\u6301\u5728\u8fb9\u7f18\u8bbe\u5907\u3001\u672c\u5730\u670d\u52a1\u5668\u548c\u4e91\u57fa\u7840\u8bbe\u65bd\u4e0a\u7684\u7075\u6d3b\u90e8\u7f72\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc792%\uff0c\u5206\u5272Dice\u5206\u6570\u8d85\u8fc791%\uff0c\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e80\u6beb\u79d2\uff0c\u5e76\u901a\u8fc7Grad-CAM\u548c\u5206\u5272\u53e0\u52a0\u7b49\u53ef\u89c6\u5316\u5de5\u5177\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u663e\u8457\u52a0\u901f\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\uff0c\u51cf\u5c11\u4e34\u5e8a\u533b\u751f\u5de5\u4f5c\u91cf\uff0c\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u533b\u7597\u73af\u5883\u4e2d\u652f\u6301\u53ef\u4fe1\u8d56\u7684AI\u96c6\u6210\u3002"}}
{"id": "2510.17115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17115", "abs": "https://arxiv.org/abs/2510.17115", "authors": ["Wei Du", "Nuowei Liu", "Jie Wang", "Jiahao Kuang", "Tao Ji", "Xiaoling Wang", "Yuanbin Wu"], "title": "DVAGen: Dynamic Vocabulary Augmented Generation", "comment": null, "summary": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.", "AI": {"tldr": "DVAGen\u662f\u4e00\u4e2a\u5f00\u6e90\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u52a8\u6001\u8bcd\u6c47\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4f20\u7edf\u56fa\u5b9a\u8bcd\u6c47\u6a21\u578b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u56fa\u5b9a\u8bcd\u6c47\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u65b0\u8bcd\u6216\u8bcd\u6c47\u5916\u8bcd\u6c47\uff0c\u73b0\u6709\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u5b58\u5728\u4ee3\u7801\u5e93\u788e\u7247\u5316\u3001\u7f3a\u4e4f\u73b0\u4ee3LLM\u652f\u6301\u3001\u63a8\u7406\u53ef\u6269\u5c55\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1DVAGen\u6846\u67b6\uff0c\u6a21\u5757\u5316\u5904\u7406\u6d41\u7a0b\u4ee5\u4fbf\u5b9a\u5236\uff0c\u65e0\u7f1d\u96c6\u6210\u5f00\u6e90LLM\uff0c\u63d0\u4f9bCLI\u548cWebUI\u5de5\u5177\u8fdb\u884c\u5b9e\u65f6\u7ed3\u679c\u68c0\u67e5\uff0c\u652f\u6301\u6279\u91cf\u63a8\u7406\u3002", "result": "\u9a8c\u8bc1\u4e86\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u5728\u73b0\u4ee3LLM\u4e0a\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u541e\u5410\u91cf\u3002", "conclusion": "DVAGen\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u8bcd\u6c47\u65b9\u6cd5\u7684\u73b0\u6709\u6311\u6218\uff0c\u4e3a\u5904\u7406\u591a\u6837\u5316\u8bcd\u6c47\u7ec4\u5408\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16624", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16624", "abs": "https://arxiv.org/abs/2510.16624", "authors": ["Sebastian Mocanu", "Emil Slusanschi", "Marius Leordeanu"], "title": "Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs", "comment": null, "summary": "This paper presents a vision-only autonomous flight system for small UAVs\noperating in controlled indoor environments. The system combines semantic\nsegmentation with monocular depth estimation to enable obstacle avoidance,\nscene exploration, and autonomous safe landing operations without requiring GPS\nor expensive sensors such as LiDAR. A key innovation is an adaptive scale\nfactor algorithm that converts non-metric monocular depth predictions into\naccurate metric distance measurements by leveraging semantic ground plane\ndetection and camera intrinsic parameters, achieving a mean distance error of\n14.4 cm. The approach uses a knowledge distillation framework where a\ncolor-based Support Vector Machine (SVM) teacher generates training data for a\nlightweight U-Net student network (1.6M parameters) capable of real-time\nsemantic segmentation. For more complex environments, the SVM teacher can be\nreplaced with a state-of-the-art segmentation model. Testing was conducted in a\ncontrolled 5x4 meter laboratory environment with eight cardboard obstacles\nsimulating urban structures. Extensive validation across 30 flight tests in a\nreal-world environment and 100 flight tests in a digital-twin environment\ndemonstrates that the combined segmentation and depth approach increases the\ndistance traveled during surveillance and reduces mission time while\nmaintaining 100% success rates. The system is further optimized through\nend-to-end learning, where a compact student neural network learns complete\nflight policies from demonstration data generated by our best-performing\nmethod, achieving an 87.5% autonomous mission success rate. This work advances\npractical vision-based drone navigation in structured environments,\ndemonstrating solutions for metric depth estimation and computational\nefficiency challenges that enable deployment on resource-constrained platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u89c6\u89c9\u7684\u5c0f\u578b\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u907f\u969c\u3001\u573a\u666f\u63a2\u7d22\u548c\u81ea\u4e3b\u5b89\u5168\u7740\u9646\uff0c\u65e0\u9700GPS\u6216\u6602\u8d35\u4f20\u611f\u5668\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u73b0\u57fa\u4e8e\u89c6\u89c9\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u5ea6\u91cf\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u989c\u8272\u7684SVM\u6559\u5e08\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7U-Net\u5b66\u751f\u7f51\u7edc\u8fdb\u884c\u5b9e\u65f6\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u5c3a\u5ea6\u56e0\u5b50\u7b97\u6cd5\u5c06\u975e\u5ea6\u91cf\u6df1\u5ea6\u9884\u6d4b\u8f6c\u6362\u4e3a\u51c6\u786e\u5ea6\u91cf\u8ddd\u79bb\u3002", "result": "\u57285x4\u7c73\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\u4e3a14.4\u5398\u7c73\uff0c30\u6b21\u771f\u5b9e\u73af\u5883\u98de\u884c\u6d4b\u8bd5\u548c100\u6b21\u6570\u5b57\u5b6a\u751f\u73af\u5883\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u589e\u52a0\u4e86\u76d1\u89c6\u8ddd\u79bb\uff0c\u51cf\u5c11\u4e86\u4efb\u52a1\u65f6\u95f4\uff0c\u4fdd\u6301100%\u6210\u529f\u7387\u3002\u7aef\u5230\u7aef\u5b66\u4e60\u8fbe\u523087.5%\u81ea\u4e3b\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u8fdb\u4e86\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u57fa\u4e8e\u89c6\u89c9\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u5b9e\u8df5\uff0c\u89e3\u51b3\u4e86\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u548c\u8ba1\u7b97\u6548\u7387\u6311\u6218\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2510.17139", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17139", "abs": "https://arxiv.org/abs/2510.17139", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Xueguang Ma", "Bingsen Chen", "Yijun Tian", "Fengran Mo", "Jie Cao", "Vivek Srikumar"], "title": "Rethinking On-policy Optimization for Query Augmentation", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u65e0\u8bad\u7ec3\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u4e0e\u66f4\u6602\u8d35\u7684RL\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002\u57fa\u4e8e\u6b64\u53d1\u73b0\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5OPQE\uff0c\u901a\u8fc7\u751f\u6210\u4f2a\u6587\u6863\u6765\u6700\u5927\u5316\u68c0\u7d22\u6027\u80fd\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548cRL\u7684\u4f18\u5316\u76ee\u6807\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u67e5\u8be2\u589e\u5f3a\u4e3b\u8981\u6709\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5728\u4e00\u81f4\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e86OPQE\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u662f\u91cd\u5199\u67e5\u8be2\uff0c\u800c\u662f\u8ba9LLM\u7b56\u7565\u5b66\u4e60\u751f\u6210\u80fd\u591f\u6700\u5927\u5316\u68c0\u7d22\u6027\u80fd\u7684\u4f2a\u6587\u6863\uff0c\u5c06\u63d0\u793a\u7684\u7075\u6d3b\u6027\u4e0eRL\u7684\u4f18\u5316\u76ee\u6807\u76f8\u7ed3\u5408\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7b80\u5355\u7684\u65e0\u8bad\u7ec3\u67e5\u8be2\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u4e0eRL\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff1bOPQE\u65b9\u6cd5\u5728\u8bc1\u636e\u641c\u7d22\u3001\u4e34\u65f6\u68c0\u7d22\u548c\u5de5\u5177\u68c0\u7d22\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5355\u72ec\u7684\u63d0\u793a\u65b9\u6cd5\u548cRL\u91cd\u5199\u65b9\u6cd5\u3002", "conclusion": "\u534f\u540c\u65b9\u6cd5\u80fd\u4ea7\u751f\u6700\u4f73\u7ed3\u679c\uff0cOPQE\u5c55\u793a\u4e86\u5c06\u63d0\u793a\u7684\u7075\u6d3b\u6027\u4e0eRL\u4f18\u5316\u76f8\u7ed3\u5408\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.16641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16641", "abs": "https://arxiv.org/abs/2510.16641", "authors": ["Young-Jun Lee", "Byung-Kwan Lee", "Jianshu Zhang", "Yechan Hwang", "Byungsoo Ko", "Han-Gyu Kim", "Dongyu Yao", "Xuankun Rong", "Eojin Joo", "Seung-Ho Han", "Bowon Ko", "Ho-Jin Choi"], "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models", "comment": "Project website:\n  https://passing2961.github.io/multiverse-project-page/", "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.", "AI": {"tldr": "MultiVerse\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b647\u4e2a\u5bf9\u8bdd\uff0c\u6db5\u76d612\u4e2a\u6d41\u884cVLM\u8bc4\u4f30\u57fa\u51c6\u7684484\u4e2a\u4efb\u52a1\uff0c\u4f7f\u7528GPT-4o\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u8bc4\u4f3018\u4e2aVLMs\u572837\u4e2a\u5173\u952e\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u4ec5\u90e8\u5206\u6355\u6349\u7528\u6237\u9047\u5230\u7684\u5404\u79cd\u5bf9\u8bdd\u573a\u666f\uff0c\u800c\u771f\u5b9e\u5e94\u7528\u9700\u8981\u66f4\u590d\u6742\u7684\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u3002", "method": "\u4ece12\u4e2aVLM\u8bc4\u4f30\u57fa\u51c6\u4e2d\u63d0\u53d6647\u4e2a\u5bf9\u8bdd\uff0c\u6bcf\u4e2a\u5bf9\u8bdd\u5e73\u57474\u8f6e\uff0c\u63d0\u51fa\u57fa\u4e8e\u68c0\u67e5\u8868\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528GPT-4o\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u3002", "result": "\u5373\u4f7f\u6700\u5f3a\u7684\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e5f\u4ec5\u8fbe\u523050%\u7684\u6210\u529f\u7387\uff0c\u63d0\u4f9b\u5b8c\u6574\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u5347\u8f83\u5c0f\u6216\u8f83\u5f31\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "MultiVerse\u662f\u8bc4\u4f30VLM\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17168", "abs": "https://arxiv.org/abs/2510.17168", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "title": "When AI companions become witty: Can human brain recognize AI-generated irony?", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as social agents\nand trained to produce humor and irony, a question emerges: when encountering\nwitty AI remarks, do people interpret these as intentional communication or\nmere computational output? This study investigates whether people adopt the\nintentional stance, attributing mental states to explain behavior,toward AI\nduring irony comprehension. Irony provides an ideal paradigm because it\nrequires distinguishing intentional contradictions from unintended errors\nthrough effortful semantic reanalysis. We compared behavioral and neural\nresponses to ironic statements from AI versus human sources using established\nERP components: P200 reflecting early incongruity detection and P600 indexing\ncognitive efforts in reinterpreting incongruity as deliberate irony. Results\ndemonstrate that people do not fully adopt the intentional stance toward\nAI-generated irony. Behaviorally, participants attributed incongruity to\ndeliberate communication for both sources, though significantly less for AI\nthan human, showing greater tendency to interpret AI incongruities as\ncomputational errors. Neural data revealed attenuated P200 and P600 effects for\nAI-generated irony, suggesting reduced effortful detection and reanalysis\nconsistent with diminished attribution of communicative intent. Notably, people\nwho perceived AI as more sincere showed larger P200 and P600 effects for\nAI-generated irony, suggesting that intentional stance adoption is calibrated\nby specific mental models of artificial agents. These findings reveal that\nsource attribution shapes neural processing of social-communicative phenomena.\nDespite current LLMs' linguistic sophistication, achieving genuine social\nagency requires more than linguistic competence, it necessitates a shift in how\nhumans perceive and attribute intentionality to artificial agents.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u4eba\u4eec\u4e0d\u4f1a\u5b8c\u5168\u5bf9AI\u751f\u6210\u7684\u8bbd\u523a\u8a00\u8bba\u91c7\u7528\u610f\u5411\u6027\u7acb\u573a\uff0c\u795e\u7ecf\u6570\u636e\u663e\u793aP200\u548cP600\u6548\u5e94\u51cf\u5f31\uff0c\u8868\u660e\u5bf9AI\u7684\u610f\u56fe\u5f52\u56e0\u51cf\u5c11\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u90e8\u7f72\u4e3a\u793e\u4ea4\u4ee3\u7406\u5e76\u8bad\u7ec3\u4ea7\u751f\u5e7d\u9ed8\u548c\u8bbd\u523a\uff0c\u9700\u8981\u7814\u7a76\u4eba\u4eec\u662f\u5426\u4f1a\u5c06AI\u7684\u673a\u667a\u8a00\u8bba\u89c6\u4e3a\u6709\u610f\u6c9f\u901a\u8fd8\u662f\u7eaf\u7cb9\u8ba1\u7b97\u8f93\u51fa\u3002", "method": "\u6bd4\u8f83\u4e86AI\u4e0e\u4eba\u7c7b\u6765\u6e90\u8bbd\u523a\u8bed\u53e5\u7684\u884c\u4e3a\u548c\u795e\u7ecf\u53cd\u5e94\uff0c\u4f7f\u7528ERP\u6210\u5206\uff1aP200\u53cd\u6620\u65e9\u671f\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\uff0cP600\u7d22\u5f15\u91cd\u65b0\u89e3\u91ca\u4e0d\u4e00\u81f4\u6027\u4e3a\u6545\u610f\u8bbd\u523a\u7684\u8ba4\u77e5\u52aa\u529b\u3002", "result": "\u884c\u4e3a\u4e0a\u53c2\u4e0e\u8005\u5bf9\u4e24\u79cd\u6765\u6e90\u90fd\u5f52\u56e0\u4e8e\u6545\u610f\u6c9f\u901a\uff0c\u4f46\u5bf9AI\u663e\u8457\u8f83\u5c11\uff0c\u66f4\u503e\u5411\u4e8e\u5c06AI\u7684\u4e0d\u4e00\u81f4\u6027\u89e3\u91ca\u4e3a\u8ba1\u7b97\u9519\u8bef\u3002\u795e\u7ecf\u6570\u636e\u663e\u793aAI\u751f\u6210\u8bbd\u523a\u7684P200\u548cP600\u6548\u5e94\u51cf\u5f31\u3002", "conclusion": "\u5c3d\u7ba1\u5f53\u524dLLM\u5177\u6709\u8bed\u8a00\u590d\u6742\u6027\uff0c\u4f46\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4ea4\u4ee3\u7406\u9700\u8981\u4eba\u7c7b\u5982\u4f55\u611f\u77e5\u548c\u5f52\u56e0\u4e8e\u4eba\u5de5\u4ee3\u7406\u7684\u610f\u5411\u6027\u53d1\u751f\u8f6c\u53d8\u3002"}}
{"id": "2510.16643", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.9; I.2.10; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16643", "abs": "https://arxiv.org/abs/2510.16643", "authors": ["Aaron Ray", "Jacob Arkin", "Harel Biggie", "Chuchu Fan", "Luca Carlone", "Nicholas Roy"], "title": "Structured Interfaces for Automated Reasoning with 3D Scene Graphs", "comment": "25 pages, 3 figures", "summary": "In order to provide a robot with the ability to understand and react to a\nuser's natural language inputs, the natural language must be connected to the\nrobot's underlying representations of the world. Recently, large language\nmodels (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for\ngrounding natural language and representing the world. In this work, we address\nthe challenge of using LLMs with 3DSGs to ground natural language. Existing\nmethods encode the scene graph as serialized text within the LLM's context\nwindow, but this encoding does not scale to large or rich 3DSGs. Instead, we\npropose to use a form of Retrieval Augmented Generation to select a subset of\nthe 3DSG relevant to the task. We encode a 3DSG in a graph database and provide\na query language interface (Cypher) as a tool to the LLM with which it can\nretrieve relevant data for language grounding. We evaluate our approach on\ninstruction following and scene question-answering tasks and compare against\nbaseline context window and code generation methods. Our results show that\nusing Cypher as an interface to 3D scene graphs scales significantly better to\nlarge, rich graphs on both local and cloud-based models. This leads to large\nperformance improvements in grounded language tasks while also substantially\nreducing the token count of the scene graph content. A video supplement is\navailable at https://www.youtube.com/watch?v=zY_YI9giZSA.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u6570\u636e\u5e93\u548cCypher\u67e5\u8be2\u8bed\u8a00\u63a5\u53e3\uff0c\u8ba9LLM\u80fd\u591f\u9ad8\u6548\u68c0\u7d223D\u573a\u666f\u56fe\u4e2d\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u573a\u666f\u56fe\u7684\u8bed\u8a00\u63a5\u5730\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c063D\u573a\u666f\u56fe\u5e8f\u5217\u5316\u4e3a\u6587\u672c\u653e\u5165LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u578b\u6216\u4e30\u5bcc\u76843D\u573a\u666f\u56fe\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u5c063D\u573a\u666f\u56fe\u7f16\u7801\u5230\u56fe\u6570\u636e\u5e93\u4e2d\uff0c\u4e3aLLM\u63d0\u4f9bCypher\u67e5\u8be2\u8bed\u8a00\u4f5c\u4e3a\u5de5\u5177\u63a5\u53e3\uff0c\u4f7f\u5176\u80fd\u591f\u68c0\u7d22\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u3002", "result": "\u5728\u6307\u4ee4\u8ddf\u968f\u548c\u573a\u666f\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u7528Cypher\u63a5\u53e3\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u56fe\u4e0a\u6269\u5c55\u6027\u66f4\u597d\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u573a\u666f\u56fe\u5185\u5bb9\u7684token\u6570\u91cf\u3002", "conclusion": "Cypher\u4f5c\u4e3a3D\u573a\u666f\u56fe\u7684\u63a5\u53e3\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bed\u8a00\u63a5\u5730\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u672c\u5730\u548c\u4e91\u7aef\u6a21\u578b\u3002"}}
{"id": "2510.17196", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17196", "abs": "https://arxiv.org/abs/2510.17196", "authors": ["Jiaqi Leng", "Xiang Hu", "Junxiong Wang", "Jianguo Li", "Wei Wu", "Yucheng Lu"], "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models", "comment": "Preprint. Work in progress", "summary": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u5206\u5757\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u8bc6\u522b\u51fa\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff1a\u8868\u8fbe\u6027\u5f3a\u7684\u5206\u5757\u7f16\u7801\u5668\u3001\u7ed5\u8fc7\u6b8b\u5dee\u8def\u5f84\u548c\u8bad\u7ec3\u4e2d\u7684\u5f3a\u5236\u9009\u62e9\u7a00\u758f\u6027\uff0c\u8fd9\u4e9b\u539f\u5219\u5171\u540c\u5b9e\u73b0\u4e86\u4ece4K\u4e0a\u4e0b\u6587\u52303200\u4e07token\u7684\u65e0\u8bad\u7ec3\u957f\u5ea6\u5916\u63a8\u3002", "motivation": "\u6709\u6548\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u662f\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\u3002\u6807\u51c6Transformer\u53d7\u9650\u4e8e\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u957f\u5ea6\u5916\u63a8\u80fd\u529b\u5dee\uff0c\u800c\u66ff\u4ee3\u67b6\u6784\u5982\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7531\u4e8e\u56fa\u5b9a\u5927\u5c0f\u7684\u5185\u5b58\u800c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5b8c\u6574\u4e0a\u4e0b\u6587\u3002\u57fa\u4e8e\u5206\u5757\u7684\u7a00\u758f\u6ce8\u610f\u529b\u5df2\u6210\u4e3a\u6781\u7aef\u957f\u5ea6\u6cdb\u5316\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u4f46\u5176\u6210\u529f\u80cc\u540e\u7684\u5173\u952e\u67b6\u6784\u539f\u5219\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u7cfb\u7edf\u5256\u6790\u8fd9\u4e9b\u6a21\u578b\u4ee5\u8bc6\u522b\u9a71\u52a8\u6027\u80fd\u7684\u6838\u5fc3\u7ec4\u4ef6\u3002\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff1a\u8868\u8fbe\u6027\u5f3a\u7684\u975e\u7ebf\u6027\u5206\u5757\u7f16\u7801\u5668\u3001\u7ed5\u8fc7\u6b8b\u5dee\u8def\u5f84\u548c\u8bad\u7ec3\u4e2d\u7684\u5f3a\u5236\u9009\u62e9\u7a00\u758f\u6027\u3002", "result": "\u7ed3\u5408\u8fd9\u4e9b\u539f\u5219\uff0c\u5728RULER\u548cBABILong\u4e0a\u5efa\u7acb\u4e86\u65e0\u8bad\u7ec3\u957f\u5ea6\u5916\u63a8\u7684\u65b0\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6210\u529f\u5c06\u8bad\u7ec3\u57284K\u4e0a\u4e0b\u6587\u7684\u6a21\u578b\u6cdb\u5316\u52303200\u4e07token\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u672a\u6765\u9ad8\u6027\u80fd\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u5957\u6e05\u6670\u4e14\u7ecf\u9a8c\u57fa\u7840\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2510.16660", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16660", "abs": "https://arxiv.org/abs/2510.16660", "authors": ["Yuntian Wang", "Xilin Yang", "Che-Yung Shen", "Nir Pillar", "Aydogan Ozcan"], "title": "Universal and Transferable Attacks on Pathology Foundation Models", "comment": "38 Pages, 8 Figures", "summary": "We introduce Universal and Transferable Adversarial Perturbations (UTAP) for\npathology foundation models that reveal critical vulnerabilities in their\ncapabilities. Optimized using deep learning, UTAP comprises a fixed and weak\nnoise pattern that, when added to a pathology image, systematically disrupts\nthe feature representation capabilities of multiple pathology foundation\nmodels. Therefore, UTAP induces performance drops in downstream tasks that\nutilize foundation models, including misclassification across a wide range of\nunseen data distributions. In addition to compromising the model performance,\nwe demonstrate two key features of UTAP: (1) universality: its perturbation can\nbe applied across diverse field-of-views independent of the dataset that UTAP\nwas developed on, and (2) transferability: its perturbation can successfully\ndegrade the performance of various external, black-box pathology foundation\nmodels - never seen before. These two features indicate that UTAP is not a\ndedicated attack associated with a specific foundation model or image dataset,\nbut rather constitutes a broad threat to various emerging pathology foundation\nmodels and their applications. We systematically evaluated UTAP across various\nstate-of-the-art pathology foundation models on multiple datasets, causing a\nsignificant drop in their performance with visually imperceptible modifications\nto the input images using a fixed noise pattern. The development of these\npotent attacks establishes a critical, high-standard benchmark for model\nrobustness evaluation, highlighting a need for advancing defense mechanisms and\npotentially providing the necessary assets for adversarial training to ensure\nthe safe and reliable deployment of AI in pathology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9488\u5bf9\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u53ef\u8fc1\u79fb\u5bf9\u6297\u6270\u52a8(UTAP)\uff0c\u8fd9\u662f\u4e00\u79cd\u56fa\u5b9a\u7684\u5f31\u566a\u58f0\u6a21\u5f0f\uff0c\u80fd\u7cfb\u7edf\u6027\u5730\u7834\u574f\u591a\u4e2a\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u63ed\u793a\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8106\u5f31\u6027\uff0c\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u5efa\u7acb\u9ad8\u6807\u51c6\u57fa\u51c6\uff0c\u63a8\u52a8\u9632\u5fa1\u673a\u5236\u53d1\u5c55\uff0c\u786e\u4fddAI\u5728\u75c5\u7406\u5b66\u4e2d\u7684\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u751f\u6210\u56fa\u5b9a\u7684\u5f31\u566a\u58f0\u6a21\u5f0f(UTAP)\uff0c\u8be5\u6270\u52a8\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff0c\u80fd\u591f\u8de8\u4e0d\u540c\u89c6\u91ce\u548c\u6570\u636e\u96c6\u5e94\u7528\uff0c\u5e76\u80fd\u6210\u529f\u653b\u51fb\u672a\u89c1\u8fc7\u7684\u9ed1\u76d2\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u3002", "result": "UTAP\u5728\u5404\u79cd\u6700\u5148\u8fdb\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u7684\u56fa\u5b9a\u566a\u58f0\u6a21\u5f0f\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "UTAP\u6784\u6210\u4e86\u5bf9\u5404\u79cd\u65b0\u5174\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u53ca\u5176\u5e94\u7528\u7684\u5e7f\u6cdb\u5a01\u80c1\uff0c\u5f3a\u8c03\u4e86\u63a8\u8fdb\u9632\u5fa1\u673a\u5236\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u5bf9\u6297\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5fc5\u8981\u8d44\u6e90\u3002"}}
{"id": "2510.17210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17210", "abs": "https://arxiv.org/abs/2510.17210", "authors": ["Chenchen Tan", "Youyang Qu", "Xinghao Li", "Hui Zhang", "Shujie Cui", "Cunjian Chen", "Longxiang Gao"], "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting", "comment": "22 pages, 10 figures", "summary": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u8f6c\u79fb\u6846\u67b6\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u9057\u5fd8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5728\u6a21\u578b\u6548\u7528\u548c\u5e7b\u89c9\u98ce\u9669\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u53ef\u80fd\u4fdd\u7559\u654f\u611f\u6570\u636e\u7684\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u5173\u952e\u56f0\u5883\uff1a\u6fc0\u8fdb\u7684\u9057\u5fd8\u4f1a\u635f\u5bb3\u6a21\u578b\u6548\u7528\uff0c\u800c\u4fdd\u5b88\u7b56\u7565\u867d\u4fdd\u7559\u6548\u7528\u4f46\u53ef\u80fd\u5bfc\u81f4\u5e7b\u89c9\u54cd\u5e94\uff0c\u9650\u5236\u4e86LLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u8f6c\u79fb\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u8bbe\u8ba1\u76ee\u6807\uff1a(1)\u4e0a\u4e0b\u6587\u4fdd\u6301\u6291\u5236\uff0c\u5728\u4e0d\u7834\u574f\u8bed\u8a00\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u8870\u51cf\u5bf9\u4e8b\u5b9e\u627f\u8f7d\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff1b(2)\u6297\u5e7b\u89c9\u54cd\u5e94\u5851\u9020\uff0c\u5728\u67e5\u8be2\u9057\u5fd8\u5185\u5bb9\u65f6\u963b\u6b62\u865a\u6784\u8865\u5168\u3002\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u6291\u5236\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u4fdd\u7559\u589e\u5f3a\u4e24\u4e2a\u6ce8\u610f\u529b\u7ea7\u5e72\u9884\u5b9e\u73b0\uff0c\u901a\u8fc7\u53cc\u635f\u5931\u76ee\u6807\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728ToFU\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u63d0\u9ad815%\uff0c\u5728TDEC\u57fa\u51c6\u4e0a\u63d0\u9ad810%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u65e0\u5e7b\u89c9\u9057\u5fd8\u6548\u679c\uff0c\u5728\u9057\u5fd8\u6548\u679c\u3001\u6cdb\u5316\u80fd\u529b\u548c\u54cd\u5e94\u53ef\u9760\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\u3002", "conclusion": "AS\u6846\u67b6\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u6709\u6548\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u6570\u636e\u5904\u7406\u65b9\u9762\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16664", "abs": "https://arxiv.org/abs/2510.16664", "authors": ["Christopher Thirgood", "Oscar Mendez", "Erin Ling", "Jon Storey", "Simon Hadfield"], "title": "HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications", "comment": null, "summary": "Hyperspectral images (HSI) promise to support a range of new applications in\ncomputer vision. Recent research has explored the feasibility of generalizable\nSpectral Reconstruction (SR), the problem of recovering a HSI from a natural\nthree-channel color image in unseen scenarios.\n  However, previous Multi-Scale Attention (MSA) works have only demonstrated\nsufficient generalizable results for very sparse spectra, while modern HSI\nsensors contain hundreds of channels.\n  This paper introduces a novel approach to spectral reconstruction via our\nHYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).\n  Using a Teacher model that encapsulates latent hyperspectral image data and a\nStudent model that learns mappings from natural images to the Teacher's encoded\ndomain, alongside a novel training method, we achieve high-quality spectral\nreconstruction.\n  This addresses key limitations of prior SR models, providing SOTA performance\nacross all metrics, including an 18\\% boost in accuracy, and faster inference\ntimes than current SOTA models at various channel depths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHYDRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u4ece\u4e09\u901a\u9053\u5f69\u8272\u56fe\u50cf\u91cd\u5efa\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u5728\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u7a00\u758f\u5149\u8c31\u7684\u95ee\u9898\uff0c\u9002\u5e94\u73b0\u4ee3\u9ad8\u5149\u8c31\u4f20\u611f\u5668\u5305\u542b\u6570\u767e\u4e2a\u901a\u9053\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u7f16\u7801\u6f5c\u5728\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u4ece\u81ea\u7136\u56fe\u50cf\u5230\u6559\u5e08\u7f16\u7801\u57df\u7684\u6620\u5c04\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5149\u8c31\u91cd\u5efa\u3002", "result": "\u5728\u6240\u6709\u6307\u6807\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u7cbe\u5ea6\u63d0\u534718%\uff0c\u5728\u4e0d\u540c\u901a\u9053\u6df1\u5ea6\u4e0b\u63a8\u7406\u901f\u5ea6\u5747\u4f18\u4e8e\u5f53\u524dSOTA\u6a21\u578b\u3002", "conclusion": "HYDRA\u65b9\u6cd5\u6210\u529f\u514b\u670d\u4e86\u5148\u524d\u5149\u8c31\u91cd\u5efa\u6a21\u578b\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17238", "abs": "https://arxiv.org/abs/2510.17238", "authors": ["Junlong Tong", "Yingqi Fan", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "StreamingThinker: Large Language Models Can Think While Reading", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}", "AI": {"tldr": "\u63d0\u51fa\u4e86StreamingThinker\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u8bfb\u53d6\u8f93\u5165\u65f6\u5c31\u5f00\u59cb\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u8f93\u5165\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8303\u5f0f\u9700\u8981\u7b49\u5f85\u5b8c\u6574\u8f93\u5165\u540e\u624d\u5f00\u59cb\u601d\u8003\uff0c\u8fd9\u5e26\u6765\u4e86\u4e0d\u5fc5\u8981\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u5728\u52a8\u6001\u573a\u666f\u4e2d\u4f1a\u524a\u5f31\u5bf9\u65e9\u671f\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u6d41\u5f0f\u601d\u7ef4\u8303\u5f0f\uff0c\u901a\u8fc7\u6d41\u5f0fCoT\u751f\u6210\u3001\u6d41\u5f0f\u7ea6\u675f\u8bad\u7ec3\u548c\u6d41\u5f0f\u5e76\u884c\u63a8\u7406\uff0c\u8ba9\u6a21\u578b\u5728\u8bfb\u53d6\u8f93\u5165\u65f6\u5c31\u5f00\u59cb\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u8d28\u91cf\u63a7\u5236\u7684\u6d41\u5f0f\u63a8\u7406\u5355\u5143\u3001\u6d41\u5f0f\u6ce8\u610f\u529b\u63a9\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u6765\u4fdd\u6301\u63a8\u7406\u987a\u5e8f\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u903b\u8f91\u63a8\u7406\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684QA\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cStreamingThinker\u4fdd\u6301\u4e86\u4e0e\u6279\u91cf\u601d\u8003\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f7f\u63a8\u7406\u5f00\u59cb\u524d\u7684token\u7b49\u5f85\u65f6\u95f4\u51cf\u5c1180%\uff0c\u6700\u7ec8\u7b54\u6848\u751f\u6210\u7684\u65f6\u95f4\u7ea7\u5ef6\u8fdf\u51cf\u5c1160%\u4ee5\u4e0a\u3002", "conclusion": "\u6d41\u5f0f\u601d\u7ef4\u8303\u5f0f\u80fd\u591f\u6709\u6548\u964d\u4f4eLLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16688", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16688", "abs": "https://arxiv.org/abs/2510.16688", "authors": ["Yejie Guo", "Yunzhong Hou", "Wufei Ma", "Meng Tang", "Ming-Hsuan Yang"], "title": "Pursuing Minimal Sufficiency in Spatial Reasoning", "comment": null, "summary": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.", "AI": {"tldr": "MSSR\u662f\u4e00\u4e2a\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6700\u5c0f\u5145\u5206\u4fe1\u606f\u96c6\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7a7a\u95f4\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u4e24\u4e2a\u57fa\u672c\u74f6\u9888\uff1a\u57fa\u4e8e2D\u9884\u8bad\u7ec3\u5bfc\u81f4\u76843D\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5197\u4f593D\u4fe1\u606f\u5f15\u53d1\u7684\u63a8\u7406\u5931\u8d25\u3002", "method": "\u91c7\u7528\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u611f\u77e5\u667a\u80fd\u4f53\u4f7f\u7528\u611f\u77e5\u5de5\u5177\u7bb1\u7a0b\u5e8f\u5316\u67e5\u8be23D\u573a\u666f\u63d0\u53d6\u5145\u5206\u4fe1\u606f\uff0c\u5305\u62ec\u65b0\u9896\u7684SOG\u6a21\u5757\u63d0\u53d6\u8bed\u8a00\u63a5\u5730\u65b9\u5411\uff1b\u63a8\u7406\u667a\u80fd\u4f53\u8fed\u4ee3\u7cbe\u70bc\u4fe1\u606f\u8ffd\u6c42\u6700\u5c0f\u5316\uff0c\u5728\u95ed\u73af\u4e2d\u4fee\u526a\u5197\u4f59\u7ec6\u8282\u5e76\u8bf7\u6c42\u7f3a\u5931\u4fe1\u606f\uff0c\u76f4\u5230\u6784\u5efa\u51fa\u6700\u5c0f\u5145\u5206\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5e76\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u8ffd\u6c42\u5145\u5206\u6027\u548c\u6700\u5c0f\u5316\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u3002"}}
{"id": "2510.17247", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17247", "abs": "https://arxiv.org/abs/2510.17247", "authors": ["Zefan Cai", "Haoyi Qiu", "Haozhe Zhao", "Ke Wan", "Jiachen Li", "Jiuxiang Gu", "Wen Xiao", "Nanyun Peng", "Junjie Hu"], "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models", "comment": null, "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86VideoBiasEval\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u4f1a\u653e\u5927\u5e76\u7a33\u5b9a\u5316\u793e\u4f1a\u504f\u89c1\uff0c\u5bfc\u81f4\u66f4\u523b\u677f\u7684\u63cf\u8ff0\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\u8c03\u4f18\u4ee5\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u4e2d\u7f16\u7801\u5e76\u653e\u5927\u4e86\u793e\u4f1a\u504f\u89c1\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u504f\u89c1\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165VideoBiasEval\u8bca\u65ad\u6846\u67b6\uff0c\u57fa\u4e8e\u793e\u4f1a\u504f\u89c1\u5206\u7c7b\u5b66\uff0c\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u7684\u63d0\u793a\u7b56\u7565\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u6f14\u5458\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u591a\u7c92\u5ea6\u6307\u6807\u8bc4\u4f30\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u9f50\u8c03\u4f18\u4e0d\u4ec5\u5f3a\u5316\u4e86\u8868\u5f81\u504f\u89c1\uff0c\u8fd8\u4f7f\u5176\u5728\u65f6\u95f4\u4e0a\u66f4\u7a33\u5b9a\uff0c\u4ea7\u751f\u66f4\u5e73\u6ed1\u4f46\u66f4\u523b\u677f\u7684\u63cf\u8ff0\u3002", "conclusion": "\u9700\u8981\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u504f\u89c1\u611f\u77e5\u7684\u8bc4\u4f30\u548c\u7f13\u89e3\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u548c\u793e\u4f1a\u8d23\u4efb\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.16702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16702", "abs": "https://arxiv.org/abs/2510.16702", "authors": ["Huy Minh Nhat Nguyen", "Triet Hoang Minh Dao", "Chau Vinh Hoang Truong", "Cuong Tuan Nguyen"], "title": "SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation", "comment": "2025 IEEE Conference on Computational Intelligence in Bioinformatics\n  and Computational Biology (CIBCB)", "summary": "Optical Coherence Tomography (OCT) is a widely used non-invasive imaging\ntechnique that provides detailed three-dimensional views of the retina, which\nare essential for the early and accurate diagnosis of ocular diseases.\nConsequently, OCT image analysis and processing have emerged as key research\nareas in biomedical imaging. However, acquiring paired datasets of clean and\nreal-world noisy OCT images for supervised denoising models remains a\nformidable challenge due to intrinsic speckle noise and practical constraints\nin clinical imaging environments. To address these issues, we propose SDPA++: A\nGeneral Framework for Self-Supervised Denoising with Patch Aggregation. Our\nnovel approach leverages only noisy OCT images by first generating\npseudo-ground-truth images through self-fusion and self-supervised denoising.\nThese refined images then serve as targets to train an ensemble of denoising\nmodels using a patch-based strategy that effectively enhances image clarity.\nPerformance improvements are validated via metrics such as Contrast-to-Noise\nRatio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge\nPreservation (EP) on the real-world dataset from the IEEE SPS Video and Image\nProcessing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT\nimages without clean references, highlighting our method's potential for\nimproving image quality and diagnostic outcomes in clinical practice.", "AI": {"tldr": "\u63d0\u51faSDPA++\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u566a\u58f0OCT\u56fe\u50cf\u8fdb\u884c\u81ea\u76d1\u7763\u53bb\u566a\uff0c\u901a\u8fc7\u81ea\u878d\u5408\u751f\u6210\u4f2a\u771f\u5b9e\u56fe\u50cf\uff0c\u7136\u540e\u8bad\u7ec3\u96c6\u6210\u53bb\u566a\u6a21\u578b\u63d0\u5347\u56fe\u50cf\u8d28\u91cf", "motivation": "OCT\u6210\u50cf\u4e2d\u56fa\u6709\u7684\u6563\u6591\u566a\u58f0\u548c\u4e34\u5e8a\u73af\u5883\u9650\u5236\u4f7f\u5f97\u83b7\u53d6\u914d\u5bf9\u7684\u5e72\u51c0-\u566a\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u975e\u5e38\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u4ec5\u4f7f\u7528\u566a\u58f0\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u53bb\u566a\u65b9\u6cd5", "method": "SDPA++\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u81ea\u878d\u5408\u548c\u81ea\u76d1\u7763\u53bb\u566a\u751f\u6210\u4f2a\u771f\u5b9e\u56fe\u50cf\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u5757\u7684\u7b56\u7565\u8bad\u7ec3\u96c6\u6210\u53bb\u566a\u6a21\u578b", "result": "\u5728IEEE SPS VIP Cup\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u901a\u8fc7CNR\u3001MSR\u3001TP\u548cEP\u7b49\u6307\u6807\u663e\u793a\u6027\u80fd\u63d0\u5347\uff0c\u8be5\u6570\u636e\u96c6\u4ec5\u5305\u542b\u566a\u58f0OCT\u56fe\u50cf\u800c\u65e0\u5e72\u51c0\u53c2\u8003", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7f3a\u4e4f\u5e72\u51c0\u53c2\u8003\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347OCT\u56fe\u50cf\u8d28\u91cf\uff0c\u5177\u6709\u6539\u5584\u4e34\u5e8a\u8bca\u65ad\u7ed3\u679c\u7684\u6f5c\u529b"}}
{"id": "2510.17252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17252", "abs": "https://arxiv.org/abs/2510.17252", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Ayesha Siddiqua", "Jungpil Shin"], "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design", "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)", "summary": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u60c5\u611f\u5206\u6790\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u4e2d\u8d1f\u9762\u60c5\u7eea\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u65b0\u95fb\u7684\u60c5\u611f\u62a5\u9053\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u65b0\u95fb\u5a92\u4f53\u901a\u8fc7\u62a5\u9053\u6846\u67b6\u5f71\u54cd\u516c\u4f17\u60c5\u7eea\uff0c\u8d1f\u9762\u6216\u60c5\u7eea\u5316\u6807\u9898\u5f80\u5f80\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\u548c\u4f20\u64ad\uff0c\u8fd9\u4fc3\u4f7f\u5a92\u4f53\u91c7\u7528\u66f4\u80fd\u5f15\u53d1\u5f3a\u70c8\u53cd\u5e94\u7684\u62a5\u9053\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Gemma-3 4B\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u5206\u6790\u4e8630\u4e07\u6761\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u548c\u5185\u5bb9\uff0c\u8bc6\u522b\u6bcf\u7bc7\u62a5\u9053\u7684\u4e3b\u5bfc\u60c5\u7eea\u548c\u6574\u4f53\u57fa\u8c03\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\u8d1f\u9762\u60c5\u7eea\u660e\u663e\u5360\u4e3b\u5bfc\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\uff0c\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u65b0\u95fb\u7684\u60c5\u611f\u62a5\u9053\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u7406\u5ff5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u65e5\u5e38\u65b0\u95fb\u4e2d\u9690\u85cf\u7684\u60c5\u611f\u6846\u67b6\u3002"}}
{"id": "2510.16704", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16704", "abs": "https://arxiv.org/abs/2510.16704", "authors": ["Tianxin Wei", "Yifan Chen", "Xinrui He", "Wenxuan Bao", "Jingrui He"], "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization", "comment": "Accepted by KDD 2025", "summary": "Distribution shifts between training and testing samples frequently occur in\npractice and impede model generalization performance. This crucial challenge\nthereby motivates studies on domain generalization (DG), which aim to predict\nthe label on unseen target domain data by solely using data from source\ndomains. It is intuitive to conceive the class-separated representations\nlearned in contrastive learning (CL) are able to improve DG, while the reality\nis quite the opposite: users observe directly applying CL deteriorates the\nperformance. We analyze the phenomenon with the insights from CL theory and\ndiscover lack of intra-class connectivity in the DG setting causes the\ndeficiency. We thus propose a new paradigm, domain-connecting contrastive\nlearning (DCCL), to enhance the conceptual connectivity across domains and\nobtain generalizable representations for DG. On the data side, more aggressive\ndata augmentation and cross-domain positive samples are introduced to improve\nintra-class connectivity. On the model side, to better embed the unseen test\ndomains, we propose model anchoring to exploit the intra-class connectivity in\npre-trained representations and complement the anchoring with generative\ntransformation loss. Extensive experiments on five standard DG benchmarks are\nperformed. The results verify that DCCL outperforms state-of-the-art baselines\neven without domain supervision. The detailed model implementation and the code\nare provided through https://github.com/weitianxin/DCCL", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u8fde\u63a5\u5bf9\u6bd4\u5b66\u4e60\uff08DCCL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u8de8\u9886\u57df\u7684\u6982\u5ff5\u8fde\u901a\u6027\u6765\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9886\u57df\u504f\u79fb\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u6027\u80fd\uff0c\u867d\u7136\u5bf9\u6bd4\u5b66\u4e60\u7684\u7c7b\u5206\u79bb\u8868\u793a\u7406\u8bba\u4e0a\u5e94\u6709\u52a9\u4e8e\u9886\u57df\u6cdb\u5316\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u539f\u56e0\u662f\u9886\u57df\u6cdb\u5316\u8bbe\u7f6e\u4e2d\u7f3a\u4e4f\u7c7b\u5185\u8fde\u901a\u6027\u3002", "method": "\u63d0\u51faDCCL\u65b9\u6cd5\uff1a\u6570\u636e\u5c42\u9762\u4f7f\u7528\u66f4\u6fc0\u8fdb\u7684\u6570\u636e\u589e\u5f3a\u548c\u8de8\u9886\u57df\u6b63\u6837\u672c\u63d0\u5347\u7c7b\u5185\u8fde\u901a\u6027\uff1b\u6a21\u578b\u5c42\u9762\u63d0\u51fa\u6a21\u578b\u951a\u5b9a\u6280\u672f\u5229\u7528\u9884\u8bad\u7ec3\u8868\u793a\u7684\u7c7b\u5185\u8fde\u901a\u6027\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u53d8\u6362\u635f\u5931\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDCCL\u5373\u4f7f\u5728\u6ca1\u6709\u9886\u57df\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u4e5f\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DCCL\u901a\u8fc7\u589e\u5f3a\u8de8\u9886\u57df\u7684\u6982\u5ff5\u8fde\u901a\u6027\uff0c\u80fd\u591f\u83b7\u5f97\u53ef\u6cdb\u5316\u7684\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2510.17256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17256", "abs": "https://arxiv.org/abs/2510.17256", "authors": ["Shahin Atakishiyev", "Housam K. B. Babiker", "Jiayi Dai", "Nawshad Farruque", "Teruaki Hayashi", "Nafisa Sadaf Hriti", "Md Abed Rahman", "Iain Smith", "Mi-Young Kim", "Osmar R. Za\u00efane", "Randy Goebel"], "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations", "comment": null, "summary": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5b9e\u9a8c\u7814\u7a76\u5206\u6790\u89e3\u91ca\u5bf9\u4fe1\u4efb\u7684\u5f71\u54cd\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u5176\u9884\u6d4b\u548c\u63a8\u7406\u8fc7\u7a0b\u5bf9\u4eba\u7c7b\u4e0d\u53ef\u7406\u89e3\uff0c\u4e14\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u9519\u8bef\uff0c\u8feb\u5207\u9700\u8981\u7406\u89e3\u548c\u89e3\u91ca\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u4ee5\u5efa\u7acb\u4fe1\u4efb\u3002", "method": "\u7efc\u8ff0\u4e86\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5e76\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u4e24\u4e2a\u5173\u952e\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u7814\u7a76\uff0c\u5206\u6790\u89e3\u91ca\u5bf9\u63a5\u6536\u8005\u7684\u4fe1\u4efb\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u4e86\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u7efc\u8ff0\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u89e3\u91ca\u5728\u5173\u952e\u9886\u57df\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "\u603b\u7ed3\u4e86LLM\u53ef\u89e3\u91ca\u6027\u9886\u57df\u7684\u673a\u9047\u3001\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u65e8\u5728\u751f\u6210\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u53ef\u4fe1LLM\u89e3\u91ca\u3002"}}
{"id": "2510.16709", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16709", "abs": "https://arxiv.org/abs/2510.16709", "authors": ["Liu Haojie", "Gao Suixiang"], "title": "HumanCM: One Step Human Motion Prediction", "comment": "6 pages, 2 figures, 2 tables", "summary": "We present HumanCM, a one-step human motion prediction framework built upon\nconsistency models. Instead of relying on multi-step denoising as in\ndiffusion-based methods, HumanCM performs efficient single-step generation by\nlearning a self-consistent mapping between noisy and clean motion states. The\nframework adopts a Transformer-based spatiotemporal architecture with temporal\nembeddings to model long-range dependencies and preserve motion coherence.\nExperiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves\ncomparable or superior accuracy to state-of-the-art diffusion models while\nreducing inference steps by up to two orders of magnitude.", "AI": {"tldr": "HumanCM\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u578b\u7684\u4e00\u6b65\u5f0f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u76f8\u6bd4\u6269\u6563\u6a21\u578b\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u9700\u8981\u591a\u6b65\u53bb\u566a\u8fc7\u7a0b\uff0c\u63a8\u7406\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u5355\u6b65\u751f\u6210\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "method": "\u91c7\u7528\u4e00\u81f4\u6027\u6a21\u578b\u5b66\u4e60\u566a\u58f0\u548c\u5e72\u51c0\u8fd0\u52a8\u72b6\u6001\u4e4b\u95f4\u7684\u81ea\u4e00\u81f4\u6620\u5c04\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u65f6\u7a7a\u67b6\u6784\u914d\u5408\u65f6\u95f4\u5d4c\u5165\u6765\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u5e76\u4fdd\u6301\u8fd0\u52a8\u8fde\u8d2f\u6027\u3002", "result": "\u5728Human3.6M\u548cHumanEva-I\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHumanCM\u5728\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6269\u6563\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\u4e86\u9ad8\u8fbe\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "HumanCM\u8bc1\u660e\u4e86\u901a\u8fc7\u4e00\u81f4\u6027\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u5355\u6b65\u8fd0\u52a8\u9884\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17263", "abs": "https://arxiv.org/abs/2510.17263", "authors": ["Avishek Lahiri", "Yufang Hou", "Debarshi Kumar Sanyal"], "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models", "comment": "This paper has been accepted at the EMNLP 2025 Main Conference", "summary": "Taxonomies play a crucial role in helping researchers structure and navigate\nknowledge in a hierarchical manner. They also form an important part in the\ncreation of comprehensive literature surveys. The existing approaches to\nautomatic survey generation do not compare the structure of the generated\nsurveys with those written by human experts. To address this gap, we present\nour own method for automated taxonomy creation that can bridge the gap between\nhuman-generated and automatically-created taxonomies. For this purpose, we\ncreate the CS-TaxoBench benchmark which consists of 460 taxonomies that have\nbeen extracted from human-written survey papers. We also include an additional\ntest set of 80 taxonomies curated from conference survey papers. We propose\nTaxoAlign, a three-phase topic-based instruction-guided method for scholarly\ntaxonomy generation. Additionally, we propose a stringent automated evaluation\nframework that measures the structural alignment and semantic coherence of\nautomatically generated taxonomies in comparison to those created by human\nexperts. We evaluate our method and various baselines on CS-TaxoBench, using\nboth automated evaluation metrics and human evaluation studies. The results\nshow that TaxoAlign consistently surpasses the baselines on nearly all metrics.\nThe code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.", "AI": {"tldr": "\u63d0\u51fa\u4e86TaxoAlign\u65b9\u6cd5\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b66\u672f\u5206\u7c7b\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86CS-TaxoBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u7efc\u8ff0\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4e0e\u4eba\u5de5\u4e13\u5bb6\u521b\u5efa\u7684\u5206\u7c7b\u6cd5\u7ed3\u6784\u5bf9\u6bd4\uff0c\u9700\u8981\u5f25\u5408\u4eba\u5de5\u751f\u6210\u4e0e\u81ea\u52a8\u521b\u5efa\u5206\u7c7b\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faTaxoAlign\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u57fa\u4e8e\u4e3b\u9898\u7684\u6307\u4ee4\u5f15\u5bfc\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u672f\u5206\u7c7b\u6cd5\u751f\u6210\u3002\u540c\u65f6\u521b\u5efa\u4e86\u5305\u542b460\u4e2a\u5206\u7c7b\u6cd5\u7684CS-TaxoBench\u57fa\u51c6\u548c80\u4e2a\u6d4b\u8bd5\u96c6\u3002", "result": "TaxoAlign\u5728\u51e0\u4e4e\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TaxoAlign\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u4e0e\u4eba\u5de5\u4e13\u5bb6\u5206\u7c7b\u6cd5\u7ed3\u6784\u5bf9\u9f50\u7684\u5b66\u672f\u5206\u7c7b\u6cd5\uff0c\u4e3a\u81ea\u52a8\u6587\u732e\u7efc\u8ff0\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2510.16714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16714", "abs": "https://arxiv.org/abs/2510.16714", "authors": ["Xiongkun Linghu", "Jiangyong Huang", "Ziyu Zhu", "Baoxiong Jia", "Siyuan Huang"], "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes", "comment": "Project page: https://scenecot.github.io/", "summary": "Existing research on 3D Large Language Models (LLMs) still struggles to\nachieve grounded question-answering, primarily due to the under-exploration of\nthe mech- anism of human-like scene-object grounded reasoning. This paper\nbridges the gap by presenting a novel framework. We first introduce a grounded\nChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a\ncomplex reasoning task into simpler and manageable problems, and building\ncorresponding visual clues based on multimodal expert modules. To enable such a\nmethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning\ndataset, consisting of 185K high-quality instances. Extensive experiments\nacross various complex 3D scene reasoning benchmarks demonstrate that our new\nframework achieves strong performance with high grounding-QA coherence. To the\nbest of our knowledge, this is the first successful application of CoT\nreasoning to 3D scene understanding, enabling step-by-step human-like reasoning\nand showing potential for extension to broader 3D scene understanding\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86SCENECOT\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u590d\u67423D\u573a\u666f\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u7b80\u5355\u5b50\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u591a\u6a21\u6001\u89c6\u89c9\u7ebf\u7d22\uff0c\u5b9e\u73b0\u4e86\u9996\u4e2a\u6210\u529f\u76843D\u573a\u666fCoT\u63a8\u7406\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a5\u5730\u95ee\u7b54\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u573a\u666f-\u5bf9\u8c61\u63a5\u5730\u63a8\u7406\u673a\u5236\u7684\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u57fa\u4e8e3D\u573a\u666f\u7684\u63a5\u5730\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5(SCENECOT)\uff0c\u5c06\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u591a\u6a21\u6001\u4e13\u5bb6\u6a21\u5757\u6784\u5efa\u89c6\u89c9\u7ebf\u7d22\u3002\u5f00\u53d1\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u63a5\u5730CoT\u63a8\u7406\u6570\u636e\u96c6SCENECOT-185K\u3002", "result": "\u5728\u591a\u4e2a\u590d\u67423D\u573a\u666f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u63a5\u5730\u95ee\u7b54\u4e00\u81f4\u6027\u3002", "conclusion": "\u8fd9\u662fCoT\u63a8\u7406\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u9996\u6b21\u6210\u529f\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u9010\u6b65\u7684\u4eba\u7c7b\u5f0f\u63a8\u7406\uff0c\u5e76\u663e\u793a\u51fa\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb3D\u573a\u666f\u7406\u89e3\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17289", "abs": "https://arxiv.org/abs/2510.17289", "authors": ["Hajar Bakarou", "Mohamed Sinane El Messoussi", "Ana\u00efs Ollagnier"], "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning", "comment": null, "summary": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u6cd5\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6CyberAgressionAdo-Large\uff0c\u8bc4\u4f30\u4e86\u53cd\u793e\u4f1a\u884c\u4e3a\u7684\u4e09\u4e2a\u4efb\u52a1\uff1a\u8fb1\u9a82\u68c0\u6d4b\u3001\u6b3a\u51cc\u884c\u4e3a\u5206\u6790\u548c\u6b3a\u51cc\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\uff08\u5982\u4ec7\u6068\u8a00\u8bba\u3001\u9a9a\u6270\u548c\u7f51\u7edc\u6b3a\u51cc\uff09\u5bf9\u5e73\u53f0\u5b89\u5168\u548c\u793e\u798f\u7949\u6784\u6210\u65e5\u76ca\u589e\u957f\u7684\u98ce\u9669\uff0c\u800c\u591a\u8f6e\u5bf9\u8bdd\u73af\u5883\u4e0b\u7684\u7814\u7a76\u56e0\u6570\u636e\u6709\u9650\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528CyberAgressionAdo-Large\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e866\u79cd\u57fa\u4e8e\u6587\u672c\u548c8\u79cd\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u8bcd\u6c47\u7ebf\u7d22\u3001\u4e92\u52a8\u52a8\u6001\u53ca\u5176\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u665a\u671f\u878d\u5408\u6a21\u578bmBERT + WD-SGCN\u5728\u8fb1\u9a82\u68c0\u6d4b\u4e0a\u8868\u73b0\u6700\u4f73\uff080.718\uff09\uff0c\u5728\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\uff080.286\uff09\u548c\u6b3a\u51cc\u5206\u6790\uff080.606\uff09\u4e0a\u4e5f\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u9690\u6666\u653b\u51fb\u3001\u89d2\u8272\u8f6c\u6362\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u654c\u610f\u7b49\u590d\u6742\u53cd\u793e\u4f1a\u884c\u4e3a\u73b0\u8c61\u3002"}}
{"id": "2510.16729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16729", "abs": "https://arxiv.org/abs/2510.16729", "authors": ["Jianbiao Mei", "Yu Yang", "Xuemeng Yang", "Licheng Wen", "Jiajun Lv", "Botian Shi", "Yong Liu"], "title": "Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models", "comment": null, "summary": "End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.", "AI": {"tldr": "IR-WM\u662f\u4e00\u79cd\u9690\u5f0f\u6b8b\u5dee\u4e16\u754c\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u5efa\u6a21\u4e16\u754c\u7684\u5f53\u524d\u72b6\u6001\u548c\u6f14\u5316\uff0c\u901a\u8fc7\u4ec5\u9884\u6d4b\u57fa\u4e8e\u81ea\u8f66\u52a8\u4f5c\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u7684\"\u6b8b\u5dee\"\u53d8\u5316\uff0c\u907f\u514d\u5197\u4f59\u5efa\u6a21\u9759\u6001\u80cc\u666f\uff0c\u663e\u8457\u63d0\u53474D\u5360\u7528\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u89c6\u89c9\u4e2d\u5fc3\u4e16\u754c\u6a21\u578b\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u5b83\u4eec\u5b8c\u5168\u91cd\u5efa\u672a\u6765\u573a\u666f\uff0c\u5c06\u5927\u91cf\u8ba1\u7b97\u80fd\u529b\u6d6a\u8d39\u5728\u5197\u4f59\u5efa\u6a21\u9759\u6001\u80cc\u666f\u4e0a\u3002", "method": "IR-WM\u9996\u5148\u4ece\u89c6\u89c9\u89c2\u6d4b\u5efa\u7acb\u5f53\u524d\u72b6\u6001\u7684\u9c81\u68d2\u9e1f\u77b0\u56fe\u8868\u793a\uff0c\u7136\u540e\u5229\u7528\u524d\u4e00\u65f6\u523b\u7684BEV\u7279\u5f81\u4f5c\u4e3a\u5f3a\u65f6\u5e8f\u5148\u9a8c\uff0c\u4ec5\u9884\u6d4b\u57fa\u4e8e\u81ea\u8f66\u52a8\u4f5c\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u6b8b\u5dee\u53d8\u5316\u3002\u8fd8\u5e94\u7528\u5bf9\u9f50\u6a21\u5757\u6821\u51c6\u8bed\u4e49\u548c\u52a8\u6001\u9519\u4f4d\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u7684\u9884\u6d4b-\u89c4\u5212\u8026\u5408\u65b9\u6848\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIR-WM\u57284D\u5360\u7528\u9884\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u65b9\u9762\u5747\u8fbe\u5230\u9876\u7ea7\u6027\u80fd\u3002", "conclusion": "\u9690\u5f0f\u6b8b\u5dee\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u4e13\u6ce8\u4e8e\u52a8\u6001\u53d8\u5316\u800c\u975e\u5b8c\u5168\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u9690\u5f0f\u672a\u6765\u72b6\u6001\u80fd\u5927\u5e45\u6539\u5584\u89c4\u5212\u7cbe\u5ea6\u3002"}}
{"id": "2510.17354", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17354", "abs": "https://arxiv.org/abs/2510.17354", "authors": ["Chenghao Zhang", "Guanting Dong", "Xinyu Yang", "Zhicheng Dou"], "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation", "comment": "This work is in progress", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Nyx\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u901a\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(URAG)\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684RAG\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6587\u672c\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u67e5\u8be2\u548c\u6587\u6863\u53ef\u80fd\u5305\u542b\u6df7\u5408\u6a21\u6001(\u5982\u6587\u672c\u548c\u56fe\u50cf)\uff0c\u9700\u8981\u89e3\u51b3\u6df7\u5408\u6a21\u6001\u4fe1\u606f\u7684\u68c0\u7d22\u548c\u63a8\u7406\u6311\u6218\u3002", "method": "1) \u6784\u5efaNyxQA\u6570\u636e\u96c6\uff1a\u901a\u8fc7\u56db\u9636\u6bb5\u81ea\u52a8\u7ba1\u9053\u4ece\u7f51\u7edc\u6587\u6863\u751f\u6210\u548c\u8fc7\u6ee4\u591a\u6837\u5316\u7684\u6df7\u5408\u6a21\u6001\u95ee\u7b54\u5bf9\uff1b2) \u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u5148\u5728NyxQA\u548c\u5f00\u6e90\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4f7f\u7528\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "Nyx\u4e0d\u4ec5\u5728\u6807\u51c6\u6587\u672cRAG\u57fa\u51c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0c\u5728\u66f4\u901a\u7528\u7684URAG\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "Nyx\u901a\u8fc7\u7edf\u4e00\u7684\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86URAG\u6311\u6218\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16730", "abs": "https://arxiv.org/abs/2510.16730", "authors": ["Tianyang Dou", "Ming Li", "Jiangying Qin", "Xuan Liao", "Jiageng Zhong", "Armin Gruen", "Mengyi Deng"], "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid", "comment": null, "summary": "Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce.", "AI": {"tldr": "UKANFormer\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u80fd\u591f\u5728\u6765\u81eaAllen Coral Atlas\u7684\u566a\u58f0\u76d1\u7763\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u73ca\u745a\u7901\u6620\u5c04\uff0c\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u7f13\u89e3\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u5168\u7403\u73ca\u745a\u7901\u4ea7\u54c1\u5982Allen Coral Atlas\u5728\u7a7a\u95f4\u7cbe\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u8fb9\u754c\u5212\u5206\u7684\u533a\u57df\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u73ca\u745a\u7901\u4fdd\u62a4\u3002", "method": "\u57fa\u4e8eUKAN\u67b6\u6784\uff0cUKANFormer\u5728\u89e3\u7801\u5668\u4e2d\u5f15\u5165\u4e86\u5168\u5c40-\u5c40\u90e8\u53d8\u6362\u5668\uff08GL-Trans\uff09\u5757\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u53d6\u5168\u5c40\u8bed\u4e49\u7ed3\u6784\u548c\u5c40\u90e8\u8fb9\u754c\u7ec6\u8282\u3002", "result": "UKANFormer\u5728\u73ca\u745a\u7c7bIoU\u8fbe\u523067.00%\uff0c\u50cf\u7d20\u7cbe\u5ea6\u8fbe\u523083.98%\uff0c\u5728\u76f8\u540c\u566a\u58f0\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ea7\u751f\u7684\u9884\u6d4b\u5728\u89c6\u89c9\u548c\u7ed3\u6784\u4e0a\u90fd\u6bd4\u8bad\u7ec3\u4f7f\u7528\u7684\u566a\u58f0\u6807\u7b7e\u66f4\u51c6\u786e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6570\u636e\u8d28\u91cf\u5e76\u4e0d\u76f4\u63a5\u9650\u5236\u6a21\u578b\u6027\u80fd\uff0c\u67b6\u6784\u8bbe\u8ba1\u53ef\u4ee5\u7f13\u89e3\u6807\u7b7e\u566a\u58f0\uff0c\u652f\u6301\u5728\u975e\u5b8c\u7f8e\u76d1\u7763\u4e0b\u8fdb\u884c\u53ef\u6269\u5c55\u6620\u5c04\uff0c\u4e3a\u751f\u6001\u76d1\u6d4b\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2510.17388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17388", "abs": "https://arxiv.org/abs/2510.17388", "authors": ["Henry Lim", "Kwan Hui Lim"], "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives", "comment": "11 pages, 1 figure, 8 tables", "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.", "AI": {"tldr": "\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u81ea\u5305\u542b\u6307\u4ee4\u6267\u884c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bf9\u9009\u9879\u6807\u7b7e\u683c\u5f0f\uff08\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7f57\u9a6c\u6570\u5b57\uff09\u654f\u611f\uff0c\u8868\u73b0\u51fa\u6307\u4ee4\u683c\u5f0f\u504f\u89c1\u548c\u6307\u4ee4\u9075\u5faa\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7d22\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u6267\u884c\u7b80\u5355\u81ea\u5305\u542b\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u590d\u6742\u6307\u4ee4\u9075\u5faa\u7684\u57fa\u7840\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5728\u4fee\u6539\u7684MMLU\u548cMMLU-Pro\u57fa\u51c6\u4e0a\u8bc4\u4f3020\u4e2a\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u9009\u9879\u6807\u7b7e\u683c\u5f0f\uff08\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7f57\u9a6c\u6570\u5b57\uff09\uff0c\u5728\u56db\u79cd\u8303\u5f0f\u4e0b\u6d4b\u8bd5\uff1a\u6709\u660e\u786e\u6307\u4ee4\u3001\u65e0\u6307\u4ee4\u3001\u79fb\u9664\u9009\u9879\u5185\u5bb9\u3001\u4f7f\u7528\u4e09\u6837\u672c\u793a\u4f8b\u3002", "result": "\u6807\u7b7e\u683c\u5f0f\u53d8\u5316\u5bfc\u81f4\u663e\u8457\u6027\u80fd\u6ce2\u52a8\uff08\u5982\u7f57\u9a6c\u6570\u5b57vs\u6570\u5b57\u4e0b\u964d30.45%\uff09\uff0c\u65e0\u6307\u4ee4\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u4e0b\u964d\uff0c\u79fb\u9664\u9009\u9879\u5185\u5bb9\u65f6\u6a21\u578b\u65e0\u6cd5\u8d85\u8d8a\u968f\u673a\u57fa\u7ebf\uff0c\u4e09\u6837\u672c\u793a\u4f8b\u65e0\u6cd5\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5927\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\u4f46\u6307\u4ee4\u9075\u5faa\u4ecd\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u539f\u5b50\u6307\u4ee4\u9075\u5faa\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2510.16732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16732", "abs": "https://arxiv.org/abs/2510.16732", "authors": ["Xinqing Li", "Xin He", "Le Zhang", "Yun Liu"], "title": "A Comprehensive Survey on World Models for Embodied AI", "comment": "https://github.com/Li-Zn-H/AwesomeWorldModels", "summary": "Embodied AI requires agents that perceive, act, and anticipate how actions\nreshape future world states. World models serve as internal simulators that\ncapture environment dynamics, enabling forward and counterfactual rollouts to\nsupport perception, prediction, and decision making. This survey presents a\nunified framework for world models in embodied AI. Specifically, we formalize\nthe problem setting and learning objectives, and propose a three-axis taxonomy\nencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)\nTemporal Modeling, Sequential Simulation and Inference vs. Global Difference\nPrediction; (3) Spatial Representation, Global Latent Vector, Token Feature\nSequence, Spatial Latent Grid, and Decomposed Rendering Representation. We\nsystematize data resources and metrics across robotics, autonomous driving, and\ngeneral video settings, covering pixel prediction quality, state-level\nunderstanding, and task performance. Furthermore, we offer a quantitative\ncomparison of state-of-the-art models and distill key open challenges,\nincluding the scarcity of unified datasets and the need for evaluation metrics\nthat assess physical consistency over pixel fidelity, the trade-off between\nmodel performance and the computational efficiency required for real-time\ncontrol, and the core modeling difficulty of achieving long-horizon temporal\nconsistency while mitigating error accumulation. Finally, we maintain a curated\nbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5177\u8eabAI\u4e16\u754c\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u62ec\u95ee\u9898\u5f62\u5f0f\u5316\u3001\u5b66\u4e60\u76ee\u6807\u548c\u4e09\u8f74\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5316\u6574\u7406\u4e86\u6570\u636e\u8d44\u6e90\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u4e86\u5b9a\u91cf\u6bd4\u8f83\uff0c\u6307\u51fa\u4e86\u5173\u952e\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u5177\u8eabAI\u9700\u8981\u80fd\u591f\u611f\u77e5\u3001\u884c\u52a8\u5e76\u9884\u6d4b\u884c\u52a8\u5982\u4f55\u91cd\u5851\u672a\u6765\u4e16\u754c\u72b6\u6001\u7684\u667a\u80fd\u4f53\u3002\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5185\u90e8\u6a21\u62df\u5668\uff0c\u80fd\u591f\u6355\u6349\u73af\u5883\u52a8\u6001\uff0c\u652f\u6301\u611f\u77e5\u3001\u9884\u6d4b\u548c\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u8f74\u5206\u7c7b\u6cd5\uff1a\u529f\u80fd\uff08\u51b3\u7b56\u8026\u5408vs\u901a\u7528\u76ee\u7684\uff09\u3001\u65f6\u95f4\u5efa\u6a21\uff08\u5e8f\u5217\u6a21\u62df\u63a8\u7406vs\u5168\u5c40\u5dee\u5f02\u9884\u6d4b\uff09\u3001\u7a7a\u95f4\u8868\u793a\uff08\u5168\u5c40\u6f5c\u5728\u5411\u91cf\u3001\u6807\u8bb0\u7279\u5f81\u5e8f\u5217\u3001\u7a7a\u95f4\u6f5c\u5728\u7f51\u683c\u3001\u5206\u89e3\u6e32\u67d3\u8868\u793a\uff09\u3002", "result": "\u7cfb\u7edf\u5316\u6574\u7406\u4e86\u673a\u5668\u4eba\u5b66\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u901a\u7528\u89c6\u9891\u8bbe\u7f6e\u7684\u6570\u636e\u8d44\u6e90\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u8986\u76d6\u50cf\u7d20\u9884\u6d4b\u8d28\u91cf\u3001\u72b6\u6001\u7ea7\u7406\u89e3\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u5b9a\u91cf\u6bd4\u8f83\u3002", "conclusion": "\u6307\u51fa\u4e86\u5173\u952e\u5f00\u653e\u6311\u6218\uff1a\u7edf\u4e00\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u3001\u9700\u8981\u8bc4\u4f30\u7269\u7406\u4e00\u81f4\u6027\u800c\u975e\u50cf\u7d20\u4fdd\u771f\u5ea6\u7684\u6307\u6807\u3001\u6a21\u578b\u6027\u80fd\u4e0e\u5b9e\u65f6\u63a7\u5236\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u3001\u5b9e\u73b0\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\u540c\u65f6\u51cf\u8f7b\u8bef\u5dee\u7d2f\u79ef\u7684\u6838\u5fc3\u5efa\u6a21\u96be\u5ea6\u3002"}}
{"id": "2510.17389", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17389", "abs": "https://arxiv.org/abs/2510.17389", "authors": ["Numaan Naeem", "Abdellah El Mekki", "Muhammad Abdul-Mageed"], "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs", "comment": "28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main", "summary": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduAdapt\u57fa\u51c6\uff0c\u5305\u542b\u8fd148k\u4e2a\u6309\u5e74\u7ea7\u6807\u6ce8\u7684\u79d1\u5b66\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u5e74\u7ea7\u6c34\u5e73\u4e0a\u7684\u9002\u5e94\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5927\u6a21\u578b\u5728\u4f4e\u5e74\u7ea7\u9002\u5e94\u6027\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u6839\u636e\u5b66\u751f\u5e74\u7ea7\u6c34\u5e73\u8c03\u6574\u56de\u7b54\uff0c\u8fd9\u5728K-12\u6559\u80b2\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u8de8\u8ba4\u77e5\u53d1\u5c55\u9636\u6bb5\u7684\u9002\u5e94\u6027\u3002", "method": "\u521b\u5efaEduAdapt\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b9\u4e2a\u79d1\u5b66\u79d1\u76ee\u7684\u8fd148k\u4e2a\u5e74\u7ea7\u6807\u6ce8\u95ee\u7b54\u5bf9\uff0c\u8986\u76d61-12\u5e74\u7ea7\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5f00\u6e90LLM\u5728\u8fd9\u4e9b\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5927\u6a21\u578b\u603b\u4f53\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u4e3a\u4f4e\u5e74\u7ea7\uff081-5\u5e74\u7ea7\uff09\u5b66\u751f\u751f\u6210\u5408\u9002\u56de\u7b54\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u5e74\u7ea7\u9002\u5e94\u6027\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u66f4\u597d\u7684\u8bad\u7ec3\u548c\u63d0\u793a\u7b56\u7565\u5f00\u53d1\u66f4\u9002\u5408\u6559\u80b2\u53d1\u5c55\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2510.16751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16751", "abs": "https://arxiv.org/abs/2510.16751", "authors": ["Erik Riise", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling", "comment": null, "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u79bb\u6563\u7684\u81ea\u56de\u5f52\u89c6\u89c9\u6a21\u578b\u6bd4\u8fde\u7eed\u6269\u6563\u6a21\u578b\u66f4\u9002\u5408\u63a8\u7406\u65f6\u641c\u7d22\u4f18\u5316\uff0c\u675f\u641c\u7d22\u80fd\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c2B\u53c2\u6570\u7684\u81ea\u56de\u5f52\u6a21\u578b\u53ef\u8d85\u8d8a12B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u63a8\u7406\u65f6\u641c\u7d22\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u5e94\u7528\u641c\u7d22\u7b56\u7565\u6548\u679c\u6709\u9650\uff0c\u7b80\u5355\u968f\u673a\u91c7\u6837\u5f80\u5f80\u8868\u73b0\u6700\u4f73\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u641c\u7d22\u7b56\u7565\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "\u91c7\u7528\u79bb\u6563\u7684\u81ea\u56de\u5f52\u89c6\u89c9\u6a21\u578b\uff0c\u5229\u7528\u675f\u641c\u7d22\u8fdb\u884c\u63a8\u7406\u65f6\u4f18\u5316\uff0c\u901a\u8fc7\u65e9\u671f\u526a\u679d\u548c\u8ba1\u7b97\u91cd\u7528\u63d0\u5347\u6548\u7387\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u79bb\u6563\u6807\u8bb0\u7a7a\u95f4\u7684\u4f18\u52bf\u3002", "result": "\u675f\u641c\u7d22\u663e\u8457\u6539\u5584\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c2B\u53c2\u6570\u81ea\u56de\u5f52\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a12B\u53c2\u6570\u6269\u6563\u6a21\u578b\uff0c\u9a8c\u8bc1\u5668\u5206\u6790\u63ed\u793a\u4e86\u901f\u5ea6\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\uff08\u800c\u975e\u4ec5\u89c4\u6a21\uff09\u5bf9\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u63a8\u7406\u65f6\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u79bb\u6563\u81ea\u56de\u5f52\u6a21\u578b\u4e3a\u641c\u7d22\u7b56\u7565\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.17402", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17402", "abs": "https://arxiv.org/abs/2510.17402", "authors": ["Jiacheng Xie", "Shuai Zeng", "Yang Yu", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.", "AI": {"tldr": "Ladder-base\u662f\u9996\u4e2a\u57fa\u4e8eGRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u7684\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e2d\u533b\u63a8\u7406\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u901a\u7528LLM\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u77e5\u8bc6\u4f53\u7cfb\u72ec\u7279\u4e14\u590d\u6742\uff0c\u73b0\u6709\u4e2d\u533b\u4e13\u7528LLM\u5b58\u5728\u5bf9\u9f50\u4e0d\u8db3\u3001\u6570\u636e\u8d28\u91cf\u5dee\u548c\u8bc4\u4f30\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eQwen2.5-7B-Instruct\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728TCM-Ladder\u57fa\u51c6\u7684\u6587\u672c\u5b50\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c80%\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\uff0c\u5269\u4f5920%\u5747\u5206\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u3002", "result": "Ladder-base\u5728\u591a\u9879\u63a8\u7406\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4\u3001Gemini 2.5\u3001Claude 3\u3001Qwen3\u7b49\u901a\u7528LLM\u4ee5\u53caBenTsao\u3001HuatuoGPT2\u3001Zhongjing\u7b49\u4e2d\u533b\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "GRPO\u4e3a\u5728\u4f20\u7edf\u533b\u5b66\u9886\u57df\u5bf9\u9f50\u4e13\u5bb6\u7ea7\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\uff0c\u652f\u6301\u5f00\u53d1\u53ef\u4fe1\u8d56\u4e14\u4e34\u5e8a\u57fa\u7840\u624e\u5b9e\u7684\u4e2d\u533b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2510.16752", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16752", "abs": "https://arxiv.org/abs/2510.16752", "authors": ["Ivan Molodetskikh", "Kirill Malyshev", "Mark Mirgaleev", "Nikita Zagainov", "Evgeney Bogatyrev", "Dmitriy Vatolin"], "title": "Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution", "comment": null, "summary": "Generative image super-resolution (SR) is rapidly advancing in visual quality\nand detail restoration. As the capacity of SR models expands, however, so does\ntheir tendency to produce artifacts: incorrect, visually disturbing details\nthat reduce perceived quality. Crucially, their perceptual impact varies: some\nartifacts are barely noticeable while others strongly degrade the image. We\nargue that artifacts should be characterized by their prominence to human\nobservers rather than treated as uniform binary defects. Motivated by this, we\npresent a novel dataset of 1302 artifact examples from 11 contemporary image-SR\nmethods, where each artifact is paired with a crowdsourced prominence score.\nBuilding on this dataset, we train a lightweight regressor that produces\nspatial prominence heatmaps and outperforms existing methods at detecting\nprominent artifacts. We release the dataset and code to facilitate\nprominence-aware evaluation and mitigation of SR artifacts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b1302\u4e2aSR\u4f2a\u5f71\u793a\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u4f2a\u5f71\u90fd\u6709\u4f17\u5305\u663e\u8457\u6027\u8bc4\u5206\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u56de\u5f52\u5668\u6765\u751f\u6210\u7a7a\u95f4\u663e\u8457\u6027\u70ed\u56fe\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u68c0\u6d4b\u663e\u8457\u4f2a\u5f71\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5bb9\u91cf\u7684\u6269\u5927\uff0c\u5b83\u4eec\u4ea7\u751f\u4f2a\u5f71\u7684\u8d8b\u52bf\u4e5f\u5728\u589e\u52a0\uff0c\u8fd9\u4e9b\u4f2a\u5f71\u4f1a\u964d\u4f4e\u611f\u77e5\u8d28\u91cf\u3002\u5173\u952e\u7684\u662f\uff0c\u5b83\u4eec\u7684\u611f\u77e5\u5f71\u54cd\u5404\u4e0d\u76f8\u540c\uff1a\u6709\u4e9b\u4f2a\u5f71\u51e0\u4e4e\u4e0d\u53ef\u89c1\uff0c\u800c\u5176\u4ed6\u4f2a\u5f71\u5219\u4e25\u91cd\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u3002\u6211\u4eec\u8ba4\u4e3a\u5e94\u8be5\u6839\u636e\u4f2a\u5f71\u5bf9\u4eba\u7c7b\u89c2\u5bdf\u8005\u7684\u663e\u8457\u6027\u6765\u8868\u5f81\u5b83\u4eec\uff0c\u800c\u4e0d\u662f\u5c06\u5176\u89c6\u4e3a\u7edf\u4e00\u7684\u4e8c\u5143\u7f3a\u9677\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1302\u4e2a\u6765\u81ea11\u79cd\u5f53\u4ee3\u56fe\u50cfSR\u65b9\u6cd5\u7684\u4f2a\u5f71\u793a\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u4f2a\u5f71\u90fd\u914d\u6709\u4f17\u5305\u663e\u8457\u6027\u8bc4\u5206\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u56de\u5f52\u5668\u6765\u751f\u6210\u7a7a\u95f4\u663e\u8457\u6027\u70ed\u56fe\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u663e\u8457\u4f2a\u5f71\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u57fa\u4e8e\u663e\u8457\u6027\u7684SR\u4f2a\u5f71\u8bc4\u4f30\u548c\u7f13\u89e3\u3002"}}
{"id": "2510.17405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17405", "abs": "https://arxiv.org/abs/2510.17405", "authors": ["Mardiyyah Oduwole", "Prince Mireku", "Fatimo Adebanjo", "Oluwatosin Olajide", "Mahi Aminu Aliyu", "Jekaterina Novikova"], "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages", "comment": null, "summary": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.", "AI": {"tldr": "AfriCaption\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u6846\u67b6\uff0c\u652f\u630120\u79cd\u975e\u6d32\u8bed\u8a00\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7ffb\u8bd1\u3001\u52a8\u6001\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u548c\u4e13\u95e8\u6a21\u578b\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u5efa\u7acb\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u63cf\u8ff0\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001AI\u7814\u7a76\u8fc7\u5ea6\u96c6\u4e2d\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u6280\u672f\u6c11\u4e3b\u5316\uff0c\u4e3a\u975e\u6d32\u8bed\u8a00\u63d0\u4f9b\u5305\u5bb9\u6027\u7684\u591a\u6a21\u6001AI\u57fa\u7840\u3002", "method": "\u6784\u5efa\u57fa\u4e8eFlickr8k\u7684\u8bed\u4e49\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u9009\u62e9\u548c\u7ffb\u8bd1\uff1b\u5efa\u7acb\u52a8\u6001\u4e0a\u4e0b\u6587\u4fdd\u6301\u7ba1\u9053\uff0c\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u548c\u81ea\u9002\u5e94\u66ff\u6362\u4fdd\u8bc1\u8d28\u91cf\uff1b\u5f00\u53d10.5B\u53c2\u6570\u7684AfriCaption\u6a21\u578b\uff0c\u96c6\u6210SigLIP\u548cNLLB200\u8fdb\u884c\u8de8\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u53ef\u6269\u5c55\u56fe\u50cf\u63cf\u8ff0\u8d44\u6e90\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6301\u7eed\u4fdd\u8bc1\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "AfriCaption\u4e3a\u771f\u6b63\u5305\u5bb9\u7684\u591a\u6a21\u6001AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u586b\u8865\u4e86\u975e\u6d32\u8bed\u8a00\u5728\u56fe\u50cf\u63cf\u8ff0\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.16765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16765", "abs": "https://arxiv.org/abs/2510.16765", "authors": ["Shengyu Zhu", "Fan", "Fuxuan Zhang"], "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement", "comment": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV),\n  Oral", "summary": "Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.", "AI": {"tldr": "\u63d0\u51faWaMaIR\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u5377\u79ef\u6269\u5927\u611f\u53d7\u91ce\uff0c\u7ed3\u5408Mamba\u901a\u9053\u611f\u77e5\u6a21\u5757\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\uff0c\u5e76\u4f7f\u7528\u591a\u5c3a\u5ea6\u7eb9\u7406\u589e\u5f3a\u635f\u5931\u6765\u63d0\u5347\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u4f20\u7edfCNN\u65b9\u6cd5\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u7531\u4e8e\u611f\u53d7\u91ce\u5c0f\u548c\u7f3a\u4e4f\u901a\u9053\u7279\u5f81\u5efa\u6a21\uff0c\u96be\u4ee5\u5145\u5206\u6062\u590d\u7cbe\u7ec6\u7eb9\u7406\u7ec6\u8282\u3002", "method": "\u91c7\u7528\u5168\u5c40\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u5377\u79ef(GMWTConvs)\u6269\u5927\u611f\u53d7\u91ce\uff0c\u5f15\u5165Mamba\u901a\u9053\u611f\u77e5\u6a21\u5757(MCAM)\u6355\u83b7\u901a\u9053\u95f4\u957f\u7a0b\u4f9d\u8d56\uff0c\u5e76\u4f7f\u7528\u591a\u5c3a\u5ea6\u7eb9\u7406\u589e\u5f3a\u635f\u5931(MTELoss)\u6307\u5bfc\u7eb9\u7406\u7ed3\u6784\u4fdd\u7559\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eWaMaIR\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u6027\u80fd\u3002", "conclusion": "WaMaIR\u6846\u67b6\u901a\u8fc7\u6269\u5927\u611f\u53d7\u91ce\u548c\u589e\u5f3a\u901a\u9053\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u7eb9\u7406\u7ec6\u8282\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2510.17415", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17415", "abs": "https://arxiv.org/abs/2510.17415", "authors": ["Jiacheng Xie", "Yang Yu", "Yibo Chen", "Hanyao Zhang", "Lening Zhao", "Jiaxuan He", "Lei Jiang", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8eChatGPT\u7684\u4e2d\u533b\u591a\u6a21\u6001\u52a9\u624bBenCao\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u800c\u975e\u53c2\u6570\u91cd\u8bad\u7ec3\uff0c\u6574\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3001\u8bca\u65ad\u6570\u636e\u548c\u4e13\u5bb6\u53cd\u9988\uff0c\u5728\u4e2d\u533b\u95ee\u7b54\u548c\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u4f9d\u8d56\u6574\u4f53\u63a8\u7406\u3001\u9690\u542b\u903b\u8f91\u548c\u591a\u6a21\u6001\u8bca\u65ad\u7ebf\u7d22\uff0c\u73b0\u6709\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u591a\u6a21\u6001\u6574\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b1000\u591a\u90e8\u7ecf\u5178\u548c\u73b0\u4ee3\u6587\u672c\u7684\u77e5\u8bc6\u5e93\uff0c\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u6307\u4ee4\u6846\u67b6\u3001\u601d\u7ef4\u94fe\u6a21\u62df\u673a\u5236\u8fdb\u884c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u6267\u4e1a\u4e2d\u533b\u5e08\u53cd\u9988\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u8fde\u63a5\u820c\u8c61\u5206\u7c7b\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\u68c0\u7d22\u7684\u5916\u90e8API\u3002", "result": "\u5728\u5355\u9009\u9898\u57fa\u51c6\u548c\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cBenCao\u5728\u8bca\u65ad\u3001\u8349\u836f\u8bc6\u522b\u548c\u4f53\u8d28\u5206\u7c7b\u65b9\u9762\u51c6\u786e\u7387\u4f18\u4e8e\u901a\u7528\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\uff0c\u5df2\u5728OpenAI GPTs\u5546\u5e97\u90e8\u7f72\uff0c\u622a\u81f32025\u5e7410\u6708\u6709\u8fd11000\u540d\u5168\u7403\u7528\u6237\u8bbf\u95ee\u3002", "conclusion": "\u8bc1\u660e\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u548c\u591a\u6a21\u6001\u6574\u5408\u5f00\u53d1\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4e0e\u4f20\u7edf\u533b\u5b66\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u548c\u53ef\u6269\u5c55\u90e8\u7f72\u8def\u5f84\u3002"}}
{"id": "2510.16772", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16772", "abs": "https://arxiv.org/abs/2510.16772", "authors": ["Thuy Phuong Vu", "Dinh-Cuong Hoang", "Minhhuy Le", "Phan Xuan Tan"], "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning", "comment": null, "summary": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git", "AI": {"tldr": "\u63d0\u51fa\u4e86Region in Context\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u7f16\u8f91\uff0c\u4f7f\u5c40\u90e8\u533a\u57df\u5728\u5168\u5c40\u56fe\u50cf\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u534f\u8c03\u7f16\u8f91", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b64\u7acb\u5904\u7406\u56fe\u50cf\u533a\u57df\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u7ebf\u7d22\uff0c\u5bfc\u81f4\u7f16\u8f91\u4e0d\u4e00\u81f4\u3001\u8fc7\u6e21\u4e0d\u81ea\u7136\u6216\u6574\u4f53\u8fde\u8d2f\u6027\u4e27\u5931", "method": "\u5f15\u5165\u53cc\u7ea7\u5f15\u5bfc\u673a\u5236\uff1a\u533a\u57df\u5728\u5168\u56fe\u50cf\u4e0a\u4e0b\u6587\u4e2d\u8868\u793a\u5e76\u4e0e\u8be6\u7ec6\u533a\u57df\u7ea7\u63cf\u8ff0\u5bf9\u9f50\uff0c\u540c\u65f6\u6574\u4e2a\u56fe\u50cf\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u573a\u666f\u7ea7\u63cf\u8ff0\u5339\u914d", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u8fde\u8d2f\u4e14\u4e0e\u6307\u4ee4\u5bf9\u9f50\u7684\u7f16\u8f91\u7ed3\u679c", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u534f\u8c03\u7684\u56fe\u50cf\u7f16\u8f91"}}
{"id": "2510.17426", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17426", "abs": "https://arxiv.org/abs/2510.17426", "authors": ["Tiancheng Hu", "Benjamin Minixhofer", "Nigel Collier"], "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging", "comment": null, "summary": "The \"alignment tax\" of post-training is typically framed as a drop in task\naccuracy. We show it also involves a severe loss of calibration, making models\noverconfident, less reliable, and model outputs less diverse. We show that this\ntrade-off can be navigated effectively via a simple post-hoc intervention:\ninterpolating between a model's weights before and after alignment. Crucially,\nthis is not a strict trade-off. We find that the process consistently reveals\nPareto-optimal interpolations - models that improve accuracy beyond both\nparents while substantially recovering the calibration lost during alignment.\nOur work demonstrates that simple model merging provides a computationally\nefficient method for mitigating the full scope of the alignment tax, yielding\nmodels that are more capable and more reliable.", "AI": {"tldr": "\u8bba\u6587\u8868\u660e\u5bf9\u9f50\u8bad\u7ec3\u4e0d\u4ec5\u5bfc\u81f4\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u8fd8\u9020\u6210\u6a21\u578b\u6821\u51c6\u4e25\u91cd\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u3001\u53ef\u9760\u6027\u964d\u4f4e\u4e14\u8f93\u51fa\u591a\u6837\u6027\u51cf\u5c11\u3002\u901a\u8fc7\u7b80\u5355\u7684\u6743\u91cd\u63d2\u503c\u65b9\u6cd5\uff0c\u53ef\u4ee5\u627e\u5230\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u6a21\u578b\uff0c\u5728\u63d0\u5347\u51c6\u786e\u7387\u7684\u540c\u65f6\u6062\u590d\u6821\u51c6\u6027\u3002", "motivation": "\u63ed\u793a\u5bf9\u9f50\u8bad\u7ec3\u5e26\u6765\u7684\u5b8c\u6574\u4ee3\u4ef7\u2014\u2014\u4e0d\u4ec5\u662f\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u8fd8\u5305\u62ec\u6821\u51c6\u6027\u635f\u5931\uff0c\u8fd9\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u548c\u53ef\u9760\u6027\u964d\u4f4e\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u540e\u5904\u7406\u5e72\u9884\uff1a\u5728\u6a21\u578b\u5bf9\u9f50\u524d\u540e\u7684\u6743\u91cd\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u5bfb\u627e\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u63d2\u503c\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u627e\u5230\u540c\u65f6\u8d85\u8d8a\u4e24\u4e2a\u7236\u6a21\u578b\u51c6\u786e\u7387\u7684\u6a21\u578b\uff0c\u5e76\u663e\u8457\u6062\u590d\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u635f\u5931\u7684\u6821\u51c6\u6027\uff0c\u8fd9\u4e0d\u662f\u4e25\u683c\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7b80\u5355\u7684\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u5bf9\u9f50\u8bad\u7ec3\u7684\u5168\u90e8\u4ee3\u4ef7\uff0c\u4ea7\u751f\u66f4\u5f3a\u5927\u4e14\u66f4\u53ef\u9760\u7684\u6a21\u578b\u3002"}}
{"id": "2510.16776", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16776", "abs": "https://arxiv.org/abs/2510.16776", "authors": ["Mingzheng Zhang", "Jinfeng Gao", "Dan Xu", "Jiangrui Yu", "Yuhan Qiao", "Lan Chen", "Jin Tang", "Xiao Wang"], "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation", "comment": null, "summary": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence that can significantly reduce diagnostic burdens for\nclinicians and patient wait times. Existing MRG models predominantly rely on\nLarge Language Models (LLMs) to improve report generation, with limited\nexploration of pre-trained vision foundation models or advanced fine-tuning\ntechniques. Mainstream frameworks either avoid fine-tuning or utilize\nsimplistic methods like LoRA, often neglecting the potential of enhancing\ncross-attention mechanisms. Additionally, while Transformer-based models\ndominate vision-language tasks, non-Transformer architectures, such as the\nMamba network, remain underexplored for medical report generation, presenting a\npromising avenue for future research. In this paper, we propose EMRRG, a novel\nX-ray report generation framework that fine-tunes pre-trained Mamba networks\nusing parameter-efficient methods. Specifically, X-ray images are divided into\npatches, tokenized, and processed by an SSM-based vision backbone for feature\nextraction, with Partial LoRA yielding optimal performance. An LLM with a\nhybrid decoder generates the medical report, enabling end-to-end training and\nachieving strong results on benchmark datasets. Extensive experiments on three\nwidely used benchmark datasets fully validated the effectiveness of our\nproposed strategies for the X-ray MRG. The source code of this paper will be\nreleased on https://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86EMRRG\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684Mamba\u7f51\u7edc\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884cX\u5c04\u7ebf\u533b\u7597\u62a5\u544a\u751f\u6210\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u62a5\u544a\u751f\u6210\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u5148\u8fdb\u5fae\u8c03\u6280\u672f\u63a2\u7d22\u6709\u9650\uff0c\u4e14\u5ffd\u7565\u4e86\u975eTransformer\u67b6\u6784\u7684\u6f5c\u529b\u3002", "method": "\u5c06X\u5c04\u7ebf\u56fe\u50cf\u5206\u5757\u6807\u8bb0\u5316\uff0c\u901a\u8fc7SSM-based\u89c6\u89c9\u9aa8\u5e72\u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528Partial LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u7ed3\u5408\u6df7\u5408\u89e3\u7801\u5668\u7684LLM\u751f\u6210\u533b\u7597\u62a5\u544a\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "EMRRG\u6846\u67b6\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3Mamba\u7f51\u7edc\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5728X\u5c04\u7ebf\u533b\u7597\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17431", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17431", "abs": "https://arxiv.org/abs/2510.17431", "authors": ["Yushi Yang", "Shreyansh Padarha", "Andrew Lee", "Adam Mahdi"], "title": "Agentic Reinforcement Learning for Search is Unsafe", "comment": null, "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.", "AI": {"tldr": "RL\u8bad\u7ec3\u7684\u641c\u7d22\u6a21\u578b\u867d\u7136\u7ee7\u627f\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u8106\u5f31\u6027\u3002\u4e24\u79cd\u7b80\u5355\u653b\u51fb\uff08\u641c\u7d22\u653b\u51fb\u548c\u591a\u641c\u7d22\u653b\u51fb\uff09\u53ef\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u66b4\u9732\u4e86\u5f53\u524dRL\u8bad\u7ec3\u5728\u5b89\u5168\u65b9\u9762\u7684\u7f3a\u9677\u3002", "motivation": "\u7814\u7a76RL\u8bad\u7ec3\u7684\u641c\u7d22\u6a21\u578b\u7684\u5b89\u5168\u7279\u6027\uff0c\u63ed\u793a\u5176\u8106\u5f31\u6027\u5e76\u63a8\u52a8\u5f00\u53d1\u5b89\u5168\u611f\u77e5\u7684\u4ee3\u7406RL\u8bad\u7ec3\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u7b80\u5355\u653b\u51fb\u65b9\u6cd5\uff1a\u641c\u7d22\u653b\u51fb\uff08\u5f3a\u5236\u6a21\u578b\u4ee5\u641c\u7d22\u5f00\u59cb\u54cd\u5e94\uff09\u548c\u591a\u641c\u7d22\u653b\u51fb\uff08\u9f13\u52b1\u6a21\u578b\u91cd\u590d\u641c\u7d22\uff09\uff0c\u5728Qwen\u548cLlama\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u653b\u51fb\u4f7f\u62d2\u7edd\u7387\u964d\u4f4e\u9ad8\u8fbe60.0%\uff0c\u7b54\u6848\u5b89\u5168\u6027\u964d\u4f4e82.5%\uff0c\u641c\u7d22\u67e5\u8be2\u5b89\u5168\u6027\u964d\u4f4e82.4%\u3002\u653b\u51fb\u901a\u8fc7\u89e6\u53d1\u6a21\u578b\u751f\u6210\u6709\u5bb3\u7684\u955c\u50cf\u641c\u7d22\u67e5\u8be2\u6765\u7ed5\u8fc7\u62d2\u7edd\u673a\u5236\u3002", "conclusion": "\u5f53\u524dRL\u8bad\u7ec3\u5b58\u5728\u6838\u5fc3\u5f31\u70b9\uff1a\u5956\u52b1\u6709\u6548\u67e5\u8be2\u751f\u6210\u800c\u4e0d\u8003\u8651\u5176\u5371\u5bb3\u6027\uff0c\u5bfc\u81f4\u641c\u7d22\u6a21\u578b\u5b58\u5728\u6613\u88ab\u5229\u7528\u7684\u6f0f\u6d1e\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5b89\u5168\u611f\u77e5\u7684\u4ee3\u7406RL\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2510.16777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16777", "abs": "https://arxiv.org/abs/2510.16777", "authors": ["Junbo Li", "Weimin Yuan", "Yinuo Wang", "Yue Zeng", "Shihao Shu", "Cai Meng", "Xiangzhi Bai"], "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation", "comment": null, "summary": "Accurate 6D pose estimation of 3D objects is a fundamental task in computer\nvision, and current research typically predicts the 6D pose by establishing\ncorrespondences between 2D image features and 3D model features. However, these\nmethods often face difficulties with textureless objects and varying\nillumination conditions. To overcome these limitations, we propose GS2POSE, a\nnovel approach for 6D object pose estimation. GS2POSE formulates a pose\nregression algorithm inspired by the principles of Bundle Adjustment (BA). By\nleveraging Lie algebra, we extend the capabilities of 3DGS to develop a\npose-differentiable rendering pipeline, which iteratively optimizes the pose by\ncomparing the input image to the rendered image. Additionally, GS2POSE updates\ncolor parameters within the 3DGS model, enhancing its adaptability to changes\nin illumination. Compared to previous models, GS2POSE demonstrates accuracy\nimprovements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and\nLineMod datasets, respectively.", "AI": {"tldr": "GS2POSE\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u675f\u8c03\u6574\u4f18\u5316\uff0c\u5728\u7eb9\u7406\u7f3a\u5931\u548c\u5149\u7167\u53d8\u5316\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf6D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d562D-3D\u7279\u5f81\u5bf9\u5e94\uff0c\u5728\u7eb9\u7406\u7f3a\u5931\u7269\u4f53\u548c\u53d8\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u675f\u8c03\u6574\u539f\u7406\uff0c\u5229\u7528\u674e\u4ee3\u6570\u6269\u5c553DGS\u6784\u5efa\u59ff\u6001\u53ef\u5fae\u5206\u6e32\u67d3\u7ba1\u7ebf\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\u56fe\u50cf\u4e0e\u6e32\u67d3\u56fe\u50cf\u7684\u5dee\u5f02\u6765\u4f30\u8ba1\u59ff\u6001\uff0c\u5e76\u66f4\u65b03DGS\u989c\u8272\u53c2\u6570\u4ee5\u9002\u5e94\u5149\u7167\u53d8\u5316\u3002", "result": "\u5728T-LESS\u3001LineMod-Occlusion\u548cLineMod\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e861.4%\u30012.8%\u548c2.5%\u7684\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "GS2POSE\u901a\u8fc7\u7ed3\u54083DGS\u548c\u675f\u8c03\u6574\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7eb9\u7406\u7f3a\u5931\u548c\u5149\u7167\u53d8\u5316\u4e0b\u76846D\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2510.17437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17437", "abs": "https://arxiv.org/abs/2510.17437", "authors": ["Manuela Daniela Danu", "George Marica", "Constantin Suciu", "Lucian Mihai Itu", "Oladimeji Farri"], "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings", "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)", "summary": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eBERT\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u5728BioASQ MultiCardioNER\u5171\u4eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u5e73\u5747\u6c34\u5e73\u7684F1\u5206\u6570\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u4ee5\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u4e34\u5e8a\u7cfb\u7edf\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7814\u7a76\u3002", "method": "\u63a2\u7d22\u5355\u8bed\u548c\u591a\u8bedBERT\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u6587\u672c\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7528\u4e8e\u4ece\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u4e2d\u63d0\u53d6\u75be\u75c5\u548c\u836f\u7269\u63d0\u53ca\u3002", "result": "\u5728\u897f\u73ed\u7259\u8bed\u75be\u75c5\u8bc6\u522b\u83b7\u5f9777.88% F1\u5206\u6570\uff0c\u897f\u73ed\u7259\u8bed\u836f\u7269\u8bc6\u522b92.09%\uff0c\u82f1\u8bed\u836f\u7269\u8bc6\u522b91.74%\uff0c\u610f\u5927\u5229\u8bed\u836f\u7269\u8bc6\u522b88.9%\uff0c\u5747\u8d85\u8fc7\u6d4b\u8bd5\u6392\u884c\u699c\u7684\u5e73\u5747\u548c\u4e2d\u4f4d\u6570\u5206\u6570\u3002", "conclusion": "\u57fa\u4e8eBERT\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5d4c\u5165\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e34\u5e8a\u6587\u672c\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00\u4e34\u5e8aNLP\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16781", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16781", "abs": "https://arxiv.org/abs/2510.16781", "authors": ["Shihao Ji", "Zihui Song"], "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features", "comment": null, "summary": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u89c6\u9891\u7406\u89e3\u91cd\u6784\u4e3a\u9ad8\u7ef4\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u81ea\u76d1\u7763\u65f6\u7a7a\u805a\u7c7b\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u56fe\u50cf\u4e0a\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5145\u5206\u5e94\u7528\u4e8e\u89c6\u9891\u9886\u57df\uff0c\u4f20\u7edf\u89c6\u9891\u7406\u89e3\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u6709\u9650\u3002", "method": "\u9996\u5148\u4f7f\u7528\u9884\u8bad\u7ec3VLM\u7684\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u5c06\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u8bed\u4e49\u7279\u5f81\u8f68\u8ff9\uff0c\u7136\u540e\u4f7f\u7528\u6838\u65f6\u95f4\u5206\u5272(KTS)\u5c06\u8fde\u7eed\u7279\u5f81\u6d41\u5206\u5272\u4e3a\u79bb\u6563\u7684\u8bed\u4e49\u8fde\u8d2f\u4e8b\u4ef6\u6bb5\uff0c\u6700\u540e\u901a\u8fc7\u65e0\u76d1\u7763\u5bc6\u5ea6\u805a\u7c7b\u8bc6\u522b\u91cd\u590d\u51fa\u73b0\u7684\u5b8f\u89c2\u573a\u666f\u548c\u4e3b\u9898\u3002", "result": "\u901a\u8fc7\u4ece\u6bcf\u4e2a\u53d1\u73b0\u7684\u805a\u7c7b\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u5173\u952e\u5e27\uff0c\u5e76\u5229\u7528VLM\u7684\u751f\u6210\u80fd\u529b\u8fdb\u884c\u6587\u672c\u63cf\u8ff0\uff0c\u81ea\u52a8\u751f\u6210\u89c6\u9891\u5185\u5bb9\u7684\u7ed3\u6784\u5316\u591a\u6a21\u6001\u6458\u8981\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u96f6\u6837\u672c\u3001\u81ea\u52a8\u5316\u7684\u89c6\u9891\u5185\u5bb9\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u9014\u5f84\u3002"}}
{"id": "2510.17460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17460", "abs": "https://arxiv.org/abs/2510.17460", "authors": ["Muhammad Farmal Khan", "Mousumi Akter"], "title": "Evaluating Large Language Models on Urdu Idiom Translation", "comment": null, "summary": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u5230\u82f1\u8bed\u7684\u4e60\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u548cNMT\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u80fd\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e14\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u6587\u672c\u6bd4\u7f57\u9a6c\u5316\u4e4c\u5c14\u90fd\u8bed\u6587\u672c\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u4e60\u8bed\u7ffb\u8bd1\u3002", "motivation": "\u4e60\u8bed\u7ffb\u8bd1\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u6b64\u524d\u7814\u7a76\u5173\u6ce8\u6709\u9650\u3002", "method": "\u6784\u5efa\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u5230\u82f1\u8bed\u4e60\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90LLM\u548cNMT\u7cfb\u7edf\uff0c\u4f7f\u7528BLEU\u3001BERTScore\u3001COMET\u548cXCOMET\u7b49\u81ea\u52a8\u6307\u6807\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u63d0\u793a\u5de5\u7a0b\u76f8\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u80fd\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f46\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u95f4\u5dee\u5f02\u8f83\u5c0f\uff1b\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u8f93\u5165\u6bd4\u7f57\u9a6c\u5316\u4e4c\u5c14\u90fd\u8bed\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u4e60\u8bed\u7ffb\u8bd1\u3002", "conclusion": "\u6587\u672c\u8868\u793a\u5f62\u5f0f\u663e\u8457\u5f71\u54cd\u7ffb\u8bd1\u8d28\u91cf\uff0c\u539f\u751f\u4e4c\u5c14\u90fd\u8bed\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7f57\u9a6c\u5316\u4e4c\u5c14\u90fd\u8bed\uff0c\u63d0\u793a\u5de5\u7a0b\u662f\u63d0\u5347\u4e60\u8bed\u7ffb\u8bd1\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.16785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16785", "abs": "https://arxiv.org/abs/2510.16785", "authors": ["Jiazhen Liu", "Long Chen"], "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs", "comment": null, "summary": "Integrating diverse visual capabilities into a unified model is a significant\ntrend in Multimodal Large Language Models (MLLMs). Among these, the inclusion\nof segmentation poses a distinct set of challenges. To equip MLLMs with\npixel-level segmentation abilities, prevailing methods require finetuning the\nmodel to produce specific outputs compatible with a mask decoder. This process\ntypically alters the model's output space and compromises its intrinsic\ngeneralization, which undermines the goal of building a unified model. We\nintroduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel\nplug-and-play solution. LENS attaches a lightweight, trainable head to a\ncompletely frozen MLLM. By refining the spatial cues embedded in attention\nmaps, LENS extracts keypoints and describes them into point-wise features\ndirectly compatible with the mask decoder. Extensive experiments validate our\napproach: LENS achieves segmentation performance competitive with or superior\nto that of retraining-based methods. Crucially, it does so while fully\npreserving the MLLM's generalization capabilities, which are significantly\ndegraded by finetuning approaches. As such, the attachable design of LENS\nestablishes an efficient and powerful paradigm for extending MLLMs, paving the\nway for truly multi-talented, unified models.", "AI": {"tldr": "LENS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u5b8c\u5168\u51bb\u7ed3\u7684MLLM\u4e0a\u9644\u52a0\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u5934\u90e8\uff0c\u5229\u7528\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u9700\u8981\u5fae\u8c03MLLM\u4ee5\u4ea7\u751f\u4e0e\u63a9\u7801\u89e3\u7801\u5668\u517c\u5bb9\u7684\u7279\u5b9a\u8f93\u51fa\uff0c\u8fd9\u4f1a\u6539\u53d8\u6a21\u578b\u7684\u8f93\u51fa\u7a7a\u95f4\u5e76\u635f\u5bb3\u5176\u5185\u5728\u6cdb\u5316\u80fd\u529b\uff0c\u8fdd\u80cc\u4e86\u6784\u5efa\u7edf\u4e00\u6a21\u578b\u7684\u76ee\u6807\u3002", "method": "LENS\u5728\u5b8c\u5168\u51bb\u7ed3\u7684MLLM\u4e0a\u9644\u52a0\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u5934\u90e8\uff0c\u901a\u8fc7\u7cbe\u70bc\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\u6765\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u5e76\u5c06\u5176\u63cf\u8ff0\u4e3a\u4e0e\u63a9\u7801\u89e3\u7801\u5668\u76f4\u63a5\u517c\u5bb9\u7684\u70b9\u7ea7\u7279\u5f81\u3002", "result": "LENS\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e\u91cd\u65b0\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u5b8c\u5168\u4fdd\u7559\u4e86MLLM\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LENS\u7684\u53ef\u9644\u52a0\u8bbe\u8ba1\u4e3a\u6269\u5c55MLLM\u5efa\u7acb\u4e86\u9ad8\u6548\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u4e3a\u771f\u6b63\u591a\u624d\u591a\u827a\u7684\u7edf\u4e00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.17476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17476", "abs": "https://arxiv.org/abs/2510.17476", "authors": ["Ipek Baris Schlicht", "Burcu Sayin", "Zhixue Zhao", "Frederik M. Labont\u00e9", "Cesare Barbera", "Marco Viviani", "Paolo Rosso", "Lucie Flek"], "title": "Disparities in Multilingual LLM-Based Healthcare Q&A", "comment": "Under review", "summary": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8de8\u8bed\u8a00\u4e8b\u5b9e\u5bf9\u9f50\u5dee\u5f02\uff0c\u5373\u4f7f\u662f\u975e\u82f1\u8bed\u63d0\u793a\uff0c\u6a21\u578b\u56de\u7b54\u4e5f\u66f4\u503e\u5411\u4e8e\u4e0e\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u5bf9\u9f50\u3002\u901a\u8fc7\u63d0\u4f9b\u975e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u6539\u5584\u4e8b\u5b9e\u5bf9\u9f50\u3002", "motivation": "\u5c06AI\u6574\u5408\u5230\u533b\u7597\u4fdd\u5065\u4e2d\u65f6\uff0c\u516c\u5e73\u83b7\u53d6\u53ef\u9760\u5065\u5eb7\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u4f46\u4e0d\u540c\u8bed\u8a00\u7684\u4fe1\u606f\u8d28\u91cf\u5b58\u5728\u5dee\u5f02\uff0c\u5f15\u53d1\u4e86\u5bf9\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u6784\u5efa\u4e86\u591a\u8bed\u8a00\u7ef4\u57fa\u533b\u7597\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u8de8\u8bed\u8a00\u533b\u7597\u8986\u76d6\u5ea6\uff0c\u8bc4\u4f30\u4e86LLM\u56de\u7b54\u4e0e\u53c2\u8003\u4fe1\u606f\u7684\u5bf9\u9f50\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8fdb\u884c\u4e86\u4e8b\u5b9e\u5bf9\u9f50\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u7ef4\u57fa\u767e\u79d1\u8986\u76d6\u5ea6\u548cLLM\u4e8b\u5b9e\u5bf9\u9f50\u90fd\u5b58\u5728\u663e\u8457\u7684\u8de8\u8bed\u8a00\u5dee\u5f02\u3002\u6240\u6709LLM\u7684\u56de\u7b54\u90fd\u66f4\u503e\u5411\u4e8e\u4e0e\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u5bf9\u9f50\uff0c\u5373\u4f7f\u63d0\u793a\u662f\u975e\u82f1\u8bed\u7684\u3002\u63d0\u4f9b\u975e\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u4e0a\u4e0b\u6587\u53ef\u4ee5\u6709\u6548\u5c06\u4e8b\u5b9e\u5bf9\u9f50\u8f6c\u5411\u6587\u5316\u76f8\u5173\u77e5\u8bc6\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u6784\u5efa\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u533b\u7597AI\u7cfb\u7edf\u7684\u5b9e\u7528\u9014\u5f84\uff0c\u901a\u8fc7\u63d0\u4f9b\u672c\u5730\u5316\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u6539\u5584\u8de8\u8bed\u8a00\u4e8b\u5b9e\u5bf9\u9f50\u3002"}}
{"id": "2510.16790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16790", "abs": "https://arxiv.org/abs/2510.16790", "authors": ["Sara Hatami Rostami", "Behrooz Nasihatkon"], "title": "Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry", "comment": "7 pages, 3 figures", "summary": "This paper presents a fully unsupervised approach for binary road\nsegmentation (road vs. non-road), eliminating the reliance on costly manually\nlabeled datasets. The method leverages scene geometry and temporal cues to\ndistinguish road from non-road regions. Weak labels are first generated from\ngeometric priors, marking pixels above the horizon as non-road and a predefined\nquadrilateral in front of the vehicle as road. In a refinement stage, temporal\nconsistency is enforced by tracking local feature points across frames and\npenalizing inconsistent label assignments using mutual information\nmaximization. This enhances both precision and temporal stability. On the\nCityscapes dataset, the model achieves an Intersection-over-Union (IoU) of\n0.82, demonstrating high accuracy with a simple design. These findings\ndemonstrate the potential of combining geometric constraints and temporal\nconsistency for scalable unsupervised road segmentation in autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u9053\u8def\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u6765\u533a\u5206\u9053\u8def\u4e0e\u975e\u9053\u8def\u533a\u57df\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u6d88\u9664\u5bf9\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u9053\u8def\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u57fa\u4e8e\u51e0\u4f55\u5148\u9a8c\u751f\u6210\u5f31\u6807\u7b7e\uff08\u5730\u5e73\u7ebf\u4ee5\u4e0a\u4e3a\u975e\u9053\u8def\uff0c\u8f66\u8f86\u524d\u65b9\u56db\u8fb9\u5f62\u4e3a\u9053\u8def\uff09\uff0c\u7136\u540e\u901a\u8fc7\u8de8\u5e27\u8ddf\u8e2a\u5c40\u90e8\u7279\u5f81\u70b9\u5e76\u5229\u7528\u4e92\u4fe1\u606f\u6700\u5927\u5316\u6765\u589e\u5f3a\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.82\u7684IoU\u5206\u6570\uff0c\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u7b80\u5355\u8bbe\u8ba1\u3002", "conclusion": "\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u53ef\u6269\u5c55\u65e0\u76d1\u7763\u9053\u8def\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.17483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17483", "abs": "https://arxiv.org/abs/2510.17483", "authors": ["Zheyue Tan", "Zhiyuan Li", "Tao Yuan", "Dong Zhou", "Weilin Liu", "Yueqing Zhuang", "Yadong Li", "Guowei Niu", "Cheng Qin", "Zhuyu Yao", "Congyi Liu", "Haiyang Xu", "Boxun Li", "Guohao Dai", "Bo Zhao", "Yu Wang"], "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.", "AI": {"tldr": "ReXMoE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u5c42\u590d\u7528\u4e13\u5bb6\u6765\u7a81\u7834\u4f20\u7edf\u5c42\u5c40\u90e8\u8def\u7531\u7684\u9650\u5236\uff0c\u5728\u56fa\u5b9a\u53c2\u6570\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u4e13\u5bb6\u7ec4\u5408\uff0c\u63d0\u5347\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfMoE\u67b6\u6784\u7684\u5c42\u5c40\u90e8\u8def\u7531\u673a\u5236\u9650\u5236\u4e86\u4e13\u5bb6\u7ef4\u5ea6\u548c\u8def\u7531\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u9700\u8981\u65b0\u7684\u8bbe\u8ba1\u6765\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faReXMoE\u67b6\u6784\uff0c\u5141\u8bb8\u8def\u7531\u5668\u5728\u76f8\u90bb\u5c42\u4e4b\u95f4\u590d\u7528\u4e13\u5bb6\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u7f29\u653e\u8def\u7531\u7b56\u7565\u9010\u6b65\u6269\u5927\u5019\u9009\u4e13\u5bb6\u6c60\u3002", "result": "\u57280.5B\u52307B\u53c2\u6570\u89c4\u6a21\u7684\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u4e0a\uff0cReXMoE\u5728\u56fa\u5b9a\u67b6\u6784\u7ef4\u5ea6\u4e0b\u6301\u7eed\u63d0\u5347\u8bed\u8a00\u5efa\u6a21\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "ReXMoE\u4e3a\u53c2\u6570\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684MoE-based LLMs\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2510.16791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16791", "abs": "https://arxiv.org/abs/2510.16791", "authors": ["Chengxuan Zhu", "Shuchen Weng", "Jiacong Fang", "Peixuan Zhang", "Si Li", "Chao Xu", "Boxin Shi"], "title": "Personalized Image Filter: Mastering Your Photographic Style", "comment": null, "summary": "Photographic style, as a composition of certain photographic concepts, is the\ncharm behind renowned photographers. But learning and transferring photographic\nstyle need a profound understanding of how the photo is edited from the unknown\noriginal appearance. Previous works either fail to learn meaningful\nphotographic concepts from reference images, or cannot preserve the content of\nthe content image. To tackle these issues, we proposed a Personalized Image\nFilter (PIF). Based on a pretrained text-to-image diffusion model, the\ngenerative prior enables PIF to learn the average appearance of photographic\nconcepts, as well as how to adjust them according to text prompts. PIF then\nlearns the photographic style of reference images with the textual inversion\ntechnique, by optimizing the prompts for the photographic concepts. PIF shows\noutstanding performance in extracting and transferring various kinds of\nphotographic style. Project page: https://pif.pages.dev/", "AI": {"tldr": "\u63d0\u51fa\u4e2a\u6027\u5316\u56fe\u50cf\u6ee4\u955cPIF\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u53cd\u6f14\u6280\u672f\u5b66\u4e60\u53c2\u8003\u56fe\u50cf\u7684\u6444\u5f71\u98ce\u683c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u63d0\u53d6\u548c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6444\u5f71\u6982\u5ff5\uff0c\u6216\u8005\u65e0\u6cd5\u4fdd\u6301\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9\u5b8c\u6574\u6027\u3002\u6444\u5f71\u98ce\u683c\u4f5c\u4e3a\u7279\u5b9a\u6444\u5f71\u6982\u5ff5\u7684\u7ec4\u5408\uff0c\u662f\u8457\u540d\u6444\u5f71\u5e08\u9b45\u529b\u7684\u6765\u6e90\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u6587\u672c\u53cd\u6f14\u6280\u672f\u4f18\u5316\u6444\u5f71\u6982\u5ff5\u7684\u63d0\u793a\u8bcd\uff0c\u5b66\u4e60\u53c2\u8003\u56fe\u50cf\u7684\u6444\u5f71\u98ce\u683c\u3002", "result": "PIF\u5728\u63d0\u53d6\u548c\u8fc1\u79fb\u5404\u79cd\u6444\u5f71\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PIF\u80fd\u591f\u6709\u6548\u5b66\u4e60\u6444\u5f71\u6982\u5ff5\u7684\u5e73\u5747\u5916\u89c2\uff0c\u5e76\u6839\u636e\u6587\u672c\u63d0\u793a\u8c03\u6574\u8fd9\u4e9b\u6982\u5ff5\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6444\u5f71\u98ce\u683c\u8fc1\u79fb\u3002"}}
{"id": "2510.17489", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17489", "abs": "https://arxiv.org/abs/2510.17489", "authors": ["Yongxin He", "Shan Zhang", "Yixuan Cao", "Lei Ma", "Ping Luo"], "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning", "comment": "To appear in NeurIPS 2025", "summary": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.", "AI": {"tldr": "\u63d0\u51faDETree\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\u5efa\u6a21\u4e0d\u540cAI\u53c2\u4e0e\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u4e13\u7528\u635f\u5931\u51fd\u6570\u5bf9\u9f50\u6587\u672c\u8868\u793a\uff0c\u5728\u6df7\u5408\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u591a\u6837\u5316\u7684\u4eba\u673a\u534f\u4f5c\u8fc7\u7a0b\u5efa\u6a21\u7c97\u7cd9\uff0c\u4e3b\u8981\u91c7\u7528\u4e8c\u5143\u5206\u7c7b\u6216\u591a\u5206\u7c7b\u65b9\u6cd5\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u5305\u542b\u591a\u79cdLLM\u53c2\u4e0e\u7684\u590d\u6742\u534f\u4f5c\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faDETree\u65b9\u6cd5\uff0c\u5c06\u4e0d\u540c\u751f\u6210\u8fc7\u7a0b\u7684\u5173\u7cfb\u5efa\u6a21\u4e3a\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e13\u7528\u635f\u5931\u51fd\u6570\u5bf9\u9f50\u6587\u672c\u8868\u793a\u4e0e\u6811\u7ed3\u6784\uff0c\u5e76\u6784\u5efaRealBench\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u65b9\u6cd5\u5728\u6df7\u5408\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u5b66\u4e60\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u8bbe\u7f6e\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c42\u6b21\u4eb2\u548c\u6811\u7ed3\u6784\u80fd\u6709\u6548\u5efa\u6a21\u590d\u6742\u7684\u4eba\u673a\u534f\u4f5c\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2510.16800", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16800", "abs": "https://arxiv.org/abs/2510.16800", "authors": ["Zhenpeng Zhang", "Yi Wang", "Shanglei Chai", "Yingying Liu", "Zekai Xie", "Wenhao Huang", "Pengyu Li", "Zipei Luo", "Dajiang Lu", "Yibin Tian"], "title": "An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting", "comment": null, "summary": "Lychee is a high-value subtropical fruit. The adoption of vision-based\nharvesting robots can significantly improve productivity while reduce reliance\non labor. High-quality data are essential for developing such harvesting\nrobots. However, there are currently no consistently and comprehensively\nannotated open-source lychee datasets featuring fruits in natural growing\nenvironments. To address this, we constructed a dataset to facilitate lychee\ndetection and maturity classification. Color (RGB) images were acquired under\ndiverse weather conditions, and at different times of the day, across multiple\nlychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset\nencompasses three different ripeness stages and contains 11,414 images,\nconsisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth\nimages. The images are annotated with 9,658 pairs of lables for lychee\ndetection and maturity classification. To improve annotation consistency, three\nindividuals independently labeled the data, and their results were then\naggregated and verified by a fourth reviewer. Detailed statistical analyses\nwere done to examine the dataset. Finally, we performed experiments using three\nrepresentative deep learning models to evaluate the dataset. It is publicly\navailable for academic", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8354\u679d\u68c0\u6d4b\u548c\u6210\u719f\u5ea6\u5206\u7c7b\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5305\u542b11,414\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u4e0d\u540c\u54c1\u79cd\u3001\u5929\u6c14\u6761\u4ef6\u548c\u6210\u719f\u9636\u6bb5\u7684\u8354\u679d\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u7edf\u8ba1\u5206\u6790\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5728\u81ea\u7136\u751f\u957f\u73af\u5883\u4e2d\u5177\u6709\u4e00\u81f4\u6027\u548c\u5168\u9762\u6807\u6ce8\u7684\u8354\u679d\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u800c\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\u4e8e\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684\u8354\u679d\u91c7\u6458\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6536\u96c6\u4e86\u591a\u4e2a\u8354\u679d\u54c1\u79cd\u5728\u4e0d\u540c\u5929\u6c14\u6761\u4ef6\u548c\u65f6\u95f4\u6bb5\u7684\u5f69\u8272\u56fe\u50cf\uff0c\u5305\u542b\u4e09\u79cd\u6210\u719f\u5ea6\u9636\u6bb5\uff0c\u901a\u8fc7\u4e09\u4eba\u72ec\u7acb\u6807\u6ce8\u548c\u7b2c\u56db\u4eba\u9a8c\u8bc1\u786e\u4fdd\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528\u4e09\u79cd\u4ee3\u8868\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b11,414\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff08878\u5f20\u539f\u59cbRGB\u56fe\u50cf\u30018,780\u5f20\u589e\u5f3aRGB\u56fe\u50cf\u548c1,756\u5f20\u6df1\u5ea6\u56fe\u50cf\uff09\uff0c\u6807\u6ce8\u4e869,658\u5bf9\u8354\u679d\u68c0\u6d4b\u548c\u6210\u719f\u5ea6\u5206\u7c7b\u6807\u7b7e\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u8354\u679d\u89c6\u89c9\u68c0\u6d4b\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u8354\u679d\u91c7\u6458\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6301\uff0c\u5e76\u5df2\u516c\u5f00\u4f9b\u5b66\u672f\u4f7f\u7528\u3002"}}
{"id": "2510.17491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17491", "abs": "https://arxiv.org/abs/2510.17491", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "comment": null, "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e1a\u667a\u80fd\u4f53\u6280\u672f\u3001\u5e94\u7528\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4ece\"\u6d41\u7a0b\u6267\u884c\u7cfb\u7edf\"\u5230\"\u81ea\u9002\u5e94\u793e\u4f1a\u7cfb\u7edf\"\u7684\u6f14\u8fdb\u8def\u5f84\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u5df2\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u524d\u6cbf\u3002\u4f46\u5982\u4f55\u5c06\u901a\u7528\u667a\u80fd\u4f53\u7814\u7a76\u6210\u679c\u8f6c\u5316\u4e3a\u63a8\u52a8\u884c\u4e1a\u53d8\u9769\u7684\u751f\u4ea7\u529b\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u652f\u6491\u667a\u80fd\u4f53\u80fd\u529b\u53d1\u5c55\u7684\u4e09\u5927\u6280\u672f\u652f\u67f1\uff1a\u8bb0\u5fc6\u3001\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u6982\u8ff0\u5728\u6570\u5b57\u5de5\u7a0b\u3001\u79d1\u5b66\u53d1\u73b0\u3001\u5177\u8eab\u667a\u80fd\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u7684\u6280\u672f\u6f14\u8fdb\u8def\u5f84\u548c\u5e94\u7528\u573a\u666f\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u8bc4\u4f30\u7cfb\u7edf\u5728\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u884c\u4e1a\u7279\u6027\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6280\u672f\u6f14\u8fdb\u4e0e\u884c\u4e1a\u5b9e\u8df5\uff0c\u4e3a\u7406\u89e3\u548c\u6784\u5efa\u4e0b\u4e00\u4ee3\u884c\u4e1a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u7406\u8bba\u57fa\u7840\uff0c\u6307\u660e\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.16822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16822", "abs": "https://arxiv.org/abs/2510.16822", "authors": ["Yahia Battach", "Abdulwahab Felemban", "Faizan Farooq Khan", "Yousef A. Radwan", "Xiang Li", "Fabio Marchese", "Sara Beery", "Burton H. Jones", "Francesca Benzoni", "Mohamed Elhoseiny"], "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification", "comment": null, "summary": "Coral reefs are rapidly declining due to anthropogenic pressures such as\nclimate change, underscoring the urgent need for scalable, automated\nmonitoring. We introduce ReefNet, a large public coral reef image dataset with\npoint-label annotations mapped to the World Register of Marine Species (WoRMS).\nReefNet aggregates imagery from 76 curated CoralNet sources and an additional\nsite from Al Wajh in the Red Sea, totaling approximately 925000 genus-level\nhard coral annotations with expert-verified labels. Unlike prior datasets,\nwhich are often limited by size, geography, or coarse labels and are not\nML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global\nscale to WoRMS. We propose two evaluation settings: (i) a within-source\nbenchmark that partitions each source's images for localized evaluation, and\n(ii) a cross-source benchmark that withholds entire sources to test domain\ngeneralization. We analyze both supervised and zero-shot classification\nperformance on ReefNet and find that while supervised within-source performance\nis promising, supervised performance drops sharply across domains, and\nperformance is low across the board for zero-shot models, especially for rare\nand visually similar genera. This provides a challenging benchmark intended to\ncatalyze advances in domain generalization and fine-grained coral\nclassification. We will release our dataset, benchmarking code, and pretrained\nmodels to advance robust, domain-adaptive, global coral reef monitoring and\nconservation.", "AI": {"tldr": "ReefNet\u662f\u4e00\u4e2a\u5927\u578b\u516c\u5f00\u73ca\u745a\u7901\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6925,000\u4e2a\u5c5e\u7ea7\u786c\u73ca\u745a\u6807\u6ce8\uff0c\u6620\u5c04\u5230\u4e16\u754c\u6d77\u6d0b\u7269\u79cd\u540d\u5f55\uff0c\u7528\u4e8e\u63a8\u52a8\u81ea\u52a8\u73ca\u745a\u76d1\u6d4b\u548c\u9886\u57df\u6cdb\u5316\u7814\u7a76\u3002", "motivation": "\u73ca\u745a\u7901\u56e0\u6c14\u5019\u53d8\u5316\u7b49\u4eba\u4e3a\u538b\u529b\u5feb\u901f\u8870\u9000\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u65b9\u6cd5\u3002\u73b0\u6709\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u5730\u7406\u8986\u76d6\u548c\u6807\u7b7e\u7c92\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u5b66\u4e60\u9700\u6c42\u3002", "method": "\u6574\u540876\u4e2aCoralNet\u6765\u6e90\u548c\u7ea2\u6d77Al Wajh\u7ad9\u70b9\u7684\u56fe\u50cf\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u5206\u7c7b\u5b66\u6620\u5c04\u7684\u6807\u6ce8\u3002\u63d0\u51fa\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a\u6e90\u5185\u57fa\u51c6\u548c\u8de8\u6e90\u57fa\u51c6\uff0c\u6d4b\u8bd5\u76d1\u7763\u5b66\u4e60\u548c\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u76d1\u7763\u5b66\u4e60\u5728\u6e90\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8de8\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u96f6\u6837\u672c\u6a21\u578b\u6574\u4f53\u8868\u73b0\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5bf9\u7a00\u6709\u548c\u89c6\u89c9\u76f8\u4f3c\u5c5e\u7684\u5206\u7c7b\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "ReefNet\u4e3a\u9886\u57df\u6cdb\u5316\u548c\u7ec6\u7c92\u5ea6\u73ca\u745a\u5206\u7c7b\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u5c06\u4fc3\u8fdb\u9c81\u68d2\u3001\u9886\u57df\u81ea\u9002\u5e94\u7684\u5168\u7403\u73ca\u745a\u7901\u76d1\u6d4b\u4e0e\u4fdd\u62a4\u3002"}}
{"id": "2510.17498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17498", "abs": "https://arxiv.org/abs/2510.17498", "authors": ["Zihan Liu", "Shun Zheng", "Xumeng Wen", "Yang Wang", "Jiang Bian", "Mao Yang"], "title": "Deep Self-Evolving Reasoning", "comment": null, "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.", "AI": {"tldr": "Deep Self-Evolving Reasoning (DSER) \u662f\u4e00\u79cd\u6982\u7387\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fed\u4ee3\u63a8\u7406\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5229\u7528\u591a\u4e2a\u5e76\u884c\u957f\u7a0b\u81ea\u6f14\u5316\u8fc7\u7a0b\u653e\u5927\u5fae\u5c0f\u6539\u8fdb\u6982\u7387\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5c0f\u578b\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u9a8c\u8bc1-\u7cbe\u70bc\u6846\u67b6\u4f9d\u8d56\u5f3a\u5927\u7684\u9a8c\u8bc1\u548c\u4fee\u6b63\u80fd\u529b\uff0c\u8fd9\u5728\u5f00\u653e\u6743\u91cd\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u5f88\u8106\u5f31\u3002DSER\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5373\u4f7f\u9a8c\u8bc1\u548c\u7cbe\u70bc\u80fd\u529b\u8f83\u5f31\uff0c\u4e5f\u80fd\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u6781\u9650\u3002", "method": "\u5c06\u8fed\u4ee3\u63a8\u7406\u6982\u5ff5\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u4ee3\u8868\u89e3\u7a7a\u95f4\u4e2d\u7684\u968f\u673a\u8f6c\u79fb\u3002\u5173\u952e\u5728\u4e8e\u53ea\u8981\u6539\u8fdb\u6982\u7387\u7565\u9ad8\u4e8e\u9000\u5316\u6982\u7387\uff0c\u5c31\u80fd\u4fdd\u8bc1\u6536\u655b\u5230\u6b63\u786e\u89e3\u3002\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u957f\u7a0b\u81ea\u6f14\u5316\u8fc7\u7a0b\u6765\u653e\u5927\u8fd9\u4e9b\u5fae\u5c0f\u6b63\u5411\u8d8b\u52bf\u3002", "result": "\u5728DeepSeek-R1-0528-Qwen3-8B\u6a21\u578b\u4e0a\u5e94\u7528DSER\uff0c\u5728AIME 2024-2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e869\u4e2a\u5148\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e2d\u76845\u4e2a\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4f7f\u8fd9\u4e2a\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u591a\u6570\u6295\u7968\u8d85\u8fc7\u4e86\u5176600B\u53c2\u6570\u6559\u5e08\u7684\u5355\u8f6e\u51c6\u786e\u7387\u3002", "conclusion": "DSER\u4e0d\u4ec5\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u8fd8\u7528\u4e8e\u8bca\u65ad\u5f53\u524d\u5f00\u653e\u6743\u91cd\u63a8\u7406\u5668\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u5185\u5728\u81ea\u6f14\u5316\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3\u6a21\u578b\u786e\u7acb\u4e86\u660e\u786e\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2510.16832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16832", "abs": "https://arxiv.org/abs/2510.16832", "authors": ["Abdur Rahman", "Mohammad Marufuzzaman", "Jason Street", "Haifeng Wang", "Veera G. Gude", "Randy Buchanan"], "title": "Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction", "comment": null, "summary": "Accurate and quick prediction of wood chip moisture content is critical for\noptimizing biofuel production and ensuring energy efficiency. The current\nwidely used direct method (oven drying) is limited by its longer processing\ntime and sample destructiveness. On the other hand, existing indirect methods,\nincluding near-infrared spectroscopy-based, electrical capacitance-based, and\nimage-based approaches, are quick but not accurate when wood chips come from\nvarious sources. Variability in the source material can alter data\ndistributions, undermining the performance of data-driven models. Therefore,\nthere is a need for a robust approach that effectively mitigates the impact of\nsource variability. Previous studies show that manually extracted texture\nfeatures have the potential to predict wood chip moisture class. Building on\nthis, in this study, we conduct a comprehensive analysis of five distinct\ntexture feature types extracted from wood chip images to predict moisture\ncontent. Our findings reveal that a combined feature set incorporating all five\ntexture features achieves an accuracy of 95% and consistently outperforms\nindividual texture features in predicting moisture content. To ensure robust\nmoisture prediction, we propose a domain adaptation method named AdaptMoist\nthat utilizes the texture features to transfer knowledge from one source of\nwood chip data to another, addressing variability across different domains. We\nalso proposed a criterion for model saving based on adjusted mutual\ninformation. The AdaptMoist method improves prediction accuracy across domains\nby 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted\nmodels. These results highlight the effectiveness of AdaptMoist as a robust\nsolution for wood chip moisture content estimation across domains, making it a\npotential solution for wood chip-reliant industries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptMoist\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u7eb9\u7406\u7279\u5f81\u9884\u6d4b\u6728\u5c51\u6c34\u5206\u542b\u91cf\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u6765\u6e90\u6728\u5c51\u6570\u636e\u5206\u5e03\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5728\u8de8\u9886\u57df\u9884\u6d4b\u4e2d\u51c6\u786e\u7387\u63d0\u534723%\u3002", "motivation": "\u73b0\u6709\u6728\u5c51\u6c34\u5206\u542b\u91cf\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u76f4\u63a5\u65b9\u6cd5\uff08\u70d8\u7bb1\u5e72\u71e5\uff09\u5904\u7406\u65f6\u95f4\u957f\u4e14\u7834\u574f\u6837\u672c\uff1b\u95f4\u63a5\u65b9\u6cd5\uff08\u8fd1\u7ea2\u5916\u5149\u8c31\u3001\u7535\u5bb9\u3001\u56fe\u50cf\uff09\u5728\u6728\u5c51\u6765\u6e90\u591a\u6837\u65f6\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6765\u6e90\u53d8\u5f02\u6027\u7684\u7a33\u5065\u65b9\u6cd5\u3002", "method": "1. \u4ece\u6728\u5c51\u56fe\u50cf\u4e2d\u63d0\u53d6\u4e94\u79cd\u4e0d\u540c\u7684\u7eb9\u7406\u7279\u5f81\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff1b2. \u63d0\u51faAdaptMoist\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u7eb9\u7406\u7279\u5f81\u5c06\u77e5\u8bc6\u4ece\u4e00\u4e2a\u6728\u5c51\u6570\u636e\u6e90\u8f6c\u79fb\u5230\u53e6\u4e00\u4e2a\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u8c03\u6574\u4e92\u4fe1\u606f\u7684\u6807\u51c6\u8fdb\u884c\u6a21\u578b\u4fdd\u5b58\u3002", "result": "1. \u7ed3\u5408\u6240\u6709\u4e94\u79cd\u7eb9\u7406\u7279\u5f81\u7684\u7279\u5f81\u96c6\u5728\u9884\u6d4b\u6c34\u5206\u542b\u91cf\u65f6\u8fbe\u523095%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5355\u4e2a\u7eb9\u7406\u7279\u5f81\uff1b2. AdaptMoist\u65b9\u6cd5\u5c06\u8de8\u9886\u57df\u9884\u6d4b\u51c6\u786e\u7387\u4ece57%\u63d0\u5347\u81f380%\uff0c\u63d0\u9ad8\u4e8623%\u3002", "conclusion": "AdaptMoist\u4f5c\u4e3a\u8de8\u9886\u57df\u6728\u5c51\u6c34\u5206\u542b\u91cf\u4f30\u8ba1\u7684\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u975e\u5e38\u6709\u6548\uff0c\u6709\u671b\u6210\u4e3a\u4f9d\u8d56\u6728\u5c51\u884c\u4e1a\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17504", "abs": "https://arxiv.org/abs/2510.17504", "authors": ["Jingshu Liu", "Raheel Qader", "Ga\u00ebtan Caillaut", "Mariam Nakhl\u00e9"], "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task", "comment": null, "summary": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u6700\u4f73\u65b9\u6cd5\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e73\u884c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u5728112\u79cd\u8bed\u8a00\u7684Tatoeba\u6570\u636e\u96c6\u4e0a\u8fbe\u523083.7%\u7684\u53cc\u6587\u672c\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86LASER\u768465.5%\u3002", "motivation": "\u867d\u7136BERT\u5728\u5355\u8bed\u53e5\u5b50\u5d4c\u5165\u5b66\u4e60\u65b9\u9762\u5f88\u6709\u6548\uff0c\u4f46\u57fa\u4e8eBERT\u7684\u8de8\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u5b66\u4e60\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86\u63a9\u7801\u8bed\u8a00\u5efa\u6a21(MLM)\u3001\u7ffb\u8bd1\u8bed\u8a00\u5efa\u6a21(TLM)\u3001\u53cc\u7f16\u7801\u5668\u7ffb\u8bd1\u6392\u5e8f\u548c\u52a0\u6027\u8fb9\u7f18softmax\u7b49\u6700\u4f73\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5c06\u6240\u9700\u5e73\u884c\u8bad\u7ec3\u6570\u636e\u51cf\u5c1180%\uff1b\u5728Tatoeba\u6570\u636e\u96c6112\u79cd\u8bed\u8a00\u4e0a\u8fbe\u523083.7%\u7684\u53cc\u6587\u672c\u68c0\u7d22\u51c6\u786e\u7387\uff1b\u5728\u5355\u8bed\u8fc1\u79fb\u5b66\u4e60\u57fa\u51c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff1b\u4eceCommonCrawl\u6316\u6398\u7684\u5e73\u884c\u6570\u636e\u53ef\u8bad\u7ec3\u51fa\u6709\u7ade\u4e89\u529b\u7684NMT\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u68c0\u7d22\u548c\u5355\u8bed\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5df2\u516c\u5f00\u53d1\u5e03\u652f\u6301109+\u79cd\u8bed\u8a00\u7684\u6a21\u578b\u3002"}}
{"id": "2510.16833", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16833", "abs": "https://arxiv.org/abs/2510.16833", "authors": ["Xiangyu Mu", "Dongliang Zhou", "Jie Hou", "Haijun Zhang", "Weili Guan"], "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display", "comment": null, "summary": "Mannequin-based clothing displays offer a cost-effective alternative to\nreal-model showcases for online fashion presentation, but lack realism and\nexpressive detail. To overcome this limitation, we introduce a new task called\nmannequin-to-human (M2H) video generation, which aims to synthesize\nidentity-controllable, photorealistic human videos from footage of mannequins.\nWe propose M2HVideo, a pose-aware and identity-preserving video generation\nframework that addresses two key challenges: the misalignment between head and\nbody motion, and identity drift caused by temporal modeling. In particular,\nM2HVideo incorporates a dynamic pose-aware head encoder that fuses facial\nsemantics with body pose to produce consistent identity embeddings across\nframes. To address the loss of fine facial details due to latent space\ncompression, we introduce a mirror loss applied in pixel space through a\ndenoising diffusion implicit model (DDIM)-based one-step denoising.\nAdditionally, we design a distribution-aware adapter that aligns statistical\ndistributions of identity and clothing features to enhance temporal coherence.\nExtensive experiments on the UBC fashion dataset, our self-constructed ASOS\ndataset, and the newly collected MannequinVideos dataset captured on-site\ndemonstrate that M2HVideo achieves superior performance in terms of clothing\nconsistency, identity preservation, and video fidelity in comparison to\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faM2HVideo\u6846\u67b6\uff0c\u5c06\u4eba\u53f0\u89c6\u9891\u8f6c\u6362\u4e3a\u8eab\u4efd\u53ef\u63a7\u3001\u903c\u771f\u7684\u4eba\u7c7b\u89c6\u9891\uff0c\u89e3\u51b3\u5934\u90e8\u4e0e\u8eab\u4f53\u8fd0\u52a8\u4e0d\u5bf9\u9f50\u548c\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u4eba\u53f0\u670d\u88c5\u5c55\u793a\u6210\u672c\u4f4e\u4f46\u7f3a\u4e4f\u771f\u5b9e\u611f\uff0c\u9700\u8981\u5c06\u4eba\u53f0\u89c6\u9891\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u4eba\u7c7b\u89c6\u9891\u6765\u63d0\u5347\u5728\u7ebf\u65f6\u5c1a\u5c55\u793a\u6548\u679c\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u59ff\u52bf\u611f\u77e5\u5934\u90e8\u7f16\u7801\u5668\u878d\u5408\u9762\u90e8\u8bed\u4e49\u4e0e\u8eab\u4f53\u59ff\u52bf\uff0c\u5f15\u5165\u955c\u50cf\u635f\u5931\u548c\u5206\u5e03\u611f\u77e5\u9002\u914d\u5668\uff0c\u57fa\u4e8eDDIM\u7684\u4e00\u6b65\u53bb\u566a\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM2HVideo\u5728\u670d\u88c5\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u9891\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "M2HVideo\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u53f0\u5230\u4eba\u7c7b\u89c6\u9891\u8f6c\u6362\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8eab\u4efd\u53ef\u63a7\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.17509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17509", "abs": "https://arxiv.org/abs/2510.17509", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Minghao Tang", "Jingtong Wu", "Zengxin Han", "Xueqi Cheng"], "title": "Annotation-Efficient Universal Honesty Alignment", "comment": null, "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86EliCal\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0LLM\u7684\u8bda\u5b9e\u5bf9\u9f50\uff1a\u5148\u4f7f\u7528\u5ec9\u4ef7\u7684\u81ea\u6211\u4e00\u81f4\u6027\u76d1\u7763\u6fc0\u53d1\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u7136\u540e\u7528\u5c11\u91cf\u6b63\u786e\u6027\u6807\u6ce8\u8fdb\u884c\u6821\u51c6\uff0c\u4ec5\u97001k\u6807\u6ce8\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bda\u5b9e\u5bf9\u9f50\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8bad\u7ec3\u514d\u8d39\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u8981\u4e48\u9700\u8981\u5927\u89c4\u6a21\u6b63\u786e\u6027\u6807\u6ce8\u8fdb\u884c\u8bad\u7ec3\u6821\u51c6\uff0c\u800c\u5b9e\u73b0\u901a\u7528\u8bda\u5b9e\u5bf9\u9f50\u9700\u8981\u6602\u8d35\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u3002", "method": "EliCal\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u81ea\u6211\u4e00\u81f4\u6027\u76d1\u7763\u6fc0\u53d1\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff1b2\uff09\u7528\u5c11\u91cf\u6b63\u786e\u6027\u6807\u6ce8\u6821\u51c6\u7f6e\u4fe1\u5ea6\u3002\u540c\u65f6\u53d1\u5e03\u4e86HonestyBench\u57fa\u51c6\uff0c\u5305\u542b10\u4e2a\u81ea\u7531\u5f62\u5f0fQA\u6570\u636e\u96c6\u3002", "result": "EliCal\u4ec5\u97001k\u6b63\u786e\u6027\u6807\u6ce8\uff08\u5168\u76d1\u7763\u76840.18%\uff09\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u5728\u672a\u89c1\u8fc7\u7684MMLU\u4efb\u52a1\u4e0a\u6bd4\u4ec5\u6821\u51c6\u7684\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "EliCal\u4e3aLLM\u7684\u901a\u7528\u8bda\u5b9e\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2510.16837", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16837", "abs": "https://arxiv.org/abs/2510.16837", "authors": ["Haofan Ren", "Qingsong Yan", "Ming Lu", "Rongfeng Lu", "Zunjie Zhu"], "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced\nneural fields, as it enables high-fidelity rendering with impressive visual\nquality. However, 3DGS has difficulty accurately representing surfaces. In\ncontrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian\ndisks. Despite advancements in geometric fidelity, rendering quality remains\ncompromised, highlighting the challenge of achieving both high-quality\nrendering and precise geometric structures. This indicates that optimizing both\ngeometric and rendering quality in a single training stage is currently\nunfeasible. To overcome this limitation, we present 2DGS-R, a new method that\nuses a hierarchical training approach to improve rendering quality while\nmaintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians\nwith the normal consistency regularization. Then 2DGS-R selects the 2D\nGaussians with inadequate rendering quality and applies a novel in-place\ncloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R\nmodel with opacity frozen. Experimental results show that compared to the\noriginal 2DGS, our method requires only 1\\% more storage and minimal additional\ntraining time. Despite this negligible overhead, it achieves high-quality\nrendering results while preserving fine geometric structures. These findings\nindicate that our approach effectively balances efficiency with performance,\nleading to improvements in both visual fidelity and geometric reconstruction\naccuracy.", "AI": {"tldr": "\u63d0\u51fa2DGS-R\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u8bad\u7ec3\u7b56\u7565\u5728\u4fdd\u6301\u51e0\u4f55\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\uff0c\u4ec5\u97001%\u989d\u5916\u5b58\u50a8\u548c\u5c11\u91cf\u8bad\u7ec3\u65f6\u95f4\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6e32\u67d3\u6548\u679c\u3002", "motivation": "3DGS\u96be\u4ee5\u51c6\u786e\u8868\u793a\u8868\u9762\uff0c\u800c2DGS\u867d\u7136\u51e0\u4f55\u4fdd\u771f\u5ea6\u6709\u6240\u63d0\u5347\u4f46\u6e32\u67d3\u8d28\u91cf\u4ecd\u53d7\u5f71\u54cd\uff0c\u76ee\u524d\u65e0\u6cd5\u5728\u5355\u9636\u6bb5\u8bad\u7ec3\u4e2d\u540c\u65f6\u4f18\u5316\u51e0\u4f55\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8bad\u7ec3\uff1a\u9996\u5148\u7528\u6cd5\u5411\u4e00\u81f4\u6027\u6b63\u5219\u5316\u8bad\u7ec3\u539f\u59cb2D\u9ad8\u65af\uff1b\u7136\u540e\u9009\u62e9\u6e32\u67d3\u8d28\u91cf\u4e0d\u8db3\u76842D\u9ad8\u65af\u8fdb\u884c\u539f\u5730\u514b\u9686\u64cd\u4f5c\uff1b\u6700\u540e\u51bb\u7ed3\u4e0d\u900f\u660e\u5ea6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u76f8\u6bd4\u539f\u59cb2DGS\u4ec5\u97001%\u989d\u5916\u5b58\u50a8\u548c\u5c11\u91cf\u8bad\u7ec3\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u6e32\u67d3\u7ed3\u679c\u540c\u65f6\u4fdd\u7559\u4e86\u7cbe\u7ec6\u51e0\u4f55\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2510.17516", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17516", "abs": "https://arxiv.org/abs/2510.17516", "authors": ["Tiancheng Hu", "Joachim Baumann", "Lorenzo Lupo", "Dirk Hovy", "Nigel Collier", "Paul R\u00f6ttger"], "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors", "comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench", "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.", "AI": {"tldr": "SimBench\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u6807\u51c6\u5316\u7684LLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7edf\u4e00\u4e8620\u4e2a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLM\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u7684\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6700\u4f73LLM\u6a21\u62df\u80fd\u529b\u6709\u9650\uff0c\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u5bf9\u6570\u7ebf\u6027\u589e\u957f\uff0c\u5b58\u5728\u5bf9\u9f50-\u6a21\u62df\u6743\u8861\uff0c\u4e14\u6a21\u578b\u5728\u6a21\u62df\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u65f6\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u5f53\u524dLLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u8bc4\u4f30\u65b9\u6cd5\u5206\u6563\u4e14\u4e0d\u53ef\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30LLM\u6a21\u62df\u7684\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\uff0c\u963b\u788d\u4e86LLM\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efaSimBench\u57fa\u51c6\uff0c\u6574\u540820\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u9053\u5fb7\u51b3\u7b56\u3001\u7ecf\u6d4e\u9009\u62e9\u7b49\u4efb\u52a1\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u5168\u7403\u53c2\u4e0e\u8005\u6570\u636e\u4f5c\u4e3a\u53c2\u8003\u6807\u51c6\u3002", "result": "\u5f53\u524d\u6700\u4f73LLM\u6a21\u62df\u5f97\u5206\u4ec540.80/100\uff0c\u6027\u80fd\u968f\u6a21\u578b\u89c4\u6a21\u5bf9\u6570\u7ebf\u6027\u589e\u957f\uff1b\u63a8\u7406\u65f6\u8ba1\u7b97\u589e\u52a0\u4e0d\u80fd\u63d0\u5347\u6027\u80fd\uff1b\u6307\u4ee4\u5fae\u8c03\u5728\u4f4e\u71b5\u95ee\u9898\u4e0a\u6539\u5584\u4f46\u5728\u9ad8\u71b5\u95ee\u9898\u4e0a\u6076\u5316\uff1b\u6a21\u578b\u6a21\u62df\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u8868\u73b0\u5dee\uff1b\u6a21\u62df\u80fd\u529b\u4e0e\u6df1\u5ea6\u77e5\u8bc6\u63a8\u7406\u5f3a\u76f8\u5173(r=0.939)\u3002", "conclusion": "\u901a\u8fc7SimBench\u57fa\u51c6\u4f7fLLM\u6a21\u62df\u80fd\u529b\u7684\u8fdb\u5c55\u53ef\u6d4b\u91cf\uff0c\u4e3a\u5f00\u53d1\u66f4\u771f\u5b9e\u7684LLM\u6a21\u62df\u5668\u63d0\u4f9b\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u5c40\u9650\u6027\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.16854", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10; I.5.4; I.4.6"], "pdf": "https://arxiv.org/pdf/2510.16854", "abs": "https://arxiv.org/abs/2510.16854", "authors": ["Akhila Kambhatla", "Taminul Islam", "Khaled R Ahmed"], "title": "ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification", "comment": "9 pages with 4 figures and 5 tables. This is a preprint submitted to\n  arXiv", "summary": "The escalating threat of weapon-related violence necessitates automated\ndetection systems capable of pixel-level precision for accurate threat\nassessment in real-time security applications. Traditional weapon detection\napproaches rely on object detection frameworks that provide only coarse\nbounding box localizations, lacking the fine-grained segmentation required for\ncomprehensive threat analysis. Furthermore, existing semantic segmentation\nmodels either sacrifice accuracy for computational efficiency or require\nexcessive computational resources incompatible with edge deployment scenarios.\nThis paper presents ArmFormer, a lightweight transformer-based semantic\nsegmentation framework that strategically integrates Convolutional Block\nAttention Module (CBAM) with MixVisionTransformer architecture to achieve\nsuperior accuracy while maintaining computational efficiency suitable for\nresource-constrained edge devices. Our approach combines CBAM-enhanced encoder\nbackbone with attention-integrated hamburger decoder to enable multi-class\nweapon segmentation across five categories: handgun, rifle, knife, revolver,\nand human. Comprehensive experiments demonstrate that ArmFormer achieves\nstate-of-the-art performance with 80.64% mIoU and 89.13% mFscore while\nmaintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M\nparameters, ArmFormer outperforms heavyweight models requiring up to 48x more\ncomputation, establishing it as the optimal solution for deployment on portable\nsecurity cameras, surveillance drones, and embedded AI accelerators in\ndistributed security infrastructure.", "AI": {"tldr": "ArmFormer\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210CBAM\u6ce8\u610f\u529b\u6a21\u5757\u548cMixVisionTransformer\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6b66\u5668\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u4f20\u7edf\u6b66\u5668\u68c0\u6d4b\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u7c97\u7565\u7684\u8fb9\u754c\u6846\u5b9a\u4f4d\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u5272\u80fd\u529b\uff1b\u73b0\u6709\u8bed\u4e49\u5206\u5272\u6a21\u578b\u8981\u4e48\u727a\u7272\u7cbe\u5ea6\u8ffd\u6c42\u6548\u7387\uff0c\u8981\u4e48\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u8fc7\u9ad8\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u3002", "method": "\u7ed3\u5408CBAM\u589e\u5f3a\u7684\u7f16\u7801\u5668\u4e3b\u5e72\u548c\u6ce8\u610f\u529b\u96c6\u6210\u6c49\u5821\u89e3\u7801\u5668\uff0c\u91c7\u7528MixVisionTransformer\u67b6\u6784\uff0c\u5b9e\u73b0\u4e94\u7c7b\u6b66\u5668\uff08\u624b\u67aa\u3001\u6b65\u67aa\u3001\u5200\u3001\u5de6\u8f6e\u624b\u67aa\u3001\u4eba\uff09\u7684\u8bed\u4e49\u5206\u5272\u3002", "result": "\u5728\u4fdd\u6301\u5b9e\u65f6\u63a8\u740682.26 FPS\u7684\u540c\u65f6\uff0c\u8fbe\u523080.64% mIoU\u548c89.13% mFscore\u7684SOTA\u6027\u80fd\uff0c\u4ec5\u97004.886G FLOPs\u548c3.66M\u53c2\u6570\uff0c\u6bd4\u91cd\u91cf\u7ea7\u6a21\u578b\u8282\u770148\u500d\u8ba1\u7b97\u91cf\u3002", "conclusion": "ArmFormer\u662f\u90e8\u7f72\u5728\u4fbf\u643a\u5b89\u5168\u6444\u50cf\u5934\u3001\u76d1\u63a7\u65e0\u4eba\u673a\u548c\u5d4c\u5165\u5f0fAI\u52a0\u901f\u5668\u4e0a\u7684\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5206\u5e03\u5f0f\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17532", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17532", "abs": "https://arxiv.org/abs/2510.17532", "authors": ["Raghu Vamshi Hemadri", "Geetha Krishna Guruju", "Kristi Topollai", "Anna Ewa Choromanska"], "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction", "comment": null, "summary": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u81ea\u56de\u5f52LLM\u4e0e\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\uff0c\u7528\u4e8e\u764c\u75c7\u6cbb\u7597\u7ed3\u679c\u9884\u6d4b\uff0c\u901a\u8fc7\u4e09\u79cd\u5bf9\u9f50\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66NLP\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9ad8\u98ce\u9669\u51b3\u7b56\u652f\u6301\u81f3\u5173\u91cd\u8981\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5f02\u8d28\u4e34\u5e8a\u6570\u636e\u65f6\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5bf9\u9f50\u7b56\u7565\uff1a\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3001\u5e26\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u76d1\u7763\u5fae\u8c03\u3001\u4ee5\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5728MSK-CHORD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u601d\u7ef4\u94fe\u63d0\u793a\u4f7fF1\u63d0\u9ad86.0%\uff0cMAE\u964d\u4f4e12%\uff1bGRPO\u5728BLEU\u3001ROUGE\u548cBERTScore\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63a8\u7406\u611f\u77e5\u5bf9\u9f50\u5728\u591a\u4efb\u52a1\u4e34\u5e8a\u5efa\u6a21\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u7684LLM\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2510.16863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16863", "abs": "https://arxiv.org/abs/2510.16863", "authors": ["Shujian Gao", "Yuan Wang", "Zekuan Yu"], "title": "BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation", "comment": "14 pages, 5 figures", "summary": "Semi-supervised medical image segmentation (SSMIS) seeks to match fully\nsupervised performance while sharply reducing annotation cost. Mainstream SSMIS\nmethods rely on \\emph{label-space consistency}, yet they overlook the equally\ncritical \\emph{representation-space alignment}. Without harmonizing latent\nfeatures, models struggle to learn representations that are both discriminative\nand spatially coherent. To this end, we introduce \\textbf{Bilateral Alignment\nin Representation and Label spaces (BARL)}, a unified framework that couples\ntwo collaborative branches and enforces alignment in both spaces. For\nlabel-space alignment, inspired by co-training and multi-scale decoding, we\ndevise \\textbf{Dual-Path Regularization (DPR)} and \\textbf{Progressively\nCognitive Bias Correction (PCBC)} to impose fine-grained cross-branch\nconsistency while mitigating error accumulation from coarse to fine scales. For\nrepresentation-space alignment, we conduct region-level and lesion-instance\nmatching between branches, explicitly capturing the fragmented, complex\npathological patterns common in medical imagery. Extensive experiments on four\npublic benchmarks and a proprietary CBCT dataset demonstrate that BARL\nconsistently surpasses state-of-the-art SSMIS methods. Ablative studies further\nvalidate the contribution of each component. Code will be released soon.", "AI": {"tldr": "BARL\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8868\u793a\u7a7a\u95f4\u548c\u6807\u7b7e\u7a7a\u95f4\u8fdb\u884c\u53cc\u8fb9\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6807\u7b7e\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u8868\u793a\u7a7a\u95f4\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002\u6ca1\u6709\u534f\u8c03\u6f5c\u5728\u7279\u5f81\uff0c\u6a21\u578b\u96be\u4ee5\u5b66\u4e60\u65e2\u5177\u6709\u5224\u522b\u6027\u53c8\u7a7a\u95f4\u4e00\u81f4\u7684\u8868\u793a\u3002", "method": "\u63d0\u51faBARL\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u4f5c\u5206\u652f\uff0c\u5728\u6807\u7b7e\u7a7a\u95f4\u4f7f\u7528\u53cc\u8def\u5f84\u6b63\u5219\u5316\u548c\u6e10\u8fdb\u8ba4\u77e5\u504f\u5dee\u6821\u6b63\u5b9e\u73b0\u5bf9\u9f50\uff0c\u5728\u8868\u793a\u7a7a\u95f4\u901a\u8fc7\u533a\u57df\u7ea7\u548c\u75c5\u7076\u5b9e\u4f8b\u5339\u914d\u6765\u5bf9\u9f50\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u79c1\u6709CBCT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBARL\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "BARL\u901a\u8fc7\u53cc\u8fb9\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4ee3\u7801\u5373\u5c06\u53d1\u5e03\u3002"}}
{"id": "2510.17548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17548", "abs": "https://arxiv.org/abs/2510.17548", "authors": ["Nisrine Rair", "Alban Goupil", "Valeriu Vrabie", "Emmanuel Chochoy"], "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity", "comment": "Accepted to appear in the Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2025, Main\n  Conference)", "summary": "Language models are often evaluated with scalar metrics like accuracy, but\nsuch measures fail to capture how models internally represent ambiguity,\nespecially when human annotators disagree. We propose a topological perspective\nto analyze how fine-tuned models encode ambiguity and more generally instances.\n  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from\ntopological data analysis, reveals that fine-tuning restructures embedding\nspace into modular, non-convex regions aligned with model predictions, even for\nhighly ambiguous cases. Over $98\\%$ of connected components exhibit $\\geq 90\\%$\nprediction purity, yet alignment with ground-truth labels drops in ambiguous\ndata, surfacing a hidden tension between structural confidence and label\nuncertainty.\n  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry\ndirectly uncovering decision regions, boundary collapses, and overconfident\nclusters. Our findings position Mapper as a powerful diagnostic tool for\nunderstanding how models resolve ambiguity. Beyond visualization, it also\nenables topological metrics that may inform proactive modeling strategies in\nsubjective NLP tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177Mapper\u6765\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u6b67\u4e49\u6027\uff0c\u53d1\u73b0\u5728\u5fae\u8c03\u540e\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\u4f1a\u5f62\u6210\u6a21\u5757\u5316\u7684\u975e\u51f8\u533a\u57df\uff0c\u4e0e\u6a21\u578b\u9884\u6d4b\u5bf9\u9f50\uff0c\u4f46\u5728\u6b67\u4e49\u6570\u636e\u4e2d\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u5bf9\u9f50\u5ea6\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u6807\u91cf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u6b67\u4e49\u6027\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6807\u6ce8\u8005\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5206\u6790\u6a21\u578b\u5982\u4f55\u7f16\u7801\u6b67\u4e49\u548c\u5b9e\u4f8b\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u5de5\u5177Mapper\u5206\u6790\u5fae\u8c03\u540e\u7684RoBERTa-Large\u6a21\u578b\u5728MD-Offense\u6570\u636e\u96c6\u4e0a\u7684\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u4e0e\u4f20\u7edf\u5de5\u5177\u5982PCA\u548cUMAP\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5fae\u8c03\u5c06\u5d4c\u5165\u7a7a\u95f4\u91cd\u6784\u4e3a\u6a21\u5757\u5316\u7684\u975e\u51f8\u533a\u57df\uff0c98%\u4ee5\u4e0a\u7684\u8fde\u901a\u7ec4\u4ef6\u663e\u793a\u51fa\u226590%\u7684\u9884\u6d4b\u7eaf\u5ea6\uff0c\u4f46\u5728\u6b67\u4e49\u6570\u636e\u4e2d\u4e0e\u771f\u5b9e\u6807\u7b7e\u7684\u5bf9\u9f50\u5ea6\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u7ed3\u6784\u7f6e\u4fe1\u5ea6\u4e0e\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u9690\u85cf\u5f20\u529b\u3002", "conclusion": "Mapper\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u7406\u89e3\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u6b67\u4e49\u6027\uff0c\u4e0d\u4ec5\u63d0\u4f9b\u53ef\u89c6\u5316\uff0c\u8fd8\u80fd\u751f\u6210\u62d3\u6251\u6307\u6807\uff0c\u4e3a\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u7684\u4e3b\u52a8\u5efa\u6a21\u7b56\u7565\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.16865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16865", "abs": "https://arxiv.org/abs/2510.16865", "authors": ["Yuyang Yu", "Zhengwei Chen", "Xuemiao Xu", "Lei Zhang", "Haoxin Yang", "Yongwei Nie", "Shengfeng He"], "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection", "comment": null, "summary": "3D anomaly detection in point-cloud data is critical for industrial quality\ncontrol, aiming to identify structural defects with high reliability. However,\ncurrent memory bank-based methods often suffer from inconsistent feature\ntransformations and limited discriminative capacity, particularly in capturing\nlocal geometric details and achieving rotation invariance. These limitations\nbecome more pronounced when registration fails, leading to unreliable detection\nresults. We argue that point-cloud registration plays an essential role not\nonly in aligning geometric structures but also in guiding feature extraction\ntoward rotation-invariant and locally discriminative representations. To this\nend, we propose a registration-induced, rotation-invariant feature extraction\nframework that integrates the objectives of point-cloud registration and\nmemory-based anomaly detection. Our key insight is that both tasks rely on\nmodeling local geometric structures and leveraging feature similarity across\nsamples. By embedding feature extraction into the registration learning\nprocess, our framework jointly optimizes alignment and representation learning.\nThis integration enables the network to acquire features that are both robust\nto rotations and highly effective for anomaly detection. Extensive experiments\non the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method\nconsistently outperforms existing approaches in effectiveness and\ngeneralizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u914d\u51c6\u7684\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u5c06\u70b9\u4e91\u914d\u51c6\u4e0e\u57fa\u4e8e\u8bb0\u5fc6\u5e93\u7684\u5f02\u5e38\u68c0\u6d4b\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u53d8\u6362\u4e0d\u4e00\u81f4\u548c\u5c40\u90e8\u51e0\u4f55\u7ec6\u8282\u6355\u6349\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bb0\u5fc6\u5e93\u76843D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u53d8\u6362\u4e0d\u4e00\u81f4\u548c\u5224\u522b\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u5c40\u90e8\u51e0\u4f55\u7ec6\u8282\u548c\u5b9e\u73b0\u65cb\u8f6c\u4e0d\u53d8\u6027\u65b9\u9762\u3002\u5f53\u914d\u51c6\u5931\u8d25\u65f6\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u68c0\u6d4b\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u914d\u51c6\u8bf1\u5bfc\u7684\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u5c06\u70b9\u4e91\u914d\u51c6\u4e0e\u57fa\u4e8e\u8bb0\u5fc6\u5e93\u7684\u5f02\u5e38\u68c0\u6d4b\u76ee\u6807\u96c6\u6210\u3002\u901a\u8fc7\u5c06\u7279\u5f81\u63d0\u53d6\u5d4c\u5165\u5230\u914d\u51c6\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u8054\u5408\u4f18\u5316\u5bf9\u9f50\u548c\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728Anomaly-ShapeNet\u548cReal3D-AD\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7279\u5f81\u63d0\u53d6\u4e0e\u914d\u51c6\u5b66\u4e60\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u7f51\u7edc\u80fd\u591f\u83b7\u5f97\u65e2\u5bf9\u65cb\u8f6c\u5177\u6709\u9c81\u68d2\u6027\u53c8\u5bf9\u5f02\u5e38\u68c0\u6d4b\u9ad8\u5ea6\u6709\u6548\u7684\u7279\u5f81\u3002"}}
{"id": "2510.17555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17555", "abs": "https://arxiv.org/abs/2510.17555", "authors": ["Collin Zhang", "Fei Huang", "Chenhan Yuan", "Junyang Lin"], "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation", "comment": null, "summary": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.", "AI": {"tldr": "LCG\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u7801\u65f6\u8fc7\u6ee4\u4ee4\u724c\u6765\u89e3\u51b3LLM\u7684\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u8bed\u8a00\u6df7\u6dc6\u800c\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u8981\u4e48\u65e0\u6cd5\u533a\u5206\u6709\u5bb3\u7684\u8bed\u8a00\u6df7\u6dc6\u548c\u53ef\u63a5\u53d7\u7684\u8bed\u7801\u8f6c\u6362\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u6df7\u6dc6\u95e8(LCG)\uff0c\u4f7f\u7528\u89c4\u8303\u8c03\u6574\u81ea\u84b8\u998f\u8bad\u7ec3\uff0c\u9884\u6d4b\u9002\u5f53\u7684\u8bed\u8a00\u5bb6\u65cf\uff0c\u5e76\u5728\u9700\u8981\u65f6\u5e94\u7528\u63a9\u7801\uff0c\u57fa\u4e8e\u8bed\u8a00\u6df7\u6dc6\u4e0d\u9891\u7e41\u3001\u6b63\u786e\u8bed\u8a00\u4ee4\u724c\u901a\u5e38\u5728\u524d\u51e0\u4f4d\u9884\u6d4b\u4e2d\u3001\u9ad8\u8d44\u6e90\u8bed\u8a00\u8f93\u51fa\u4ee4\u724c\u5d4c\u5165\u89c4\u8303\u66f4\u5927\u7684\u53d1\u73b0\u3002", "result": "\u5728\u5305\u62ecQwen3\u3001GPT-OSS\u3001Gemma3\u3001Llama3.1\u5728\u5185\u7684\u5404\u79cd\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cLCG\u663e\u8457\u964d\u4f4e\u4e86\u8bed\u8a00\u6df7\u6dc6\uff0c\u901a\u5e38\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "LCG\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u663e\u8457\u51cf\u5c11LLM\u7684\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.16870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16870", "abs": "https://arxiv.org/abs/2510.16870", "authors": ["Yudan Ren", "Xinlong Wang", "Kexin Wang", "Tian Xia", "Zihan Ma", "Zhaowei Li", "Xiangrong Bi", "Xiao Li", "Xiaowei He"], "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding", "comment": "14 pages, 7 figures", "summary": "While brain-inspired artificial intelligence(AI) has demonstrated promising\nresults, current understanding of the parallels between artificial neural\nnetworks (ANNs) and human brain processing remains limited: (1) unimodal ANN\nstudies fail to capture the brain's inherent multimodal processing\ncapabilities, and (2) multimodal ANN research primarily focuses on high-level\nmodel outputs, neglecting the crucial role of individual neurons. To address\nthese limitations, we propose a novel neuron-level analysis framework that\ninvestigates the multimodal information processing mechanisms in\nvision-language models (VLMs) through the lens of human brain activity. Our\napproach uniquely combines fine-grained artificial neuron (AN) analysis with\nfMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP\nand METER. Our analysis reveals four key findings: (1) ANs successfully predict\nbiological neurons (BNs) activities across multiple functional networks\n(including language, vision, attention, and default mode), demonstrating shared\nrepresentational mechanisms; (2) Both ANs and BNs demonstrate functional\nredundancy through overlapping neural representations, mirroring the brain's\nfault-tolerant and collaborative information processing mechanisms; (3) ANs\nexhibit polarity patterns that parallel the BNs, with oppositely activated BNs\nshowing mirrored activation trends across VLM layers, reflecting the complexity\nand bidirectional nature of neural information processing; (4) The\narchitectures of CLIP and METER drive distinct BNs: CLIP's independent branches\nshow modality-specific specialization, whereas METER's cross-modal design\nyields unified cross-modal activation, highlighting the architecture's\ninfluence on ANN brain-like properties. These results provide compelling\nevidence for brain-like hierarchical processing in VLMs at the neuronal level.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u5927\u8111\u6d3b\u52a8\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u673a\u5236\uff0c\u53d1\u73b0\u4eba\u5de5\u795e\u7ecf\u5143\u4e0e\u751f\u7269\u795e\u7ecf\u5143\u5728\u529f\u80fd\u7f51\u7edc\u3001\u5197\u4f59\u6027\u3001\u6781\u6027\u6a21\u5f0f\u548c\u67b6\u6784\u5f71\u54cd\u65b9\u9762\u5b58\u5728\u663e\u8457\u76f8\u4f3c\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u5927\u8111\u5904\u7406\u4e4b\u95f4\u76f8\u4f3c\u6027\u7684\u7406\u89e3\u6709\u9650\uff1a\u5355\u6a21\u6001\u7814\u7a76\u65e0\u6cd5\u6355\u6349\u5927\u8111\u56fa\u6709\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\uff0c\u800c\u591a\u6a21\u6001\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9ad8\u5c42\u6a21\u578b\u8f93\u51fa\uff0c\u5ffd\u89c6\u4e86\u5355\u4e2a\u795e\u7ecf\u5143\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u7ed3\u5408\u7ec6\u7c92\u5ea6\u4eba\u5de5\u795e\u7ecf\u5143\u5206\u6790\u548c\u57fa\u4e8efMRI\u7684\u4f53\u7d20\u7f16\u7801\uff0c\u7814\u7a76\u4e86CLIP\u548cMETER\u4e24\u79cd\u67b6\u6784\u4e0d\u540c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u522b\u5206\u6790\u6846\u67b6\u6bd4\u8f83\u4eba\u5de5\u795e\u7ecf\u5143\u4e0e\u751f\u7269\u795e\u7ecf\u5143\u7684\u6d3b\u52a8\u3002", "result": "\u53d1\u73b0\u56db\u4e2a\u5173\u952e\u7ed3\u679c\uff1a1\uff09\u4eba\u5de5\u795e\u7ecf\u5143\u6210\u529f\u9884\u6d4b\u591a\u4e2a\u529f\u80fd\u7f51\u7edc\u4e2d\u751f\u7269\u795e\u7ecf\u5143\u7684\u6d3b\u52a8\uff1b2\uff09\u4e24\u8005\u90fd\u8868\u73b0\u51fa\u529f\u80fd\u5197\u4f59\u6027\uff1b3\uff09\u4eba\u5de5\u795e\u7ecf\u5143\u5448\u73b0\u4e0e\u751f\u7269\u795e\u7ecf\u5143\u76f8\u4f3c\u7684\u6781\u6027\u6a21\u5f0f\uff1b4\uff09\u4e0d\u540c\u67b6\u6784\u9a71\u52a8\u4e0d\u540c\u7684\u751f\u7269\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7c7b\u8111\u5c42\u6b21\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\uff0c\u8868\u660e\u5728\u795e\u7ecf\u5143\u7ea7\u522b\u4e0a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u5927\u8111\u5904\u7406\u673a\u5236\u5177\u6709\u663e\u8457\u76f8\u4f3c\u6027\u3002"}}
{"id": "2510.17591", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17591", "abs": "https://arxiv.org/abs/2510.17591", "authors": ["Guang Yang", "Yujie Zhu"], "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection", "comment": "Accepted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025) as a findings long paper", "summary": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u9002\u914d\u5668HGAdapter\uff0c\u901a\u8fc7\u6355\u83b7\u4ee3\u7801\u4e2d\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u8003\u8651\u4ee3\u7801\u4e2d\u6f5c\u5728\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4ee3\u7801\u9ad8\u9636\u76f8\u5173\u6027\uff08\u62bd\u8c61\u8bed\u6cd5\u6811\u5bb6\u65cf\u76f8\u5173\u6027\u3001\u8bcd\u6c47\u76f8\u5173\u6027\u548c\u884c\u76f8\u5173\u6027\uff09\uff0c\u6784\u5efa\u4e86tokens\u548chyperedges\u751f\u6210\u5668\uff0c\u5e76\u6539\u8fdb\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7ed3\u5408\u9002\u914d\u5668\u8c03\u4f18\u63d0\u51faHGAdapter\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7a0b\u5ea6\u4e0a\u63d0\u5347\u4e86PLMs\u5728\u4ee3\u7801\u6458\u8981\u548c\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6709\u52a9\u4e8e\u63d0\u9ad8\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u7684\u6548\u679c\uff0cHGAdapter\u80fd\u591f\u6709\u6548\u7f16\u7801\u8fd9\u4e9b\u76f8\u5173\u6027\u5e76\u589e\u5f3a\u5404\u79cdPLMs\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16887", "abs": "https://arxiv.org/abs/2510.16887", "authors": ["Nusrat Munia", "Abdullah Imran"], "title": "Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis", "comment": "EMBC 2025", "summary": "Generative models, especially Diffusion Models, have demonstrated remarkable\ncapability in generating high-quality synthetic data, including medical images.\nHowever, traditional class-conditioned generative models often struggle to\ngenerate images that accurately represent specific medical categories, limiting\ntheir usefulness for applications such as skin cancer diagnosis. To address\nthis problem, we propose a classification-induced diffusion model, namely,\nClass-N-Diff, to simultaneously generate and classify dermoscopic images. Our\nClass-N-Diff model integrates a classifier within a diffusion model to guide\nimage generation based on its class conditions. Thus, the model has better\ncontrol over class-conditioned image synthesis, resulting in more realistic and\ndiverse images. Additionally, the classifier demonstrates improved performance,\nhighlighting its effectiveness for downstream diagnostic tasks. This unique\nintegration in our Class-N-Diff makes it a robust tool for enhancing the\nquality and utility of diffusion model-based synthetic dermoscopic image\ngeneration. Our code is available at https://github.com/Munia03/Class-N-Diff.", "AI": {"tldr": "\u63d0\u51faClass-N-Diff\u6a21\u578b\uff0c\u5c06\u5206\u7c7b\u5668\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u751f\u6210\u548c\u5206\u7c7b\u76ae\u80a4\u955c\u56fe\u50cf\uff0c\u63d0\u9ad8\u7c7b\u522b\u63a7\u5236\u80fd\u529b\u548c\u56fe\u50cf\u8d28\u91cf", "motivation": "\u4f20\u7edf\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u751f\u6210\u7279\u5b9a\u533b\u5b66\u7c7b\u522b\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5728\u76ae\u80a4\u764c\u8bca\u65ad\u7b49\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027", "method": "\u5728\u6269\u6563\u6a21\u578b\u4e2d\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u57fa\u4e8e\u7c7b\u522b\u6761\u4ef6\u6307\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u7c7b\u522b\u63a7\u5236", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u591a\u6837\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u5206\u7c7b\u5668\u6027\u80fd\u4e5f\u5f97\u5230\u63d0\u5347", "conclusion": "Class-N-Diff\u662f\u589e\u5f3a\u6269\u6563\u6a21\u578b\u5408\u6210\u76ae\u80a4\u955c\u56fe\u50cf\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u7684\u5f3a\u5927\u5de5\u5177"}}
{"id": "2510.17602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17602", "abs": "https://arxiv.org/abs/2510.17602", "authors": ["Huiyuan Xie", "Chenyang Li", "Huining Zhu", "Chubin Zhang", "Yuxiao Ye", "Zhenghao Liu", "Zhiyuan Liu"], "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis", "comment": null, "summary": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86LawChain\u6846\u67b6\u6765\u5efa\u6a21\u4e2d\u56fd\u4fb5\u6743\u6c11\u4e8b\u6848\u4ef6\u7684\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e86\u8bc4\u4f30\u57fa\u51c6LawChain_eval\uff0c\u5e76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u8be5\u6846\u67b6\u7684\u57fa\u7ebf\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u63a8\u7406\u8ba1\u7b97\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u901a\u7528\u63a8\u7406\u6846\u67b6\uff0c\u672a\u80fd\u5168\u9762\u8003\u5bdf\u6cd5\u5f8b\u63a8\u7406\u7684\u7ec6\u5fae\u8fc7\u7a0b\uff0c\u4e14\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5211\u4e8b\u6848\u4ef6\uff0c\u5bf9\u6c11\u4e8b\u6848\u4ef6\u5efa\u6a21\u4e0d\u8db3\u3002", "method": "\u5c06\u4fb5\u6743\u5206\u6790\u4e2d\u7684\u6cd5\u5f8b\u63a8\u7406\u8fc7\u7a0b\u64cd\u4f5c\u5316\u4e3aLawChain\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u7531\u591a\u4e2a\u7ec6\u7c92\u5ea6\u5b50\u6b65\u9aa4\u7ec4\u6210\u3002\u57fa\u4e8e\u8be5\u6846\u67b6\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86\u901a\u8fc7\u63d0\u793a\u6216\u540e\u8bad\u7ec3\u663e\u5f0f\u878d\u5165LawChain\u63a8\u7406\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u8868\u660e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fb5\u6743\u6cd5\u5f8b\u63a8\u7406\u7684\u5173\u952e\u8981\u7d20\u5904\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u4fb5\u6743\u76f8\u5173\u6cd5\u5f8b\u63a8\u7406\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u76f8\u5173\u6cd5\u5f8b\u5206\u6790\u4efb\u52a1\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u6cd5\u5f8b\u63a8\u7406\u94fe\u80fd\u6709\u6548\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0cLawChain\u6846\u67b6\u4e3a\u63d0\u5347\u6cd5\u5f8b\u63a8\u7406\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16888", "abs": "https://arxiv.org/abs/2510.16888", "authors": ["Zongjian Li", "Zheyuan Liu", "Qihui Zhang", "Bin Lin", "Shenghai Yuan", "Zhiyuan Yan", "Yang Ye", "Wangbo Yu", "Yuwei Niu", "Li Yuan"], "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback", "comment": null, "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.", "AI": {"tldr": "Edit-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u7b56\u7565\u4f18\u5316\u7684\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7DiffusionNFT\u65b9\u6cd5\u548cMLLM\u5956\u52b1\u6a21\u578b\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u6807\u6ce8\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63a2\u7d22\u548c\u8d85\u8d8a\u8bad\u7ec3\u5206\u5e03\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Diffusion Negative-aware Finetuning (DiffusionNFT)\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bad\u7ec3\u65e0\u5173\u7684\u7edf\u4e00\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4f4e\u65b9\u5dee\u7ec4\u8fc7\u6ee4\u673a\u5236\u6765\u51cf\u5c11\u8bc4\u5206\u566a\u58f0\u3002", "result": "UniWorld-V2\u5728ImgEdit\u548cGEdit-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u83b7\u5f974.49\u548c7.83\u5206\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u8be5\u6846\u67b6\u5bf9Qwen-Image-Edit\u548cFLUX-Kontext\u7b49\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u90fd\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Edit-R1\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.17620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17620", "abs": "https://arxiv.org/abs/2510.17620", "authors": ["Yuefeng Peng", "Parnian Afshar", "Megan Ganji", "Thomas Butler", "Amir Houmansadr", "Mingxian Wang", "Dezhi Hong"], "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models", "comment": null, "summary": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u76ee\u6807\u77e5\u8bc6\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\uff0c\u6062\u590d\u4e86\u6a21\u578b\u5728\u63d0\u793a\u4e2d\u91cd\u65b0\u5f15\u5165\u88ab\u9057\u5fd8\u77e5\u8bc6\u65f6\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u77e5\u8bc6\u7684\u9057\u5fd8\u7a0b\u5ea6\u548c\u4fdd\u7559\u96c6\u7684\u6027\u80fd\uff0c\u4f46\u5ffd\u89c6\u4e86\u6a21\u578b\u5728\u88ab\u9057\u5fd8\u77e5\u8bc6\u91cd\u65b0\u51fa\u73b0\u5728\u63d0\u793a\u4e2d\u65f6\u80fd\u5426\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u7684\u91cd\u8981\u53ef\u7528\u6027\u65b9\u9762\u3002", "method": "\u5728\u516d\u79cd\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u589e\u52a0\u4e00\u4e2a\u63d2\u4ef6\u9879\u6765\u589e\u5f3a\u9057\u5fd8\u5b66\u4e60\u76ee\u6807\uff0c\u4ee5\u4fdd\u6301\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u5b58\u5728\u88ab\u9057\u5fd8\u77e5\u8bc6\u65f6\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u4e0a\u4e0b\u6587\u6548\u7528\u6062\u590d\u5230\u63a5\u8fd1\u539f\u59cb\u6c34\u5e73\uff0c\u540c\u65f6\u4ecd\u4fdd\u6301\u6709\u6548\u7684\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u96c6\u6548\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u9057\u5fd8\u5b66\u4e60\u4e2d\u7684\u4e0a\u4e0b\u6587\u6548\u7528\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\u6062\u590d\u4e86\u6a21\u578b\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2510.16891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16891", "abs": "https://arxiv.org/abs/2510.16891", "authors": ["Ramon Dalmau", "Gabriel Jarry", "Philippe Very"], "title": "Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data", "comment": null, "summary": "Aviation's non-CO2 effects, particularly contrails, are a significant\ncontributor to its climate impact. Persistent contrails can evolve into\ncirrus-like clouds that trap outgoing infrared radiation, with radiative\nforcing potentially comparable to or exceeding that of aviation's CO2\nemissions. While physical models simulate contrail formation, evolution and\ndissipation, validating and calibrating these models requires linking observed\ncontrails to the flights that generated them, a process known as\ncontrail-to-flight attribution. Satellite-based attribution is challenging due\nto limited spatial and temporal resolution, as contrails often drift and deform\nbefore detection. In this paper, we evaluate an alternative approach using\nground-based cameras, which capture contrails shortly after formation at high\nspatial and temporal resolution, when they remain thin, linear, and visually\ndistinct. Leveraging the ground visible camera contrail sequences (GVCCS)\ndataset, we introduce a modular framework for attributing contrails observed\nusing ground-based cameras to theoretical contrails derived from aircraft\nsurveillance and meteorological data. The framework accommodates multiple\ngeometric representations and distance metrics, incorporates temporal\nsmoothing, and enables flexible probability-based assignment strategies. This\nwork establishes a strong baseline and provides a modular framework for future\nresearch in linking contrails to their source flight.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5730\u9762\u6444\u50cf\u673a\u7684\u51dd\u7ed3\u5c3e\u8ff9\u5230\u822a\u73ed\u5f52\u56e0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u536b\u661f\u5f52\u56e0\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0a\u7684\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u822a\u7a7a\u4e1a\u7684\u975eCO2\u6548\u5e94\uff08\u7279\u522b\u662f\u51dd\u7ed3\u5c3e\u8ff9\uff09\u5bf9\u6c14\u5019\u53d8\u5316\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u9a8c\u8bc1\u548c\u6821\u51c6\u7269\u7406\u6a21\u578b\u9700\u8981\u5c06\u89c2\u6d4b\u5230\u7684\u51dd\u7ed3\u5c3e\u8ff9\u4e0e\u751f\u6210\u5b83\u4eec\u7684\u822a\u73ed\u5173\u8054\u8d77\u6765\u3002\u536b\u661f\u5f52\u56e0\u65b9\u6cd5\u56e0\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6709\u9650\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528\u5730\u9762\u53ef\u89c1\u6444\u50cf\u673a\u51dd\u7ed3\u5c3e\u8ff9\u5e8f\u5217(GVCCS)\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u5730\u9762\u6444\u50cf\u673a\u89c2\u6d4b\u5230\u7684\u51dd\u7ed3\u5c3e\u8ff9\u4e0e\u57fa\u4e8e\u98de\u673a\u76d1\u89c6\u548c\u6c14\u8c61\u6570\u636e\u63a8\u5bfc\u7684\u7406\u8bba\u51dd\u7ed3\u5c3e\u8ff9\u8fdb\u884c\u5173\u8054\u3002\u8be5\u6846\u67b6\u5305\u542b\u591a\u79cd\u51e0\u4f55\u8868\u793a\u548c\u8ddd\u79bb\u5ea6\u91cf\uff0c\u6574\u5408\u4e86\u65f6\u95f4\u5e73\u6ed1\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u6982\u7387\u7684\u7075\u6d3b\u5206\u914d\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u51dd\u7ed3\u5c3e\u8ff9\u4e0e\u6e90\u822a\u73ed\u5173\u8054\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u6846\u67b6\u3002", "conclusion": "\u5730\u9762\u6444\u50cf\u673a\u65b9\u6cd5\u80fd\u591f\u5728\u51dd\u7ed3\u5c3e\u8ff9\u5f62\u6210\u540e\u4e0d\u4e45\u4ee5\u9ad8\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6355\u6349\u5b83\u4eec\uff0c\u6b64\u65f6\u51dd\u7ed3\u5c3e\u8ff9\u4ecd\u7136\u8f83\u8584\u3001\u7ebf\u6027\u4e14\u89c6\u89c9\u4e0a\u53ef\u533a\u5206\uff0c\u4e3a\u51dd\u7ed3\u5c3e\u8ff9\u5f52\u56e0\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.17652", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17652", "abs": "https://arxiv.org/abs/2510.17652", "authors": ["Joseph McInerney"], "title": "Qomhra: A Bilingual Irish-English Large Language Model", "comment": null, "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.", "AI": {"tldr": "Qomhr'a\u662f\u4e00\u4e2a\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u5f00\u53d1\u7684\u7231\u5c14\u5170\u8bed-\u82f1\u8bed\u53cc\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u8bed\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u5b8c\u6574\u6d41\u7a0b\u6784\u5efa\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u5f00\u53d1\u7231\u5c14\u5170\u8bed\u53cc\u8bedLLM\uff0c\u6539\u5584\u7231\u5c14\u5170\u8bed\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u82f1\u8bed\u80fd\u529b\uff0c\u89e3\u51b3\u7231\u5c14\u5170\u8bed\u8d44\u6e90\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u6df7\u5408\u548c\u6574\u7406\u65b0\u83b7\u53d6\u7684\u7231\u5c14\u5170\u8bed\u8bed\u6599\u4e0e\u82f1\u8bed\u6587\u672c\uff0c\u4f7f\u7528Gemini-2.5-Pro\u5408\u6210\u6307\u4ee4\u5fae\u8c03\u548c\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u53cc\u8bed\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728\u7ffb\u8bd1\u3001\u6027\u522b\u7406\u89e3\u3001\u4e3b\u9898\u8bc6\u522b\u548c\u4e16\u754c\u77e5\u8bc6\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7231\u5c14\u5170\u8bed\u6027\u80fd\u63d0\u5347\u8fbe29%\uff0c\u82f1\u8bed\u6027\u80fd\u63d0\u5347\u8fbe44%\uff0c\u5728\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u53d6\u5f97\u660e\u663e\u8fdb\u5c55\u3002", "conclusion": "Qomhr'a\u6210\u529f\u5c55\u793a\u4e86\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u5f00\u53d1\u9ad8\u8d28\u91cf\u53cc\u8bedLLM\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7231\u5c14\u5170\u8bedAI\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2510.16913", "categories": ["cs.CV", "68T07, 68U10, 68U35", "I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16913", "abs": "https://arxiv.org/abs/2510.16913", "authors": ["Akhila Kambhatla", "Ahmed R Khaled"], "title": "Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation", "comment": "9 Images with 1 figure and 3 Tables. This is a preprint submitted to\n  arXiv", "summary": "Thermal weapon segmentation is crucial for surveillance and security\napplications, enabling robust detection under lowlight and visually obscured\nconditions where RGB-based systems fail. While convolutional neural networks\n(CNNs) dominate thermal segmentation literature, their ability to capture\nlong-range dependencies and fine structural details is limited. Vision\nTransformers (ViTs), with their global context modeling capabilities, have\nachieved state-of-the-art results in RGB segmentation tasks, yet their\npotential in thermal weapon segmentation remains underexplored. This work\nadapts and evaluates four transformer-based architectures SegFormer,\nDeepLabV3\\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a\ncustom thermal dataset comprising 9,711 images collected from real world\nsurveillance videos and automatically annotated using SAM2. We employ standard\naugmentation strategies within the MMSegmentation framework to ensure robust\nmodel training and fair architectural comparison. Experimental results\ndemonstrate significant improvements in segmentation performance: SegFormer-b5\nachieves the highest mIoU (94.15\\%) and Pixel Accuracy (97.04\\%), while\nSegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive\nmIoU (90.84\\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and\n92.24\\% mIoU, and DeepLabV3\\+ R101-D8 reaches 92.76\\% mIoU at 29.86 FPS. The\ntransformer architectures demonstrate robust generalization capabilities for\nweapon detection in low-light and occluded thermal environments, with flexible\naccuracy-speed trade-offs suitable for diverse real-time security applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff08SegFormer\u3001DeepLabV3+\u3001SegNeXt\u548cSwin Transformer\uff09\u5728\u70ed\u6210\u50cf\u6b66\u5668\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u57289,711\u5f20\u771f\u5b9e\u76d1\u63a7\u89c6\u9891\u56fe\u50cf\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u70ed\u6210\u50cf\u6b66\u5668\u5206\u5272\u5728\u4f4e\u5149\u7167\u548c\u89c6\u89c9\u906e\u6321\u6761\u4ef6\u4e0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CNN\u65b9\u6cd5\u5728\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002Transformer\u5728RGB\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u70ed\u6210\u50cf\u6b66\u5668\u5206\u5272\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u56db\u79cdTransformer\u67b6\u6784\uff08SegFormer\u3001DeepLabV3+\u3001SegNeXt\u3001Swin Transformer\uff09\u5728\u81ea\u5b9a\u4e49\u70ed\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e8c\u503c\u6b66\u5668\u5206\u5272\uff0c\u91c7\u7528MMSegmentation\u6846\u67b6\u548c\u6807\u51c6\u6570\u636e\u589e\u5f3a\u7b56\u7565\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u516c\u5e73\u6bd4\u8f83\u3002", "result": "SegFormer-b5\u83b7\u5f97\u6700\u9ad8mIoU\uff0894.15%\uff09\u548c\u50cf\u7d20\u7cbe\u5ea6\uff0897.04%\uff09\uff0cSegFormer-b0\u63d0\u4f9b\u6700\u5feb\u63a8\u7406\u901f\u5ea6\uff0898.32 FPS\uff09\u4e14mIoU\u8fbe90.84%\u3002SegNeXt-mscans\u572885.12 FPS\u4e0b\u8fbe\u523092.24% mIoU\uff0cDeepLabV3+ R101-D8\u8fbe\u523092.76% mIoU\uff0829.86 FPS\uff09\u3002", "conclusion": "Transformer\u67b6\u6784\u5728\u4f4e\u5149\u7167\u548c\u906e\u6321\u70ed\u6210\u50cf\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u7cbe\u5ea6-\u901f\u5ea6\u6743\u8861\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5b9e\u65f6\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2510.17698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17698", "abs": "https://arxiv.org/abs/2510.17698", "authors": ["Liqun He", "Manolis Mavrikis", "Mutlu Cukurova"], "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues", "comment": null, "summary": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u8bdd\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5b66\u4e60\u8005\u4e0eLLM\u7684\u5bf9\u8bdd\u4e2d\u8bc6\u522b\u6709\u6548\u7684\u6559\u5b66\u7b56\u7565\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u5b66\u4e60\u8005-LLM\u4e92\u52a8\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6559\u80b2\u5e94\u7528\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u6027\u80fd\u6216\u5b66\u4e60\u6210\u679c\uff0c\u800c\u5ffd\u89c6\u4e86\u5b66\u4e60\u8005\u4e0eLLM\u4e4b\u95f4\u7684\u4e92\u52a8\u8fc7\u7a0b\uff0c\u9700\u8981\u66f4\u5173\u6ce8\u5bf9\u8bdd\u52a8\u6001\u548c\u6559\u5b66\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5bf9\u8bdd\u5206\u6790\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u3001\u5bf9\u8bdd\u884c\u4e3a\u6807\u6ce8\u3001\u5bf9\u8bdd\u6a21\u5f0f\u6316\u6398\u548c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u56db\u4e2a\u6b65\u9aa4\u3002", "result": "\u7814\u7a76\u5904\u4e8e\u8fdb\u884c\u4e2d\uff0c\u5df2\u83b7\u5f97\u521d\u6b65\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u6559\u80b2\u5e94\u7528\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u5bf9\u8bdd\u52a8\u6001\u548c\u6559\u5b66\u7b56\u7565\uff0c\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5bf9\u8bdd\u5206\u6790\u5728\u6559\u80b2\u6280\u672f\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16926", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16926", "abs": "https://arxiv.org/abs/2510.16926", "authors": ["Chenxu Li", "Zhicai Wang", "Yuan Sheng", "Xingyu Zhu", "Yanbin Hao", "Xiang Wang"], "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input", "comment": "23 pages,19 figures", "summary": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement.", "AI": {"tldr": "\u63d0\u51fa\u4e86Res-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u7a33\u5b9a\u6027\uff0c\u5f15\u5165\u65b0\u7684\u9c81\u68d2\u6027\u6307\u6807\u6765\u8861\u91cf\u5206\u8fa8\u7387\u53d8\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u6027\u80fd\uff0c\u5ffd\u7565\u4e86\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u5373\u6a21\u578b\u6027\u80fd\u662f\u5426\u5728\u4e0d\u540c\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "method": "\u6784\u5efa\u5305\u542b14,400\u4e2a\u6837\u672c\u7684Res-Bench\u57fa\u51c6\uff0c\u6db5\u76d612\u4e2a\u5206\u8fa8\u7387\u7ea7\u522b\u548c6\u4e2a\u6838\u5fc3\u80fd\u529b\u7ef4\u5ea6\uff1b\u8bbe\u8ba1\u4e86\u5305\u542bSpearman\u76f8\u5173\u6027\u548c\u7edd\u5bf9/\u76f8\u5bf9\u8fde\u7eed\u8bef\u5dee\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5bf9\u9886\u5148\u7684MLLMs\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5305\u62ec\u6a21\u578b\u4e2d\u5fc3\u4e0e\u4efb\u52a1\u4e2d\u5fc3\u7684\u9c81\u68d2\u6027\u5206\u6790\u3001\u9884\u5904\u7406\u7b56\u7565\u7814\u7a76\u4ee5\u53ca\u5fae\u8c03\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2510.17715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17715", "abs": "https://arxiv.org/abs/2510.17715", "authors": ["Hanxu Hu", "Xingxing Zhang", "Jannis Vamvas", "Rico Sennrich", "Furu Wei"], "title": "QueST: Incentivizing LLMs to Generate Difficult Problems", "comment": "20 pages, 7 figures", "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86QueST\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u56fe\u91c7\u6837\u548c\u62d2\u7edd\u5fae\u8c03\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ade\u4e89\u6027\u7f16\u7a0b\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff08\u4ec5\u6570\u5343\u5230\u6570\u4e07\u95ee\u9898\uff09\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u56fe\u91c7\u6837\u548c\u96be\u5ea6\u611f\u77e5\u62d2\u7edd\u5fae\u8c03\uff0c\u76f4\u63a5\u4f18\u5316\u4e13\u95e8\u7684\u751f\u6210\u5668\u6765\u521b\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u7f16\u7a0b\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u84b8\u998f\u6216\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0b\u6e38\u6a21\u578b\u3002", "result": "\u751f\u6210\u76848B\u6a21\u578b\u5728LiveCodeBench\u4e0a\u8d85\u8d8a\u4e86\u539f\u59cbQwen3-8B\uff0c\u4ec5\u752828K\u4eba\u5de5\u95ee\u9898\u914d\u4ee5\u5408\u6210\u89e3\u51b3\u65b9\u6848\u5c31\u8fbe\u5230\u4e86DeepSeek-R1-671B\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7QueST\u751f\u6210\u590d\u6742\u95ee\u9898\u4e3a\u63a8\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u548c\u63a8\u7406\u65b9\u9762\u7684\u524d\u6cbf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16973", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16973", "abs": "https://arxiv.org/abs/2510.16973", "authors": ["Praveenbalaji Rajendran", "Mojtaba Safari", "Wenfeng He", "Mingzhe Hu", "Shansong Wang", "Jun Zhou", "Xiaofeng Yang"], "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis", "comment": null, "summary": "Recent advancements in artificial intelligence (AI), particularly foundation\nmodels (FMs), have revolutionized medical image analysis, demonstrating strong\nzero- and few-shot performance across diverse medical imaging tasks, from\nsegmentation to report generation. Unlike traditional task-specific AI models,\nFMs leverage large corpora of labeled and unlabeled multimodal datasets to\nlearn generalized representations that can be adapted to various downstream\nclinical applications with minimal fine-tuning. However, despite the rapid\nproliferation of FM research in medical imaging, the field remains fragmented,\nlacking a unified synthesis that systematically maps the evolution of\narchitectures, training paradigms, and clinical applications across modalities.\nTo address this gap, this review article provides a comprehensive and\nstructured analysis of FMs in medical image analysis. We systematically\ncategorize studies into vision-only and vision-language FMs based on their\narchitectural foundations, training strategies, and downstream clinical tasks.\nAdditionally, a quantitative meta-analysis of the studies was conducted to\ncharacterize temporal trends in dataset utilization and application domains. We\nalso critically discuss persistent challenges, including domain adaptation,\nefficient fine-tuning, computational constraints, and interpretability along\nwith emerging solutions such as federated learning, knowledge distillation, and\nadvanced prompting. Finally, we identify key future research directions aimed\nat enhancing the robustness, explainability, and clinical integration of FMs,\nthereby accelerating their translation into real-world medical practice.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u6587\u7ae0\u7cfb\u7edf\u5206\u6790\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5305\u62ec\u89c6\u89c9\u4e13\u7528\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63a2\u8ba8\u4e86\u5176\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u4e34\u5e8a\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u5143\u5206\u6790\u63ed\u793a\u4e86\u6570\u636e\u96c6\u4f7f\u7528\u8d8b\u52bf\u548c\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u8be5\u9886\u57df\u4ecd\u7136\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u5bf9\u67b6\u6784\u6f14\u53d8\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u4e34\u5e8a\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002", "method": "\u5c06\u7814\u7a76\u7cfb\u7edf\u5206\u7c7b\u4e3a\u89c6\u89c9\u4e13\u7528\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5206\u6790\u5176\u67b6\u6784\u57fa\u7840\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u4e0b\u6e38\u4e34\u5e8a\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u5b9a\u91cf\u5143\u5206\u6790\u4ee5\u8868\u5f81\u6570\u636e\u96c6\u4f7f\u7528\u7684\u65f6\u95f4\u8d8b\u52bf\u3002", "result": "\u63ed\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\u8d8b\u52bf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u9886\u57df\u9002\u5e94\u3001\u9ad8\u6548\u5fae\u8c03\u7b49\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u96c6\u6210\u7b49\u5173\u952e\u95ee\u9898\uff0c\u4ee5\u52a0\u901f\u5176\u5728\u771f\u5b9e\u533b\u7597\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.17720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17720", "abs": "https://arxiv.org/abs/2510.17720", "authors": ["Nanda Kumar Rengarajan", "Jun Yan", "Chun Wang"], "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5c11\u6837\u672cNER\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5b9e\u73b0\u4e0eSOTA\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3NER\u4efb\u52a1\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u96f6\u6837\u672c\u548c\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u9886\u57df\u7279\u5b9a\u5b9e\u4f53\u4e14\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6709\u9650\u6570\u636e", "method": "1) \u65b0\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\uff0c\u7b80\u5316\u8f93\u51fa\u683c\u5f0f\u4ee5\u5229\u7528\u73b0\u4ee3LLM\u7684\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b2) \u6218\u7565\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4fdd\u6301\u5b9e\u4f53\u4fe1\u606f\u7684\u540c\u65f6\u5bf9\u4e0a\u4e0b\u6587\u8fdb\u884c\u6539\u5199", "result": "\u5728CrossNER\u6570\u636e\u96c6\u4e0a\u5c11\u6837\u672c\u65b9\u6cd5\u5e73\u5747F1\u8fbe80.1\uff0c\u6570\u636e\u589e\u5f3a\u7248\u672c\u6bd4\u57fa\u7ebf\u63d0\u5347\u6700\u591a17\u4e2aF1\u70b9", "conclusion": "\u4e3a\u62e5\u6709\u6709\u9650NER\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u7fa4\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.16983", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16983", "abs": "https://arxiv.org/abs/2510.16983", "authors": ["Yuanzhi Zhu", "Eleftherios Tsonis", "Lucas Degeorge", "Vicky Kalogeiton"], "title": "One-step Diffusion Models with Bregman Density Ratio Matching", "comment": "work in progress", "summary": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Di-Bregman\u6846\u67b6\uff0c\u901a\u8fc7Bregman\u6563\u5ea6\u5bc6\u5ea6\u6bd4\u5339\u914d\u6765\u52a0\u901f\u6269\u6563\u6a21\u578b\u91c7\u6837\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e00\u6b65\u751f\u6210\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u6a21\u578b\u751f\u6210\u8d28\u91cf\u9ad8\u4f46\u8ba1\u7b97\u6602\u8d35\uff0c\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u6269\u6563\u84b8\u998f\u6784\u5efa\u4e3a\u57fa\u4e8eBregman\u6563\u5ea6\u7684\u5bc6\u5ea6\u6bd4\u5339\u914d\u95ee\u9898\uff0c\u63d0\u4f9b\u51f8\u5206\u6790\u89c6\u89d2\u7edf\u4e00\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u5728CIFAR-10\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u53cd\u5411KL\u84b8\u998f\u83b7\u5f97\u66f4\u597d\u7684\u4e00\u6b65FID\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "Bregman\u5bc6\u5ea6\u6bd4\u5339\u914d\u4e3a\u9ad8\u6548\u4e00\u6b65\u6269\u6563\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7406\u8bba\u57fa\u7840\u7684\u9014\u5f84\u3002"}}
{"id": "2510.17725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17725", "abs": "https://arxiv.org/abs/2510.17725", "authors": ["Haozhen Zhang", "Tao Feng", "Pengrui Han", "Jiaxuan You"], "title": "AcademicEval: Live Long-Context LLM Benchmark", "comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval", "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval", "AI": {"tldr": "\u63d0\u51fa\u4e86AcademicEval\uff0c\u4e00\u4e2a\u57fa\u4e8earXiv\u8bba\u6587\u7684\u5b9e\u65f6\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6807\u9898\u3001\u6458\u8981\u3001\u5f15\u8a00\u548c\u76f8\u5173\u5de5\u4f5c\u56db\u4e2a\u5b66\u672f\u5199\u4f5c\u4efb\u52a1\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u4e14\u80fd\u907f\u514d\u6807\u7b7e\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u957f\u6587\u672cLLM\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u56fa\u5b9a\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u65f6\u6807\u7b7e\u6cc4\u9732\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528arXiv\u8bba\u6587\u6784\u5efa\u5b66\u672f\u5199\u4f5c\u4efb\u52a1\uff0c\u6574\u5408\u9ad8\u8d28\u91cf\u4e13\u5bb6\u7b56\u5212\u7684\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u652f\u6301\u7075\u6d3b\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u8bc4\u4f30\u3002", "result": "LLMs\u5728\u5177\u6709\u5c42\u6b21\u62bd\u8c61\u7ea7\u522b\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u957f\u7684\u5c11\u6837\u672c\u6f14\u793a\uff0c\u51f8\u663e\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u6311\u6218\u6027\u3002", "conclusion": "AcademicEval\u4e3a\u8bc4\u4f30LLMs\u7684\u957f\u6587\u672c\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u63d0\u5347LLMs\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.16988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16988", "abs": "https://arxiv.org/abs/2510.16988", "authors": ["Junhao Zhao", "Zishuai Liu", "Ruili Fang", "Jin Lu", "Linghan Zhang", "Fei Dou"], "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams", "comment": null, "summary": "The recognition of Activities of Daily Living (ADLs) from event-triggered\nambient sensors is an essential task in Ambient Assisted Living, yet existing\nmethods remain constrained by representation-level limitations. Sequence-based\napproaches preserve temporal order of sensor activations but are sensitive to\nnoise and lack spatial awareness, while image-based approaches capture global\npatterns and implicit spatial correlations but compress fine-grained temporal\ndynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)\nfail to enforce alignment between sequence- and image-based representation\nviews, underutilizing their complementary strengths. We propose Contrastive\nAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an\nend-to-end framework that jointly optimizes representation learning via\nSequence-Image Contrastive Alignment (SICA) and classification via\ncross-entropy, ensuring both cross-representation alignment and task-specific\ndiscriminability. CARE integrates (i) time-aware, noise-resilient sequence\nencoding with (ii) spatially-informed and frequency-sensitive image\nrepresentations, and employs (iii) a joint contrastive-classification objective\nfor end-to-end learning of aligned and discriminative embeddings. Evaluated on\nthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% on\nMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to\nsensor malfunctions and layout variability, highlighting its potential for\nreliable ADL recognition in smart homes.", "AI": {"tldr": "\u63d0\u51fa\u4e86CARE\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217-\u56fe\u50cf\u5bf9\u6bd4\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3ADL\u8bc6\u522b\u4e2d\u5e8f\u5217\u548c\u56fe\u50cf\u8868\u793a\u4e4b\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u4e09\u4e2aCASAS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u5c42\u9762\u7684\u9650\u5236\uff1a\u5e8f\u5217\u65b9\u6cd5\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u4f46\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\uff0c\u56fe\u50cf\u65b9\u6cd5\u6355\u83b7\u5168\u5c40\u6a21\u5f0f\u4f46\u538b\u7f29\u65f6\u95f4\u52a8\u6001\u5e76\u626d\u66f2\u4f20\u611f\u5668\u5e03\u5c40\uff0c\u7b80\u5355\u878d\u5408\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\u4e24\u79cd\u8868\u793a\u89c6\u56fe\u3002", "method": "CARE\u6846\u67b6\u5305\u542b\uff1a(1)\u65f6\u95f4\u611f\u77e5\u3001\u6297\u566a\u58f0\u7684\u5e8f\u5217\u7f16\u7801\uff1b(2)\u7a7a\u95f4\u611f\u77e5\u548c\u9891\u7387\u654f\u611f\u7684\u56fe\u50cf\u8868\u793a\uff1b(3)\u8054\u5408\u5bf9\u6bd4-\u5206\u7c7b\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u5e8f\u5217-\u56fe\u50cf\u5bf9\u6bd4\u5bf9\u9f50(SICA)\u548c\u4ea4\u53c9\u71b5\u8054\u5408\u4f18\u5316\u8868\u793a\u5b66\u4e60\u548c\u5206\u7c7b\u3002", "result": "\u5728\u4e09\u4e2aCASAS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aMilan 89.8%\u3001Cairo 88.9%\u3001Kyoto7 73.3%\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u4f20\u611f\u5668\u6545\u969c\u548c\u5e03\u5c40\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CARE\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u6709\u6548\u7ed3\u5408\u4e86\u5e8f\u5217\u548c\u56fe\u50cf\u8868\u793a\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4e3a\u667a\u80fd\u5bb6\u5c45\u4e2d\u53ef\u9760\u7684ADL\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17733", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17733", "abs": "https://arxiv.org/abs/2510.17733", "authors": ["Tong Chen", "Akari Asai", "Luke Zettlemoyer", "Hannaneh Hajishirzi", "Faeze Brahman"], "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations", "comment": null, "summary": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u5916\u6e90\u6027\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u4efb\u52a1\u6027\u80fd\u4e0d\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bad\u7ec3\u6570\u636e\u4e0d\u652f\u6301\u7684\u4e8b\u5b9e\u9519\u8bef\u4fe1\u606f\uff08\u5916\u6e90\u6027\u5e7b\u89c9\uff09\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5728\u5f00\u653e\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u53ea\u6709\u5f53\u6a21\u578b\u8f93\u51fa\u5b8c\u5168\u6b63\u786e\u65f6\u5956\u52b1\u4e3a1\uff0c\u5426\u5219\u4e3a0\u3002\u5728Qwen3\u63a8\u7406\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5e7b\u89c9\u7387\u964d\u4f4e39.3%\uff1b\u5728\u77ed\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cPopQA\u548cGPQA\u7684\u9519\u8bef\u7b54\u6848\u5206\u522b\u51cf\u5c1144.4%\u548c21.7%\uff1b\u540c\u65f6\u4fdd\u6301\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u7684\u6027\u80fd\u4e0d\u4e0b\u964d\u3002", "conclusion": "\u4e8c\u5143\u5956\u52b1\u65b9\u6848\u5728\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u6027\u80fd\u56de\u5f52\uff0c\u800c\u8fde\u7eed\u5956\u52b1RL\u867d\u7136\u4e5f\u80fd\u6539\u5584\u4e8b\u5b9e\u6027\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002"}}
{"id": "2510.16989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16989", "abs": "https://arxiv.org/abs/2510.16989", "authors": ["Luca Zanella", "Massimiliano Mancini", "Yiming Wang", "Alessio Tonioni", "Elisa Ricci"], "title": "Training-free Online Video Step Grounding", "comment": "NeurIPS 2025. Project website at https://lucazanella.github.io/baglm/", "summary": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims\nto detect which steps are performed in a video. Standard approaches for this\ntask require a labeled training set (e.g., with step-level annotations or\nnarrations), which may be costly to collect. Moreover, they process the full\nvideo offline, limiting their applications for scenarios requiring online\ndecisions. Thus, in this work, we explore how to perform VSG online and without\ntraining. We achieve this by exploiting the zero-shot capabilities of recent\nLarge Multimodal Models (LMMs). In particular, we use LMMs to predict the step\nassociated with a restricted set of frames, without access to the whole video.\nWe show that this online strategy without task-specific tuning outperforms\noffline and training-based models. Motivated by this finding, we develop\nBayesian Grounding with Large Multimodal Models (BaGLM), further injecting\nknowledge of past frames into the LMM-based predictions. BaGLM exploits\nBayesian filtering principles, modeling step transitions via (i) a dependency\nmatrix extracted through large language models and (ii) an estimation of step\nprogress. Experiments on three datasets show superior performance of BaGLM over\nstate-of-the-art training-based offline methods.", "AI": {"tldr": "\u63d0\u51faBaGLM\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u5728\u7ebf\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6574\u5408\u5386\u53f2\u5e27\u4fe1\u606f\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u8bad\u7ec3\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u65b9\u6cd5\u9700\u8981\u5e26\u6807\u6ce8\u7684\u8bad\u7ec3\u96c6\u4e14\u53ea\u80fd\u79bb\u7ebf\u5904\u7406\uff0c\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u6ee1\u8db3\u5728\u7ebf\u51b3\u7b56\u9700\u6c42\u3002\u672c\u6587\u63a2\u7d22\u5982\u4f55\u5728\u4e0d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5728\u7ebf\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u9884\u6d4b\u6709\u9650\u5e27\u7684\u6b65\u9aa4\uff0c\u63d0\u51faBaGLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6574\u5408\u5386\u53f2\u5e27\u4fe1\u606f\uff0c\u4f7f\u7528LLM\u63d0\u53d6\u7684\u4f9d\u8d56\u77e9\u9635\u548c\u6b65\u9aa4\u8fdb\u5ea6\u4f30\u8ba1\u6765\u5efa\u6a21\u6b65\u9aa4\u8f6c\u6362\u3002", "result": "\u5728\u7ebf\u65e0\u8bad\u7ec3\u7b56\u7565\u4f18\u4e8e\u79bb\u7ebf\u8bad\u7ec3\u6a21\u578b\uff0cBaGLM\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u53ef\u4ee5\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u5728\u7ebf\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\uff0cBaGLM\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.17764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17764", "abs": "https://arxiv.org/abs/2510.17764", "authors": ["Xiao Ye", "Jacob Dineen", "Zhaonan Li", "Zhikun Xu", "Weiyu Chen", "Shijie Lu", "Yuxi Huang", "Ming Shen", "Phu Tran", "Ji-Eun Irene Yum", "Muhammad Ali Khan", "Muhammad Umar Afzal", "Irbaz Bin Riaz", "Ben Zhou"], "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications", "comment": null, "summary": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u901a\u8fc7\u81ea\u4e3b\u6027\u5c42\u7ea7\u6846\u67b6(L0-L3)\u91cd\u65b0\u5b9a\u4e49\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0c\u4ece\u4fe1\u606f\u5de5\u5177\u5230\u76d1\u7763\u4ee3\u7406\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u4e0e\u5404\u5c42\u7ea7\u5141\u8bb8\u7684\u64cd\u4f5c\u548c\u98ce\u9669\u5bf9\u9f50\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c42\u7ea7\u7684\u8bc4\u4f30\u84dd\u56fe\u3002", "motivation": "\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5c06\u8fd9\u4e9b\u7ed3\u679c\u8f6c\u5316\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b89\u5168\u53ef\u9760\u7684\u6027\u80fd\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u53ef\u4fe1\u3001\u98ce\u9669\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u4e3b\u6027\u5c42\u7ea7\u6846\u67b6(L0-L3)\uff0c\u5c06\u8bc4\u4f30\u5206\u4e3a\u4fe1\u606f\u5de5\u5177\u3001\u4fe1\u606f\u8f6c\u6362\u4e0e\u805a\u5408\u3001\u51b3\u7b56\u652f\u6301\u548c\u76d1\u7763\u4ee3\u7406\u56db\u4e2a\u5c42\u7ea7\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u548c\u6307\u6807\u4e0e\u5404\u5c42\u7ea7\u7684\u5141\u8bb8\u64cd\u4f5c\u548c\u98ce\u9669\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5c42\u7ea7\u7684\u8bc4\u4f30\u84dd\u56fe\uff0c\u5305\u62ec\u6307\u6807\u9009\u62e9\u3001\u8bc1\u636e\u6536\u96c6\u548c\u58f0\u660e\u62a5\u544a\u7684\u65b9\u6cd5\uff0c\u5c06\u8bc4\u4f30\u4e0e\u76d1\u7ba1\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u901a\u8fc7\u4ee5\u81ea\u4e3b\u6027\u4e3a\u4e2d\u5fc3\uff0c\u8be5\u8c03\u67e5\u63a8\u52a8\u9886\u57df\u8d85\u8d8a\u57fa\u4e8e\u5206\u6570\u7684\u58f0\u660e\uff0c\u8f6c\u5411\u4e3a\u771f\u5b9e\u4e34\u5e8a\u4f7f\u7528\u63d0\u4f9b\u53ef\u4fe1\u3001\u98ce\u9669\u611f\u77e5\u7684\u8bc1\u636e\u3002"}}
{"id": "2510.17007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17007", "abs": "https://arxiv.org/abs/2510.17007", "authors": ["Ignacio M. De la Jara", "Cristian Rodriguez-Opazo", "Edison Marrese-Taylor", "Felipe Bravo-Marquez"], "title": "An empirical study of the effect of video encoders on Temporal Video Grounding", "comment": null, "summary": "Temporal video grounding is a fundamental task in computer vision, aiming to\nlocalize a natural language query in a long, untrimmed video. It has a key role\nin the scientific community, in part due to the large amount of video generated\nevery day. Although we find extensive work in this task, we note that research\nremains focused on a small selection of video representations, which may lead\nto architectural overfitting in the long run. To address this issue, we propose\nan empirical study to investigate the impact of different video features on a\nclassical architecture. We extract features for three well-known benchmarks,\nCharades-STA, ActivityNet-Captions and YouCookII, using video encoders based on\nCNNs, temporal reasoning and transformers. Our results show significant\ndifferences in the performance of our model by simply changing the video\nencoder, while also revealing clear patterns and errors derived from the use of\ncertain features, ultimately indicating potential feature complementarity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u5bf9\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4ec5\u6539\u53d8\u89c6\u9891\u7f16\u7801\u5668\u5c31\u80fd\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u7279\u5f81\u4e92\u8865\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u7814\u7a76\u8fc7\u4e8e\u96c6\u4e2d\u5728\u5c11\u6570\u89c6\u9891\u8868\u793a\u65b9\u6cd5\u4e0a\uff0c\u53ef\u80fd\u5bfc\u81f4\u67b6\u6784\u8fc7\u62df\u5408\u3002\u9700\u8981\u7814\u7a76\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u5bf9\u7ecf\u5178\u67b6\u6784\u7684\u5f71\u54cd\u3002", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u57fa\u4e8eCNN\u3001\u65f6\u5e8f\u63a8\u7406\u548ctransformer\u7684\u89c6\u9891\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728\u7ecf\u5178\u67b6\u6784\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4ec5\u6539\u53d8\u89c6\u9891\u7f16\u7801\u5668\u5c31\u80fd\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4f7f\u7528\u7279\u5b9a\u7279\u5f81\u4ea7\u751f\u7684\u660e\u663e\u6a21\u5f0f\u548c\u9519\u8bef\u3002", "conclusion": "\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u548c\u6f5c\u5728\u7684\u7279\u5f81\u4e92\u8865\u6027\uff0c\u8fd9\u4e3a\u672a\u6765\u7684\u7279\u5f81\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.17793", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17793", "abs": "https://arxiv.org/abs/2510.17793", "authors": ["Austin Xu", "Xuan-Phi Nguyen", "Yilun Zhou", "Chien-Sheng Wu", "Caiming Xiong", "Shafiq Joty"], "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains", "comment": "29 pages, 9 tables, 6 figures", "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FARE\uff08\u57fa\u7840\u81ea\u52a8\u63a8\u7406\u8bc4\u4f30\u5668\uff09\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u8bad\u7ec3\u4e868B\u548c20B\u53c2\u6570\u7684\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u4e13\u7528\u8bc4\u4f30\u5668\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u8bc4\u4f30\u5668\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65b0\u65b9\u6cd5\u5982\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u5ffd\u89c6\u4e86\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u5f00\u53d1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u6269\u5c55\u6765\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u3002", "method": "\u6536\u96c6\u4e86250\u4e07\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u4e94\u79cd\u8bc4\u4f30\u4efb\u52a1\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u8bad\u7ec3FARE\u8bc4\u4f30\u5668\u3002", "result": "FARE-8B\u6311\u6218\u4e86\u66f4\u5927\u7684\u4e13\u7528RL\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0cFARE-20B\u4e3a\u5f00\u6e90\u8bc4\u4f30\u5668\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8d85\u8d8a\u4e8670B+\u7684\u4e13\u7528\u8bc4\u4f30\u5668\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cFARE-20B\u5728MATH\u4e0a\u8fbe\u5230\u63a5\u8fd1oracle\u6027\u80fd\uff0c\u5728RL\u8bad\u7ec3\u4e2d\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u6027\u80fd14.1%\u3002", "conclusion": "\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u591f\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u7684\u8bc4\u4f30\u5668\uff0cFARE\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8bc4\u4f30\u5668\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.17014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17014", "abs": "https://arxiv.org/abs/2510.17014", "authors": ["Ani Vanyan", "Alvard Barseghyan", "Hakob Tamazyan", "Tigran Galstyan", "Vahan Huroyan", "Naira Hovakimyan", "Hrant Khachatrian"], "title": "Do Satellite Tasks Need Special Pretraining?", "comment": null, "summary": "Foundation models have advanced machine learning across various modalities,\nincluding images. Recently multiple teams trained foundation models specialized\nfor remote sensing applications. This line of research is motivated by the\ndistinct characteristics of remote sensing imagery, specific applications and\ntypes of robustness useful for satellite image analysis. In this work we\nsystematically challenge the idea that specific foundation models are more\nuseful than general-purpose vision foundation models, at least in the small\nscale. First, we design a simple benchmark that measures generalization of\nremote sensing models towards images with lower resolution for two downstream\ntasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,\nan ImageNet-scale satellite imagery dataset, with several modifications\nspecific to remote sensing. We show that none of those pretrained models bring\nconsistent improvements upon general-purpose baselines at the ViT-B scale.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u4e13\u7528\u9065\u611f\u57fa\u7840\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u89c2\u5ff5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bad\u7ec3iBOT\u6a21\u578b\uff0c\u53d1\u73b0\u5728ViT-B\u89c4\u6a21\u4e0b\u4e13\u7528\u6a21\u578b\u5e76\u672a\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u4e13\u7528\u9065\u611f\u57fa\u7840\u6a21\u578b\u662f\u5426\u771f\u7684\u6bd4\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u66f4\u6709\u7528\uff0c\u7279\u522b\u662f\u5728\u5c0f\u89c4\u6a21\u573a\u666f\u4e0b\uff0c\u8003\u8651\u9065\u611f\u56fe\u50cf\u7684\u72ec\u7279\u7279\u6027\u548c\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u6d4b\u91cf\u9065\u611f\u6a21\u578b\u5411\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5728MillionAID\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86iBOT\u81ea\u76d1\u7763\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u8fdb\u884c\u4e86\u9065\u611f\u7279\u5b9a\u7684\u4fee\u6539\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728ViT-B\u89c4\u6a21\u4e0b\uff0c\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u90fd\u6ca1\u6709\u6bd4\u901a\u7528\u57fa\u7ebf\u5e26\u6765\u4e00\u81f4\u7684\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u81f3\u5c11\u5728\u8f83\u5c0f\u89c4\u6a21\u4e0b\uff0c\u4e13\u7528\u9065\u611f\u57fa\u7840\u6a21\u578b\u5e76\u4e0d\u6bd4\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u66f4\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.17795", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17795", "abs": "https://arxiv.org/abs/2510.17795", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "title": "Executable Knowledge Graphs for Replicating AI Research", "comment": "Work in progress", "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31\uff08xKG\uff09\u6765\u89e3\u51b3AI\u7814\u7a76\u590d\u73b0\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u6280\u672f\u89c1\u89e3\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u7814\u7a76\u590d\u73b0\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u80cc\u666f\u77e5\u8bc6\u4e0d\u8db3\u548cRAG\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u53c2\u8003\u6587\u732e\u4e2d\u7684\u6f5c\u5728\u6280\u672f\u7ec6\u8282\uff0c\u540c\u65f6\u7f3a\u4e4f\u652f\u6301\u591a\u7c92\u5ea6\u68c0\u7d22\u548c\u91cd\u7528\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3002", "method": "\u63d0\u51fa\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31\uff08xKG\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u63d2\u62d4\u7684\u77e5\u8bc6\u5e93\uff0c\u81ea\u52a8\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u6280\u672f\u89c1\u89e3\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u5e76\u96c6\u6210\u5230\u4ee3\u7406\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u7406\u6846\u67b6\u548c\u4e24\u79cd\u4e0d\u540cLLM\u4e0a\u96c6\u6210xKG\u540e\uff0c\u5728PaperBench\u4e0a\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08o3-mini\u63d0\u534710.9%\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u81ea\u52a8\u5316AI\u7814\u7a76\u590d\u73b0\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "xKG\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u5728AI\u7814\u7a76\u590d\u73b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u8868\u793a\u548c\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17023", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17023", "abs": "https://arxiv.org/abs/2510.17023", "authors": ["Shraman Pramanick", "Effrosyni Mavroudi", "Yale Song", "Rama Chellappa", "Lorenzo Torresani", "Triantafyllos Afouras"], "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs", "comment": "ICCV 2025 (Highlights)", "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding\nutilizing multi-modal large language models. Our approach harnesses the\ncapabilities of multimodal LLMs to jointly process text and video, in order to\neffectively localize natural language queries in videos through a two-stage\nprocess. Rather than being directly grounded, language queries are initially\ntransformed into enriched sentences that incorporate missing details and cues\nto aid in grounding. In the second stage, these enriched queries are grounded,\nusing a lightweight decoder, which specializes at predicting accurate\nboundaries conditioned on contextualized representations of the enriched\nqueries. To mitigate noise and reduce the impact of hallucinations, our model\nis trained with a multiple-instance-learning objective that dynamically selects\nthe optimal version of the query for each training sample. We demonstrate\nstate-of-the-art results across various benchmarks in temporal video grounding\nand paragraph grounding settings. Experiments reveal that our method\nsignificantly outperforms all previously proposed LLM-based temporal grounding\napproaches and is either superior or comparable to specialized models, while\nmaintaining a clear advantage against them in zero-shot evaluation scenarios.", "AI": {"tldr": "ED-VTG\u662f\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5904\u7406\u5c06\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u589e\u5f3a\u53e5\u5b50\uff0c\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u8fdb\u884c\u5b9a\u4f4d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u6765\u8054\u5408\u5904\u7406\u6587\u672c\u548c\u89c6\u9891\uff0c\u6709\u6548\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u89e3\u51b3\u76f4\u63a5\u5b9a\u4f4d\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u7ec6\u8282\u7f3a\u5931\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5c06\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u5305\u542b\u7f3a\u5931\u7ec6\u8282\u548c\u7ebf\u7d22\u7684\u589e\u5f3a\u53e5\u5b50\uff0c\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u57fa\u4e8e\u589e\u5f3a\u67e5\u8be2\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u9884\u6d4b\u51c6\u786e\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\u76ee\u6807\u52a8\u6001\u9009\u62e9\u6700\u4f73\u67e5\u8be2\u7248\u672c\u3002", "result": "\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u548c\u6bb5\u843d\u5b9a\u4f4d\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6240\u6709\u5148\u524d\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u573a\u666f\u4e2d\u4fdd\u6301\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "ED-VTG\u65b9\u6cd5\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u4f18\u4e8e\u6216\u53ef\u4e0e\u4e13\u95e8\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001LLM\u5728\u7ec6\u7c92\u5ea6\u89c6\u9891\u5b9a\u4f4d\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17797", "abs": "https://arxiv.org/abs/2510.17797", "authors": ["Akshara Prabhakar", "Roshan Ram", "Zixiang Chen", "Silvio Savarese", "Frank Wang", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics", "comment": "Technical report; 13 pages plus references and appendices", "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\uff08EDR\uff09\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u3001\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53\u3001\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3001\u53ef\u89c6\u5316\u667a\u80fd\u4f53\u548c\u53cd\u601d\u673a\u5236\uff0c\u5b9e\u73b0\u4f01\u4e1a\u6570\u636e\u7684\u81ea\u52a8\u5316\u7814\u7a76\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u4f01\u4e1a\u9762\u4e34\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u7684\u538b\u529b\uff0c\u73b0\u6709\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u4f01\u4e1a\u96c6\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u3001\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53\uff08\u901a\u7528\u3001\u5b66\u672f\u3001GitHub\u3001LinkedIn\uff09\u3001MCP\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3001\u53ef\u89c6\u5316\u667a\u80fd\u4f53\u548c\u53cd\u601d\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u5728DeepResearch Bench\u548cDeepConsult\u7b49\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEDR\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "EDR\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\uff0c\u63a8\u8fdb\u591a\u667a\u80fd\u4f53\u63a8\u7406\u5e94\u7528\u7684\u7814\u7a76\u3002"}}
{"id": "2510.17034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17034", "abs": "https://arxiv.org/abs/2510.17034", "authors": ["Yutong Zhong"], "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding", "comment": null, "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.", "AI": {"tldr": "\u63d0\u51faW2R2\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u6377\u5f84\u6291\u5236\u6765\u89e3\u51b3\u591a\u6a21\u60013D\u5b9a\u4f4d\u4e2d\u76842D\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u63d0\u53473D\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u60013D\u5b9a\u4f4d\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\"2D\u8bed\u4e49\u504f\u5dee\"\uff0c\u8fc7\u5ea6\u4f9d\u8d562D\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u7c97\u5b9a\u4f4d\uff0c\u800c\u5ffd\u89c63D\u51e0\u4f55\u8f93\u5165\uff0c\u5bfc\u81f4\u878d\u5408\u6027\u80fd\u4e0d\u4f73\u3002", "method": "W2R2\u6846\u67b6\u5c062D\u7279\u5f81\u6307\u5b9a\u4e3a\u8bed\u4e49\u4fe1\u6807\u8fdb\u884c\"\u662f\u4ec0\u4e48\"\u8bc6\u522b\uff0c3D\u7279\u5f81\u4f5c\u4e3a\u7a7a\u95f4\u951a\u70b9\u8fdb\u884c\"\u5728\u54ea\u91cc\"\u5b9a\u4f4d\uff0c\u91c7\u7528\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff0c\u5305\u62ec\u7528\u4e8e\u591a\u6a21\u6001\u534f\u540c\u7684\u5bf9\u9f50\u635f\u5931\u548c\u901a\u8fc7\u8fb9\u9645\u673a\u5236\u60e9\u7f5a2D\u4e3b\u5bfc\u4f2a\u8f93\u51fa\u7684\u4f2a\u6807\u7b7e\u635f\u5931\u3002", "result": "\u5728ScanRefer\u548cScanQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cW2R2\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u6742\u4e71\u6237\u5916\u573a\u666f\u4e2d\u3002", "conclusion": "W2R2\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u6377\u5f84\u6291\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u60013D\u5b9a\u4f4d\u4e2d\u76842D\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u76843D\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2510.17035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17035", "abs": "https://arxiv.org/abs/2510.17035", "authors": ["Syed Konain Abbas", "Sandip Purnapatra", "M. G. Sarwar Murshed", "Conor Miller-Lynch", "Lambert Igene", "Soumyabrata Dey", "Stephanie Schuckers", "Faraz Hussain"], "title": "Conditional Synthetic Live and Spoof Fingerprint Generation", "comment": null, "summary": "Large fingerprint datasets, while important for training and evaluation, are\ntime-consuming and expensive to collect and require strict privacy measures.\nResearchers are exploring the use of synthetic fingerprint data to address\nthese issues. This paper presents a novel approach for generating synthetic\nfingerprint images (both spoof and live), addressing concerns related to\nprivacy, cost, and accessibility in biometric data collection. Our approach\nutilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce\nhigh-resolution synthetic live fingerprints, conditioned on specific finger\nidentities (thumb through little finger). Additionally, we employ CycleGANs to\ntranslate these into realistic spoof fingerprints, simulating a variety of\npresentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof\nfingerprints are crucial for developing robust spoof detection systems. Through\nthese generative models, we created two synthetic datasets (DB2 and DB3), each\ncontaining 1,500 fingerprint images of all ten fingers with multiple\nimpressions per finger, and including corresponding spoofs in eight material\ntypes. The results indicate robust performance: our StyleGAN3 model achieves a\nFr\\'echet Inception Distance (FID) as low as 5, and the generated fingerprints\nachieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The\nStyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess\nfingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,\nmatching experiments confirm strong privacy preservation, with no significant\nevidence of identity leakage, confirming the strong privacy-preserving\nproperties of our synthetic datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6761\u4ef6StyleGAN2-ADA\u548cStyleGAN3\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u6d3b\u4f53\u6307\u7eb9\uff0c\u5e76\u901a\u8fc7CycleGANs\u8f6c\u6362\u4e3a\u5404\u79cd\u6750\u6599\u7c7b\u578b\u7684\u6b3a\u9a97\u6307\u7eb9\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u4e24\u4e2a\u5408\u6210\u6307\u7eb9\u6570\u636e\u96c6\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6307\u7eb9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u6307\u7eb9\u6570\u636e\u96c6\u6536\u96c6\u8017\u65f6\u6602\u8d35\u3001\u9700\u8981\u4e25\u683c\u9690\u79c1\u4fdd\u62a4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6307\u7eb9\u6570\u636e\u6765\u5e94\u5bf9\u751f\u7269\u8bc6\u522b\u6570\u636e\u6536\u96c6\u4e2d\u7684\u9690\u79c1\u3001\u6210\u672c\u548c\u53ef\u8bbf\u95ee\u6027\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6761\u4ef6StyleGAN2-ADA\u548cStyleGAN3\u67b6\u6784\u751f\u6210\u7279\u5b9a\u624b\u6307\u8eab\u4efd\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u6d3b\u4f53\u6307\u7eb9\uff0c\u7136\u540e\u5229\u7528CycleGANs\u5c06\u8fd9\u4e9b\u6307\u7eb9\u8f6c\u6362\u4e3a\u6a21\u62df\u5404\u79cd\u5448\u73b0\u653b\u51fb\u6750\u6599\uff08\u5982EcoFlex\u3001Play-Doh\uff09\u7684\u903c\u771f\u6b3a\u9a97\u6307\u7eb9\u3002", "result": "StyleGAN3\u6a21\u578b\u5b9e\u73b0\u4e86\u4f4e\u81f35\u7684Fr\u00e9chet Inception Distance (FID)\uff0c\u751f\u6210\u7684\u6307\u7eb9\u57280.01%\u9519\u8bef\u63a5\u53d7\u7387\u4e0b\u8fbe\u523099.47%\u7684\u771f\u5b9e\u63a5\u53d7\u7387\uff0cStyleGAN2-ADA\u6a21\u578b\u8fbe\u523098.67%\u7684\u771f\u5b9e\u63a5\u53d7\u7387\u3002\u5339\u914d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u6ca1\u6709\u663e\u8457\u7684\u8eab\u4efd\u6cc4\u9732\u8bc1\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6307\u7eb9\u6570\u636e\u96c6\uff0c\u5728\u4fdd\u6301\u6307\u7eb9\u8bc6\u522b\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684\u6b3a\u9a97\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2510.17039", "categories": ["cs.CV", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17039", "abs": "https://arxiv.org/abs/2510.17039", "authors": ["Mohammad R. Salmanpour", "Sonya Falahati", "Amir Hossein Pouria", "Amin Mousavi", "Somayeh Sadat Mehrnia", "Morteza Alizadeh", "Arman Gorji", "Zeinab Farsangi", "Alireza Safarian", "Mehdi Maghsudi", "Carlos Uribe", "Arman Rahmim", "Ren Yuan"], "title": "Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework", "comment": "13 pages, 2 figures, and 2 tables", "summary": "Lung cancer remains the leading cause of cancer mortality, with CT imaging\ncentral to screening, prognosis, and treatment. Manual segmentation is variable\nand time-intensive, while deep learning (DL) offers automation but faces\nbarriers to clinical adoption. Guided by the Knowledge-to-Action framework,\nthis study develops a clinician-in-the-loop DL pipeline to enhance\nreproducibility, prognostic accuracy, and clinical trust. Multi-center CT data\nfrom 999 patients across 12 public datasets were analyzed using five DL models\n(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against\nexpert contours on whole and click-point cropped images. Segmentation\nreproducibility was assessed using 497 PySERA-extracted radiomic features via\nSpearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic\nmodeling compared supervised (SL) and semi-supervised learning (SSL) across 38\ndimensionality reduction strategies and 24 classifiers. Six physicians\nqualitatively evaluated masks across seven domains, including clinical\nmeaningfulness, boundary quality, prognostic value, trust, and workflow\nintegration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),\nradiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive\naccuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed\nSL across models. Radiologists favored VNet for peritumoral representation and\nsmoother boundaries, preferring AI-generated initial masks for refinement\nrather than replacement. These results demonstrate that integrating VNet with\nSSL yields accurate, reproducible, and clinically trusted CT-based lung cancer\nprognosis, highlighting a feasible path toward physician-centered AI\ntranslation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u7528\u4e8e\u80ba\u764cCT\u56fe\u50cf\u5206\u5272\u548c\u9884\u540e\u9884\u6d4b\u3002VNet\u6a21\u578b\u5728\u5206\u5272\u6027\u80fd\u3001\u653e\u5c04\u7ec4\u5b66\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u59cb\u7ec8\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u3002\u653e\u5c04\u79d1\u533b\u751f\u66f4\u503e\u5411\u4e8e\u4f7f\u7528AI\u751f\u6210\u7684\u521d\u59cb\u63a9\u6a21\u8fdb\u884c\u7cbe\u70bc\u800c\u975e\u5b8c\u5168\u66ff\u4ee3\u3002", "motivation": "\u80ba\u764c\u662f\u764c\u75c7\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0cCT\u6210\u50cf\u5728\u7b5b\u67e5\u3001\u9884\u540e\u548c\u6cbb\u7597\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u624b\u52a8\u5206\u5272\u5b58\u5728\u53d8\u5f02\u6027\u4e14\u8017\u65f6\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u63d0\u4f9b\u81ea\u52a8\u5316\u4f46\u9762\u4e34\u4e34\u5e8a\u91c7\u7528\u969c\u788d\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u7684\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3001\u9884\u540e\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u3002", "method": "\u4f7f\u7528\u6765\u81ea12\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684999\u540d\u60a3\u8005\u7684\u591a\u4e2d\u5fc3CT\u6570\u636e\uff0c\u91c7\u7528\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff083D Attention U-Net\u3001ResUNet\u3001VNet\u3001ReconNet\u3001SAM-Med3D\uff09\uff0c\u5728\u5b8c\u6574\u56fe\u50cf\u548c\u70b9\u51fb\u70b9\u88c1\u526a\u56fe\u50cf\u4e0a\u4e0e\u4e13\u5bb6\u8f6e\u5ed3\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u901a\u8fc7497\u4e2aPySERA\u63d0\u53d6\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u8bc4\u4f30\u5206\u5272\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u6bd4\u8f83\u76d1\u7763\u5b66\u4e60\u548c\u534a\u76d1\u7763\u5b66\u4e60\u572838\u79cd\u964d\u7ef4\u7b56\u7565\u548c24\u79cd\u5206\u7c7b\u5668\u4e0a\u7684\u9884\u540e\u5efa\u6a21\u6027\u80fd\u3002\u516d\u540d\u533b\u751f\u5728\u4e03\u4e2a\u9886\u57df\u5bf9\u63a9\u6a21\u8fdb\u884c\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "VNet\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff08Dice = 0.83\uff0cIoU = 0.71\uff09\u3001\u653e\u5c04\u7ec4\u5b66\u7a33\u5b9a\u6027\uff08\u5e73\u5747\u76f8\u5173\u6027 = 0.76\uff0cICC = 0.65\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\u4e0b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff08\u51c6\u786e\u7387 = 0.88\uff0cF1 = 0.83\uff09\u3002\u534a\u76d1\u7763\u5b66\u4e60\u5728\u6240\u6709\u6a21\u578b\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u3002\u653e\u5c04\u79d1\u533b\u751f\u66f4\u503e\u5411\u4e8eVNet\u7684\u7624\u5468\u8868\u793a\u548c\u66f4\u5e73\u6ed1\u7684\u8fb9\u754c\uff0c\u504f\u597d\u4f7f\u7528AI\u751f\u6210\u7684\u521d\u59cb\u63a9\u6a21\u8fdb\u884c\u7cbe\u70bc\u800c\u975e\u5b8c\u5168\u66ff\u4ee3\u3002", "conclusion": "\u5c06VNet\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ea7\u751f\u51c6\u786e\u3001\u53ef\u91cd\u590d\u4e14\u4e34\u5e8a\u53ef\u4fe1\u7684\u57fa\u4e8eCT\u7684\u80ba\u764c\u9884\u540e\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u5411\u4ee5\u533b\u751f\u4e3a\u4e2d\u5fc3\u7684AI\u8f6c\u5316\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.17043", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.17043", "abs": "https://arxiv.org/abs/2510.17043", "authors": ["Md Ahmed Al Muzaddid", "William J. Beksi"], "title": "Person Re-Identification via Generalized Class Prototypes", "comment": "18 pages, 11 figures, and 4 tables", "summary": "Advanced feature extraction methods have significantly contributed to\nenhancing the task of person re-identification. In addition, modifications to\nobjective functions have been developed to further improve performance.\nNonetheless, selecting better class representatives is an underexplored area of\nresearch that can also lead to advancements in re-identification performance.\nAlthough past works have experimented with using the centroid of a gallery\nimage class during training, only a few have investigated alternative\nrepresentations during the retrieval stage. In this paper, we demonstrate that\nthese prior techniques yield suboptimal results in terms of re-identification\nmetrics. To address the re-identification problem, we propose a generalized\nselection method that involves choosing representations that are not limited to\nclass centroids. Our approach strikes a balance between accuracy and mean\naverage precision, leading to improvements beyond the state of the art. For\nexample, the actual number of representations per class can be adjusted to meet\nspecific application requirements. We apply our methodology on top of multiple\nre-identification embeddings, and in all cases it substantially improves upon\ncontemporary results", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e7f\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u4e0d\u9650\u4e8e\u7c7b\u8d28\u5fc3\u7684\u8868\u793a\u6765\u6539\u8fdb\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5728\u51c6\u786e\u7387\u548c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "motivation": "\u867d\u7136\u5148\u8fdb\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u548c\u76ee\u6807\u51fd\u6570\u6539\u8fdb\u5df2\u663e\u8457\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u4f46\u9009\u62e9\u66f4\u597d\u7684\u7c7b\u4ee3\u8868\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7814\u7a76\u9886\u57df\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u7d22\u9636\u6bb5\u4f7f\u7528\u7c7b\u8d28\u5fc3\u4f5c\u4e3a\u4ee3\u8868\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u4e0d\u9650\u4e8e\u4f7f\u7528\u7c7b\u8d28\u5fc3\u4f5c\u4e3a\u4ee3\u8868\uff0c\u800c\u662f\u9009\u62e9\u66f4\u4f18\u7684\u7c7b\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u8c03\u6574\u6bcf\u4e2a\u7c7b\u7684\u5b9e\u9645\u8868\u793a\u6570\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u91cd\u8bc6\u522b\u5d4c\u5165\u6a21\u578b\u4e0a\u5e94\u7528\uff0c\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u663e\u8457\u6539\u8fdb\u4e86\u5f53\u4ee3\u7ed3\u679c\uff0c\u5728\u51c6\u786e\u7387\u548c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\u65b9\u9762\u53d6\u5f97\u5e73\u8861\u63d0\u5347\u3002", "conclusion": "\u5e7f\u4e49\u9009\u62e9\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u7c7b\u8868\u793a\u9009\u62e9\uff0c\u6709\u6548\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\uff0c\u4e14\u5177\u6709\u7075\u6d3b\u8c03\u6574\u8868\u793a\u6570\u91cf\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.17045", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17045", "abs": "https://arxiv.org/abs/2510.17045", "authors": ["Deepak Sridhar", "Kartikeya Bhardwaj", "Jeya Pradha Jeyaraj", "Nuno Vasconcelos", "Ankita Nayak", "Harris Teague"], "title": "Video Reasoning without Training", "comment": null, "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.", "AI": {"tldr": "\u63d0\u51faV-Reason\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u4f18\u5316\u8c03\u6574LMM\u7684\u503c\u7f13\u5b58\uff0c\u5728\u63a8\u7406\u65f6\u6539\u5584\u6a21\u578b\u7684\u5fae\u63a2\u7d22\u548c\u5229\u7528\u884c\u4e3a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u89c6\u9891\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63a8\u7406LMM\u4f9d\u8d56\u6602\u8d35\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u5197\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u601d\u7ef4\u8fc7\u7a0b\u63a7\u5236\u673a\u5236\u6709\u9650\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u8f93\u51fa\u71b5\u4f5c\u4e3a\u4fe1\u53f7\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u5c0f\u578b\u53ef\u8bad\u7ec3\u63a7\u5236\u5668\u5bf9LMM\u503c\u7f13\u5b58\u8fdb\u884c\u57fa\u4e8e\u71b5\u76ee\u6807\u7684\u4f18\u5316\uff0c\u6539\u5584\u5fae\u63a2\u7d22\u548c\u5229\u7528\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7840\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u4e0eRL\u8bad\u7ec3\u6a21\u578b\u5dee\u8ddd\u7f29\u5c0f\u81f30.6%\uff0c\u8f93\u51fatoken\u51cf\u5c1158.6%\u3002", "conclusion": "\u57fa\u4e8e\u71b5\u7684\u63a8\u7406\u65f6\u4f18\u5316\u80fd\u6709\u6548\u63d0\u5347LMM\u7684\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u63a5\u8fd1RL\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2510.17205", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17205", "abs": "https://arxiv.org/abs/2510.17205", "authors": ["Yingqi Fan", "Anhao Zhao", "Jinlan Fu", "Junlong Tong", "Hui Su", "Yijie Pan", "Wei Zhang", "Xiaoyu Shen"], "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs", "comment": "EMNLP 2025 Main", "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VisiPruner\u8bad\u7ec3\u65e0\u5173\u7684\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790MLLMs\u7684\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u89c6\u89c9\u76f8\u5173\u6ce8\u610f\u529b\u8ba1\u7b97\u548cFLOPs\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u4e8c\u6b21\u589e\u957f\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9MLLMs\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u673a\u5236\u7684\u57fa\u672c\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u53d1\u73b0\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u8fc7\u7a0b\uff1a\u6d45\u5c42\u8bc6\u522b\u4efb\u52a1\u610f\u56fe\u3001\u4e2d\u5c42\u5173\u952e\u89c6\u89c9token\u9a71\u52a8\u878d\u5408\u3001\u6df1\u5c42\u4e22\u5f03\u89c6\u89c9token\u4e13\u6ce8\u8bed\u8a00\u7cbe\u70bc\u3002\u57fa\u4e8e\u6b64\u63d0\u51faVisiPruner\u8bad\u7ec3\u65e0\u5173\u526a\u679d\u6846\u67b6\u3002", "result": "\u5728LLaVA-v1.5 7B\u4e0a\u51cf\u5c11\u9ad8\u8fbe99%\u7684\u89c6\u89c9\u76f8\u5173\u6ce8\u610f\u529b\u8ba1\u7b97\u548c53.9%\u7684FLOPs\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u79cdMLLMs\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u3002", "conclusion": "\u9664\u4e86\u526a\u679d\uff0c\u7814\u7a76\u7ed3\u679c\u8fd8\u4e3a\u8bad\u7ec3\u9ad8\u6548MLLMs\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u6307\u5bfc\uff0c\u901a\u8fc7\u4f7f\u6a21\u578b\u67b6\u6784\u4e0e\u5176\u5185\u5728\u5c42\u7ea7\u5904\u7406\u52a8\u6001\u5bf9\u9f50\u6765\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2510.17051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17051", "abs": "https://arxiv.org/abs/2510.17051", "authors": ["Masoud Khairi Atani", "Alon Harell", "Hyomin Choi", "Runyu Yang", "Fabien Racape", "Ivan V. Bajic"], "title": "How Universal Are SAM2 Features?", "comment": "This work has been accepted for publication in IEEE Picture Coding\n  Symposium (PCS) 2025", "summary": "The trade-off between general-purpose foundation vision models and their\nspecialized counterparts is critical for efficient feature coding design and is\nnot yet fully understood. We investigate this trade-off by comparing the\nfeature versatility of the general-purpose Hiera encoder against the\nsegmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,\ntrainable neck to probe the adaptability of their frozen features, we quantify\nthe information-theoretic cost of specialization. Our results reveal that while\nSAM2's specialization is highly effective for spatially-related tasks like\ndepth estimation, it comes at a cost. The specialized SAM2 encoder\nunderperforms its generalist predecessor, Hiera, on conceptually distant tasks\nsuch as pose estimation and image captioning, demonstrating a measurable loss\nof broader semantic information. A novel cross-neck analysis on SAM2 reveals\nthat each level of adaptation creates a further representational bottleneck.\nOur analysis illuminates these trade-offs in feature universality, providing a\nquantitative foundation for designing efficient feature coding and adaptation\nstrategies for diverse downstream applications.", "AI": {"tldr": "\u6bd4\u8f83\u901a\u7528\u89c6\u89c9\u6a21\u578bHiera\u548c\u4e13\u7528\u5206\u5272\u6a21\u578bSAM2\u7684\u7279\u5f81\u901a\u7528\u6027\uff0c\u53d1\u73b0\u4e13\u7528\u5316\u867d\u7136\u63d0\u5347\u7a7a\u95f4\u76f8\u5173\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u4f1a\u635f\u5931\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u6982\u5ff5\u8ddd\u79bb\u8f83\u8fdc\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0b\u964d\u3002", "motivation": "\u7406\u89e3\u901a\u7528\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u4e0e\u4e13\u7528\u6a21\u578b\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u9ad8\u6548\u7279\u5f81\u7f16\u7801\u8bbe\u8ba1\u63d0\u4f9b\u5b9a\u91cf\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u53ef\u8bad\u7ec3\u9888\u90e8\u7f51\u7edc\u6765\u63a2\u6d4b\u51bb\u7ed3\u7279\u5f81\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u6210\u672c\u91cf\u5316\u4e13\u7528\u5316\u4ee3\u4ef7\uff0c\u5e76\u8fdb\u884c\u8de8\u9888\u90e8\u5206\u6790\u3002", "result": "SAM2\u5728\u6df1\u5ea6\u4f30\u8ba1\u7b49\u7a7a\u95f4\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u59ff\u6001\u4f30\u8ba1\u548c\u56fe\u50cf\u63cf\u8ff0\u7b49\u8bed\u4e49\u4efb\u52a1\u4e0a\u4e0d\u5982Hiera\uff0c\u663e\u793a\u4e13\u7528\u5316\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u635f\u5931\u3002", "conclusion": "\u4e13\u7528\u5316\u5728\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u4f1a\u727a\u7272\u7279\u5f81\u901a\u7528\u6027\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u7684\u7279\u5f81\u7f16\u7801\u548c\u9002\u5e94\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cf\u5316\u6307\u5bfc\u3002"}}
{"id": "2510.17790", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17790", "abs": "https://arxiv.org/abs/2510.17790", "authors": ["Yuhao Yang", "Zhen Yang", "Zi-Yi Dou", "Anh Nguyen", "Keen You", "Omar Attia", "Andrew Szot", "Michael Feng", "Ram Ramrakhya", "Alexander Toshev", "Chao Huang", "Yinfei Yang", "Zhe Gan"], "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action", "comment": null, "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.", "AI": {"tldr": "UltraCUA\u662f\u4e00\u4e2a\u878d\u5408GUI\u57fa\u672c\u64cd\u4f5c\u4e0e\u9ad8\u7ea7\u7a0b\u5e8f\u5316\u5de5\u5177\u8c03\u7528\u7684\u591a\u6a21\u6001\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u52a8\u4f5c\u673a\u5236\u663e\u8457\u63d0\u5347\u6267\u884c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u4ec5\u4f9d\u8d56\u70b9\u51fb\u3001\u8f93\u5165\u3001\u6eda\u52a8\u7b49\u57fa\u672c\u64cd\u4f5c\uff0c\u9700\u8981\u7cbe\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u5197\u957f\u7684\u6267\u884c\u94fe\uff0c\u5bfc\u81f4\u7ea7\u8054\u5931\u8d25\u548c\u6027\u80fd\u74f6\u9888\uff0c\u800c\u5176\u4ed6\u4ee3\u7406\u53ef\u4ee5\u5229\u7528\u4e30\u5bcc\u7684\u7a0b\u5e8f\u5316\u63a5\u53e3\u3002", "method": "\u5305\u542b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u81ea\u52a8\u5316\u5de5\u5177\u6269\u5c55\u6d41\u6c34\u7ebf\u3001\u5408\u6210\u6570\u636e\u5f15\u64ce\u751f\u621017000+\u53ef\u9a8c\u8bc1\u4efb\u52a1\u3001\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6df7\u5408\u52a8\u4f5c\u8f68\u8ff9\u6536\u96c6\u3001\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728OSWorld\u4e0a\u76f8\u5bf9\u57fa\u51c6\u6a21\u578b\u63d0\u534722%\u5e73\u5747\u6027\u80fd\uff0c\u6b65\u9aa4\u51cf\u5c1111%\uff1b\u5728WindowsAgentArena\u4e0a\u8fbe\u523021.7%\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5728Windows\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6df7\u5408\u52a8\u4f5c\u673a\u5236\u5bf9\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u548c\u4fdd\u6301\u6267\u884c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u6210\u529f\u5f25\u5408\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u4e0e\u7a0b\u5e8f\u5316\u5de5\u5177\u80fd\u529b\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2510.17068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17068", "abs": "https://arxiv.org/abs/2510.17068", "authors": ["Zhe Luo", "Wenjing Jia", "Stuart Perry"], "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding", "comment": null, "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet", "AI": {"tldr": "\u63d0\u51faProDAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bc6\u5ea6\u611f\u77e5\u7684\u5c3e\u4e22\u5f03\u673a\u5236\u5b9e\u73b0\u70b9\u4e91\u7684\u6e10\u8fdb\u5f0f\u7f16\u7801\uff0c\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u652f\u6301\u591a\u6bd4\u7279\u7387\u89e3\u7801\uff0c\u663e\u8457\u63d0\u5347\u7f16\u7801\u6548\u7387\u3002", "motivation": "3D\u70b9\u4e91\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u5b9e\u65f6\u5904\u7406\u548c\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u5927\u6570\u636e\u91cf\u548c\u5e26\u5bbd\u9650\u5236\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9ad8\u8d28\u91cf\u670d\u52a1\u90e8\u7f72\u3002\u73b0\u6709\u5b66\u4e60\u578b\u7f16\u7801\u65b9\u6cd5\u7684\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\u4e0d\u652f\u6301\u6e10\u8fdb\u5f0f\u89e3\u7801\u3002", "method": "\u4f7f\u7528\u5bc6\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u6307\u5bfc\u4fe1\u53f7\uff0c\u57fa\u4e8e\u91cd\u8981\u6027\u81ea\u9002\u5e94\u89e3\u7801\u6f5c\u5728\u7279\u5f81\u548c\u5750\u6807\uff0c\u901a\u8fc7\u5bc6\u5ea6\u611f\u77e5\u7684\u5c3e\u4e22\u5f03\u673a\u5236\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u7f16\u7801\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cProDAT\u4e0d\u4ec5\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u7f16\u7801\uff0c\u800c\u4e14\u6bd4\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u578b\u7f16\u7801\u6280\u672f\u83b7\u5f97\u66f4\u597d\u7684\u7f16\u7801\u6548\u7387\uff0c\u5728SemanticKITTI\u4e0aPSNR-D2\u7684BD-rate\u63d0\u5347\u8d85\u8fc728.6%\uff0c\u5728ShapeNet\u4e0a\u8d85\u8fc718.15%\u3002", "conclusion": "ProDAT\u6210\u529f\u89e3\u51b3\u4e86\u70b9\u4e91\u6e10\u8fdb\u5f0f\u7f16\u7801\u95ee\u9898\uff0c\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u591a\u6bd4\u7279\u7387\u89e3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u6027\u80fd\u3002"}}
{"id": "2510.17800", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17800", "abs": "https://arxiv.org/abs/2510.17800", "authors": ["Jiale Cheng", "Yusen Liu", "Xinyu Zhang", "Yulin Fei", "Wenyi Hong", "Ruiliang Lyu", "Weihan Wang", "Zhe Su", "Xiaotao Gu", "Xiao Liu", "Yushi Bai", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "Glyph: Scaling Context Windows via Visual-Text Compression", "comment": null, "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.", "AI": {"tldr": "Glyph\u6846\u67b6\u901a\u8fc7\u5c06\u957f\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u5b9e\u73b0\u4e863-4\u500d\u7684\u6587\u672c\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u9700\u6c42\u7684\u589e\u52a0\uff0c\u6269\u5c55\u5230\u767e\u4e07token\u7ea7\u522b\u4f1a\u5e26\u6765\u8fc7\u9ad8\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u9650\u5236\u4e86\u957f\u4e0a\u4e0b\u6587LLMs\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faGlyph\u6846\u67b6\uff0c\u5c06\u957f\u6587\u672c\u6e32\u67d3\u6210\u56fe\u50cf\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff1b\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u9057\u4f20\u641c\u7d22\u6765\u5bfb\u627e\u6700\u4f73\u89c6\u89c9\u6e32\u67d3\u914d\u7f6e\uff0c\u5e73\u8861\u51c6\u786e\u6027\u548c\u538b\u7f29\u7387\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e863-4\u500d\u7684token\u538b\u7f29\uff0c\u51c6\u786e\u6027\u4e0eQwen3-8B\u7b49\u9886\u5148LLMs\u76f8\u5f53\uff1b\u9884\u586b\u5145\u548c\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u7ea64\u500d\uff0cSFT\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\uff1b128K\u4e0a\u4e0b\u6587\u7684VLM\u53ef\u6269\u5c55\u5230\u5904\u74061M token\u7ea7\u522b\u7684\u6587\u672c\u4efb\u52a1\u3002", "conclusion": "\u89c6\u89c9\u4e0a\u4e0b\u6587\u6269\u5c55\u65b9\u6cd5\u4e3a\u957f\u6587\u672c\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e3a\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u4efb\u52a1\u5e26\u6765\u76ca\u5904\u3002"}}
{"id": "2510.17078", "categories": ["cs.CV", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.17078", "abs": "https://arxiv.org/abs/2510.17078", "authors": ["Jad Berjawi", "Yoann Dupas", "Christophe C'erin"], "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection", "comment": "8 pages, 8 figures, accepted at ICCV 2025 MIRA Workshop", "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.", "AI": {"tldr": "FMCAF\u662f\u4e00\u79cd\u7528\u4e8eRGB\u548c\u7ea2\u5916\u56fe\u50cf\u878d\u5408\u7684\u9884\u5904\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u9891\u57df\u6ee4\u6ce2\u548c\u8de8\u6ce8\u610f\u529b\u878d\u5408\u63d0\u5347\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5728LLVIP\u548cVEDAI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u4f20\u611f\u5668\u6a21\u6001\u7684\u4e92\u8865\u7ebf\u7d22\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faFMCAF\u67b6\u6784\uff0c\u5305\u542b\u9891\u57df\u6ee4\u6ce2\u5757(Freq-Filter)\u6765\u6291\u5236\u5197\u4f59\u9891\u8c31\u7279\u5f81\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8de8\u6ce8\u610f\u529b\u7684\u878d\u5408\u6a21\u5757(MCAF)\u6765\u6539\u5584\u6a21\u6001\u95f4\u7279\u5f81\u5171\u4eab\u3002", "result": "\u5728LLVIP(\u4f4e\u5149\u884c\u4eba\u68c0\u6d4b)\u548cVEDAI(\u822a\u7a7a\u8f66\u8f86\u68c0\u6d4b)\u6570\u636e\u96c6\u4e0a\uff0cFMCAF\u4f18\u4e8e\u4f20\u7edf\u62fc\u63a5\u878d\u5408\u65b9\u6cd5\uff0cVEDAI\u4e0amAP@50\u63d0\u534713.9%\uff0cLLVIP\u4e0a\u63d0\u53471.1%\u3002", "conclusion": "FMCAF\u4f5c\u4e3a\u4e00\u79cd\u7075\u6d3b\u7684\u9884\u5904\u7406\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u68c0\u6d4b\u7ba1\u9053\u4e2d\u7684\u9c81\u68d2\u878d\u5408\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u5177\u6709\u8f83\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2510.17095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17095", "abs": "https://arxiv.org/abs/2510.17095", "authors": ["Ruitong Gan", "Junran Peng", "Yang Liu", "Chuanchen Luo", "Qing Li", "Zhaoxiang Zhang"], "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation", "comment": null, "summary": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines.", "AI": {"tldr": "GSPlane\u901a\u8fc7\u5f15\u5165\u5e73\u9762\u5148\u9a8c\u6765\u6539\u8fdb\u9ad8\u65af\u6cfc\u6e85\u76843D\u91cd\u5efa\uff0c\u7279\u522b\u9488\u5bf9\u5e73\u9762\u533a\u57df\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u5e73\u6ed1\u7684\u51e0\u4f55\u91cd\u5efa\u548c\u7ed3\u6784\u5316\u7f51\u683c\u8868\u793a", "motivation": "\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u91cd\u5efa\u5e73\u9762\u533a\u57df\u65f6\u5b58\u5728\u5e73\u6ed1\u5ea6\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u5e73\u9762\u662f3D\u573a\u666f\u4e2d\u7684\u91cd\u8981\u57fa\u5143\uff0c\u7279\u522b\u662f\u5728\u4eba\u9020\u73af\u5883\u4e2d", "method": "\u5229\u7528\u73b0\u6210\u7684\u5206\u5272\u548c\u6cd5\u7ebf\u9884\u6d4b\u6a21\u578b\u63d0\u53d6\u5e73\u9762\u5148\u9a8c\uff0c\u5efa\u7acb\u7ed3\u6784\u5316\u5e73\u9762\u9ad8\u65af\u5750\u6807\u8868\u793a\uff1b\u5f15\u5165\u52a8\u6001\u9ad8\u65af\u91cd\u5206\u7c7b\u5668\u81ea\u9002\u5e94\u91cd\u65b0\u5206\u7c7b\u9ad8\u68af\u5ea6\u5e73\u9762\u9ad8\u65af\uff1b\u5229\u7528\u4f18\u5316\u540e\u7684\u5e73\u9762\u5148\u9a8c\u4f18\u5316\u7f51\u683c\u5e03\u5c40", "result": "\u5728\u4e0d\u727a\u7272\u6e32\u67d3\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63d0\u53d6\u7f51\u683c\u7684\u51e0\u4f55\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u9876\u70b9\u548c\u9762\u6570\uff0c\u6539\u5584\u4e86\u62d3\u6251\u7ed3\u6784", "conclusion": "GSPlane\u901a\u8fc7\u5e73\u9762\u5148\u9a8c\u7684\u5f15\u5165\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u65af\u6cfc\u6e85\u5728\u5e73\u9762\u91cd\u5efa\u4e2d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u7ed3\u6784\u5316\u76843D\u573a\u666f\u91cd\u5efa"}}
{"id": "2510.17105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17105", "abs": "https://arxiv.org/abs/2510.17105", "authors": ["Xiaogang Xu", "Jian Wang", "Yunfan Lu", "Ruihang Chu", "Ruixing Wang", "Jiafei Wu", "Bei Yu", "Liang Lin"], "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement", "comment": null, "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6f5c\u5728\u7ec6\u5316\u7ba1\u9053\u548c\u53cc\u5411\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u771f\u5b9e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5185\u5bb9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u65b9\u6cd5\u5728\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u727a\u7272\u5185\u5bb9\u4fdd\u771f\u5ea6\u6765\u83b7\u5f97\u66f4\u9ad8\u7684\u611f\u77e5\u771f\u5b9e\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u7167\u573a\u666f\u4e0b\uff0c\u4e25\u91cd\u9000\u5316\u7684\u4fe1\u606f\u9650\u5236\u4e86\u6709\u6548\u63a7\u5236\u3002", "method": "\u5f15\u5165\u6f5c\u5728\u7ec6\u5316\u7ba1\u9053\u6765\u6062\u590dVAE\u7f16\u7801\u8fc7\u7a0b\u4e2d\u4e22\u5931\u7684\u7a7a\u95f4\u7ec6\u8282\uff0c\u5e76\u5efa\u7acb\u6761\u4ef6\u6f5c\u5728\u4e0e\u566a\u58f0\u6f5c\u5728\u4e4b\u95f4\u7684\u52a8\u6001\u53cc\u5411\u4ea4\u4e92\u673a\u5236\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u6269\u6563\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5bb9\u4fdd\u771f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u4f18\u5316\u7b56\u7565\u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u7f51\u7edc\u4e2d\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u771f\u5b9e\u6027\u548c\u7f8e\u5b66\u8d28\u91cf\u7684\u540c\u65f6\u589e\u5f3a\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2510.17114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17114", "abs": "https://arxiv.org/abs/2510.17114", "authors": ["Hodaka Kawachi", "Tomoya Nakamura", "Hiroaki Santo", "SaiKiran Kumar Tedla", "Trevor Dalton Canham", "Yasushi Yagi", "Michael S. Brown"], "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras", "comment": null, "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f7f\u7528LED\u73af\u5883\u7167\u660e\u751f\u6210\u4eba\u773c\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316LED\u5149\u8c31\u8f6e\u5ed3\u4f7f\u5176\u5bf9\u4eba\u773c\u51e0\u4e4e\u4e0d\u53ef\u89c1\u4f46\u5bf9\u6d88\u8d39\u7ea7\u76f8\u673a\u9ad8\u5ea6\u53ef\u68c0\u6d4b\uff0c\u652f\u6301\u5728\u6807\u51c6\u5e27\u7387\u4e0b\u63d0\u53d6\u6c34\u5370\u3002", "motivation": "\u4e3a\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u63d0\u4f9b\u4e00\u79cd\u89c6\u89c9\u4e0d\u53ef\u611f\u77e5\u7684\u6c34\u5370\u6280\u672f\uff0c\u5229\u7528LED\u7167\u660e\u548c\u76f8\u673a\u4f20\u611f\u5668\u7684\u5149\u8c31\u7279\u6027\u5dee\u5f02\u6765\u5b9e\u73b0\u9690\u853d\u7684\u4fe1\u606f\u5d4c\u5165\u3002", "method": "\u91c7\u7528\u5149\u8c31\u8c03\u5236\u800c\u975e\u5f3a\u5ea6\u8c03\u5236\uff0c\u8054\u5408\u8003\u8651\u4eba\u773c\u89c6\u89c9\u7cfb\u7edf\u654f\u611f\u6027\u3001\u76f8\u673a\u4f20\u611f\u5668\u5149\u8c31\u54cd\u5e94\u548c\u7a84\u5e26LED\u751f\u6210\u767d\u5149\u7684\u7279\u6027\uff0c\u4f18\u5316LED\u5149\u8c31\u8f6e\u5ed3\u4ee5\u5b9e\u73b0\u4eba\u773c\u4e0d\u53ef\u89c1\u4f46\u76f8\u673a\u53ef\u68c0\u6d4b\u7684\u6c34\u5370\u3002", "result": "\u80fd\u591f\u5728\u6807\u51c6\u4f4e\u5e27\u7387\uff0830-60 fps\uff09\u4e0b\u63d0\u53d6\u6c34\u5370\uff0c10\u79d2\u89c6\u9891\u7247\u6bb5\u53ef\u5d4c\u5165128\u4f4d\u4fe1\u606f\uff0c\u8db3\u4ee5\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u6240\u9700\u7684\u5143\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e0d\u53ef\u611f\u77e5\u7684\u6c34\u5370\u5d4c\u5165\uff0c\u4e3a\u6d88\u8d39\u7ea7\u76f8\u673a\u73af\u5883\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17131", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17131", "abs": "https://arxiv.org/abs/2510.17131", "authors": ["Xin Gao", "Jiyao Liu", "Guanghao Li", "Yueming Lyu", "Jianxiong Gao", "Weichen Yu", "Ningsheng Xu", "Liang Wang", "Caifeng Shan", "Ziwei Liu", "Chenyang Si"], "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection", "comment": "28 pages, 16 figures, conference", "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.", "AI": {"tldr": "GOOD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5f15\u5bfc\u673a\u5236\u76f4\u63a5\u6307\u5bfc\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u751f\u6210OOD\u6837\u672c\uff0c\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6270\u52a8\u6587\u672c\u6761\u4ef6\u5d4c\u5165\u751f\u6210OOD\u6837\u672c\uff0c\u5b58\u5728\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u504f\u79fb\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9eOOD\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u4f7f\u7528\u73b0\u6210\u7684ID\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u56fe\u50cf\u7ea7\u5f15\u5bfc\uff08\u57fa\u4e8e\u5bf9\u6570\u5206\u5272\u68af\u5ea6\u51cf\u5c11\u8f93\u5165\u4f3c\u7136\uff09\u548c\u7279\u5f81\u7ea7\u5f15\u5bfc\uff08\u57fa\u4e8ek-NN\u8ddd\u79bb\u5728\u5206\u7c7b\u5668\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4fc3\u8fdb\u7a00\u758f\u533a\u57df\u91c7\u6837\uff09\u7684\u53cc\u91cd\u5f15\u5bfc\u8bbe\u8ba1", "result": "\u8bad\u7ec3\u4f7f\u7528GOOD\u751f\u6210\u7684\u6837\u672c\u53ef\u4ee5\u663e\u8457\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd", "conclusion": "GOOD\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u5f15\u5bfc\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u548c\u591a\u6837\u5316\u7684OOD\u6837\u672c\u751f\u6210\uff0c\u589e\u5f3a\u4e86OOD\u68c0\u6d4b\u7684\u9c81\u68d2\u6027"}}
{"id": "2510.17137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17137", "abs": "https://arxiv.org/abs/2510.17137", "authors": ["WenBo Xu", "Liu Liu", "Li Zhang", "Ran Zhang", "Hao Wu", "Dan Guo", "Meng Wang"], "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation", "comment": null, "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.", "AI": {"tldr": "KineDiff3D\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5\u6269\u6563\u6a21\u578b\u4ece\u5355\u89c6\u56fe\u8f93\u5165\u91cd\u5efa\u548c\u751f\u6210\u7c7b\u522b\u7ea7\u94f0\u63a5\u7269\u4f53\u5f62\u72b6\uff0c\u5e76\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\uff08\u5982\u7b14\u8bb0\u672c\u7535\u8111\u548c\u62bd\u5c49\uff09\u7531\u4e8e\u5176\u591a\u90e8\u4ef6\u51e0\u4f55\u7ed3\u6784\u548c\u53ef\u53d8\u5173\u8282\u914d\u7f6e\uff0c\u57283D\u91cd\u5efa\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5728\u4e0d\u540c\u72b6\u6001\u4e0b\u8868\u73b0\u51fa\u591a\u6837\u6027\u3002", "method": "\u9996\u5148\u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5VAE\u5c06\u5b8c\u6574\u51e0\u4f55\uff08SDF\uff09\u3001\u5173\u8282\u89d2\u5ea6\u548c\u90e8\u4ef6\u5206\u5272\u7f16\u7801\u5230\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff1b\u7136\u540e\u4f7f\u7528\u4e24\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5206\u522b\u56de\u5f52\u5168\u5c40\u59ff\u6001\uff08SE(3)\uff09\u548c\u5173\u8282\u53c2\u6570\uff0c\u4ee5\u53ca\u4ece\u90e8\u5206\u89c2\u6d4b\u751f\u6210\u8fd0\u52a8\u5b66\u611f\u77e5\u6f5c\u5728\u4ee3\u7801\uff1b\u6700\u540e\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6a21\u5757\u53cc\u5411\u4f18\u5316\u91cd\u5efa\u7cbe\u5ea6\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u3001\u534a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u94f0\u63a5\u7269\u4f53\u5e76\u4f30\u8ba1\u5176\u8fd0\u52a8\u5b66\u5c5e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684KineDiff3D\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u94f0\u63a5\u7269\u4f53\u76843D\u91cd\u5efa\u548c\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u91cd\u5efa\u548c\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2510.17157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17157", "abs": "https://arxiv.org/abs/2510.17157", "authors": ["Yinghui Wang", "Xinyu Zhang", "Peng Du"], "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image", "comment": null, "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.", "AI": {"tldr": "GACO-CAD\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u7ec4\u957f\u5ea6\u5956\u52b1\uff0c\u63d0\u5347\u4ece\u5355\u56fe\u50cf\u751f\u6210\u53ef\u7f16\u8f91CAD\u6a21\u578b\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u5efa\u6a21\u7b80\u6d01\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece2D\u56fe\u50cf\u51c6\u786e\u63a8\u65ad3D\u51e0\u4f55\u7684\u56f0\u96be\uff0c\u964d\u4f4e\u5de5\u4e1a\u6982\u5ff5\u8bbe\u8ba1\u7684\u95e8\u69db\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u591a\u901a\u9053\u8f93\u5165\u7684\u51e0\u4f55\u5148\u9a8c\uff1b\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u7ec4\u957f\u5ea6\u5956\u52b1\u6765\u9f13\u52b1\u7b80\u6d01\u5efa\u6a21\u5e8f\u5217\u3002", "result": "\u5728DeepCAD\u548cFusion360\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u6709\u6548\u6027\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u5efa\u6a21\u7b80\u6d01\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GACO-CAD\u901a\u8fc7\u51e0\u4f55\u5148\u9a8c\u548c\u7b80\u6d01\u6027\u5956\u52b1\u6709\u6548\u63d0\u5347\u4e86\u4ece\u5355\u56fe\u50cf\u751f\u6210CAD\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.17169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17169", "abs": "https://arxiv.org/abs/2510.17169", "authors": ["Roland Croft", "Brian Du", "Darcy Joseph", "Sharath Kumar"], "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition", "comment": "Accepted for publication in DICTA 2025", "summary": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u9884\u5904\u7406\u6b65\u9aa4\u5bf9\u5bf9\u6297\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u7684\u9009\u62e9\u4f1a\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u800c\u63d0\u51fa\u7684\u9884\u5904\u7406\u4e0d\u53d8\u65b9\u6cd5\u53ef\u63d0\u5347\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u8fbe27%\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6837\u672c\u653b\u51fb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u9884\u5904\u7406\u6b65\u9aa4\u5728\u9ed1\u76d2\u653b\u51fb\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u9884\u5904\u7406\u6280\u672f\u5bf9\u5bf9\u6297\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u73b0\u6210\u7684\u6700\u5148\u8fdb\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u4e0d\u540c\u9884\u5904\u7406\u6280\u672f\u4e0b\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5206\u6790\u4e86\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u548c\u964d\u91c7\u6837\u63d2\u503c\u65b9\u6cd5\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f93\u5165\u53d8\u6362\u7684\u9884\u5904\u7406\u4e0d\u53d8\u65b9\u6cd5\u6765\u63d0\u5347\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u3002", "result": "\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u7684\u9009\u62e9\u53ef\u4f7f\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u9ad8\u8fbe78%\uff0c\u800c\u63d2\u503c\u65b9\u6cd5\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002\u9884\u5904\u7406\u8981\u6c42\u5373\u4f7f\u5728\u767d\u76d2\u8bbe\u7f6e\u4e0b\u4e5f\u4f1a\u964d\u4f4e\u653b\u51fb\u5f3a\u5ea6\u3002\u63d0\u51fa\u7684\u9884\u5904\u7406\u4e0d\u53d8\u65b9\u6cd5\u53ef\u5c06\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u63d0\u5347\u9ad8\u8fbe27%\u3002", "conclusion": "\u9884\u5904\u7406\u5728\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8003\u8651\u9884\u5904\u7406\u56e0\u7d20\u6765\u63d0\u9ad8\u9762\u90e8\u5bf9\u6297\u6837\u672c\u7684\u5bf9\u6297\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17171", "abs": "https://arxiv.org/abs/2510.17171", "authors": ["Feihong Yan", "Peiru Wang", "Yao Zhu", "Kaiyu Pang", "Qingyan Wei", "Huiqi Li", "Linfeng Zhang"], "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling", "comment": "12 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.", "AI": {"tldr": "GtR\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u7ed3\u6784\u751f\u6210\u548c\u7ec6\u8282\u91cd\u5efa\u4e24\u4e2a\u9636\u6bb5\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b03.72\u500d\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3MAR\u6a21\u578b\u5728\u5e76\u884c\u751f\u6210\u89c6\u89c9\u5185\u5bb9\u65f6\uff0c\u7531\u4e8e\u7a7a\u95f4\u76f8\u5173\u89c6\u89c9\u6807\u8bb0\u7684\u5efa\u6a21\u590d\u6742\u6027\u800c\u5bfc\u81f4\u7684\u52a0\u901f\u6f5c\u529b\u53d7\u9650\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7ed3\u6784\u751f\u6210\u9636\u6bb5\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u6846\u67b6\uff0c\u7ec6\u8282\u91cd\u5efa\u9636\u6bb5\u9ad8\u6548\u5b8c\u6210\u5269\u4f59\u6807\u8bb0\u3002\u540c\u65f6\u63d0\u51fa\u9891\u7387\u52a0\u6743\u6807\u8bb0\u9009\u62e9(FTS)\uff0c\u4e3a\u56fe\u50cf\u7ec6\u8282\u533a\u57df\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728ImageNet\u7c7b\u6761\u4ef6\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cMAR-H\u6a21\u578b\u5b9e\u73b03.72\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8d28\u91cf(FID:1.59, IS:304.4 vs \u539f\u59cb1.59, 299.1)\u3002", "conclusion": "GtR\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3aMAR\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17179", "abs": "https://arxiv.org/abs/2510.17179", "authors": ["Yingzi Han", "Jiakai He", "Chuanlong Xie", "Jianping Li"], "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring", "comment": null, "summary": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6d6e\u6e38\u751f\u7269\u8bc6\u522b\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u7cfb\u7edf\u6027\u7684OoD\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8622\u79cdOoD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0ViM\u65b9\u6cd5\u5728\u8fdcOoD\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u6d6e\u6e38\u751f\u7269\u8bc6\u522b\u6a21\u578b\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u5206\u5e03\u504f\u79fb\u6311\u6218\uff0c\u7531\u4e8e\u6d6e\u6e38\u751f\u7269\u5f62\u6001\u590d\u6742\u3001\u7269\u79cd\u591a\u6837\u6027\u5927\u4e14\u4e0d\u65ad\u6709\u65b0\u7269\u79cd\u53d1\u73b0\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u51fa\u73b0\u4e0d\u53ef\u9884\u6d4b\u9519\u8bef\u3002\u8be5\u9886\u57df\u7f3a\u4e4f\u6700\u65b0\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u7cfb\u7edf\u6574\u5408\u548c\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u57fa\u4e8eDYB-PlanktonNet\u6570\u636e\u96c6\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u6a21\u62df\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u573a\u666f\u7684OoD\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e8622\u79cdOoD\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eViM\u65b9\u6cd5\u5728\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8fdcOoD\u573a\u666f\u4e2d\u5173\u952e\u6307\u6807\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u81ea\u52a8\u6d6e\u6e38\u751f\u7269\u8bc6\u522b\u4e2d\u7684\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u9760\u53c2\u8003\uff0c\u5e76\u4e3a\u6d6e\u6e38\u751f\u7269OoD\u68c0\u6d4b\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u662f\u8be5\u9886\u57df\u9996\u6b21\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u8bc4\u4f30\u5206\u6790\u3002"}}
{"id": "2510.17181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17181", "abs": "https://arxiv.org/abs/2510.17181", "authors": ["Haonan He", "Yufeng Zheng", "Jie Song"], "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video", "comment": "ICCV 2025", "summary": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8054\u5408\u5b66\u4e60\u8be6\u7ec6\u5934\u90e8\u5316\u8eab\u548c\u624b-\u9762\u90e8\u4e92\u52a8\u5f15\u8d77\u7684\u975e\u521a\u6027\u53d8\u5f62\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u81ea\u7136\u624b-\u9762\u90e8\u4e92\u52a8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u533a\u57df\uff0c\u5ffd\u7565\u4e86\u4f20\u8fbe\u8ba4\u77e5\u72b6\u6001\u7684\u81ea\u7136\u624b-\u9762\u90e8\u4e92\u52a8\uff08\u5982\u624b\u6258\u4e0b\u5df4\u3001\u624b\u6307\u8f7b\u89e6\u8138\u988a\u7b49\uff09\uff0c\u8fd9\u4e9b\u4e92\u52a8\u5bf9\u4e8e\u903c\u771f\u76843D\u5934\u90e8\u5316\u8eab\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u987a\u5e8f\u635f\u5931\u548c\u63a5\u89e6\u6b63\u5219\u5316\u8fdb\u884c\u59ff\u6001\u8ddf\u8e2a\uff0c\u5b66\u4e60\u624b\u5f15\u8d77\u9762\u90e8\u53d8\u5f62\u7684PCA\u57fa\u7840\uff0c\u5e76\u5f15\u5165\u63a5\u89e6\u635f\u5931\u51cf\u5c11\u7a7f\u63d2\u4f2a\u5f71\uff0c\u63d0\u9ad8\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5728iPhone\u62cd\u6444\u7684RGB(D)\u89c6\u9891\u4e0a\u8bc4\u4f30\uff0c\u5e76\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u6355\u6349\u66f4\u597d\u7684\u5916\u89c2\u548c\u66f4\u51c6\u786e\u7684\u9762\u90e8\u53d8\u5f62\u51e0\u4f55\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6355\u6349\u624b-\u9762\u90e8\u4e92\u52a8\u5f15\u8d77\u7684\u9762\u90e8\u53d8\u5f62\uff0c\u751f\u6210\u66f4\u903c\u771f\u548c\u7269\u7406\u5408\u7406\u76843D\u5934\u90e8\u5316\u8eab\u3002"}}
{"id": "2510.17188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17188", "abs": "https://arxiv.org/abs/2510.17188", "authors": ["Vaibhav Rathore", "Divyam Gupta", "Biplab Banerjee"], "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery", "comment": "Accpeted at NeurIPS (2025) Main Conference", "summary": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.", "AI": {"tldr": "HIDISC\u662f\u4e00\u4e2a\u53cc\u66f2\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u4e0e\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u95ee\u9898\uff0c\u901a\u8fc7GPT\u5f15\u5bfc\u7684\u6269\u6563\u589e\u5f3a\u548c\u5207\u7ebf\u7a7a\u95f4\u63d2\u503c\u7b49\u65b9\u6cd5\uff0c\u5728\u672a\u89c1\u9886\u57df\u548c\u7c7b\u522b\u4e0a\u5b9e\u73b0\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709GCD\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6765\u81ea\u540c\u4e00\u9886\u57df\uff0c\u9650\u5236\u4e86\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002DG-GCD\u8981\u6c42\u6a21\u578b\u6cdb\u5316\u5230\u5305\u542b\u65b0\u7c7b\u522b\u7684\u672a\u89c1\u9886\u57df\uff0c\u800c\u73b0\u6709\u65b9\u6cd5DG2CD-Net\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "method": "\u4f7f\u7528GPT\u5f15\u5bfc\u7684\u6269\u6563\u8fdb\u884c\u6e90\u57df\u589e\u5f3a\uff1b\u5f15\u5165\u5207\u7ebfCutMix\u8fdb\u884c\u66f2\u7387\u611f\u77e5\u63d2\u503c\uff1b\u7ed3\u5408\u60e9\u7f5aBusemann\u5bf9\u9f50\u3001\u6df7\u5408\u53cc\u66f2\u5bf9\u6bd4\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u79bb\u7fa4\u70b9\u6392\u65a5\u7684\u7edf\u4e00\u635f\u5931\u51fd\u6570\uff1b\u53ef\u5b66\u4e60\u7684\u66f2\u7387\u53c2\u6570\u9002\u5e94\u6570\u636e\u96c6\u590d\u6742\u6027\u3002", "result": "\u5728PACS\u3001Office-Home\u548cDomainNet\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2(DG)-GCD\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HIDISC\u901a\u8fc7\u53cc\u66f2\u8868\u793a\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u548c\u7c7b\u522b\u7ea7\u522b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e86\u60c5\u666f\u6a21\u62df\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8de8\u9886\u57df\u7c7b\u522b\u53d1\u73b0\u3002"}}
{"id": "2510.17197", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17197", "abs": "https://arxiv.org/abs/2510.17197", "authors": ["Pu Zhang", "Yuwei Li", "Xingyuan Xian", "Guoming Tang"], "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models", "comment": null, "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c\u7684\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u4efb\u52a1\u76f8\u5173\u6027\u548c\u4fe1\u606f\u591a\u6837\u6027\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u80fd\u529b\u589e\u5f3a\uff0c\u89c6\u89c9token\u5197\u4f59\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6587\u672c\u63d0\u793a\u7684\u6307\u5bfc\uff0c\u65e0\u6cd5\u4f18\u5148\u8003\u8651\u4efb\u52a1\u76f8\u5173\u6027", "method": "\u5206\u5c42\u65b9\u6cd5\uff1a\u5148\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u7684\u6838\u5fc3\u89c6\u89c9token\uff0c\u518d\u8865\u5145\u591a\u6837\u6027token\u4ee5\u4fdd\u7559\u66f4\u5e7f\u6cdb\u4e0a\u4e0b\u6587", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5373\u4f7f\u526a\u679d90%\u7684token\u4e5f\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ec5\u9020\u6210\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11GPU\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u5ef6\u8fdf", "conclusion": "\u63d0\u51fa\u7684\u63d0\u793a\u611f\u77e5\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2510.17198", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17198", "abs": "https://arxiv.org/abs/2510.17198", "authors": ["M Saifuzzaman Rafat", "Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Jungpil Shin"], "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh", "comment": "Submitted to the International Conference on Data and Applied\n  Analytics (IDAA 2025). 15 pages, 5 figures, 4 tables", "summary": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path.", "AI": {"tldr": "\u4f7f\u7528Segment Anything Model (SAM)\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u76d1\u6d4b\u5b5f\u52a0\u62c9\u56fd\u6cb3\u6d41\u4fb5\u8680\u7684AI\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03SAM\u7684\u63a9\u7801\u89e3\u7801\u5668\u6765\u8bc6\u522b\u6cb3\u5cb8\u4fb5\u8680\u7684\u7ec6\u5fae\u7279\u5f81\uff0c\u5728\u571f\u5730-\u6c34\u4f53\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8686.30%\u7684mIoU\u548c92.60%\u7684Dice\u5206\u6570\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u7684\u4e3b\u8981\u6cb3\u6d41\u65e2\u662f\u5546\u4e1a\u548c\u751f\u5b58\u7684\u52a8\u8109\uff0c\u4e5f\u662f\u65e0\u60c5\u7684\u7834\u574f\u8005\uff0c\u6bcf\u5e74\u541e\u566c\u6574\u4e2a\u6751\u5e84\u548c\u5927\u7247\u519c\u7530\uff0c\u5bfc\u81f4\u6570\u5343\u5bb6\u5ead\u6d41\u79bb\u5931\u6240\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8ddf\u8e2a\u8fd9\u79cd\u7f13\u6162\u53d1\u751f\u7684\u707e\u96be\u3002", "method": "\u9996\u5148\u4f7f\u7528\u7b80\u5355\u7684\u989c\u8272\u901a\u9053\u5206\u6790\u8fdb\u884c\u7c97\u7565\u7684\u571f\u5730-\u6c34\u4f53\u5206\u5272\uff0c\u7136\u540e\u5fae\u8c03SAM\u7684\u63a9\u7801\u89e3\u7801\u5668\u6765\u8bc6\u522b\u6cb3\u5cb8\u4fb5\u8680\u7684\u7ec6\u5fae\u7279\u5f81\u3002\u6784\u5efa\u4e86\u5305\u542b2003-2025\u5e74\u5b5f\u52a0\u62c9\u56fd\u6700\u8106\u5f31\u5730\u533a\u5386\u53f2Google Earth\u5f71\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u5305\u542b\u6d88\u5931\u5b9a\u5c45\u70b9\u624b\u52a8\u6807\u6ce8\u7684\u6570\u636e\u96c6\u3002", "result": "\u5f00\u53d1\u7684\u6a21\u578b\u5728\u8bc6\u522b\u6cb3\u6d41\u4fb5\u8680\u7834\u574f\u8fc7\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4\u8fbe\u523086.30%\uff0cDice\u5206\u6570\u8fbe\u523092.60%\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6210\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a\u5b5f\u52a0\u62c9\u56fd\u56e0\u6cb3\u6d41\u4fb5\u8680\u800c\u6d88\u5931\u5b9a\u5c45\u70b9\u7684\u9996\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\uff1b\u4e13\u95e8\u4e3a\u6b64\u5173\u952e\u4efb\u52a1\u5fae\u8c03\u7684AI\u6a21\u578b\uff1b\u4ee5\u53ca\u901a\u8fc7\u5f15\u4eba\u6ce8\u76ee\u7684\u89c6\u89c9\u8bc1\u636e\u91cf\u5316\u571f\u5730\u635f\u5931\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u5de5\u5177\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u707e\u5bb3\u7ba1\u7406\u673a\u6784\u76d1\u63a7\u4fb5\u8680\u3001\u9884\u6d4b\u5176\u8f68\u8ff9\u5e76\u4fdd\u62a4\u8106\u5f31\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.17199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17199", "abs": "https://arxiv.org/abs/2510.17199", "authors": ["Nirai Hayakawa", "Kazumasa Shimari", "Kazuma Yamasaki", "Hirotatsu Hoshikawa", "Rikuto Tsuchida", "Kenichi Matsumoto"], "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis", "comment": "Accepted to IEEE 2025 Conference on Games", "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.", "AI": {"tldr": "\u57fa\u4e8eTimeSformer\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790VALORANT\u6bd4\u8d5b\u5f55\u50cf\u4e2d\u7684\u5c0f\u5730\u56fe\u4fe1\u606f\uff0c\u63d0\u53d6\u6218\u672f\u7279\u5f81\u6765\u9884\u6d4b\u56de\u5408\u7ed3\u679c\uff0c\u5728\u589e\u5f3a\u6218\u672f\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ea681%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7535\u7ade\u6bd4\u8d5b\u7ed3\u679c\u9884\u6d4b\u7814\u7a76\u591a\u57fa\u4e8e\u6bd4\u8d5b\u65e5\u5fd7\u6570\u636e\u548c\u7edf\u8ba1\u4fe1\u606f\uff0c\u672c\u7814\u7a76\u9488\u5bf9\u9700\u8981\u590d\u6742\u7b56\u7565\u7684FPS\u6e38\u620fVALORANT\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u6bd4\u8d5b\u5f55\u50cf\u4e2d\u7684\u5c0f\u5730\u56fe\u4fe1\u606f\u6765\u6784\u5efa\u56de\u5408\u7ed3\u679c\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u57fa\u4e8eTimeSformer\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u4ece\u5c0f\u5730\u56fe\u4fe1\u606f\u4e2d\u63d0\u53d6\u8be6\u7ec6\u7684\u6218\u672f\u7279\u5f81\uff0c\u5305\u62ec\u89d2\u8272\u4f4d\u7f6e\u4fe1\u606f\u548c\u5176\u4ed6\u6e38\u620f\u5185\u4e8b\u4ef6\uff0c\u6784\u5efa\u589e\u5f3a\u6218\u672f\u6807\u7b7e\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u589e\u5f3a\u6218\u672f\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fbe\u5230\u7ea681%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5728\u56de\u5408\u4e2d\u671f\u9636\u6bb5\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u5c0f\u5730\u56fe\u4fe1\u606f\u672c\u8eab\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5229\u7528\u6bd4\u8d5b\u5f55\u50cf\u4e2d\u7684\u6218\u672f\u7279\u5f81\u5bf9\u4e8e\u9884\u6d4bVALORANT\u56de\u5408\u7ed3\u679c\u975e\u5e38\u6709\u6548\uff0c\u8bc1\u660e\u4e86\u6218\u672f\u5206\u6790\u5728\u7535\u7ade\u9884\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17200", "abs": "https://arxiv.org/abs/2510.17200", "authors": ["Bingrong Liu", "Jun Shi", "Yushan Zheng"], "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification", "comment": null, "summary": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment.", "AI": {"tldr": "EndoCIL\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5185\u7aa5\u955c\u56fe\u50cf\u8bca\u65ad\u8bbe\u8ba1\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u89e3\u51b3\u9886\u57df\u5dee\u5f02\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u9700\u8981\u6a21\u578b\u80fd\u591f\u6301\u7eed\u9002\u5e94\u65b0\u7684\u4e34\u5e8a\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5df2\u5b66\u77e5\u8bc6\u7684\u6027\u80fd\u3002\u73b0\u6709\u91cd\u653e\u65b9\u6cd5\u7531\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u4e2d\u4e25\u91cd\u7684\u9886\u57df\u5dee\u5f02\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u65e0\u6cd5\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "EndoCIL\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u7684\u91cd\u653e(MDBR)\u9009\u62e9\u591a\u6837\u4ee3\u8868\u6027\u6837\u672c\uff0c\u5148\u9a8c\u6b63\u5219\u5316\u7c7b\u522b\u5e73\u8861\u635f\u5931(PRCBL)\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5168\u8fde\u63a5\u68af\u5ea6\u6821\u51c6(CFG)\u8c03\u6574\u5206\u7c7b\u5668\u68af\u5ea6\u51cf\u5c11\u65b0\u7c7b\u504f\u5dee\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEndoCIL\u5728\u4e0d\u540c\u7f13\u51b2\u533a\u5927\u5c0f\u548c\u8bc4\u4f30\u6307\u6807\u4e0b\u666e\u904d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684CIL\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u7ec8\u8eab\u5185\u7aa5\u955c\u8bca\u65ad\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u4e34\u5e8a\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2510.17201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17201", "abs": "https://arxiv.org/abs/2510.17201", "authors": ["Mika Feng", "Pierre Gallin-Martel", "Koichi Ito", "Takafumi Aoki"], "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing", "comment": "ICCV 2025 Workshop FAS", "summary": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDINOv2\u7684\u6d3b\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528DINOv2\u548c\u5bc4\u5b58\u5668\u63d0\u53d6\u53ef\u6cdb\u5316\u7279\u5f81\uff0c\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u6270\u52a8\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u771f\u5b9e\u4eba\u8138\u548c\u4f2a\u9020\u4eba\u8138\u4e4b\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u867d\u7136\u5bf9\u59ff\u6001\u3001\u5149\u7167\u548c\u6a21\u7cca\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u6076\u610f\u653b\u51fb\u8005\u53ef\u80fd\u4f7f\u7528\u6ce8\u518c\u7528\u6237\u7684\u7167\u7247\u7ed5\u8fc7\u8ba4\u8bc1\u3002\u9700\u8981\u5728\u4eba\u8138\u8bc6\u522b\u524d\u68c0\u6d4b\u8fd9\u79cd\u6b3a\u9a97\u653b\u51fb\u3002", "method": "\u4f7f\u7528\u5e26\u6709\u5bc4\u5b58\u5668\u7684DINOv2\u6a21\u578b\u63d0\u53d6\u53ef\u6cdb\u5316\u7279\u5f81\uff0c\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u6270\u52a8\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e13\u6ce8\u4e8e\u5173\u952e\u7ec6\u5fae\u7279\u5f81\u6765\u533a\u5206\u771f\u5b9e\u548c\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u3002", "result": "\u5728The 6th Face Anti-Spoofing Workshop\u6570\u636e\u96c6\u548cSiW\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8eDINOv2\u7684\u6b3a\u9a97\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u771f\u5b9e\u548c\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u4e4b\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.17218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17218", "abs": "https://arxiv.org/abs/2510.17218", "authors": ["Zhuo Cao", "Heming Du", "Bingqing Zhang", "Xin Yu", "Xue Li", "Sen Wang"], "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions", "comment": "Accepted to NeurIPS 2025", "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u65f6\u523b\u68c0\u7d22\u4efb\u52a1\u548cQV-M\u00b2\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86FlashMMR\u6846\u67b6\u901a\u8fc7\u540e\u9a8c\u8bc1\u6a21\u5757\u6539\u8fdb\u65f6\u523b\u8fb9\u754c\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u5355\u65f6\u523b\u68c0\u7d22\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u4e00\u4e2a\u67e5\u8be2\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2a\u76f8\u5173\u65f6\u523b\uff0c\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u652f\u6301\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u3002", "method": "\u63d0\u51faFlashMMR\u6846\u67b6\uff0c\u5305\u542b\u591a\u65f6\u523b\u540e\u9a8c\u8bc1\u6a21\u5757\uff0c\u901a\u8fc7\u7ea6\u675f\u65f6\u5e8f\u8c03\u6574\u548c\u9a8c\u8bc1\u6a21\u5757\u91cd\u65b0\u8bc4\u4f30\u5019\u9009\u7247\u6bb5\uff0c\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u63d0\u6848\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u65f6\u523b\u5bf9\u9f50\u3002", "result": "\u5728QV-M\u00b2\u6570\u636e\u96c6\u4e0a\uff0cFlashMMR\u76f8\u6bd4\u4e4b\u524dSOTA\u65b9\u6cd5\u5728G-mAP\u3001mAP@3+tgt\u548cmR@3\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53473.00%\u30012.70%\u548c2.56%\u3002", "conclusion": "QV-M\u00b2\u6570\u636e\u96c6\u548cFlashMMR\u65b9\u6cd5\u4e3a\u63a8\u8fdb\u66f4\u73b0\u5b9e\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17264", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17264", "abs": "https://arxiv.org/abs/2510.17264", "authors": ["Akihito Yoshii", "Ryosuke Sonoda", "Ramya Srinivasan"], "title": "Fair and Interpretable Deepfake Detection in Videos", "comment": "10 pages (including References)", "summary": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u516c\u5e73\u611f\u77e5\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u6574\u5408\u65f6\u5e8f\u7279\u5f81\u5b66\u4e60\u548c\u4eba\u53e3\u7edf\u8ba1\u611f\u77e5\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u63d0\u5347\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u4e14\u672a\u80fd\u6355\u6349\u65f6\u5e8f\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u95f4\u7684\u51b3\u7b56\u504f\u89c1\u548c\u4e0d\u53ef\u9760\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u65f6\u5e8f\u5efa\u6a21\u7684\u5e8f\u5217\u805a\u7c7b\u548c\u6982\u5ff5\u63d0\u53d6\uff0c\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u611f\u77e5\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5305\u62ec\u5e73\u8861\u6b20\u4ee3\u8868\u7fa4\u4f53\u548c\u5e94\u7528\u9891\u57df\u53d8\u6362\u4ee5\u4fdd\u7559\u6df1\u5ea6\u4f2a\u9020\u4f2a\u5f71\u3002", "result": "\u5728FaceForensics++\u3001DFD\u3001Celeb-DF\u548cDFDC\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Xception\u548cResNet\u67b6\u6784\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u83b7\u5f97\u6700\u4f73\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17269", "abs": "https://arxiv.org/abs/2510.17269", "authors": ["Luis Wiedmann", "Orr Zohar", "Amir Mahla", "Xiaohan Wang", "Rui Li", "Thibaud Frere", "Leandro von Werra", "Aritra Roy Gosthipaty", "Andr\u00e9s Marafioti"], "title": "FineVision: Open Data Is All You Need", "comment": null, "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.", "AI": {"tldr": "FineVision\u662f\u4e00\u4e2a\u7cbe\u5fc3\u6536\u96c6\u548c\u6574\u7406\u76842400\u4e07\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u7edf\u4e00\u4e86200\u591a\u4e2a\u6570\u636e\u6e90\uff0c\u5e76\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u53bb\u91cd\u548c\u53bb\u6c61\u67d3\u5904\u7406\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u53d7\u5230\u6570\u636e\u96c6\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u6c61\u67d3\u95ee\u9898\u7684\u963b\u788d\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u7edf\u4e00\u6570\u636e\u96c6\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u5316\u3001\u4eba\u673a\u534f\u540c\u7684\u6d41\u7a0b\uff1a\u81ea\u52a8\u5316\u8fdb\u884c\u6279\u91cf\u6444\u5165\u548c\u6a21\u5f0f\u6620\u5c04\uff0c\u4eba\u5de5\u5ba1\u6838\u5458\u68c0\u67e5\u6620\u5c04\u8d28\u91cf\u3001\u6837\u672c\u591a\u6837\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u8fdb\u884c\u4e25\u683c\u7684\u53bb\u91cd\u548c\u53bb\u6c61\u67d3\u5904\u7406\u3002", "result": "\u5728FineVision\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u5f00\u653e\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u89c4\u6a21\u3001\u6570\u636e\u536b\u751f\u548c\u4eba\u673a\u534f\u540c\u5e73\u8861\u7684\u4f18\u52bf\u3002", "conclusion": "FineVision\u6570\u636e\u96c6\u548c\u6574\u7406\u5de5\u5177\u5c06\u52a0\u901f\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u6536\u96c6\u548c\u6574\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17274", "abs": "https://arxiv.org/abs/2510.17274", "authors": ["Katie Luo", "Jingwei Ji", "Tong He", "Runsheng Xu", "Yichen Xie", "Dragomir Anguelov", "Mingxing Tan"], "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models", "comment": "In proceedings of IROS 2025", "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.", "AI": {"tldr": "\u63d0\u51faPlug-and-Forecast (PnF)\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u73b0\u6709\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u590d\u6742\u573a\u666f\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u7406\u6539\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u96be\u4ee5\u7ecf\u6d4e\u9ad8\u6548\u5730\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u73b0\u5b9e\u573a\u666f\u3002", "method": "\u8bbe\u8ba1\u63d0\u793a\u8bcd\u4eceMLLMs\u63d0\u53d6\u7ed3\u6784\u5316\u573a\u666f\u7406\u89e3\uff0c\u5c06\u5176\u84b8\u998f\u4e3a\u53ef\u5b66\u4e60\u5d4c\u5165\u6765\u589e\u5f3a\u73b0\u6709\u884c\u4e3a\u9884\u6d4b\u6a21\u578b\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728Waymo Open Motion Dataset\u548cnuScenes Dataset\u4e0a\u9a8c\u8bc1\uff0c\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u5747\u83b7\u5f97\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PnF\u65b9\u6cd5\u901a\u8fc7MLLMs\u7684\u81ea\u7136\u8bed\u8a00\u80fd\u529b\u6709\u6548\u63d0\u5347\u8fd0\u52a8\u9884\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.17278", "categories": ["cs.CV", "68T07, 92C55", "I.4.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17278", "abs": "https://arxiv.org/abs/2510.17278", "authors": ["Mehdi Zekriyapanah Gashti", "Mostafa Mohammadpour", "Ghasem Farjamnia"], "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation", "comment": null, "summary": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows.", "AI": {"tldr": "\u63d0\u51faSG-CLDFF\u6846\u67b6\uff0c\u7ed3\u5408\u663e\u8457\u6027\u5f15\u5bfc\u9884\u5904\u7406\u4e0e\u591a\u5c3a\u5ea6\u6df1\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u63d0\u5347\u767d\u7ec6\u80de\u5206\u5272\u548c\u5206\u7c7b\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u767d\u7ec6\u80de\u663e\u5fae\u56fe\u50cf\u5206\u6790\u5bf9\u8840\u6db2\u75be\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u67d3\u8272\u53d8\u5f02\u3001\u590d\u6742\u80cc\u666f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218", "method": "\u4f7f\u7528\u663e\u8457\u6027\u5148\u9a8c\u7a81\u51fa\u767d\u7ec6\u80de\u533a\u57df\uff0c\u91c7\u7528EfficientSwin\u98ce\u683c\u6df7\u5408\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u591a\u5206\u8fa8\u7387\u7279\u5f81\uff0c\u901a\u8fc7ResNeXt-CC\u542f\u53d1\u7684\u8de8\u5c42\u878d\u5408\u6a21\u5757\u6574\u5408\u6d45\u5c42\u548c\u6df1\u5c42\u7279\u5f81\uff0c\u591a\u4efb\u52a1\u8bad\u7ec3\u7ed3\u5408\u5206\u5272\u548c\u5206\u7c7b\u5934", "result": "\u5728BCCD\u3001LISC\u3001ALL-IDB\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIoU\u3001F1\u5206\u6570\u548c\u5206\u7c7b\u51c6\u786e\u7387\u5747\u4f18\u4e8eCNN\u548cTransformer\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "SG-CLDFF\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u767d\u7ec6\u80de\u81ea\u52a8\u5206\u6790\u8def\u5f84"}}
{"id": "2510.17287", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17287", "abs": "https://arxiv.org/abs/2510.17287", "authors": ["Amir Gharghabi", "Mahdi Hakiminezhad", "Maryam Shafaei", "Shaghayegh Gharghabi"], "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation", "comment": null, "summary": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eYOLOv11\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u7684\u81ea\u52a8\u624b\u672f\u7167\u660e\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bc6\u522b\u84dd\u8272\u6807\u8bb0\u81ea\u52a8\u8c03\u6574LED\u5149\u6e90\u4f4d\u7f6e\uff0c\u51cf\u5c11\u5916\u79d1\u533b\u751f\u75b2\u52b3\u5e76\u63d0\u9ad8\u7167\u660e\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u7167\u660e\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\uff0c\u5bfc\u81f4\u5916\u79d1\u533b\u751f\u75b2\u52b3\u3001\u9888\u90e8\u52b3\u635f\u4ee5\u53ca\u56e0\u6f02\u79fb\u548c\u9634\u5f71\u9020\u6210\u7684\u4e0d\u4e00\u81f4\u7167\u660e\u95ee\u9898\u3002", "method": "\u4f7f\u7528YOLOv11\u7b97\u6cd5\u68c0\u6d4b\u624b\u672f\u533a\u57df\u4e0a\u65b9\u7684\u84dd\u8272\u7403\u5f62\u6807\u8bb0\uff0c\u901a\u8fc7\u4e24\u4e2a\u4f3a\u670d\u7535\u673a\u63a7\u5236LED\u5149\u6e90\u7684\u503e\u659c\u548c\u65cb\u8f6c\uff0c\u5b9e\u73b0\u81ea\u52a8\u5b9a\u4f4d\u7167\u660e\u3002", "result": "YOLO\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523096.7% mAP@50\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u96c6\u5305\u542b\u6a21\u62df\u624b\u672f\u573a\u666f\u7684\u6807\u6ce8\u56fe\u50cf\u3002", "conclusion": "\u8be5\u673a\u5668\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u81ea\u52a8\u5316\u4e86\u7167\u660e\u8fc7\u7a0b\uff0c\u51cf\u8f7b\u4e86\u5916\u79d1\u533b\u751f\u7684\u8eab\u4f53\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e86\u7167\u660e\u4e00\u81f4\u6027\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u624b\u672f\u6548\u679c\u3002"}}
{"id": "2510.17299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17299", "abs": "https://arxiv.org/abs/2510.17299", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning", "comment": "Accepted by NeurIPS 2025", "summary": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u53cd\u76f4\u89c9\u73b0\u8c61\uff1a\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u4f1a\u635f\u5bb3\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\uff0c\u79f0\u4e3a\u81ea\u76d1\u7763\u5bc6\u96c6\u9000\u5316(SDD)\u3002\u4f5c\u8005\u63d0\u51fa\u4e86DSE\u6307\u6807\u6765\u65e0\u76d1\u7763\u8bc4\u4f30\u5bc6\u96c6\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u6a21\u578b\u9009\u62e9\u7b56\u7565\u548c\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "motivation": "\u89c2\u5bdf\u5230\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u53cd\u800c\u4f1a\u635f\u5bb3\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u6027\u80fd\u7684\u53cd\u76f4\u89c9\u73b0\u8c61\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u8bc4\u4f30\u5bc6\u96c6\u8868\u793a\u8d28\u91cf\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86Dense representation Structure Estimator (DSE)\uff0c\u5305\u542b\u7c7b\u522b\u76f8\u5173\u6027\u5ea6\u91cf\u548c\u6709\u6548\u7ef4\u5ea6\u5ea6\u91cf\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u6a21\u578b\u9009\u62e9\u7b56\u7565\u548cDSE\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u572816\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\u548c4\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u9009\u62e9\u7b56\u7565\u5e73\u5747\u63d0\u5347mIoU 3.0%\uff0cDSE\u6b63\u5219\u5316\u80fd\u6301\u7eed\u7f13\u89e3\u5bc6\u96c6\u9000\u5316\u6548\u5e94\u3002", "conclusion": "DSE\u6307\u6807\u80fd\u6709\u6548\u8bc4\u4f30\u5bc6\u96c6\u8868\u793a\u8d28\u91cf\uff0c\u57fa\u4e8eDSE\u7684\u6a21\u578b\u9009\u62e9\u548c\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u663e\u8457\u6539\u5584\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.17305", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17305", "abs": "https://arxiv.org/abs/2510.17305", "authors": ["ZhaoYang Han", "Qihan Lin", "Hao Liang", "Bowen Chen", "Zhou Liu", "Wentao Zhang"], "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding", "comment": "Submitted to ARR Rolling Review", "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.", "AI": {"tldr": "LongInsightBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6574\u5408\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u591a\u6a21\u6001\uff0c\u5305\u542b\u7ea61000\u4e2a\u4fe1\u606f\u5bc6\u96c6\u7684\u957f\u89c6\u9891\u548c6\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u573a\u666f\uff0c\u63ed\u793a\u4e86\u5168\u6a21\u6001\u6a21\u578b\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u7a0b\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u9700\u8981\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff08\u89c6\u89c9\u3001\u97f3\u9891\u3001\u6587\u672c\uff09\u6765\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u3001\u89c2\u70b9\u3001\u884c\u4e3a\u7b49\u4e0a\u4e0b\u6587\u5143\u7d20\u7684\u4efb\u52a1\u3002", "method": "\u4eceFineVideo\u6570\u636e\u96c6\u4e2d\u7cbe\u5fc3\u6311\u9009\u7ea61000\u4e2a\u957f\u65f6\u957f\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u89c6\u9891\uff08\u5982\u8bb2\u5ea7\u3001\u8bbf\u8c08\u3001vlog\uff09\uff0c\u8bbe\u8ba16\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u573a\u666f\uff08\u5305\u62ec\u4e8b\u4ef6\u5185\u548c\u4e8b\u4ef6\u95f4\u4efb\u52a1\uff09\uff0c\u5e76\u5f00\u53d1\u4e09\u6b65\u534a\u81ea\u52a8\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u786e\u4fdd\u95ee\u9898\u96be\u5ea6\u548c\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5168\u6a21\u6001\u6a21\u578b\u5728\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u7a0b\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u6269\u5c55\u5b9e\u9a8c\u63ed\u793a\u4e86OLMs\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5904\u7406\u504f\u5dee\u95ee\u9898\u3002", "conclusion": "LongInsightBench\u4e3a\u8bc4\u4f30\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5168\u6a21\u6001\u6a21\u578b\u5728\u591a\u6a21\u6001\u957f\u89c6\u9891\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2510.17318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17318", "abs": "https://arxiv.org/abs/2510.17318", "authors": ["Sangyoon Bae", "Jiook Cha"], "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference", "comment": null, "summary": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function.", "AI": {"tldr": "CausalMamba\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684fMRI\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u901a\u8fc7BOLD\u53bb\u5377\u79ef\u548c\u6761\u4ef6Mamba\u67b6\u6784\u89e3\u51b3\u795e\u7ecf\u56e0\u679c\u63a8\u65ad\u7684\u9006\u95ee\u9898\uff0c\u5728\u6a21\u62df\u6570\u636e\u4e0a\u6bd4DCM\u51c6\u786e\u7387\u9ad837%\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u80fd\u6062\u590d88%\u7684\u5df2\u77e5\u795e\u7ecf\u901a\u8def\u3002", "motivation": "\u89e3\u51b3fMRI\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u4ece\u8840\u6d41\u52a8\u529b\u5b66\u626d\u66f2\u7684BOLD\u4fe1\u53f7\u63a8\u65ad\u795e\u7ecf\u56e0\u679c\u5173\u7cfb\u7684\u75c5\u6001\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\uff08\u5982DCM\uff09\u7684\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u3002", "method": "\u5c06\u590d\u6742\u9006\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u53ef\u5904\u7406\u9636\u6bb5\uff1aBOLD\u53bb\u5377\u79ef\u6062\u590d\u6f5c\u5728\u795e\u7ecf\u6d3b\u52a8\uff0c\u7136\u540e\u4f7f\u7528\u65b0\u9896\u7684\u6761\u4ef6Mamba\u67b6\u6784\u8fdb\u884c\u56e0\u679c\u56fe\u63a8\u65ad\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e0a\u6bd4DCM\u51c6\u786e\u7387\u9ad837%\uff1b\u5728\u771f\u5b9e\u4efb\u52a1fMRI\u6570\u636e\u4e2d\u6062\u590d88%\u7684\u5df2\u77e5\u795e\u7ecf\u901a\u8def\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u572899%\u4ee5\u4e0a\u53d7\u8bd5\u8005\u4e2d\u65e0\u6cd5\u8bc6\u522b\u8fd9\u4e9b\u5178\u578b\u56de\u8def\uff1b\u5de5\u4f5c\u8bb0\u5fc6\u6570\u636e\u5206\u6790\u663e\u793a\u5927\u8111\u6839\u636e\u523a\u6fc0\u7b56\u7565\u6027\u5730\u8f6c\u79fb\u4e3b\u8981\u56e0\u679c\u67a2\u7ebd\u3002", "conclusion": "\u4e3a\u795e\u7ecf\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u56e0\u679c\u63a8\u65ad\uff0c\u6355\u6349\u8ba4\u77e5\u529f\u80fd\u80cc\u540e\u7684\u57fa\u672c\u56de\u8def\u6a21\u5f0f\u548c\u7075\u6d3b\u7f51\u7edc\u52a8\u6001\u3002"}}
{"id": "2510.17322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17322", "abs": "https://arxiv.org/abs/2510.17322", "authors": ["Wei Zhang", "Zhanhao Hu", "Xiao Li", "Xiaopei Zhu", "Xiaolin Hu"], "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World", "comment": "13 pages, 8 figures", "summary": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u73b0\u6709\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u5728\u9762\u5bf9\u5927\u5c3a\u5bf8\u5bf9\u6297\u8863\u7269\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u5355\u7eaf\u589e\u5927\u8865\u4e01\u5c3a\u5bf8\u5c31\u80fd\u4f7f\u73b0\u6709\u9632\u5fa1\u5931\u6548\uff0c\u5bf9\u6297\u8863\u7269\u5728\u6570\u5b57\u548c\u7269\u7406\u4e16\u754c\u90fd\u80fd\u6709\u6548\u7a81\u7834\u591a\u79cd\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u9a8c\u53d1\u73b0\u7b80\u5355\u589e\u5927\u5bf9\u6297\u8865\u4e01\u5c3a\u5bf8\u5c31\u80fd\u4f7f\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u9632\u5fa1\u65b9\u6cd5\u5728\u9762\u5bf9\u5927\u5c3a\u5bf8\u5bf9\u6297\u8863\u7269\u65f6\u7684\u8868\u73b0\uff0c\u5bf9\u6297\u8863\u7269\u4e0d\u4ec5\u5c3a\u5bf8\u5927\u4e14\u6bd4\u4eba\u4f53\u4e0a\u7684\u5927\u8865\u4e01\u770b\u8d77\u6765\u66f4\u81ea\u7136\u3002", "method": "\u901a\u8fc7\u5236\u4f5c\u5bf9\u6297\u8863\u7269\u6765\u6d4b\u8bd5\u591a\u79cd\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728\u6570\u5b57\u4e16\u754c\u548c\u7269\u7406\u4e16\u754c\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7279\u522b\u9488\u5bf9Faster R-CNN\u5236\u4f5c\u4e86\u80fd\u7a81\u7834\u591a\u79cd\u9632\u5fa1\u7684\u5355\u5957\u8863\u7269\u3002", "result": "\u6240\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u9762\u5bf9\u5bf9\u6297\u8863\u7269\u65f6\u8868\u73b0\u90fd\u5f88\u5dee\uff0c\u5236\u4f5c\u7684\u5355\u5957\u8863\u7269\u5728\u7269\u7406\u4e16\u754c\u5bf9\u672a\u9632\u5fa1\u68c0\u6d4b\u5668\u7684\u653b\u51fb\u6210\u529f\u7387\u8fbe96.06%\uff0c\u5bf9\u4e5d\u4e2a\u9632\u5fa1\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u5747\u8d85\u8fc764.84%\u3002", "conclusion": "\u73b0\u6709\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u5728\u9762\u5bf9\u5bf9\u6297\u8863\u7269\u65f6\u5b58\u5728\u5171\u540c\u8106\u5f31\u6027\uff0c\u5927\u5c3a\u5bf8\u7684\u5bf9\u6297\u8863\u7269\u80fd\u6709\u6548\u7a81\u7834\u591a\u79cd\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2510.17330", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17330", "abs": "https://arxiv.org/abs/2510.17330", "authors": ["Gyuhwan Park", "Kihyun Na", "Injung Kim"], "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration", "comment": "11 pages, 6 figures", "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.", "AI": {"tldr": "\u63d0\u51faCharDiff\u6846\u67b6\uff0c\u4f7f\u7528\u5b57\u7b26\u7ea7\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u6765\u6062\u590d\u548c\u8bc6\u522b\u4e25\u91cd\u9000\u5316\u7684\u8f66\u724c\u56fe\u50cf\uff0c\u5728\u6062\u590d\u8d28\u91cf\u548c\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8f66\u724c\u56fe\u50cf\u6062\u590d\u4e0d\u4ec5\u5bf9\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\u9884\u5904\u7406\u5f88\u91cd\u8981\uff0c\u8fd8\u80fd\u63d0\u9ad8\u8bc1\u636e\u4ef7\u503c\u3001\u589e\u5f3a\u89c6\u89c9\u754c\u9762\u6e05\u6670\u5ea6\uff0c\u4fc3\u8fdb\u8f66\u724c\u56fe\u50cf\u7684\u8fdb\u4e00\u6b65\u5229\u7528\u3002", "method": "CharDiff\u5229\u7528\u5916\u90e8\u5206\u5272\u548cOCR\u6a21\u5757\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5b57\u7b26\u7ea7\u5148\u9a8c\uff0c\u901a\u8fc7\u65b0\u9896\u7684CHARM\u6a21\u5757\u5b9e\u73b0\u5b57\u7b26\u5f15\u5bfc\u6ce8\u610f\u529b\uff0c\u786e\u4fdd\u6bcf\u4e2a\u5b57\u7b26\u7684\u5f15\u5bfc\u4ec5\u9650\u4e8e\u81ea\u8eab\u533a\u57df\u3002", "result": "\u5728Roboflow-LP\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6027\u80fd\u6700\u4f73\u7684\u57fa\u7ebf\u6a21\u578b\uff0cCharDiff\u5b9e\u73b0\u4e8628%\u7684\u76f8\u5bf9CER\u964d\u4f4e\uff0c\u5728\u6062\u590d\u8d28\u91cf\u548c\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7ed3\u6784\u5316\u5b57\u7b26\u5f15\u5bfc\u6761\u4ef6\u5316\u6709\u6548\u589e\u5f3a\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u8f66\u724c\u6062\u590d\u548c\u8bc6\u522b\u5728\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17332", "abs": "https://arxiv.org/abs/2510.17332", "authors": ["Zhaoran Zhao", "Xinli Yue", "Jianhui Sun", "Yuhao Xie", "Tao Shao", "Liangchao Yao", "Fan Xia", "Yuetang Deng"], "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA", "comment": "Accepted to ICCV 2025 Workshop", "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.", "AI": {"tldr": "\u63d0\u51fa\u4e86iDETEX\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u6267\u884c\u8d28\u91cf\u5b9a\u4f4d\u3001\u611f\u77e5\u548c\u63cf\u8ff0\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\uff0c\u5728ViDA-UGC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4ece\u6807\u91cf\u8d28\u91cf\u9884\u6d4b\u5411\u66f4\u53ef\u89e3\u91ca\u3001\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bc4\u4f30\u8303\u5f0f\u53d1\u5c55\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u8be6\u7ec6\u4e14\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u79bb\u7ebf\u589e\u5f3a\u6a21\u5757\u548c\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u8f85\u4ee5\u5728\u7ebf\u589e\u5f3a\u7b56\u7565\u5145\u5206\u5229\u7528\u591a\u6e90\u76d1\u7763\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728ViDA-UGC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728ICCV MIPI 2025\u8be6\u7ec6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "iDETEX\u6a21\u578b\u5728\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17338", "abs": "https://arxiv.org/abs/2510.17338", "authors": ["Jiahao Huo", "Mufhumudzi Muthivhi", "Terence L. van Zyl", "Fredrik Gustafsson"], "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition", "comment": null, "summary": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u5f00\u653e\u96c6\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u7279\u5f81\u4e0e\u9884\u6d4blogits\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u6765\u8bc6\u522b\u672a\u77e5\u7c7b\u522b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u91ce\u751f\u52a8\u7269\u5206\u7c7b\u6a21\u578b\u5728\u5c01\u95ed\u4e16\u754c\u8bbe\u7f6e\u4e0b\u8bad\u7ec3\uff0c\u5f53\u9047\u5230\u672a\u77e5\u7c7b\u522b\u65f6\u4f1a\u8fc7\u5ea6\u81ea\u4fe1\u3002\u73b0\u6709OSR\u65b9\u6cd5\u5927\u591a\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u8f93\u5165\u5230\u6700\u8fd1\u7c7b\u5747\u503c\u7684\u8ddd\u79bb\u6784\u5efa\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u5c06\u8be5\u5206\u5e03\u4e0esoftmax\u6982\u7387\u8fdb\u884c\u6bd4\u8f83\uff0c\u8861\u91cfNCM\u548c\u5206\u7c7b\u5934\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u6392\u540d\u524d\u4e09\uff0cAUROC\u5206\u522b\u8fbe\u523093.41\uff08\u975e\u6d32\u52a8\u7269\uff09\u548c95.35\uff08\u745e\u5178\u52a8\u7269\uff09\uff0c\u6027\u80fd\u7a33\u5b9a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u540e\u5904\u7406OSR\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e00\u81f4\u4e14\u4f18\u5f02\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17347", "abs": "https://arxiv.org/abs/2510.17347", "authors": ["Jingqian Wu", "Shengpeng Xu", "Yunbo Jia", "Edmund Y. Lam"], "title": "Exploring The Missing Semantics In Event Modality", "comment": null, "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.", "AI": {"tldr": "Semantic-E2VID\u662f\u4e00\u4e2a\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\uff0c\u5229\u7528SAM\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u6765\u589e\u5f3a\u4e8b\u4ef6\u6570\u636e\u7684\u8bed\u4e49\u4fe1\u606f\u91cd\u5efa\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u53ea\u6355\u6349\u5f3a\u5ea6\u53d8\u5316\uff0c\u5ffd\u7565\u9759\u6001\u7269\u4f53\u548c\u80cc\u666f\uff0c\u5bfc\u81f4\u6355\u83b7\u7684\u4e8b\u4ef6\u6a21\u6001\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u3002\u73b0\u6709E2V\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8bed\u4e49\u4fe1\u606f\u5728\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5c06SAM\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u4e49\u8f6c\u79fb\u5230\u4e8b\u4ef6\u7f16\u7801\u5668\uff1b\u8bbe\u8ba1\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u5757\u6574\u5408\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u7279\u5f81\uff1b\u5f15\u5165\u8bed\u4e49\u611f\u77e5E2V\u76d1\u7763\u5229\u7528SAM\u751f\u6210\u7684\u7c7b\u522b\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u5e27\u8d28\u91cf\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684E2V\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\uff0cSemantic-E2VID\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6a21\u6001\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u4e86\u4e8b\u4ef6\u5230\u89c6\u9891\u7684\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2510.17363", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17363", "abs": "https://arxiv.org/abs/2510.17363", "authors": ["U. V. B. L Udugama", "George Vosselman", "Francesco Nex"], "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). 8 pages, 7 figures", "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.", "AI": {"tldr": "M2H\u662f\u4e00\u4e2a\u7528\u4e8e\u5355\u76ee\u56fe\u50cf\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u652f\u6301\u8bed\u4e49\u5206\u5272\u3001\u6df1\u5ea6\u3001\u8fb9\u7f18\u548c\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u4ea4\u6362\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u5404\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5b9e\u65f6\u7a7a\u95f4\u611f\u77e5\u9700\u8981\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u65e2\u8981\u5229\u7528\u4e92\u8865\u4efb\u52a1\u4fe1\u606f\uff0c\u53c8\u8981\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002\u4f20\u7edf\u72ec\u7acb\u5355\u4efb\u52a1\u6a21\u578b\u6216\u5171\u4eab\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u63d0\u51faWindow-Based Cross-Task Attention Module\u5b9e\u73b0\u7ed3\u6784\u5316\u7279\u5f81\u4ea4\u6362\uff0c\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u7ec6\u8282\uff1b\u57fa\u4e8e\u8f7b\u91cf\u7ea7ViT-based DINOv2\u9aa8\u5e72\u7f51\u7edc\uff0c\u4f18\u5316\u5b9e\u65f6\u90e8\u7f72\u3002", "result": "\u5728NYUDv2\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5728Hypersim\u4e0a\u8d85\u8fc7\u5355\u4efb\u52a1\u6df1\u5ea6\u548c\u8bed\u4e49\u57fa\u7ebf\uff0c\u5728Cityscapes\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u7b14\u8bb0\u672c\u7535\u8111\u786c\u4ef6\u4e0a\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "M2H\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u76843D\u573a\u666f\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5355\u76ee\u7a7a\u95f4\u611f\u77e5\u7cfb\u7edf\u57fa\u7840\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.17364", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17364", "abs": "https://arxiv.org/abs/2510.17364", "authors": ["Vaggelis Dorovatas", "Soroush Seifi", "Gunshi Gupta", "Rahaf Aljundi"], "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs", "comment": "NeurIPS 2025", "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u3001\u9012\u5f52\u5904\u7406\u5386\u53f2\u4ee4\u724c\u548c\u57fa\u4e8e\u5b57\u5e55\u7684\u95ee\u7b54\uff0c\u5b9e\u73b0\u89c6\u9891\u6d41\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u5904\u7406\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6d41\u5f0f\u957f\u89c6\u9891\u65f6\u9762\u4e34\u6548\u7387\u6311\u6218\uff0c\u65e0\u6cd5\u5728\u89c6\u9891\u64ad\u653e\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u54cd\u5e94\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5904\u7406\u65b9\u6cd5\u3002", "method": "1) \u57fa\u4e8eLLM\u6ce8\u610f\u529b\u9009\u62e9\u91cd\u8981\u89c6\u89c9\u4ee4\u724c\uff0c\u4e22\u5f0395%\u4e0d\u91cd\u8981\u4ee4\u724c\uff1b2) \u9012\u5f52\u5904\u7406\u5386\u53f2\u9009\u62e9\u7684\u4ee4\u724c\u4ee5\u4fdd\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\uff1b3) \u4f7f\u7528\u57fa\u4e8e\u5b57\u5e55\u7684\u8f7b\u91cf\u7ea7\u95ee\u7b54\u673a\u5236", "result": "\u5728\u6d41\u5f0f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u9891\u6d41\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u4e0e\u6807\u51c6Video-LLMs\u517c\u5bb9\u3002"}}
{"id": "2510.17372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17372", "abs": "https://arxiv.org/abs/2510.17372", "authors": ["Pawe\u0142 Borsukiewicz", "Fadi Boutros", "Iyiola E. Olatunji", "Charles Beumier", "Wendk\u00fbuni C. Ouedraogo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise", "comment": null, "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f3025\u4e2a\u5408\u6210\u4eba\u8138\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u5408\u6210\u4eba\u8138\u6570\u636e\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u80fd\u591f\u8fbe\u5230\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u76f8\u5f53\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u6700\u4f73\u5408\u6210\u6570\u636e\u96c6\u6027\u80fd\u751a\u81f3\u8d85\u8d8a\u67d0\u4e9b\u771f\u5b9e\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u4f7f\u7528\u771f\u5b9e\u4eba\u8138\u6570\u636e\u5e26\u6765\u7684\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\uff0c\u540c\u65f6\u586b\u8865\u5408\u6210\u4eba\u8138\u6570\u636e\u7f3a\u4e4f\u5168\u9762\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u8bc6\u522b25\u4e2a\u5408\u6210\u4eba\u8138\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e03\u4e2a\u5173\u952e\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\uff0c\u6d89\u53ca\u8d85\u8fc71000\u4e07\u5408\u6210\u6837\u672c\u7684\u6d4b\u8bd5\u3002", "result": "\u6700\u4f73\u5408\u6210\u6570\u636e\u96c6VariFace\u548cVIGFace\u5206\u522b\u8fbe\u523095.67%\u548c94.91%\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u8d85\u8d8aCASIA-WebFace\u7b49\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u540c\u65f6\u80fd\u6709\u6548\u63a7\u5236\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u3002", "conclusion": "\u5408\u6210\u4eba\u8138\u6570\u636e\u662f\u79d1\u5b66\u53ef\u884c\u4e14\u4f26\u7406\u5fc5\u8981\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u4eba\u8138\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2510.17373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17373", "abs": "https://arxiv.org/abs/2510.17373", "authors": ["Yintao Zhou", "Wei Huang", "Zhengyu Li", "Jing Huang", "Meng Pang"], "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing", "comment": "3 pages, 2 figures, accepted by MIND 2025", "summary": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u9762\u90e8\u8868\u60c5\u7279\u5f81\u7684\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9762\u90e8\u8868\u60c5\u7684PD\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u8868\u60c5\u7c7b\u578b\u6613\u5bfc\u81f4\u8bef\u8bca\uff0c\u5ffd\u89c6\u4e0d\u540cPD\u9636\u6bb5\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e14\u5927\u591a\u53ea\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u800c\u975e\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u3002", "method": "\u96c6\u6210\u591a\u79cd\u9762\u90e8\u8868\u60c5\u7279\u5f81\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u7279\u5f81\u878d\u5408\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u6839\u636e\u7c7b\u522b\u5206\u5e03\u548c\u5206\u7c7b\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6837\u672c\u8d21\u732e\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728PD\u4e25\u91cd\u7a0b\u5ea6\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6ce8\u610f\u529b\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u5747\u6709\u6548\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bca\u65adPD\u4e25\u91cd\u7a0b\u5ea6\uff0c\u591a\u8868\u60c5\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u7c7b\u522b\u5e73\u8861\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u3002"}}
{"id": "2510.17384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17384", "abs": "https://arxiv.org/abs/2510.17384", "authors": ["Jiajin Tang", "Zhengxuan Wei", "Ge Zheng", "Sibei Yang"], "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding", "comment": "Accepted at ICCV 2025", "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.", "AI": {"tldr": "LoopTrans\u662f\u4e00\u4e2a\u95ed\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb\u5728\u81ea\u6211\u4e2d\u5fc3\u548c\u4ed6\u8005\u4e2d\u5fc3\u89c6\u89d2\u95f4\u8fdb\u884c\u4ea4\u4e92\u5f0f\u5b66\u4e60\uff0c\u63d0\u5347\u5f31\u76d1\u7763\u529f\u80fd\u57fa\u7840\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4ece\u4ed6\u8005\u4e2d\u5fc3\u56fe\u50cf\u5355\u5411\u8f6c\u79fb\u5230\u81ea\u6211\u4e2d\u5fc3\u56fe\u50cf\uff0c\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u9700\u8981\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb\u6765\u589e\u5f3a\u529f\u80fd\u57fa\u7840\u80fd\u529b", "method": "\u63d0\u51fa\u95ed\u73af\u6846\u67b6LoopTrans\uff0c\u5305\u542b\u7edf\u4e00\u8de8\u6a21\u6001\u5b9a\u4f4d\u548c\u53bb\u566a\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u5b9e\u73b0\u81ea\u6211\u4e2d\u5fc3\u548c\u4ed6\u8005\u4e2d\u5fc3\u89c6\u89d2\u95f4\u7684\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6240\u6709\u6307\u6807\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5373\u4f7f\u5728\u4eba\u4f53\u5b8c\u5168\u906e\u6321\u7269\u4f53\u4ea4\u4e92\u533a\u57df\u7684\u6311\u6218\u6027\u573a\u666f\u4e0b\u4e5f\u80fd\u5904\u7406", "conclusion": "LoopTrans\u901a\u8fc7\u95ed\u73af\u53cc\u5411\u77e5\u8bc6\u8f6c\u79fb\u6709\u6548\u5f25\u5408\u4e86\u89c6\u89d2\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u529f\u80fd\u57fa\u7840\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2510.17409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17409", "abs": "https://arxiv.org/abs/2510.17409", "authors": ["Dmitrii Galimzianov", "Viacheslav Vyshegorodtsev", "Ivan Nezhivykh"], "title": "Monitoring Horses in Stalls: From Object to Event Detection", "comment": "12 pages, 4 figures, 4 tables", "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u9a6c\u53a9\u76d1\u63a7\u7cfb\u7edf\uff0c\u4f7f\u7528YOLOv11\u548cBoT-SORT\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u8ddf\u8e2a\u9a6c\u5339\u4e0e\u4eba\u5458\uff0c\u901a\u8fc7\u7269\u4f53\u8f68\u8ff9\u548c\u7a7a\u95f4\u5173\u7cfb\u63a8\u65ad\u4e8b\u4ef6\u72b6\u6001\u3002", "motivation": "\u4f20\u7edf\u9a6c\u5339\u884c\u4e3a\u76d1\u6d4b\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u65e9\u671f\u53d1\u73b0\u5065\u5eb7\u4e0e\u798f\u5229\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u6280\u672f\uff0c\u5229\u7528CLIP\u548cGroundingDINO\u6784\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u7269\u4f53\u8f68\u8ff9\u548c\u7a7a\u95f4\u5173\u7cfb\u63a8\u65ad\u4e8b\u4ef6\u72b6\u6001\u3002", "result": "\u7cfb\u7edf\u80fd\u533a\u5206\u4e94\u79cd\u4e8b\u4ef6\u7c7b\u578b\u5e76\u8003\u8651\u76f8\u673a\u76f2\u533a\uff0c\u5bf9\u9a6c\u76f8\u5173\u4e8b\u4ef6\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u4eba\u5458\u68c0\u6d4b\u56e0\u6570\u636e\u7a00\u7f3a\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u4e3a\u9a6c\u573a\u5b9e\u65f6\u884c\u4e3a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5bf9\u52a8\u7269\u798f\u5229\u548c\u53a9\u820d\u7ba1\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.17422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17422", "abs": "https://arxiv.org/abs/2510.17422", "authors": ["Shaharyar Ahmed Khan Tareen", "Filza Khan Tareen"], "title": "DeepDetect: Learning All-in-One Dense Keypoints", "comment": "6 pages, 6 figures, 2 tables, 7 equations", "summary": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches).", "AI": {"tldr": "DeepDetect\u662f\u4e00\u4e2a\u667a\u80fd\u3001\u5168\u529f\u80fd\u7684\u5bc6\u96c6\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u878d\u5408\u4f20\u7edf\u68c0\u6d4b\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u4f18\u52bf\uff0c\u5728\u5173\u952e\u70b9\u5bc6\u5ea6\u3001\u91cd\u590d\u6027\u548c\u5339\u914d\u6570\u91cf\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff08SIFT\u3001SURF\u7b49\uff09\u548c\u5b66\u4e60\u65b9\u6cd5\uff08SuperPoint\u3001R2D2\u7b49\uff09\u5b58\u5728\u5bf9\u5149\u5ea6\u53d8\u5316\u654f\u611f\u3001\u5173\u952e\u70b9\u5bc6\u5ea6\u4f4e\u3001\u91cd\u590d\u6027\u5dee\u3001\u9002\u5e94\u6027\u6709\u9650\u4ee5\u53ca\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u7b49\u95ee\u9898\u3002", "method": "\u9996\u5148\u878d\u54087\u4e2a\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u548c2\u4e2a\u8fb9\u7f18\u68c0\u6d4b\u5668\u7684\u8f93\u51fa\u521b\u5efa\u771f\u5b9e\u63a9\u7801\uff0c\u63d0\u53d6\u591a\u6837\u89c6\u89c9\u7279\u5f81\uff1b\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7ESPNet\u6a21\u578b\u8bad\u7ec3\uff0c\u4f7fDeepDetect\u80fd\u591f\u8bed\u4e49\u805a\u7126\u56fe\u50cf\u5e76\u4ea7\u751f\u5bc6\u96c6\u5173\u952e\u70b9\u3002", "result": "\u5728Oxford Affine Covariant Regions\u6570\u636e\u96c6\u4e0a\uff0cDeepDetect\u5728\u5173\u952e\u70b9\u5bc6\u5ea6\uff080.5143\uff09\u3001\u91cd\u590d\u6027\uff080.9582\uff09\u548c\u6b63\u786e\u5339\u914d\u6570\uff0859,003\uff09\u65b9\u9762\u8fbe\u5230\u6700\u5927\u503c\uff0c\u8d85\u8d8a\u5176\u4ed6\u68c0\u6d4b\u5668\u3002", "conclusion": "DeepDetect\u901a\u8fc7\u878d\u5408\u4f20\u7edf\u68c0\u6d4b\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5728\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u5177\u6709\u9ad8\u5bc6\u5ea6\u3001\u9ad8\u91cd\u590d\u6027\u548c\u5f3a\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u3002"}}
{"id": "2510.17434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17434", "abs": "https://arxiv.org/abs/2510.17434", "authors": ["Julien Zouein", "Hossein Javidnia", "Fran\u00e7ois Piti\u00e9", "Anil Kokaram"], "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching", "comment": "Accepted ICIR 2025, camera-ready version", "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.", "AI": {"tldr": "\u5229\u7528AV1\u8fd0\u52a8\u5411\u91cf\u751f\u6210\u5bc6\u96c6\u4e9a\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\u548c\u7ecf\u8fc7\u4f59\u5f26\u4e00\u81f4\u6027\u8fc7\u6ee4\u7684\u77ed\u8f68\u8ff9\uff0c\u5728\u538b\u7f29\u57df\u524d\u7aef\u5b9e\u73b0\u4e0eSIFT\u76f8\u5f53\u7684\u6027\u80fd\u4f46CPU\u4f7f\u7528\u66f4\u5c11\uff0c\u5e76\u4ea7\u751f\u66f4\u5bc6\u96c6\u7684\u5339\u914d\u70b9\u3002", "motivation": "\u63a2\u7d22\u538b\u7f29\u57df\u5bf9\u5e94\u5173\u7cfb\u4f5c\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u524d\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5b8c\u6574\u89c6\u89c9\u7ba1\u9053\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "method": "\u91cd\u65b0\u5229\u7528AV1\u8fd0\u52a8\u5411\u91cf\u751f\u6210\u5bc6\u96c6\u4e9a\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u4e00\u81f4\u6027\u8fc7\u6ee4\u77ed\u8f68\u8ff9\u3002", "result": "\u5728117\u5e27\u89c6\u9891\u4e0a\uff0c\u8fd0\u52a8\u5411\u91cf\u5339\u914d\u6210\u529f\u6ce8\u518c\u6240\u6709\u56fe\u50cf\u5e76\u91cd\u5efa46-62\u4e07\u4e2a\u70b9\uff0c\u91cd\u6295\u5f71\u8bef\u5dee\u4e3a0.51-0.53\u50cf\u7d20\uff1bBA\u65f6\u95f4\u968f\u5339\u914d\u5bc6\u5ea6\u589e\u957f\u3002", "conclusion": "\u538b\u7f29\u57df\u5bf9\u5e94\u5173\u7cfb\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u524d\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5728\u5b8c\u6574\u7ba1\u9053\u4e2d\u6269\u5c55\u7684\u6e05\u6670\u8def\u5f84\u3002"}}
{"id": "2510.17440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17440", "abs": "https://arxiv.org/abs/2510.17440", "authors": ["Qiyuan Guan", "Xiang Chen", "Guiyue Jin", "Jiyu Jin", "Shumin Fan", "Tianyu Song", "Jinshan Pan"], "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation", "comment": "Accepted by NeurIPS 2025", "summary": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u591c\u95f4\u56fe\u50cf\u53bb\u96e8\u57fa\u51c6\u6570\u636e\u96c6HQ-NightRain\u548c\u6709\u6548\u7684\u989c\u8272\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edcCST-Net\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u989c\u8272\u7a7a\u95f4\u8f6c\u6362\u5668\u548c\u9690\u5f0f\u5149\u7167\u5f15\u5bfc\u6765\u66f4\u597d\u5730\u53bb\u9664\u591c\u95f4\u590d\u6742\u96e8\u7eb9\u3002", "motivation": "\u591c\u95f4\u56fe\u50cf\u53bb\u96e8\u76f8\u6bd4\u767d\u5929\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u591c\u95f4\u573a\u666f\u7684\u56fa\u6709\u590d\u6742\u6027\u4ee5\u53ca\u7f3a\u4e4f\u51c6\u786e\u8868\u793a\u96e8\u4e0e\u5149\u7167\u8026\u5408\u6548\u5e94\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86\u989c\u8272\u7a7a\u95f4\u8f6c\u6362\u7f51\u7edcCST-Net\uff0c\u5305\u542b\u53ef\u5b66\u4e60\u7684\u989c\u8272\u7a7a\u95f4\u8f6c\u6362\u5668\u6765\u5728Y\u901a\u9053\u8fdb\u884c\u96e8\u7eb9\u53bb\u9664\uff0c\u5e76\u5f15\u5165\u9690\u5f0f\u5149\u7167\u5f15\u5bfc\u6765\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u6570\u636e\u96c6\u7684\u4ef7\u503c\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684HQ-NightRain\u6570\u636e\u96c6\u548cCST-Net\u65b9\u6cd5\u5728\u591c\u95f4\u56fe\u50cf\u53bb\u96e8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17479", "abs": "https://arxiv.org/abs/2510.17479", "authors": ["Feng Zhou", "Wenkai Guo", "Pu Cao", "Zhicheng Zhang", "Jianqin Yin"], "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS", "comment": "A preprint paper", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6e85\u5c04\u521d\u59cb\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5SfM\u30013DGS\u81ea\u521d\u59cb\u5316\u548c\u70b9\u4e91\u6b63\u5219\u5316\u6765\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u4e0b\u76843D\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6e85\u5c04\u5bb9\u6613\u8fc7\u62df\u5408\u8bad\u7ec3\u89c6\u56fe\uff0c\u5bfc\u81f4\u65b0\u89c6\u89d2\u6e32\u67d3\u51fa\u73b0\u6a21\u7cca\u7b49\u4f2a\u5f71\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u589e\u5f3a\u521d\u59cb\u5316\u6216\u6dfb\u52a0\u8bad\u7ec3\u65f6\u7ea6\u675f\u6765\u89e3\u51b3\uff0c\u4f46\u5b9e\u9a8c\u8868\u660e\u521d\u59cb\u5316\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\u3002", "method": "1. \u9891\u7387\u611f\u77e5SfM\uff1a\u901a\u8fc7\u4f4e\u9891\u89c6\u56fe\u589e\u5f3a\u548c\u5bbd\u677e\u591a\u89c6\u56fe\u5bf9\u5e94\u6539\u8fdb\u4f4e\u7eb9\u7406\u533a\u57df\u8986\u76d6\uff1b2. 3DGS\u81ea\u521d\u59cb\u5316\uff1a\u5229\u7528\u5149\u5ea6\u76d1\u7763\u751f\u6210\u989d\u5916\u70b9\uff0c\u8865\u5145SfM\u7a00\u758f\u533a\u57df\uff1b3. \u70b9\u4e91\u6b63\u5219\u5316\uff1a\u901a\u8fc7\u51e0\u4f55/\u53ef\u89c1\u6027\u5148\u9a8c\u786e\u4fdd\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u5747\u5300\u7a7a\u95f4\u8986\u76d6\u3002", "result": "\u5728LLFF\u548cMip-NeRF360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6210\u4e3a\u4e00\u4e2a\u66f4\u5f3a\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002", "conclusion": "\u521d\u59cb\u5316\u662f\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6e85\u5c04\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u7684\u7efc\u5408\u521d\u59cb\u5316\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.17482", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17482", "abs": "https://arxiv.org/abs/2510.17482", "authors": ["Chenxu Dang", "Haiyan Liu", "Guangjun Bao", "Pei An", "Xinyue Tang", "Jie Ma", "Bingchuan Sun", "Yan Wang"], "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries", "comment": "Under Review", "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.", "AI": {"tldr": "SparseWorld\u662f\u4e00\u4e2a\u65b0\u9896\u76844D\u5360\u7528\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u52a8\u6001\u67e5\u8be2\u5b9e\u73b0\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u548c\u9ad8\u6548\u7684\u573a\u666f\u7406\u89e3\uff0c\u5728\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5360\u7528\u4e16\u754c\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u56fa\u5b9a\u5d4c\u5165\u6216\u7f51\u683c\uff0c\u9650\u5236\u4e86\u611f\u77e5\u7684\u7075\u6d3b\u6027\uff0c\u4e14\u5176\"\u539f\u5730\u5206\u7c7b\"\u65b9\u6cd5\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u52a8\u6001\u8fde\u7eed\u6027\u5b58\u5728\u6f5c\u5728\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faRange-Adaptive Perception\u6a21\u5757\uff08\u53ef\u5b66\u4e60\u67e5\u8be2\u7531ego\u72b6\u6001\u8c03\u5236\u5e76\u589e\u5f3a\u65f6\u7a7a\u5173\u8054\uff09\u548cState-Conditioned Forecasting\u6a21\u5757\uff08\u7528\u56de\u5f52\u5f15\u5bfc\u516c\u5f0f\u66ff\u4ee3\u57fa\u4e8e\u5206\u7c7b\u7684\u9884\u6d4b\uff09\uff0c\u4ee5\u53caTemporal-Aware Self-Scheduling\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "SparseWorld\u901a\u8fc7\u7a00\u758f\u52a8\u6001\u67e5\u8be2\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u5360\u7528\u4e16\u754c\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a4D\u73af\u5883\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17484", "abs": "https://arxiv.org/abs/2510.17484", "authors": ["Muhammad Umer Ramzan", "Ali Zia", "Abdelwahed Khamis", "Noman Ali", "Usman Ali", "Wei Xiang"], "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment", "comment": null, "summary": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models.", "AI": {"tldr": "POTNet\u901a\u8fc7\u71b5\u5f15\u5bfc\u7684\u53cc\u805a\u7c7b\u5934\u548c\u6700\u4f18\u4f20\u8f93\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u63a9\u7801\uff0cAutoSOD\u5229\u7528\u8fd9\u4e9b\u4f2a\u63a9\u7801\u5b9e\u73b0\u65e0\u76d1\u7763\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u9700\u8981\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u6602\u3002\u4f5c\u8005\u8ba4\u4e3a\u53ea\u8981\u6709\u53ef\u9760\u7684\u4f2a\u63a9\u7801\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faPOTNet\uff0c\u7528\u71b5\u5f15\u5bfc\u7684\u53cc\u805a\u7c7b\u5934\u66ff\u4ee3\u5355k-means\uff1a\u9ad8\u71b5\u50cf\u7d20\u7528\u8c31\u805a\u7c7b\uff0c\u4f4e\u71b5\u50cf\u7d20\u7528k-means\uff0c\u4e24\u7ec4\u539f\u578b\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\uff0c\u751f\u6210\u66f4\u6e05\u6670\u7684\u4f2a\u63a9\u7801\u3002AutoSOD\u4f7f\u7528\u8fd9\u4e9b\u4f2a\u63a9\u7801\u8bad\u7ec3\u6807\u51c6MaskFormer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoSOD\u6bd4\u65e0\u76d1\u7763\u65b9\u6cd5F-measure\u63d0\u534726%\uff0c\u6bd4\u5f31\u76d1\u7763\u65b9\u6cd5\u63d0\u534736%\uff0c\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u4e0e\u5168\u76d1\u7763\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u539f\u578b\u8d28\u91cf\u548c\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u65e0\u76d1\u7763\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u53ef\u4ee5\u8fbe\u5230\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002"}}
{"id": "2510.17501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17501", "abs": "https://arxiv.org/abs/2510.17501", "authors": ["Yuanli Wu", "Long Zhang", "Yue Du", "Bin Li"], "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "comment": null, "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u63d0\u793a\u6846\u67b6\uff0c\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u6784\u5efa\u6570\u636e\u96c6\u81ea\u9002\u5e94\u7684\u8bc4\u5206\u6807\u51c6\u6765\u6307\u5bfc\u53ef\u89e3\u91ca\u7684\u573a\u666f\u8bc4\u4f30\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u89c6\u9891\u6458\u8981\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u5c42\u6b21\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u7ebf\u7d22\uff0c\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u5bf9\u4eba\u5de5\u63d0\u793a\u6a21\u677f\u548c\u6570\u636e\u96c6\u7279\u5b9a\u5206\u6570\u5f52\u4e00\u5316\u654f\u611f\u3002", "method": "\u5c06\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u8f6c\u5316\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\uff0c\u805a\u5408\u6210\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\uff1b\u63a8\u7406\u65f6\u57fa\u4e8e\u63cf\u8ff0\u5bf9\u9996\u5c3e\u7247\u6bb5\u8bc4\u5206\uff0c\u4e2d\u95f4\u7247\u6bb5\u7ed3\u5408\u76f8\u90bb\u573a\u666f\u7684\u4e0a\u4e0b\u6587\u6458\u8981\u6765\u8bc4\u4f30\u53d9\u4e8b\u8fdb\u5c55\u548c\u5197\u4f59\u5ea6\u3002", "result": "\u5728SumMe\u548cTVSum\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523057.58\u548c63.05\u7684F1\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u65e0\u76d1\u7763\u548c\u5148\u524d\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u3002", "conclusion": "\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u80fd\u6709\u6548\u7a33\u5b9a\u57fa\u4e8eLLM\u7684\u8bc4\u5206\uff0c\u4e3a\u89c6\u9891\u6458\u8981\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u96f6\u6837\u672c\u8303\u5f0f\u3002"}}
{"id": "2510.17519", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17519", "abs": "https://arxiv.org/abs/2510.17519", "authors": ["Yongshun Zhang", "Zhongyi Fan", "Yonghang Zhang", "Zhangzikang Li", "Weifeng Chen", "Zhongwei Feng", "Chaoyue Wang", "Peng Hou", "Anxiang Zeng"], "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models", "comment": "Technical Report; Project Page:\n  \\href{https://github.com/Shopee-MUG/MUG-V}", "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u7684\u56db\u652f\u67f1\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\uff0c\u5f00\u53d1\u4e86MUG-V 10B\u6a21\u578b\uff0c\u5728\u7535\u5546\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5f00\u6e90\u57fa\u7ebf\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u6280\u672f\u6808\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u8de8\u6a21\u6001\u6587\u672c-\u89c6\u9891\u5bf9\u9f50\u3001\u957f\u5e8f\u5217\u548c\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u7b49\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u8bad\u7ec3\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u8d44\u6e90\u5bc6\u96c6\u578b\u95ee\u9898\u3002", "method": "\u4f18\u5316\u56db\u4e2a\u5173\u952e\u652f\u67f1\uff1a\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u89c6\u9891\u538b\u7f29\u3001\u53c2\u6570\u7f29\u653e\u3001\u8bfe\u7a0b\u5f0f\u9884\u8bad\u7ec3\u548c\u5bf9\u9f50\u540e\u8bad\u7ec3\u3002", "result": "MUG-V 10B\u6a21\u578b\u6574\u4f53\u6027\u80fd\u8fbe\u5230\u8fd1\u671f\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u7535\u5546\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u8bad\u7ec3\u6846\u67b6\uff0c\u5f00\u6e90\u4e86\u5b8c\u6574\u6280\u672f\u6808\uff0c\u9996\u6b21\u516c\u5f00\u5229\u7528Megatron-Core\u5b9e\u73b0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u8fd1\u7ebf\u6027\u591a\u8282\u70b9\u6269\u5c55\u7684\u89c6\u9891\u751f\u6210\u8bad\u7ec3\u4ee3\u7801\u3002"}}
{"id": "2510.17529", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17529", "abs": "https://arxiv.org/abs/2510.17529", "authors": ["Yovin Yahathugoda", "Davide Prezzi", "Piyalitt Ittichaiwong", "Vicky Goh", "Sebastien Ourselin", "Michela Antonelli"], "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation", "comment": null, "summary": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.", "AI": {"tldr": "\u63d0\u51faMambaX-Net\uff0c\u4e00\u79cd\u7528\u4e8e\u524d\u5217\u817a\u764c\u4e3b\u52a8\u76d1\u6d4b\u7684\u534a\u76d1\u7763\u53cc\u626b\u63cf3D\u5206\u5272\u67b6\u6784\uff0c\u5229\u7528\u65f6\u95f4\u5e8f\u5217MRI\u6570\u636e\u6539\u8fdb\u524d\u5217\u817a\u533a\u57df\u5206\u5272", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u6a21\u578b\u5728\u7eb5\u5411\u4e3b\u52a8\u76d1\u6d4b\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u5355\u65f6\u95f4\u70b9\u548c\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u65f6\u95f4\u70b9\u548c\u4e13\u5bb6\u6807\u6ce8\u7a00\u7f3a\u7684\u60c5\u51b5", "method": "\u63d0\u51faMamba\u589e\u5f3a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u5f62\u72b6\u63d0\u53d6\u5668\u6a21\u5757\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u81ea\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u9884\u8bad\u7ec3nnU-Net\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u5b66\u4e60", "result": "\u5728\u7eb5\u5411\u4e3b\u52a8\u76d1\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMambaX-Net\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684U-Net\u548cTransformer\u6a21\u578b\uff0c\u5373\u4f7f\u5728\u6709\u9650\u548c\u566a\u58f0\u6570\u636e\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u4f18\u5f02\u7684\u524d\u5217\u817a\u533a\u57df\u5206\u5272", "conclusion": "MambaX-Net\u4e3a\u524d\u5217\u817a\u764c\u4e3b\u52a8\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u534a\u76d1\u7763\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u6539\u8fdb\u5206\u5272\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u4e13\u5bb6\u6807\u6ce8\u7684\u4f9d\u8d56"}}
{"id": "2510.17566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17566", "abs": "https://arxiv.org/abs/2510.17566", "authors": ["Nachuan Ma", "Zhengfei Song", "Qiang Hu", "Xiaoyu Tang", "Chengxi Zhang", "Rui Fan", "Lihua Xie"], "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection", "comment": null, "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.", "AI": {"tldr": "WP-CrackNet\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u9053\u8def\u88c2\u7f1d\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u5373\u53ef\u5b9e\u73b0\u50cf\u7d20\u7ea7\u68c0\u6d4b\uff0c\u901a\u8fc7\u5206\u7c7b\u5668\u3001\u91cd\u5efa\u5668\u548c\u68c0\u6d4b\u5668\u7684\u5bf9\u6297\u5b66\u4e60\u673a\u5236\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u51cf\u5c11\u5bf9\u6602\u8d35\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u4e2d\u53ef\u6269\u5c55\u7684\u9053\u8def\u88c2\u7f1d\u68c0\u6d4b\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u751f\u6210\u7c7b\u6fc0\u6d3b\u56fe\u7684\u5206\u7c7b\u5668\u3001\u6d4b\u91cf\u7279\u5f81\u53ef\u63a8\u65ad\u6027\u7684\u91cd\u5efa\u5668\u3001\u4ea7\u751f\u50cf\u7d20\u7ea7\u68c0\u6d4b\u7ed3\u679c\u7684\u68c0\u6d4b\u5668\uff1b\u8bbe\u8ba1\u8def\u5f84\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e2d\u5fc3\u589e\u5f3aCAM\u4e00\u81f4\u6027\u6a21\u5757\u3002", "result": "\u5728\u4e09\u4e2a\u56fe\u50cf\u7ea7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWP-CrackNet\u8fbe\u5230\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63a8\u8fdb\u4e86\u53ef\u6269\u5c55\u9053\u8def\u68c0\u6d4b\uff0c\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.17568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17568", "abs": "https://arxiv.org/abs/2510.17568", "authors": ["Kaichen Zhou", "Yuhan Wang", "Grace Chen", "Xinhai Chang", "Gaspard Beaudouin", "Fangneng Zhan", "Paul Pu Liang", "Mengyu Wang"], "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception", "comment": null, "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.", "AI": {"tldr": "PAGE-4D\u662f\u4e00\u4e2a\u524d\u9988\u6a21\u578b\uff0c\u6269\u5c55\u4e86VGGT\u4ee5\u5904\u7406\u52a8\u6001\u573a\u666f\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u6df1\u5ea6\u9884\u6d4b\u548c\u70b9\u4e91\u91cd\u5efa\uff0c\u65e0\u9700\u540e\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u76843D\u524d\u9988\u6a21\u578b\uff08\u5982VGGT\uff09\u5728\u9759\u6001\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u96be\u4ee5\u5904\u7406\u5305\u542b\u590d\u6742\u52a8\u6001\u5143\u7d20\uff08\u5982\u79fb\u52a8\u4eba\u7269\u6216\u53ef\u53d8\u5f62\u7269\u4f53\uff09\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u611f\u77e5\u805a\u5408\u5668\uff0c\u901a\u8fc7\u9884\u6d4b\u52a8\u6001\u611f\u77e5\u63a9\u7801\u6765\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\u2014\u2014\u5728\u59ff\u6001\u4f30\u8ba1\u65f6\u6291\u5236\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5728\u51e0\u4f55\u91cd\u5efa\u65f6\u589e\u5f3a\u5b83\u4eec\u3002", "result": "\u5728\u52a8\u6001\u573a\u666f\u4e2d\uff0cPAGE-4D\u6301\u7eed\u4f18\u4e8e\u539f\u59cbVGGT\uff0c\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u5355\u76ee\u548c\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca\u5bc6\u96c6\u70b9\u4e91\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u66f4\u4f18\u7ed3\u679c\u3002", "conclusion": "PAGE-4D\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4efb\u52a14D\u91cd\u5efa\u4e2d\u7684\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u76843D\u5c5e\u6027\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17585", "abs": "https://arxiv.org/abs/2510.17585", "authors": ["Chuhong Wang", "Hua Li", "Chongyi Li", "Huazhong Liu", "Xiongxin Tang", "Sam Kwong"], "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset", "comment": null, "summary": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u6c34\u4e0b\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6UCIS4K\u548c\u57fa\u4e8eSegment Anything Model\u7684\u6c34\u4e0b\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u7f51\u7edcUCIS-SAM\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6a21\u5757\u89e3\u51b3\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u989c\u8272\u5931\u771f\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u5728\u9646\u5730\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6c34\u4e0b\u6837\u672c\u6709\u9650\uff0c\u5728\u6c34\u4e0b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6c34\u4e0b\u9000\u5316\u73af\u5883\uff08\u989c\u8272\u5931\u771f\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u6a21\u7cca\uff09\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faUCIS-SAM\u7f51\u7edc\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u901a\u9053\u5e73\u8861\u4f18\u5316\u6a21\u5757(CBOM)\u589e\u5f3a\u901a\u9053\u7279\u5f81\u5b66\u4e60\uff0c\u9891\u57df\u771f\u503c\u6574\u5408\u6a21\u5757(FDTIM)\u5f3a\u8c03\u5185\u5728\u7269\u4f53\u7279\u5f81\u51cf\u5c11\u4f2a\u88c5\u6a21\u5f0f\u5e72\u6270\uff0c\u591a\u5c3a\u5ea6\u7279\u5f81\u9891\u7387\u805a\u5408\u6a21\u5757(MFFAM)\u5728\u591a\u9891\u6bb5\u52a0\u5f3a\u4f4e\u5bf9\u6bd4\u5ea6\u4f2a\u88c5\u5b9e\u4f8b\u8fb9\u754c\u3002", "result": "\u5728\u63d0\u51fa\u7684UCIS4K\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUCIS-SAM\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "UCIS-SAM\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6c34\u4e0b\u7279\u5f81\u589e\u5f3a\u548c\u9891\u7387\u57df\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u4f2a\u88c5\u5b9e\u4f8b\u5206\u5272\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.17603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17603", "abs": "https://arxiv.org/abs/2510.17603", "authors": ["Shuyuan Zhang", "Chenhan Jiang", "Zuoou Li", "Jiankang Deng"], "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling", "comment": "NeurIPS 2025 Poster", "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.", "AI": {"tldr": "ShapeCraft\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u4ee3\u7406\u6846\u67b6\u7684\u6587\u672c\u52303D\u751f\u6210\u7cfb\u7edf\uff0c\u4f7f\u7528\u56fe\u7ed3\u6784\u7a0b\u5e8f\u5316\u5f62\u72b6\u8868\u793a\u6765\u751f\u6210\u7ed3\u6784\u5316\u3001\u53ef\u4ea4\u4e92\u76843D\u8d44\u4ea7\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u52303D\u751f\u6210\u65b9\u6cd5\u4ea7\u751f\u7684\u7f51\u683c\u7ed3\u6784\u6df7\u4e71\u3001\u4ea4\u4e92\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u827a\u672f\u521b\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u7a0b\u5e8f\u5316\u5f62\u72b6\u8868\u793a\uff0c\u5c06\u590d\u6742\u81ea\u7136\u8bed\u8a00\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528LLM\u4ee3\u7406\u5206\u5c42\u89e3\u6790\u7528\u6237\u8f93\u5165\u5e76\u8fed\u4ee3\u4f18\u5316\u7a0b\u5e8f\u5316\u5efa\u6a21\u548c\u7ed8\u5236\u8fc7\u7a0b\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u663e\u793aShapeCraft\u5728\u751f\u6210\u51e0\u4f55\u7cbe\u786e\u548c\u8bed\u4e49\u4e30\u5bcc\u76843D\u8d44\u4ea7\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u65b9\u6cd5\u3002", "conclusion": "ShapeCraft\u5c55\u793a\u4e86\u5728\u52a8\u753b\u548c\u7528\u6237\u81ea\u5b9a\u4e49\u7f16\u8f91\u65b9\u9762\u7684\u591a\u529f\u80fd\u6027\uff0c\u5177\u6709\u66f4\u5e7f\u6cdb\u4ea4\u4e92\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17609", "abs": "https://arxiv.org/abs/2510.17609", "authors": ["Siqi Chen", "Shanyue Guan"], "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation", "comment": null, "summary": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u76843D\u70b9\u4e91\u81ea\u52a8\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u65e0\u4eba\u673a\u626b\u63cf\u7684\u771f\u5b9e\u70b9\u4e91\u548cBIM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u7ed3\u6784\u7ec4\u4ef6\u5206\u5272\u4e2d\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u957f\u4e14\u6613\u51fa\u9519\u7684\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u6280\u672f\u7ed3\u5408\u6444\u5f71\u6d4b\u91cf\u80fd\u591f\u9ad8\u6548\u83b7\u53d6\u57fa\u7840\u8bbe\u65bd\u76843D\u6a21\u578b\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u5206\u5272\u7ed3\u6784\u7ec4\u4ef6\u7684\u65b9\u6cd5\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u771f\u5b9e\u65e0\u4eba\u673a\u626b\u63cf\u70b9\u4e91\u548cBIM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5229\u7528\u4e24\u79cd\u6570\u636e\u7684\u4e92\u8865\u4f18\u52bf\u6765\u514b\u670d\u624b\u52a8\u6807\u6ce8\u7684\u9650\u5236\u3002", "result": "\u5728\u94c1\u8def\u8f68\u9053\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u548c\u5206\u5272\u94c1\u8f68\u3001\u8f68\u6795\u7b49\u4e3b\u8981\u7ec4\u4ef6\uff0c\u4f7f\u7528\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u7ed3\u5408BIM\u6570\u636e\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u9ad8\u4e863D\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\u5206\u5272\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\u4e0eBIM\u6280\u672f\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u548c\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u4e2d\u7684\u96c6\u6210\u5e94\u7528\u3002"}}
{"id": "2510.17611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17611", "abs": "https://arxiv.org/abs/2510.17611", "authors": ["Jia Guo", "Shuai Lu", "Lei Fan", "Zelin Li", "Donglin Di", "Yang Song", "Weihang Zhang", "Wenbing Zhu", "Hong Yan", "Fang Chen", "Huiqi Li", "Hongen Liao"], "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection", "comment": "Extended version of CVPR2025", "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.", "AI": {"tldr": "Dinomaly2\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u7b80\u5355\u5143\u7d20\u7684\u7ec4\u5408\u5728\u6807\u51c6\u91cd\u5efa\u6846\u67b6\u4e2d\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u586b\u8865\u4e86\u591a\u7c7b\u6a21\u578b\u4e0e\u5355\u7c7b\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u6269\u5c55\u5230\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\u8bbe\u7f6e\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u663e\u8457\u843d\u540e\u4e8e\u6700\u5148\u8fdb\u7684\u5355\u7c7b\u6a21\u578b\uff0c\u4e14\u8be5\u9886\u57df\u5df2\u5206\u88c2\u4e3a\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u4e13\u95e8\u65b9\u6cd5\uff0c\u9700\u8981\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u6765\u514b\u670d\u90e8\u7f72\u969c\u788d\u3002", "method": "\u57fa\u4e8e\"\u5c11\u5373\u662f\u591a\"\u7406\u5ff5\uff0c\u5728\u6807\u51c6\u91cd\u5efa\u6846\u67b6\u4e2d\u534f\u8c03\u4e94\u4e2a\u7b80\u5355\u5143\u7d20\uff0c\u8fd9\u79cd\u6700\u5c0f\u5316\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u81ea\u7136\u6269\u5c55\u5230\u5404\u79cd\u4efb\u52a1\u800c\u65e0\u9700\u4fee\u6539\u3002", "result": "\u572812\u4e2a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDinomaly2\u5728\u591a\u79cd\u6a21\u6001\u3001\u4efb\u52a1\u8bbe\u7f6e\u548c\u5e94\u7528\u9886\u57df\u5c55\u73b0\u51fa\u5168\u8c31\u7cfb\u4f18\u52bf\u3002\u591a\u7c7b\u6a21\u578b\u5728MVTec-AD\u548cVisA\u4e0a\u5206\u522b\u8fbe\u523099.9%\u548c99.3%\u7684\u56fe\u50cf\u7ea7AUROC\uff0c\u4ec5\u7528\u6bcf\u4e2a\u7c7b\u522b8\u4e2a\u6b63\u5e38\u6837\u672c\u5c31\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5168\u6837\u672c\u6a21\u578b\u3002", "conclusion": "Dinomaly2\u51ed\u501f\u6700\u5c0f\u5316\u8bbe\u8ba1\u3001\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u9002\u7528\u6027\uff0c\u6210\u4e3a\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u5e94\u7528\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u6027\u662f\u771f\u6b63\u666e\u9002\u6027\u7684\u57fa\u7840\u3002"}}
{"id": "2510.17626", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17626", "abs": "https://arxiv.org/abs/2510.17626", "authors": ["Fr\u00e9d\u00e9ric LIN", "Biruk Abere Ambaw", "Adrian Popescu", "Hejer Ammar", "Romaric Audigier", "Herv\u00e9 Le Borgne"], "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation", "comment": "To be published in NeurIPS 2025 Track on Datasets and Benchmarks", "summary": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.", "AI": {"tldr": "CaMiT\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\uff0c\u6355\u6349\u6c7d\u8f66\u6a21\u578b\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u8fc7\u7a0b\uff0c\u652f\u6301\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u3002\u7814\u7a76\u53d1\u73b0\u9759\u6001\u9884\u8bad\u7ec3\u5728\u8de8\u5e74\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u63d0\u51fa\u4e86\u65f6\u95f4\u589e\u91cf\u5206\u7c7b\u8bbe\u7f6e\u548c\u4e24\u79cd\u7b56\u7565\u6765\u63d0\u5347\u65f6\u95f4\u9c81\u68d2\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u65f6\u95f4\u611f\u77e5\u56fe\u50cf\u751f\u6210\u3002", "motivation": "AI\u7cfb\u7edf\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u89c6\u89c9\u73af\u5883\uff0c\u7279\u522b\u662f\u5728\u5bf9\u8c61\u5916\u89c2\u968f\u65f6\u95f4\u53d8\u5316\u7684\u9886\u57df\u3002\u6c7d\u8f66\u6a21\u578b\u4f5c\u4e3a\u6280\u672f\u4ea7\u54c1\u7684\u4ee3\u8868\u6027\u7c7b\u522b\uff0c\u5176\u5916\u89c2\u6f14\u53d8\u9700\u8981\u88ab\u7814\u7a76\u3002", "method": "\u6784\u5efaCaMiT\u6570\u636e\u96c6\uff08787K\u6807\u6ce8\u6837\u672c\u548c5.1M\u672a\u6807\u6ce8\u6837\u672c\uff09\uff0c\u8bc4\u4f30\u9759\u6001\u9884\u8bad\u7ec3\u6027\u80fd\uff0c\u63d0\u51fa\u65f6\u95f4\u589e\u91cf\u5206\u7c7b\u8bbe\u7f6e\uff0c\u5305\u62ec\u65f6\u95f4\u589e\u91cf\u9884\u8bad\u7ec3\uff08\u66f4\u65b0\u4e3b\u5e72\u7f51\u7edc\uff09\u548c\u65f6\u95f4\u589e\u91cf\u5206\u7c7b\u5668\u5b66\u4e60\uff08\u4ec5\u66f4\u65b0\u6700\u540e\u4e00\u5c42\uff09\uff0c\u5e76\u63a2\u7d22\u65f6\u95f4\u611f\u77e5\u56fe\u50cf\u751f\u6210\u3002", "result": "\u9759\u6001\u9884\u8bad\u7ec3\u5728\u9886\u57df\u5185\u6570\u636e\u4e0a\u80fd\u8fbe\u5230\u4e0e\u5927\u89c4\u6a21\u901a\u7528\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u4e14\u66f4\u9ad8\u6548\uff0c\u4f46\u5728\u8de8\u5e74\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u4e0b\u964d\u3002\u65f6\u95f4\u589e\u91cf\u7b56\u7565\u80fd\u63d0\u5347\u65f6\u95f4\u9c81\u68d2\u6027\uff0c\u65f6\u95f4\u611f\u77e5\u56fe\u50cf\u751f\u6210\u80fd\u4ea7\u751f\u66f4\u771f\u5b9e\u7684\u8f93\u51fa\u3002", "conclusion": "CaMiT\u4e3a\u7814\u7a76\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u548c\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u9002\u5e94\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u7684\u65f6\u95f4\u589e\u91cf\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u89c6\u89c9\u73af\u5883\u7684\u65f6\u95f4\u6f14\u53d8\u6311\u6218\u3002"}}
{"id": "2510.17644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17644", "abs": "https://arxiv.org/abs/2510.17644", "authors": ["Zexian Huang", "Mashnoon Islam", "Brian Armstrong", "Kourosh Khoshelham", "Martin Tomko"], "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives", "comment": null, "summary": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.", "AI": {"tldr": "\u63d0\u51faDINO-CV\u5206\u5272\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u673a\u8f7dLiDAR DEM\u81ea\u52a8\u6620\u5c04\u4f4e\u77ee\u5e72\u77f3\u5899\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8de8\u89c6\u56fe\u9884\u8bad\u7ec3\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u690d\u88ab\u8986\u76d6\u533a\u57df\u5b9e\u73b0\u6709\u6548\u68c0\u6d4b\u3002", "motivation": "\u5e72\u77f3\u5899\u5177\u6709\u91cd\u8981\u9057\u4ea7\u548c\u73af\u5883\u4ef7\u503c\uff0c\u4f46\u8bb8\u591a\u5899\u4f53\u56e0\u96be\u4ee5\u63a5\u8fd1\u548c\u4eba\u5de5\u6d4b\u7ed8\u6210\u672c\u9ad8\u800c\u672a\u88ab\u8bc6\u522b\u3002\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u690d\u88ab\u906e\u6321\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u4e24\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528LiDAR DEM\u514b\u670d\u690d\u88ab\u906e\u6321\uff0c\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u81ea\u76d1\u7763\u8de8\u89c6\u56fe\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5b66\u4e60\u591a\u4e2aDEM\u884d\u751f\u7269\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728Budj Bim UNESCO\u4e16\u754c\u9057\u4ea7\u5730\u5e94\u7528\u4e2d\uff0c\u8bc6\u522b\u51fa\u6fb3\u5927\u5229\u4e9a\u6700\u5bc6\u96c6\u7684\u6b96\u6c11\u65f6\u671f\u5e72\u77f3\u5899\uff0c\u6d4b\u8bd5\u533a\u57dfmIoU\u8fbe68.6%\uff0c\u4ec5\u752810%\u6807\u8bb0\u6570\u636e\u5fae\u8c03\u540e\u4ecd\u4fdd\u630163.8% mIoU\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u9ad8\u5206\u8fa8\u7387DEM\u884d\u751f\u7269\u4e0a\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u5728\u690d\u88ab\u8986\u76d6\u548c\u9057\u4ea7\u4e30\u5bcc\u4f46\u6807\u6ce8\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u5e72\u77f3\u5899\u81ea\u52a8\u6620\u5c04\u3002"}}
{"id": "2510.17651", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17651", "abs": "https://arxiv.org/abs/2510.17651", "authors": ["S\u00e9bastien Thuau", "Siba Haidar", "Ayush Bajracharya", "Rachid Chelouah"], "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs", "comment": "7 pages, 1 figure, FLTA 2025", "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cd\u8282\u4fed\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u66b4\u529b\u68c0\u6d4b\uff1a\u96f6\u6837\u672c\u548c\u8054\u90a6\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e0e\u4e2a\u6027\u5316\u8bad\u7ec3\u7d27\u51d13D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN3D)\u3002\u4e24\u79cd\u65b9\u6cd5\u51c6\u786e\u7387\u5747\u8d8590%\uff0cCNN3D\u5728ROC AUC\u548clog loss\u4e0a\u7565\u4f18\u4e14\u80fd\u8017\u66f4\u4f4e\uff0cVLMs\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u4e0a\u66f4\u4f18\u3002\u63d0\u51fa\u4e86\u6df7\u5408\u6a21\u578b\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u8282\u4fed\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u66b4\u529b\u68c0\u6d4b\uff0c\u7279\u522b\u5173\u6ce8\u80fd\u6e90\u6548\u7387\u548c\u73af\u5883\u6307\u6807\uff0c\u4e3a\u89c6\u9891\u76d1\u63a7\u63d0\u4f9b\u8d1f\u8d23\u4efb\u3001\u8d44\u6e90\u611f\u77e5\u7684AI\u57fa\u51c6\u3002", "method": "\u4f7f\u7528LLaVA-7B\u548c65.8M\u53c2\u6570CNN3D\u4f5c\u4e3a\u4ee3\u8868\u6848\u4f8b\uff0c\u5728\u73b0\u5b9e\u975eIID\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u80fd\u8017\u3002\u6bd4\u8f83\u96f6\u6837\u672c\u548c\u8054\u90a6\u5fae\u8c03VLMs\u4e0e\u4e2a\u6027\u5316\u8bad\u7ec3CNN3D\u4e24\u79cd\u7b56\u7565\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u51c6\u786e\u7387\u5747\u8d85\u8fc790%\u3002CNN3D\u5728ROC AUC\u548clog loss\u4e0a\u7565\u4f18\u4e8eLoRA\u8c03\u4f18\u7684VLMs\uff0c\u4e14\u80fd\u8017\u66f4\u4f4e\u3002VLMs\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u4fdd\u6301\u4f18\u52bf\u3002\u91cf\u5316\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u80fd\u8017\u548cCO2\u6392\u653e\u3002", "conclusion": "\u652f\u6301\u6df7\u5408\u6a21\u578b\uff1a\u8f7b\u91cf\u7ea7CNN\u7528\u4e8e\u5e38\u89c4\u5206\u7c7b\uff0c\u9009\u62e9\u6027\u6fc0\u6d3bVLM\u7528\u4e8e\u590d\u6742\u6216\u63cf\u8ff0\u6027\u573a\u666f\u3002\u8be5\u6846\u67b6\u4e3a\u89c6\u9891\u76d1\u63a7\u4e2d\u8d1f\u8d23\u4efb\u3001\u8d44\u6e90\u611f\u77e5\u7684AI\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.17664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17664", "abs": "https://arxiv.org/abs/2510.17664", "authors": ["Ling Liu", "Jun Tian", "Li Yi"], "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads", "comment": null, "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.", "AI": {"tldr": "4DSegStreamer\u662f\u4e00\u4e2a\u7528\u4e8e4D\u5168\u666f\u5206\u5272\u7684\u6d41\u5f0f\u5904\u7406\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u7ebf\u7a0b\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u67093D/4D\u5206\u5272\u65b9\u6cd5\u4e2d\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u7ec6\u7c92\u5ea6\u611f\u77e5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\uff08\u5982\u5bc6\u96c6\u4eba\u7fa4\u758f\u6563\u3001\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff09\u4e2d\u9700\u8981\u5b9e\u65f6\u7ec6\u7c92\u5ea6\u611f\u77e5\u7684\u95ee\u9898\uff0c\u5728\u6709\u9650\u65f6\u95f4\u9884\u7b97\u5185\u5b9e\u73b04D\u5168\u666f\u5206\u5272\u3002", "method": "\u91c7\u7528\u53cc\u7ebf\u7a0b\u7cfb\u7edf\uff1a\u9884\u6d4b\u7ebf\u7a0b\u5229\u7528\u5386\u53f2\u8fd0\u52a8\u548c\u51e0\u4f55\u4fe1\u606f\u63d0\u53d6\u7279\u5f81\u5e76\u9884\u6d4b\u672a\u6765\u52a8\u6001\uff1b\u63a8\u7406\u7ebf\u7a0b\u901a\u8fc7\u5bf9\u9f50\u6700\u65b0\u8bb0\u5fc6\u5e76\u8865\u507f\u81ea\u8fd0\u52a8\u548c\u52a8\u6001\u7269\u4f53\u79fb\u52a8\uff0c\u786e\u4fdd\u5bf9\u8f93\u5165\u5e27\u7684\u53ca\u65f6\u9884\u6d4b\u3002", "result": "\u5728\u5ba4\u5185HOI4D\u6570\u636e\u96c6\u548c\u5ba4\u5916SemanticKITTI\u3001nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u51c6\u786e\u9884\u6d4b\u52a8\u6001\u5bf9\u8c61\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9ad8FPS\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u6d41\u5f0f\u611f\u77e5\u65b9\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "4DSegStreamer\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6d41\u5f0f4D\u5168\u666f\u5206\u5272\u6846\u67b6\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u9002\u5408\u590d\u6742\u573a\u666f\u7684\u52a8\u6001\u5bf9\u8c61\u9884\u6d4b\u3002"}}
{"id": "2510.17681", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17681", "abs": "https://arxiv.org/abs/2510.17681", "authors": ["Yuandong Pu", "Le Zhuo", "Songhao Han", "Jinbo Xing", "Kaiwen Zhu", "Shuo Cao", "Bin Fu", "Si Liu", "Hongsheng Li", "Yu Qiao", "Wenlong Zhang", "Xi Chen", "Yihao Liu"], "title": "PICABench: How Far Are We from Physically Realistic Image Editing?", "comment": null, "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.", "AI": {"tldr": "\u63d0\u51fa\u4e86PICABench\u57fa\u51c6\u6d4b\u8bd5\u548cPICAEval\u8bc4\u4f30\u534f\u8bae\uff0c\u7cfb\u7edf\u8bc4\u4f30\u56fe\u50cf\u7f16\u8f91\u7684\u7269\u7406\u771f\u5b9e\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u6548\u679c\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6307\u4ee4\u5b8c\u6210\u5ea6\uff0c\u4f46\u5ffd\u7565\u4e86\u7f16\u8f91\u64cd\u4f5c\u4f34\u968f\u7684\u7269\u7406\u6548\u679c\uff08\u5982\u9634\u5f71\u3001\u53cd\u5c04\u3001\u7269\u4f53\u4ea4\u4e92\u7b49\uff09\uff0c\u8fd9\u4e9b\u5bf9\u751f\u6210\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efaPICABench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d68\u4e2a\u7269\u7406\u5b50\u7ef4\u5ea6\uff08\u5149\u5b66\u3001\u529b\u5b66\u3001\u72b6\u6001\u8f6c\u6362\u7b49\uff09\u548c\u5e38\u89c1\u7f16\u8f91\u64cd\u4f5c\uff1b\u63d0\u51faPICAEval\u8bc4\u4f30\u534f\u8bae\uff0c\u4f7f\u7528VLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u7ed3\u5408\u533a\u57df\u7ea7\u4eba\u5de5\u6807\u6ce8\u548c\u95ee\u9898\uff1b\u6784\u5efaPICA-100K\u8bad\u7ec3\u6570\u636e\u96c6\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u7269\u7406\u77e5\u8bc6\u3002", "result": "\u8bc4\u4f30\u4e3b\u6d41\u6a21\u578b\u540e\u53d1\u73b0\uff0c\u7269\u7406\u771f\u5b9e\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5b58\u5728\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u7269\u7406\u771f\u5b9e\u6027\u662f\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u7684\u91cd\u8981\u6311\u6218\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u89e3\u51b3\u65b9\u6848\u4e3a\u4ece\u7b80\u5355\u5185\u5bb9\u7f16\u8f91\u5411\u7269\u7406\u4e00\u81f4\u6027\u771f\u5b9e\u611f\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17684", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17684", "abs": "https://arxiv.org/abs/2510.17684", "authors": ["Xinwei Zhang", "Hu Chen", "Zhe Yuan", "Sukun Tian", "Peng Feng"], "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model", "comment": null, "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.", "AI": {"tldr": "\u63d0\u51faIC-MoE\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u8bed\u4e49\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u548c\u7834\u574f\u9884\u8bad\u7ec3\u6743\u91cd\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u7840\u4e13\u5bb6\u3001\u8bed\u4e49\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u4e13\u5bb6\uff0c\u91c7\u7528\u50cf\u7d20\u6982\u7387\u81ea\u9002\u5e94\u6295\u7968\u7b56\u7565\u8fdb\u884c\u4e13\u5bb6\u9009\u62e9\u548c\u878d\u5408\uff0c\u5e76\u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIC-MoE\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "IC-MoE\u6709\u6548\u8865\u5145\u4e86\u57fa\u7840\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.17685", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17685", "abs": "https://arxiv.org/abs/2510.17685", "authors": ["Min Cao", "Xinyu Zhou", "Ding Jiang", "Bo Du", "Mang Ye", "Min Zhang"], "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning", "comment": "Final version published in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI). Xplore link:\n  https://ieeexplore.ieee.org/document/11199360", "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.", "AI": {"tldr": "\u63d0\u51faBi-IRRA\u6846\u67b6\u89e3\u51b3\u591a\u8bed\u8a00\u6587\u672c-\u56fe\u50cf\u884c\u4eba\u68c0\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u548c\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u6765\u5b66\u4e60\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u5bf9\u9f50", "motivation": "\u73b0\u6709\u6587\u672c-\u56fe\u50cf\u884c\u4eba\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u5f02\u8d28\u6027\u6311\u6218\uff0c\u5168\u5c40\u65b9\u6cd5\u5ffd\u7565\u7ec6\u7c92\u5ea6\u5dee\u5f02\uff0c\u5c40\u90e8\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u4fe1\u606f\uff0c\u4e14\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u9650\u5236\u4e86\u591a\u8bed\u8a00\u5e94\u7528", "method": "\u5f00\u53d1\u591a\u8bed\u8a00TIPR\u57fa\u51c6\uff0c\u63d0\u51faBi-IRRA\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u6a21\u5757\uff08\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u548c\u6587\u672c\u7684\u53cc\u5411\u9884\u6d4b\u589e\u5f3a\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u5c40\u90e8\u5173\u7cfb\u5efa\u6a21\uff09\u548c\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u6a21\u5757", "result": "\u5728\u6240\u6709\u591a\u8bed\u8a00TIPR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c", "conclusion": "Bi-IRRA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u6587\u672c-\u56fe\u50cf\u884c\u4eba\u68c0\u7d22\u4e2d\u7684\u6a21\u6001\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u591a\u8bed\u8a00\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2510.17686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17686", "abs": "https://arxiv.org/abs/2510.17686", "authors": ["Taichi Liu", "Zhenyu Wang", "Ruofeng Liu", "Guang Wang", "Desheng Zhang"], "title": "Towards 3D Objectness Learning in an Open World", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.", "AI": {"tldr": "OP3Det\u662f\u4e00\u4e2a\u65e0\u9700\u6587\u672c\u63d0\u793a\u7684\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\uff0c\u5229\u75282D\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u591a\u6a21\u6001\u878d\u5408\u6765\u68c0\u6d4b3D\u573a\u666f\u4e2d\u7684\u6240\u6709\u5bf9\u8c61\uff0c\u5305\u62ec\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u65b0\u7c7b\u522b\u3002", "motivation": "\u4f20\u7edf\u5c01\u95ed\u5f0f3D\u68c0\u6d4b\u5668\u96be\u4ee5\u6cdb\u5316\u5230\u5f00\u653e\u4e16\u754c\u573a\u666f\uff0c\u800c\u76f4\u63a5\u5e94\u75283D\u5f00\u653e\u8bcd\u6c47\u6a21\u578b\u53c8\u9762\u4e34\u8bcd\u6c47\u6269\u5c55\u548c\u8bed\u4e49\u91cd\u53e0\u95ee\u9898\uff0c\u9700\u8981\u5b66\u4e60\u901a\u7528\u76843D\u7269\u4f53\u6027\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u7ed3\u54082D\u8bed\u4e49\u5148\u9a8c\u548c3D\u51e0\u4f55\u5148\u9a8c\u8fdb\u884c\u7c7b\u522b\u65e0\u5173\u7684\u63d0\u8bae\u751f\u6210\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4e13\u5bb6\u6df7\u5408\u52a8\u6001\u8def\u7531\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u7279\u5f81\u6765\u5b66\u4e60\u901a\u75283D\u7269\u4f53\u6027\u3002", "result": "\u5728AR\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\u8fbe16.0%\uff0c\u76f8\u6bd4\u5c01\u95ed\u4e16\u754c3D\u68c0\u6d4b\u5668\u63d0\u534713.5%\u3002", "conclusion": "OP3Det\u901a\u8fc7\u878d\u54082D\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5f00\u653e\u4e16\u754c3D\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u901a\u75283D\u7269\u4f53\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.17699", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17699", "abs": "https://arxiv.org/abs/2510.17699", "authors": ["Aleksandr Oganov", "Ilya Bykov", "Eva Neudachina", "Mishan Aliev", "Alexander Tolmachev", "Alexander Sidorov", "Aleksandr Zuev", "Andrey Okhotin", "Denis Rakitin", "Aibek Alanov"], "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver", "comment": null, "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u91c7\u6837\u65b9\u6cd5\u2014\u2014\u5e7f\u4e49\u5bf9\u6297\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u7b80\u5355\u7684ODE\u6c42\u89e3\u5668\u53c2\u6570\u5316\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u91c7\u6837\u6b65\u6570\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u91c7\u6837\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u901a\u8fc7\u84b8\u998f\u65b9\u6cd5\u51cf\u5c11\u91c7\u6837\u6b65\u6570\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u8bad\u7ec3\u6280\u5de7\u4e14\u4e0d\u80fd\u5f88\u597d\u5730\u4fdd\u7559\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u6c42\u89e3\u5668\u53c2\u6570\u5316ODE\u91c7\u6837\u5668\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6280\u5de7\uff1b\u7ed3\u5408\u539f\u59cb\u84b8\u998f\u635f\u5931\u548c\u5bf9\u6297\u8bad\u7ec3\u6765\u51cf\u5c11\u4f2a\u5f71\u5e76\u589e\u5f3a\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u76f8\u4f3c\u8d44\u6e90\u7ea6\u675f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6c42\u89e3\u5668\u8bad\u7ec3\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u5e7f\u4e49\u5bf9\u6297\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u91c7\u6837\u65b9\u6848\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2510.17700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17700", "abs": "https://arxiv.org/abs/2510.17700", "authors": ["Walter Simoncini", "Michael Dorkenwald", "Tijmen Blankevoort", "Cees G. M. Snoek", "Yuki M. Asano"], "title": "Elastic ViTs from Pretrained Models without Retraining", "comment": "Accepted at NeurIPS 2025", "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/", "AI": {"tldr": "SnapViT\u662f\u4e00\u79cd\u540e\u9884\u8bad\u7ec3\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8fde\u7eed\u8ba1\u7b97\u9884\u7b97\u8303\u56f4\u5185\u5b9e\u73b0\u5f39\u6027\u63a8\u7406\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u6216\u6807\u7b7e\u6570\u636e\uff0c\u57285\u5206\u949f\u5185\u751f\u6210\u53ef\u8c03\u6574\u7684\u5f39\u6027\u6a21\u578b\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u53ea\u6709\u6709\u9650\u7684\u9884\u5b9a\u4e49\u5c3a\u5bf8\uff0c\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u96be\u4ee5\u8fdb\u884c\u6700\u4f18\u90e8\u7f72\u9009\u62e9\u3002", "method": "\u7ed3\u5408\u68af\u5ea6\u4fe1\u606f\u4e0e\u8de8\u7f51\u7edc\u7ed3\u6784\u76f8\u5173\u6027\uff08\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u8fd1\u4f3c\uff09\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u91cd\u8981\u6027\u8bc4\u5206\u673a\u5236\uff0c\u65e0\u9700\u5206\u7c7b\u5934\u5373\u53ef\u6cdb\u5316\u3002", "result": "\u5728DINO\u3001SigLIPv2\u3001DeIT\u548cAugReg\u6a21\u578b\u4e0a\uff0c\u5728\u5404\u79cd\u7a00\u758f\u5ea6\u4e0b\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5355\u4e2aA100 GPU\u4e0a\u4e0d\u52305\u5206\u949f\u5373\u53ef\u751f\u6210\u5f39\u6027\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u526a\u679d\u7b56\u7565\u3001\u65b0\u9896\u7684Hessian\u975e\u5bf9\u89d2\u7ed3\u6784\u8fdb\u5316\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4ee5\u53ca\u65e0\u9700\u91cd\u8bad\u7ec3\u6216\u6807\u7b7e\u7684\u81ea\u76d1\u7763\u91cd\u8981\u6027\u8bc4\u5206\u673a\u5236\u3002"}}
{"id": "2510.17703", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17703", "abs": "https://arxiv.org/abs/2510.17703", "authors": ["Mhd Adnan Albani", "Riad Sonbol"], "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns", "comment": "19 pages, 2 figures, 9 tables", "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u5757\u5904\u7406\u548c\u96c6\u6210\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u60a3\u8005\u6570\u636e\u4e0a\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5e15\u91d1\u68ee\u75c5\u624b\u7ed8\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u9762\u5bf9\u672a\u89c1\u60a3\u8005\u6570\u636e\u65f6\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u6309\u7ed8\u56fe\u7c7b\u578b\u5206\u7c7b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u56fe\u50cf\u5206\u4e3a2x2\u5757\u5206\u522b\u63d0\u53d6\u7279\u5f81\uff0c\u6700\u540e\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u5408\u5e76\u5404\u5757\u51b3\u7b56\u3002", "result": "\u5728NewHandPD\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u5df2\u89c1\u60a3\u8005\u51c6\u786e\u7387\u8fbe97.08%\uff0c\u5bf9\u672a\u89c1\u60a3\u8005\u8fbe94.91%\uff0c\u6027\u80fd\u5dee\u8ddd\u4ec52.17\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5757\u7b56\u7565\u548c\u96c6\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u60a3\u8005\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2510.17716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17716", "abs": "https://arxiv.org/abs/2510.17716", "authors": ["Suqiang Ma", "Subhadeep Sengupta", "Yao Lee", "Beikang Gu", "Xianyan Chen", "Xianqiao Wang", "Yang Liu", "Mengjia Xu", "Galit H. Frydman", "He Li"], "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging", "comment": null, "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5206\u6790\u5faa\u73af\u8840\u7ec6\u80de\u7c07\u56fe\u50cf\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u4f7f\u7528YOLOv11\u6a21\u578b\u5206\u7c7b\u7ec6\u80de\u7c07\uff0c\u5e76\u901a\u8fc7\u591a\u901a\u9053\u8367\u5149\u67d3\u8272\u8bc6\u522b\u7ec6\u80de\u7c7b\u578b\uff0c\u51c6\u786e\u7387\u8d85\u8fc795%\u3002", "motivation": "\u5faa\u73af\u8840\u7ec6\u80de\u7c07\u662f\u8840\u6813\u3001\u611f\u67d3\u548c\u708e\u75c7\u7684\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u81ea\u52a8\u5206\u6790\u7ec6\u80de\u7c07\u56fe\u50cf\u7684\u5de5\u5177\u3002\u7ec6\u80de\u7c07\u5f62\u72b6\u4e0d\u89c4\u5219\u3001\u5927\u5c0f\u4e0d\u4e00\uff0c\u4e14\u5305\u542b\u5f02\u8d28\u7ec6\u80de\u7c7b\u578b\uff0c\u9700\u8981\u591a\u901a\u9053\u67d3\u8272\u6765\u8bc6\u522b\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u5206\u6790\u7b56\u7565\uff1a1) \u4f7f\u7528YOLOv11\u6a21\u578b\u5fae\u8c03\uff0c\u5c06\u56fe\u50cf\u5206\u7c7b\u4e3a\u7ec6\u80de\u7c07\u548c\u975e\u7ec6\u80de\u7c07\u7ec4\uff1b2) \u901a\u8fc7\u53e0\u52a0\u7ec6\u80de\u7c07\u8f6e\u5ed3\u4e0e\u591a\u901a\u9053\u8367\u5149\u67d3\u8272\u533a\u57df\u6765\u8bc6\u522b\u7ec6\u80de\u7c7b\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7ec6\u80de\u7c07\u5206\u7c7b\u548c\u8868\u578b\u8bc6\u522b\u65b9\u9762\u5747\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\u548cViT\u6a21\u578b\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6846\u67b6\u6709\u6548\u5206\u6790\u6d41\u5f0f\u7ec6\u80de\u672f\u4e2d\u7684\u5faa\u73af\u8840\u7ec6\u80de\u7c07\u56fe\u50cf\uff0c\u5229\u7528\u660e\u573a\u548c\u8367\u5149\u6570\u636e\uff0c\u5177\u6709\u6269\u5c55\u5230\u5206\u6790\u514d\u75ab\u7ec6\u80de\u548c\u80bf\u7624\u7ec6\u80de\u7c07\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17719", "abs": "https://arxiv.org/abs/2510.17719", "authors": ["Zhiqiang Teng", "Beibei Lin", "Tingting Chen", "Zifeng Yuan", "Xuanyi Li", "Xuanyu Zhang", "Shunli Zhang"], "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions", "comment": null, "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.", "AI": {"tldr": "RaindropGS\u662f\u4e00\u4e2a\u8bc4\u4f303D\u9ad8\u65af\u6e85\u5c04\u5728\u96e8\u6ef4\u6761\u4ef6\u4e0b\u6027\u80fd\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65e0\u7ea6\u675f\u96e8\u6ef4\u56fe\u50cf\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6d41\u7a0b\u7ec4\u4ef6\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u4f7f\u7528\u5df2\u77e5\u76f8\u673a\u59ff\u6001\u7684\u5408\u6210\u96e8\u6ef4\u56fe\u50cf\u8bc4\u4f303DGS\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u96e8\u6ef4\u4f1a\u5e72\u6270\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u70b9\u4e91\u521d\u59cb\u5316\uff0c\u4e14\u5408\u6210\u4e0e\u771f\u5b9e\u96e8\u6ef4\u5b58\u5728\u663e\u8457\u9886\u57df\u5dee\u8ddd\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u5bf9\u9f50\u56fe\u50cf\u96c6\uff08\u96e8\u6ef4\u805a\u7126\u3001\u80cc\u666f\u805a\u7126\u548c\u65e0\u96e8\u5730\u9762\u5b9e\u51b5\uff09\u7684\u771f\u5b9e\u4e16\u754c\u96e8\u6ef4\u91cd\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u5b8c\u6574\u8bc4\u4f30\u6d41\u7a0b\uff1a\u6570\u636e\u51c6\u5907\u3001\u6570\u636e\u5904\u7406\u548c\u96e8\u6ef4\u611f\u77e53DGS\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u67093DGS\u65b9\u6cd5\u5728\u65e0\u7ea6\u675f\u96e8\u6ef4\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u9650\u5236\uff0c\u4ee5\u53ca\u76f8\u673a\u7126\u70b9\u4f4d\u7f6e\u3001\u4e0d\u51c6\u786e\u59ff\u6001\u548c\u70b9\u4e91\u521d\u59cb\u5316\u5bf9\u91cd\u5efa\u7684\u5e72\u6270\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u5728\u96e8\u6ef4\u6761\u4ef6\u4e0b\u66f4\u9c81\u68d2\u76843DGS\u65b9\u6cd5\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002"}}
{"id": "2510.17722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17722", "abs": "https://arxiv.org/abs/2510.17722", "authors": ["Yaning Pan", "Zekun Wang", "Qianqian Xie", "Yongqian Wen", "Yuanxing Zhang", "Guohui Zhang", "Haoxuan Hu", "Zhiyu Pan", "Yibing Huang", "Zhidong Gan", "Yonghong Lin", "An Ping", "Tianhao Peng", "Jiaheng Liu"], "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues", "comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench", "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86MT-Video-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b987\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6db5\u76d66\u4e2a\u6838\u5fc3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4ec5\u9650\u4e8e\u5355\u8f6e\u95ee\u7b54\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaMT-Video-Bench\u57fa\u51c6\uff0c\u4e3b\u8981\u8bc4\u4f306\u4e2a\u5173\u6ce8\u611f\u77e5\u6027\u548c\u4ea4\u4e92\u6027\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u6db5\u76d6987\u4e2a\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5e76\u4e0e\u73b0\u5b9e\u5e94\u7528\u4e25\u683c\u5bf9\u9f50\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u4e86\u5404\u79cd\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90MLLMs\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5904\u7406\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u65f6\u7684\u663e\u8457\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5c06\u516c\u5f00\u53ef\u7528\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\uff0c\u5e2e\u52a9\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.17724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17724", "abs": "https://arxiv.org/abs/2510.17724", "authors": ["Matheus Ramos Parracho"], "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization", "comment": "Undergraduate thesis (preprint)---submitted to Escola Polit\\'ecnica,\n  Universidade Federal do Rio de Janeiro (POLI/UFRJ). The final version will\n  include official signatures and defense approval", "summary": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u7b7e\u540d\u4f2a\u9020\u68c0\u6d4b\u7684\u7279\u5f81\u5b66\u4e60\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8\u63d0\u9ad8\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u539f\u59cb\u7b7e\u540d\u56fe\u50cf\u548c\u58f3\u9884\u5904\u7406\u4e24\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u7b7e\u540d\u9a8c\u8bc1\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u79bb\u7ebf\u7b7e\u540d\u9a8c\u8bc1\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u7b14\u8ff9\u98ce\u683c\u548c\u91c7\u96c6\u534f\u8bae\u7684\u5dee\u5f02\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\uff08CEDAR\u3001ICDAR\u548cGPDS Synthetic\uff09\uff0c\u5f00\u53d1\u4e86\u4e24\u4e2a\u5b9e\u9a8c\u6d41\u7a0b\uff1a\u4e00\u4e2a\u57fa\u4e8e\u539f\u59cb\u7b7e\u540d\u56fe\u50cf\uff0c\u53e6\u4e00\u4e2a\u91c7\u7528\u58f3\u9884\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u539f\u59cb\u56fe\u50cf\u6a21\u578b\u5728\u8de8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u800c\u57fa\u4e8e\u58f3\u9884\u5904\u7406\u7684\u65b9\u6cd5\u663e\u793a\u51fa\u672a\u6765\u6539\u8fdb\u4ee5\u5b9e\u73b0\u7a33\u5065\u8de8\u57df\u7b7e\u540d\u9a8c\u8bc1\u7684\u6f5c\u529b\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u539f\u59cb\u56fe\u50cf\u6a21\u578b\u6027\u80fd\u66f4\u597d\uff0c\u4f46\u58f3\u9884\u5904\u7406\u65b9\u6cd5\u5177\u6709\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u8de8\u57df\u7b7e\u540d\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.17731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17731", "abs": "https://arxiv.org/abs/2510.17731", "authors": ["Aaron Appelle", "Jerome P. Lynch"], "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?", "comment": "Appeared in the ICML 2025 Workshop on Building Physically Plausible\n  World Models, July 2025, https://physical-world-modeling.github.io/", "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u6a21\u578b\u80fd\u5426\u751f\u6210\u62e5\u6324\u516c\u5171\u573a\u666f\u4e2d\u771f\u5b9e\u7684\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f", "motivation": "\u63a2\u7d22\u9ad8\u6027\u80fdI2V\u6a21\u578b\u5728\u5927\u578b\u89c6\u9891\u6570\u636e\u96c6\u8bad\u7ec3\u540e\u662f\u5426\u5177\u5907\u751f\u6210\u771f\u5b9e\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\u7684\u80fd\u529b", "method": "\u901a\u8fc7\u5728\u884c\u4eba\u8f68\u8ff9\u57fa\u51c6\u4e2d\u63d0\u53d6\u5173\u952e\u5e27\u6765\u6761\u4ef6\u5316I2V\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u884c\u4eba\u52a8\u529b\u5b66\u5b9a\u91cf\u6307\u6807\u8bc4\u4f30\u5176\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c", "conclusion": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86I2V\u6a21\u578b\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b"}}
{"id": "2510.17739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17739", "abs": "https://arxiv.org/abs/2510.17739", "authors": ["Timur Ismagilov", "Shakaiba Majeed", "Michael Milford", "Tan Viet Tuyen Nguyen", "Sarvapali D. Ramchurn", "Shoaib Ehsan"], "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition", "comment": "13 pages", "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63cf\u8ff0\u7b26\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u5c06\u591a\u4e2a\u53c2\u8003\u63cf\u8ff0\u7b26\u8054\u5408\u5efa\u6a21\u4e3a\u57fa\u8868\u793a\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6295\u5f71\u7684\u6b8b\u5dee\u5339\u914d\uff0c\u7528\u4e8e\u591a\u53c2\u8003\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u591a\u53c2\u8003\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4e2d\uff0c\u9762\u5bf9\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u53c2\u8003\u96c6\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u591a\u6837\u6027\u548c\u6a21\u578b\u590d\u6742\u6027\u589e\u52a0\u65f6\u5e26\u6765\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u77e9\u9635\u5206\u89e3\u5c06\u591a\u4e2a\u53c2\u8003\u63cf\u8ff0\u7b26\u5206\u89e3\u4e3a\u57fa\u8868\u793a\uff0c\u901a\u8fc7\u6295\u5f71\u6b8b\u5dee\u5339\u914d\u8fdb\u884c\u5730\u70b9\u8bc6\u522b\uff0c\u65e0\u9700\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u5916\u89c2\u6570\u636e\u4e0a\uff0cRecall@1\u6bd4\u5355\u53c2\u8003\u65b9\u6cd5\u63d0\u5347\u7ea618%\uff0c\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u63d0\u5347\u7ea65%\uff0c\u4f18\u4e8e\u73b0\u6709\u591a\u53c2\u8003\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\uff0c\u5728\u5e94\u5bf9\u5916\u89c2\u548c\u89c6\u89d2\u53d8\u5316\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.17773", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17773", "abs": "https://arxiv.org/abs/2510.17773", "authors": ["Md. Enamul Atiq", "Shaikh Anowarul Fattah"], "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion", "comment": "15 pages, 7 Figures, 3 Tables", "summary": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u7f16\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u75c5\u7076\u5206\u5272\u548c\u4e34\u5e8a\u5143\u6570\u636e\u6765\u63d0\u5347\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u76ae\u80a4\u764c\u65e9\u671f\u68c0\u6d4b\u5bf9\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\"\u9ed1\u7bb1\"\u95ee\u9898\uff0c\u4e14\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u5b58\u5728\u7c7b\u5185\u5dee\u5f02\u5927\u3001\u7c7b\u95f4\u5dee\u5f02\u5c0f\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5e26\u53cc\u6ce8\u610f\u529b\u95e8\u548c\u7a7a\u6d1e\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\u7684Deep-UNet\u8fdb\u884c\u75c5\u7076\u5206\u5272\uff0c\u7136\u540e\u91c7\u7528\u53ccDenseNet201\u7f16\u7801\u5668\u5206\u522b\u5904\u7406\u539f\u59cb\u56fe\u50cf\u548c\u5206\u5272\u75c5\u7076\uff0c\u901a\u8fc7\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u7279\u5f81\uff0c\u5e76\u96c6\u6210\u57fa\u4e8etransformer\u7684\u60a3\u8005\u5143\u6570\u636e\u6a21\u5757\u3002", "result": "\u5728HAM10000\u3001ISIC 2018\u548c2019\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548c\u5e73\u5747AUC\uff0cGrad-CAM\u70ed\u56fe\u9a8c\u8bc1\u4e86\u6a21\u578b\u5173\u6ce8\u75c5\u7076\u533a\u57df\u800c\u975e\u80cc\u666f\u4f2a\u7279\u5f81\u3002", "conclusion": "\u5c06\u7cbe\u786e\u7684\u75c5\u7076\u5206\u5272\u3001\u4e34\u5e8a\u6570\u636e\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u6784\u5efa\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u76ae\u80a4\u764c\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2510.17777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17777", "abs": "https://arxiv.org/abs/2510.17777", "authors": ["Samir Khaki", "Junxian Guo", "Jiaming Tang", "Shang Yang", "Yukang Chen", "Konstantinos N. Plataniotis", "Yao Lu", "Song Han", "Zhijian Liu"], "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference", "comment": null, "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.", "AI": {"tldr": "SparseVILA\u662f\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u9884\u586b\u5145\u9636\u6bb5\u526a\u679d\u5197\u4f59\u89c6\u89c9token\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u4ec5\u68c0\u7d22\u67e5\u8be2\u76f8\u5173token\uff0c\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\u800c\u4e0d\u635f\u5931\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u6027\u53d7\u5230\u89c6\u89c9token\u6570\u91cf\u589e\u957f\u7684\u5236\u7ea6\uff0c\u8fd9\u4e9btoken\u4e3b\u5bfc\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7684\u89c6\u89c9\u7a00\u758f\u5316\u65b9\u6cd5\uff1a\u9884\u586b\u5145\u9636\u6bb5\u526a\u679d\u5197\u4f59\u89c6\u89c9token\uff0c\u89e3\u7801\u9636\u6bb5\u68c0\u7d22\u67e5\u8be2\u76f8\u5173token\uff1b\u57fa\u4e8eAWQ\u4f18\u5316\u7684\u63a8\u7406\u6d41\u6c34\u7ebf\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u89c6\u9891\u4efb\u52a1\u4e0a\u5b9e\u73b04.0\u500d\u9884\u586b\u5145\u52a0\u901f\u30012.5\u500d\u89e3\u7801\u52a0\u901f\u548c2.6\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u5728\u6587\u6863\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u67e5\u8be2\u65e0\u5173\u526a\u679d\u548c\u67e5\u8be2\u611f\u77e5\u68c0\u7d22\uff0cSparseVILA\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u67b6\u6784\u65e0\u5173\u7684\u52a0\u901f\u6846\u67b6\u3002"}}
{"id": "2510.17803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17803", "abs": "https://arxiv.org/abs/2510.17803", "authors": ["Zixin Yin", "Ling-Hao Chen", "Lionel Ni", "Xili Dai"], "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing", "comment": "SIGGRAPH Asia 2025", "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.", "AI": {"tldr": "\u63d0\u51faConsistEdit\u65b9\u6cd5\uff0c\u9488\u5bf9MM-DiT\u67b6\u6784\u8bbe\u8ba1\u6ce8\u610f\u529b\u63a7\u5236\u673a\u5236\uff0c\u89e3\u51b3\u73b0\u6709\u8bad\u7ec3\u81ea\u7531\u7f16\u8f91\u65b9\u6cd5\u5728\u7f16\u8f91\u5f3a\u5ea6\u4e0e\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u652f\u6301\u591a\u8f6e\u548c\u89c6\u9891\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u81ea\u7531\u6ce8\u610f\u529b\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5f3a\u7f16\u8f91\u80fd\u529b\u548c\u6e90\u56fe\u50cf\u4e00\u81f4\u6027\uff0c\u5728\u591a\u8f6e\u548c\u89c6\u9891\u7f16\u8f91\u4e2d\u89c6\u89c9\u8bef\u5dee\u4f1a\u7d2f\u79ef\uff0c\u4e14\u5168\u5c40\u4e00\u81f4\u6027\u9650\u5236\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u57fa\u4e8eMM-DiT\u67b6\u6784\u5206\u6790\uff0c\u63d0\u51faConsistEdit\u65b9\u6cd5\uff0c\u5305\u542b\u89c6\u89c9\u4e13\u7528\u6ce8\u610f\u529b\u63a7\u5236\u3001\u63a9\u7801\u5f15\u5bfc\u9884\u6ce8\u610f\u529b\u878d\u5408\u3001\u4ee5\u53ca\u67e5\u8be2-\u952e-\u503c\u4ee4\u724c\u7684\u5dee\u5f02\u5316\u64cd\u4f5c\u3002", "result": "\u5728\u5e7f\u6cdb\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u7ed3\u6784\u4e00\u81f4\u548c\u4e0d\u4e00\u81f4\u573a\u666f\uff0c\u662f\u9996\u4e2a\u65e0\u9700\u624b\u5de5\u8bbe\u8ba1\u5373\u53ef\u5728\u6240\u6709\u63a8\u7406\u6b65\u9aa4\u548c\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u7f16\u8f91\u7684\u65b9\u6cd5\u3002", "conclusion": "ConsistEdit\u663e\u8457\u63d0\u5347\u4e86\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\uff0c\u652f\u6301\u7a33\u5065\u7684\u591a\u8f6e\u548c\u591a\u533a\u57df\u7f16\u8f91\uff0c\u5e76\u80fd\u6e10\u8fdb\u8c03\u6574\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002"}}
