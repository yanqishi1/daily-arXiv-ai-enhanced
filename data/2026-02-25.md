<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 86]
- [cs.CL](#cs.CL) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport](https://arxiv.org/abs/2602.20205)
*Xiwen Chen,Wenhui Zhu,Gen Li,Xuanzhao Dong,Yujian Xiong,Hao Wang,Peijie Qiu,Qingquan Song,Zhipeng Wang,Shao Tang,Yalin Wang,Abolfazl Razi*

Main category: cs.CV

TL;DR: OTPrune：基于最优传输的免训练视觉令牌剪枝框架，通过分布对齐减少多模态大语言模型推理成本


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型推理成本高，现有视觉令牌剪枝方法忽略了视觉表示的分布结构，需要更有效的剪枝方法

Method: 将剪枝问题形式化为通过最优传输进行分布对齐，最小化完整令牌和剪枝后令牌分布之间的2-Wasserstein距离，并推导出可处理的子模目标函数

Result: 在广泛基准测试中，OTPrune相比最先进方法实现了更优的性能-效率权衡

Conclusion: OTPrune通过分布对齐实现了稳定且语义忠实的视觉令牌剪枝，为多模态大语言模型的高效推理提供了理论基础和实用框架

Abstract: Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.

</details>


### [2] [De-rendering, Reasoning, and Repairing Charts with Vision-Language Models](https://arxiv.org/abs/2602.20291)
*Valentin Bonas,Martin Sinnona,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: 提出一个结合图表解构、自动分析和迭代改进的框架，为可视化设计提供可操作的反馈，通过LLM驱动的推荐系统提升图表质量和用户的可视化素养。


<details>
  <summary>Details</summary>
Motivation: 数据可视化在科学传播、新闻和日常决策中至关重要，但常存在错误。基于规则的检查工具缺乏上下文理解，而通用LLM在可视化质量评估上不可靠，需要更智能的反馈系统。

Method: 开发了一个框架，包含图表解构（从图像重建图表结构）、视觉语言推理分析设计缺陷、基于可视化研究原则提出具体修改建议，并支持用户选择性应用改进和重新渲染的迭代循环。

Result: 在Chart2Code基准的1000个图表上评估，系统生成了10,452条设计建议，聚类为10个连贯类别（如轴格式化、颜色可访问性、图例一致性），验证了LLM驱动推荐系统的有效性。

Conclusion: LLM驱动的推荐系统能够为可视化设计提供结构化、基于原则的反馈，为开发更智能、更易用的创作工具开辟了新途径，同时促进可视化素养的提升。

Abstract: Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.

</details>


### [3] [N4MC: Neural 4D Mesh Compression](https://arxiv.org/abs/2602.20312)
*Guodong Chen,Huanshuo Dong,Mallesham Dasari*

Main category: cs.CV

TL;DR: N4MC是首个4D神经压缩框架，通过利用时间冗余高效压缩时变网格序列，采用类似2D视频编解码器的帧间压缩思想，在长网格序列中学习运动补偿。


<details>
  <summary>Details</summary>
Motivation: 现有神经网格压缩方法将每个网格帧独立处理，忽略了时间冗余。受2D视频编解码器中帧间压缩的启发，需要开发能够利用网格序列时间相关性的压缩方法。

Method: 将连续的不规则网格帧转换为规则的4D张量作为统一紧凑表示；使用自解码器压缩张量以捕获时空相关性；引入基于Transformer的插值模型，通过跟踪体积中心的潜在嵌入预测中间网格帧，消除运动模糊。

Result: N4MC在率失真性能上优于现有最先进方法，同时能够实现4D网格序列的实时解码。

Conclusion: N4MC成功实现了首个4D神经网格压缩框架，通过有效利用时间冗余显著提升了压缩效率，为时变网格序列的实时处理提供了可行方案。

Abstract: We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.

</details>


### [4] [GSNR: Graph Smooth Null-Space Representation for Inverse Problems](https://arxiv.org/abs/2602.20328)
*Romario Gualdrón-Hurtado,Roman Jacome,Rafael S. Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GSNR（图平滑零空间表示）的方法，通过将图拉普拉斯结构引入逆问题的零空间分量，改善图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 成像逆问题通常是病态的，存在无限多个与测量一致的解。传统的图像先验（如稀疏性、平滑性或得分函数）只约束一般图像流形，而不约束零空间分量，可能导致重建结果有偏差。因此需要将有意义的零空间信息纳入重建框架。

Method: 提出Graph-Smooth Null-Space Representation (GSNR)方法：基于图的平滑图像表示，通过图拉普拉斯构造零限制拉普拉斯矩阵，编码零空间信号中相邻像素的相似性。设计从p个最平滑的谱图模式（最低图频率）到低维投影矩阵，仅对不可见分量施加结构。

Result: GSNR方法具有三个重要优势：1）通过零空间图正则化改善收敛性；2）更好的覆盖度（p个模式捕获的零空间方差）；3）高可预测性（这些模式从测量中推断的能力）。在图像去模糊、压缩感知、去马赛克和超分辨率四个场景中，GSNR集成到PnP、DIP和扩散求解器中，相比基线公式PSNR提升高达4.3dB，相比端到端学习模型提升高达1dB。

Conclusion: GSNR通过将图平滑结构专门引入逆问题的零空间分量，有效改善了图像重建质量，在多种逆问题求解器中都能带来一致的性能提升。

Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.

</details>


### [5] [Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking](https://arxiv.org/abs/2602.20330)
*Jingcheng Yang,Tianhu Xiong,Shengyi Qian,Klara Nahrstedt,Mingyuan Wu*

Main category: cs.CV

TL;DR: 提出了首个用于视觉语言模型透明电路追踪的框架，通过跨模态分析方法揭示了VLM如何分层整合视觉和语义概念，并证明了特定视觉特征电路在数学推理和跨模态关联中的作用。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然功能强大，但仍然是黑盒系统，缺乏透明度。为了系统分析多模态推理过程，需要开发能够揭示VLM内部工作机制的透明化分析框架。

Method: 使用transcoders、attribution graphs和基于注意力的方法，构建了透明电路追踪框架。通过特征引导和电路修补技术验证电路的功能性和因果性。

Result: 发现VLM能够分层整合视觉和语义概念，特定的视觉特征电路可以处理数学推理并支持跨模态关联。验证表明这些电路具有因果性和可控性。

Conclusion: 该框架为视觉语言模型提供了首个透明电路追踪方法，揭示了多模态推理的内部机制，为开发更可解释和可靠的VLM奠定了基础。

Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.

</details>


### [6] [Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques](https://arxiv.org/abs/2602.20342)
*Christos Maikos,Georgios Angelidis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 提出一个端到端无人机视频流实时3D重建系统，结合3D高斯泼溅技术，实现低延迟的AR/VR可视化应用


<details>
  <summary>Details</summary>
Motivation: 无人机在实时感知应用中广泛使用，3D高斯泼溅技术展现出实时神经渲染潜力，但将其集成到端到端无人机重建系统中仍待探索

Method: 结合RTMP流媒体直播视频采集、传感器融合同步、相机姿态估计和3DGS优化，构建连续模型更新和低延迟部署的架构

Result: 相比NeRF方法，在保持4-7%离线参考质量的同时，显著提升渲染性能并大幅降低端到端延迟

Conclusion: 该系统适用于实时、可扩展的空中平台增强感知应用，在视觉保真度和渲染性能方面表现优异

Abstract: In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.

</details>


### [7] [3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism](https://arxiv.org/abs/2602.20354)
*Bhavik Chandna,Kelsey R. Allen*

Main category: cs.CV

TL;DR: 3DSPA是一个自动评估视频真实性的框架，通过3D时空点自编码器结合3D点轨迹、深度线索和语义特征，无需参考视频即可评估生成视频的真实性、时间一致性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前AI视频生成发展迅速，但评估生成视频的真实性仍然主要依赖人工标注或有限范围的评估数据集，需要一种自动化、全面的评估方法来捕捉语义和连贯的3D结构。

Method: 提出3DSPA方法，这是一个3D时空点自编码器，将3D点轨迹、深度线索和DINO语义特征整合到统一的视频表示中，建模物体运动和场景内容，从而评估视频的真实性。

Result: 实验表明3DSPA能可靠识别违反物理规律的视频，对运动伪影更敏感，在多个数据集上与人类对视频质量和真实性的判断更一致。

Conclusion: 将3D语义信息融入基于轨迹的表示，为生成视频模型的基准测试提供了更强的基础，并能隐式捕捉物理规则违反情况，代码和预训练模型权重将开源。

Abstract: AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.

</details>


### [8] [SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images](https://arxiv.org/abs/2602.20412)
*Aayush Dhakal,Subash Khanal,Srikumar Sastry,Jacob Arndt,Philipe Ambrozio Dias,Dalton Lunga,Nathan Jacobs*

Main category: cs.CV

TL;DR: 提出SimLBR框架，通过潜在混合正则化学习紧致的真实图像决策边界，将伪造图像视为汇类，显著提升跨生成器泛化能力，训练速度比现有方法快几个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的伪造图像检测方法容易过拟合训练数据，在分布偏移的困难测试集上表现灾难性失败。需要更原则性的方法来学习真实图像的紧致决策边界。

Method: 提出SimLBR框架，采用潜在混合正则化(LBR)学习真实图像的紧致决策边界，将伪造图像类别视为汇类。强调面向可靠性的评估，引入风险调整指标和最坏情况估计。

Result: 在Chameleon基准测试上，SimLBR显著提升跨生成器泛化能力，达到+24.85%准确率和+69.62%召回率。训练速度比现有方法快几个数量级。

Conclusion: SimLBR提供了一种简单高效的伪造图像检测框架，通过潜在混合正则化学习紧致决策边界，显著提升泛化能力和训练效率，同时强调可靠性导向的评估方法的重要性。

Abstract: The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\% accuracy and +69.62\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.

</details>


### [9] [gQIR: Generative Quanta Image Reconstruction](https://arxiv.org/abs/2602.20417)
*Aryan Garg,Sizhuo Ma,Mohit Gupta*

Main category: cs.CV

TL;DR: 该研究提出了一种将大型文本到图像潜在扩散模型适配到光子受限量子爆发成像领域的方法，通过处理稀疏、噪声的二进制光子探测数据，实现高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 单光子雪崩二极管传感器在传统相机失效的极端低光条件下有潜力实现高质量成像，但原始量子帧包含稀疏、噪声的二进制光子探测数据，需要处理对齐、去噪和去马赛克等挑战，而现有标准恢复流程或现代生成模型无法处理这种噪声统计特性。

Method: 该方法将大型文本到图像潜在扩散模型适配到光子受限的量子爆发成像领域，利用互联网规模扩散模型的结构和语义先验，同时引入处理伯努利光子统计的机制，通过潜在空间恢复与爆发级时空推理相结合。

Result: 该方法在合成基准测试和新的真实世界数据集（包括首个彩色SPAD爆发数据集和具有挑战性的Deforming视频基准）上进行了评估，在所有设置中都显著提高了感知质量，超越了经典和现代基于学习的基线方法。

Conclusion: 该研究展示了将大型生成先验适配到极端光子受限传感领域的潜力，能够产生既保持光度保真度又具有良好感知质量的图像重建，即使在高速运动条件下也是如此。

Abstract: Capturing high-quality images from only a few detected photons is a fundamental challenge in computational imaging. Single-photon avalanche diode (SPAD) sensors promise high-quality imaging in regimes where conventional cameras fail, but raw \emph{quanta frames} contain only sparse, noisy, binary photon detections. Recovering a coherent image from a burst of such frames requires handling alignment, denoising, and demosaicing (for color) under noise statistics far outside those assumed by standard restoration pipelines or modern generative models. We present an approach that adapts large text-to-image latent diffusion models to the photon-limited domain of quanta burst imaging. Our method leverages the structural and semantic priors of internet-scale diffusion models while introducing mechanisms to handle Bernoulli photon statistics. By integrating latent-space restoration with burst-level spatio-temporal reasoning, our approach produces reconstructions that are both photometrically faithful and perceptually pleasing, even under high-speed motion. We evaluate the method on synthetic benchmarks and new real-world datasets, including the first color SPAD burst dataset and a challenging \textit{Deforming (XD)} video benchmark. Across all settings, the approach substantially improves perceptual quality over classical and modern learning-based baselines, demonstrating the promise of adapting large generative priors to extreme photon-limited sensing. Code at \href{https://github.com/Aryan-Garg/gQIR}{https://github.com/Aryan-Garg/gQIR}.

</details>


### [10] [MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation](https://arxiv.org/abs/2602.20423)
*Taha Koleilat,Hojat Asgariandehkordi,Omid Nejati Manzari,Berardino Barile,Yiming Xiao,Hassan Rivaz*

Main category: cs.CV

TL;DR: MedCLIPSeg：基于CLIP的概率视觉语言模型，用于数据高效、不确定性感知的医学图像分割


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临标注数据有限、解剖特征模糊和域偏移等挑战。虽然CLIP等视觉语言模型提供强大的跨模态表示能力，但它们在密集、文本引导的医学图像分割方面的潜力尚未充分探索。

Method: 提出MedCLIPSeg框架，通过概率跨模态注意力机制利用补丁级CLIP嵌入，实现图像和文本标记的双向交互，并显式建模预测不确定性。同时采用软补丁级对比损失，促进跨不同文本提示的更细致语义学习。

Result: 在跨越5种成像模态和6个器官的16个数据集上进行广泛实验，MedCLIPSeg在准确性、效率和鲁棒性方面优于先前方法，同时提供可解释的不确定性图，突出分割结果的局部可靠性。

Conclusion: 这项工作展示了概率视觉语言建模在文本驱动医学图像分割中的潜力，为数据高效、不确定性感知的医学图像分析提供了新思路。

Abstract: Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.

</details>


### [11] [SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens](https://arxiv.org/abs/2602.20476)
*Anindita Ghosh,Vladislav Golyanik,Taku Komura,Philipp Slusallek,Christian Theobalt,Rishabh Dabral*

Main category: cs.CV

TL;DR: SceMoS是一个基于2D场景表示的场景感知3D人体运动合成框架，通过分离全局规划和局部执行，使用轻量级2D线索替代昂贵的3D监督，在效率和保真度之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 当前基于3D场景数据（如点云、体素占用网格）的方法计算成本高，需要同时学习高级规划和低级接触推理。本文旨在探索结构化2D场景表示能否作为全3D监督的有效替代方案，实现物理基础的运动合成。

Method: 提出SceMoS框架：1）基于文本条件的自回归全局运动规划器，使用DINOv2特征编码的鸟瞰图作为场景表示；2）几何基础的运动分词器，通过条件VQ-VAE训练，使用2D局部场景高度图，将表面物理直接嵌入离散词汇表。

Result: 在TRUMANS基准测试中达到最先进的运动真实性和接触准确性，场景编码的可训练参数数量减少超过50%，证明2D场景线索能有效支撑3D人-场景交互。

Conclusion: 结构化2D场景表示可以作为全3D监督的强大替代方案，通过分离全局规划和局部执行，使用轻量级2D线索实现物理基础的3D人体运动合成，在效率和保真度之间达到良好平衡。

Abstract: Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent ("walk to the couch") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.

</details>


### [12] [Path-Decoupled Hyperbolic Flow Matching for Few-Shot Adaptation](https://arxiv.org/abs/2602.20479)
*Lin Li,Ziqi Jiang,Gefan Ye,Zhenqi He,Jiahui Li,Jun Xiao,Kwang-Ting Cheng,Long Chen*

Main category: cs.CV

TL;DR: HFM提出双曲流匹配方法，通过洛伦兹流形的指数扩展解决欧几里得流匹配中的路径纠缠问题，在跨模态少样本适应任务中实现新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于欧几里得几何的流匹配方法存在根本性局限，平坦几何的多项式体积增长无法适应多样化的特征分布，导致严重的路径纠缠问题。

Method: 提出路径解耦的双曲流匹配(HFM)：1)向心双曲对齐：通过锚定文本根构建向心层次结构，将视觉叶节点推向边界以初始化有序流；2)路径解耦目标：作为"语义护栏"，通过逐步监督将轨迹严格限制在孤立的类特定测地线走廊内；3)自适应直径停止机制：基于内在语义尺度防止过度传输到拥挤的源点。

Result: 在11个基准测试上的广泛消融实验表明，HFM建立了新的最先进性能，一致优于其欧几里得对应方法。

Conclusion: HFM通过利用双曲几何的指数扩展特性有效解决了跨模态少样本适应中的路径纠缠问题，为视觉-语义对齐提供了更优的流匹配框架。

Abstract: Recent advances in cross-modal few-shot adaptation treat visual-semantic alignment as a continuous feature transport problem via Flow Matching (FM). However, we argue that Euclidean-based FM overlooks fundamental limitations of flat geometry, where polynomial volume growth fails to accommodate diverse feature distributions, leading to severe path entanglement. To this end, we propose path-decoupled Hyperbolic Flow Matching (HFM), leveraging the Lorentz manifold's exponential expansion for trajectory decoupling. HFM structures the transport via two key designs: 1) Centripetal hyperbolic alignment: It constructs a centripetal hierarchy by anchoring textual roots, which pushes visual leaves to the boundary to initialize orderly flows. 2) Path-decoupled objective: It acts as a ``semantic guardrail'' rigidly confining trajectories within isolated class-specific geodesic corridors via step-wise supervision. Furthermore, we devise an adaptive diameter-based stopping to prevent over-transportation into the crowded origin based on the intrinsic semantic scale. Extensive ablations on 11 benchmarks have shown that HFM establishes a new state-of-the-art, consistently outperforming its Euclidean counterparts. Our codes and models will be released.

</details>


### [13] [Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching](https://arxiv.org/abs/2602.20496)
*Jintu Zheng,Qizhe Liu,HuangXin Xu,Zhuojie Chen*

Main category: cs.CV

TL;DR: 该论文提出PipStereo，一种面向边缘设备的实时高精度立体匹配方法，通过渐进迭代剪枝、单目先验迁移框架和硬件优化的FlashGRU算子，在保持大模型精度的同时实现边缘实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有迭代式立体匹配方法依赖RNN，阻碍了在边缘设备上的部署。研究发现视差更新存在空间稀疏性和时间冗余性，这为优化提供了机会。

Method: 1. 渐进迭代剪枝策略：抑制冗余更新步骤，将递归计算压缩为近似单次推理；2. 协作式单目先验迁移框架：隐式嵌入深度先验，无需专用单目编码器；3. FlashGRU：利用结构化稀疏和I/O感知设计的硬件感知RNN算子。

Result: 在NVIDIA Jetson Orin NX上处理320×640帧仅需75ms（FP16），RTX 4090上仅需19ms，精度与大型迭代模型相当，泛化能力和精度远超现有实时方法。FlashGRU在2K分辨率下相比原生ConvGRUs实现7.28倍加速、76.6%内存峰值降低和80.9%全局内存请求减少。

Conclusion: PipStereo实现了在边缘硬件上的实时高保真立体匹配，解决了迭代式立体匹配的边缘部署难题，在保持高精度的同时显著提升了推理效率。

Abstract: While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\times$ speedup, 76.6\% memory peak reduction and 80.9\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.

</details>


### [14] [LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration](https://arxiv.org/abs/2602.20497)
*Peiliang Cai,Jiacheng Liu,Haowen Xu,Xinyu Wang,Chang Zou,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出LESA预测器框架，通过可学习的阶段感知特征预测加速扩散模型，在保持生成质量的同时实现5-6倍加速


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了显著成功，但扩散变换器（DiTs）的高计算需求限制了实际部署。现有的特征缓存方法难以适应扩散过程的复杂阶段依赖性动态，导致质量下降且无法保持与标准去噪过程的一致性

Method: 提出基于两阶段训练的LEarnable Stage-Aware (LESA)预测器框架，利用Kolmogorov-Arnold Network (KAN)从数据中准确学习时间特征映射，并引入多阶段、多专家架构，为不同噪声级别阶段分配专门的预测器

Result: 在FLUX.1-dev上实现5.00倍加速且质量下降仅1.0%；在Qwen-Image上实现6.25倍加速且质量提升20.2%；在HunyuanVideo上实现5.00倍加速且PSNR提升24.7%。在文本到图像和文本到视频合成任务上均达到最先进性能

Conclusion: LESA框架通过可学习的阶段感知特征预测，在保持高保真生成的同时显著加速扩散模型，验证了该训练框架在不同模型上的有效性和泛化能力

Abstract: Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.

</details>


### [15] [Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models](https://arxiv.org/abs/2602.20501)
*Qing Zhang,Xuesong Li,Jing Zhang*

Main category: cs.CV

TL;DR: 通过融合DINO的几何原型和Flux的交互映射，在零样本训练下实现了与弱监督方法相竞争的affordance估计，证实了几何感知和交互感知是视觉基础模型中affordance理解的基本构建模块。


<details>
  <summary>Details</summary>
Motivation: 探索视觉系统真正理解affordance（可供性）的含义，认为这种理解依赖于两个互补能力：几何感知（识别支持交互的对象结构部分）和交互感知（建模智能体动作如何与这些部分互动）。

Method: 对视觉基础模型进行系统探测，发现DINO编码了部分级几何结构，而Flux等生成模型包含丰富的动词条件空间注意力映射作为隐式交互先验。通过以训练免费、零样本方式融合DINO的几何原型和Flux的交互映射来实现affordance估计。

Result: 几何感知和交互感知不仅是相关的，而且是affordance的可组合元素。通过简单融合这两种表示，在affordance估计上达到了与弱监督方法竞争的性能。

Conclusion: 几何感知和交互感知是视觉基础模型中affordance理解的基本构建模块，为感知如何支撑行动提供了机制性解释。

Abstract: What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.

</details>


### [16] [How Do Inpainting Artifacts Propagate to Language?](https://arxiv.org/abs/2602.20520)
*Pratham Yashwante,Davit Abrahamyan,Shresth Grover,Sukruth Rao*

Main category: cs.CV

TL;DR: 研究扩散修复引入的视觉伪影如何影响视觉语言模型的语言生成，通过两阶段诊断框架分析修复保真度与下游字幕质量的关系


<details>
  <summary>Details</summary>
Motivation: 研究扩散修复技术引入的视觉伪影如何影响视觉语言模型的语言生成能力，理解视觉重建质量与语言生成质量之间的关系

Method: 采用两阶段诊断框架：1) 对掩码图像区域进行扩散修复重建；2) 将原始图像和重建图像分别输入字幕生成模型进行对比分析。通过多个数据集分析重建保真度与字幕质量的关系，并分析中间视觉表示和注意力模式

Result: 发现像素级和感知重建指标与词汇和语义字幕性能之间存在一致关联。修复伪影导致模型行为的系统性、层次依赖性变化

Conclusion: 提供了一个实用的诊断框架，用于检验视觉重建质量如何影响多模态系统中的语言生成，揭示了修复伪影对模型行为的系统性影响

Abstract: We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.

</details>


### [17] [A Lightweight Vision-Language Fusion Framework for Predicting App Ratings from User Interfaces and Metadata](https://arxiv.org/abs/2602.20531)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级视觉-语言框架，结合移动应用UI布局和语义信息来预测应用评分，相比现有仅使用文本或UI特征的方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有应用评分预测模型主要局限于文本数据或用户界面特征，忽视了同时利用UI和语义信息的重要性。为了克服这些限制，需要开发能够整合多模态信息的框架。

Method: 提出轻量级视觉-语言框架：使用MobileNetV3提取UI布局的视觉特征，DistilBERT提取文本特征，通过带Swish激活函数的门控融合模块融合多模态特征，最后使用多层感知机回归头进行预测。

Result: 经过20个epoch训练后，模型达到MAE=0.1060，RMSE=0.1433，MSE=0.0205，R²=0.8529，Pearson相关系数=0.9251。消融研究验证了不同视觉和文本编码器组合的有效性。

Conclusion: 该轻量级框架为开发者和最终用户提供了有价值的见解，支持可持续的应用开发，并能在边缘设备上高效部署，实现了UI和语义信息的有效整合。

Abstract: App ratings are among the most significant indicators of the quality, usability, and overall user satisfaction of mobile applications. However, existing app rating prediction models are largely limited to textual data or user interface (UI) features, overlooking the importance of jointly leveraging UI and semantic information. To address these limitations, this study proposes a lightweight vision--language framework that integrates both mobile UI and semantic information for app rating prediction. The framework combines MobileNetV3 to extract visual features from UI layouts and DistilBERT to extract textual features. These multimodal features are fused through a gated fusion module with Swish activations, followed by a multilayer perceptron (MLP) regression head. The proposed model is evaluated using mean absolute error (MAE), root mean square error (RMSE), mean squared error (MSE), coefficient of determination (R2), and Pearson correlation. After training for 20 epochs, the model achieves an MAE of 0.1060, an RMSE of 0.1433, an MSE of 0.0205, an R2 of 0.8529, and a Pearson correlation of 0.9251. Extensive ablation studies further demonstrate the effectiveness of different combinations of visual and textual encoders. Overall, the proposed lightweight framework provides valuable insights for developers and end users, supports sustainable app development, and enables efficient deployment on edge devices.

</details>


### [18] [PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning](https://arxiv.org/abs/2602.20537)
*Xinyong Cai,Changbin Sun,Yong Wang,Hongyu Yang,Yuankai Wu*

Main category: cs.CV

TL;DR: PFGNet：一种基于像素级频率引导门控的全卷积时空预测框架，通过动态调制感受野来适应性地捕捉空间变化的运动模式，在保持高效率的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 纯卷积模型在时空预测学习中具有优越的效率和完全并行性，但其固定的感受野限制了适应性地捕捉空间变化运动模式的能力。需要一种能够动态调制感受野的方法来提升性能。

Method: 提出PFGNet框架，核心是外围频率门控（PFG）块，通过提取局部频谱线索，自适应地融合多尺度大核外围响应与可学习的中心抑制，形成空间自适应带通滤波器。所有大核都分解为可分离的1D卷积以保持效率。

Result: 在Moving MNIST、TaxiBJ、Human3.6M和KTH数据集上的实验表明，PFGNet以显著更少的参数和FLOPs实现了SOTA或接近SOTA的预测性能。

Conclusion: PFGNet通过像素级频率引导门控实现了结构感知的时空建模，无需循环或注意力机制，在保持高效率的同时达到了优异的预测性能。

Abstract: Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \times k$ followed by $k \times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.

</details>


### [19] [Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing](https://arxiv.org/abs/2602.20543)
*Subhra Jyoti Mandal,Lara Rachidi,Puneet Jain,Matthieu Duvinage,Sander W. Timmer*

Main category: cs.CV

TL;DR: 该研究开发了一个结合深度学习和视觉语言模型的多智能体框架，用于自动检测菌落形成单位，显著提高了制药行业微生物质量控制的自动化水平。


<details>
  <summary>Details</summary>
Motivation: 制药制造中的菌落形成单位检测是环境监测的关键环节，传统人工计数劳动密集且易出错，现有深度学习方法虽然准确但易受样本质量变化和伪影影响，且无法满足制药级的高精度要求。

Method: 开发了一个多智能体框架：首先使用视觉语言模型对培养皿进行分类（有效/无效）；对于有效样本，深度学习和视觉语言模型分别独立估计菌落数量；当预测结果在5%误差范围内一致时自动记录，否则交由专家审核；专家反馈用于持续重新训练和系统自我改进。

Result: 基于深度学习的自动化将人工验证减少了50%，结合视觉语言模型后增加到85%；定制Detectron2模型在超过50,000张培养皿图像上达到99%检测率、2%假阳性和0.6%假阴性；系统显著提升了操作效率并节省了成本。

Conclusion: 提出的系统为微生物质量控制提供了一个可扩展、可审计且符合监管要求的解决方案，推动了生物制药生产的自动化进程，在保证高精度的同时大幅减少了人工干预。

Abstract: Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.

</details>


### [20] [Robust Spiking Neural Networks Against Adversarial Attacks](https://arxiv.org/abs/2602.20548)
*Shuai Wang,Malu Zhang,Yulin Jiang,Dehao Zhang,Ammar Belatreche,Yu Liang,Yimeng Shan,Zijian Zhou,Yang Yang,Haizhou Li*

Main category: cs.CV

TL;DR: 本文提出阈值守卫优化方法，通过将神经元膜电位远离阈值和引入噪声尖峰神经元，显著提升直接训练SNN在对抗环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SNN因其生物合理性和尖峰驱动特性在神经形态计算中具有能效优势，但在复杂对抗环境中的鲁棒性受到显著限制。研究发现阈值邻近的尖峰神经元是限制直接训练SNN鲁棒性的关键因素。

Method: 提出阈值守卫优化方法：1) 在损失函数中加入额外约束，使神经元膜电位远离阈值，增加梯度稀疏性，降低对抗攻击的理论上限；2) 引入噪声尖峰神经元，将神经元发放机制从确定性转变为概率性，减少因微小扰动导致的状态翻转概率。

Result: 在标准对抗场景中的大量实验证明，该方法显著增强了直接训练SNN的鲁棒性。

Conclusion: 该研究为推进现实应用中更可靠、更安全的神经形态计算铺平了道路。

Abstract: Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. However, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs. We find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.

</details>


### [21] [The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation](https://arxiv.org/abs/2602.20550)
*Chengshuai Yang*

Main category: cs.CV

TL;DR: 论文证明所有成像前向模型都可用11个基本原语的DAG图近似表示，建立了物理世界模型的数学基础。


<details>
  <summary>Details</summary>
Motivation: 传统计算成像前向模型通常以特定模态的单一代码实现，缺乏统一的理论框架。本文旨在为各种成像模态建立统一的数学表示基础。

Method: 定义了广泛的成像算子类Cimg，证明其中所有算子都可用11个基本原语的类型化有向无环图近似表示。提供了构造算法，并证明该原语库是最小的。

Result: 在31个线性模态上验证了相对算子误差低于0.01，最多使用5个节点和深度5。为9个非线性模态提供了构造性DAG分解。

Conclusion: 建立了有限原语基定理，为物理世界模型框架提供了数学基础，实现了成像前向模型的统一表示。

Abstract: Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.

</details>


### [22] [CAD-Prompted SAM3: Geometry-Conditioned Instance Segmentation for Industrial Objects](https://arxiv.org/abs/2602.20551)
*Zhenran Tang,Rohan Nagabhirava,Changliu Liu*

Main category: cs.CV

TL;DR: 提出基于CAD模型提示的分割框架，使用CAD多视角渲染作为提示输入，解决语言和外观提示在工业场景中的局限性


<details>
  <summary>Details</summary>
Motivation: 语言提示受自然语言表达能力限制，难以描述不常见或特定实例的物体；图像提示主要编码外观特征（颜色、纹理），但在工业环境中，同一部件可能由不同材料、表面处理或颜色制成，导致外观提示不可靠。工业对象通常有精确的CAD模型定义其几何特征

Method: 基于SAM3构建CAD提示分割框架，使用CAD模型的多视角渲染作为提示输入，提供与表面外观无关的几何条件。通过仿真中网格渲染生成的合成数据进行训练，支持多样化的视角和场景上下文

Result: 实现了单阶段、CAD提示的掩码预测，将可提示分割扩展到无法仅通过语言或外观可靠描述的对象

Conclusion: CAD提示分割框架有效解决了工业环境中语言和外观提示的局限性，利用CAD模型的几何信息实现了更可靠的对象分割

Abstract: Verbal-prompted segmentation is inherently limited by the expressiveness of natural language and struggles with uncommon, instance-specific, or difficult-to-describe objects: scenarios frequently encountered in manufacturing and 3D printing environments. While image exemplars provide an alternative, they primarily encode appearance cues such as color and texture, which are often unrelated to a part's geometric identity. In industrial settings, a single component may be produced in different materials, finishes, or colors, making appearance-based prompting unreliable. In contrast, such objects are typically defined by precise CAD models that capture their canonical geometry. We propose a CAD-prompted segmentation framework built on SAM3 that uses canonical multi-view renderings of a CAD model as prompt input. The rendered views provide geometry-based conditioning independent of surface appearance. The model is trained using synthetic data generated from mesh renderings in simulation under diverse viewpoints and scene contexts. Our approach enables single-stage, CAD-prompted mask prediction, extending promptable segmentation to objects that cannot be robustly described by language or appearance alone.

</details>


### [23] [WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos](https://arxiv.org/abs/2602.20556)
*Hanhui Li,Xuan Huang,Wanquan Liu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang,Chenqiang Gao*

Main category: cs.CV

TL;DR: WildGHand是一个基于优化的框架，通过动态扰动解耦模块和扰动感知优化策略，在野外视频中实现自适应的3D高斯溅射，重建高质量手部化身。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部重建方法大多依赖受控环境数据，在真实世界存在手-物交互、极端姿态、光照变化和运动模糊等扰动时性能下降，需要能在野外视频中鲁棒重建手部化身的方法。

Method: 提出WildGHand框架，包含两个核心组件：1) 动态扰动解耦模块，将扰动表示为3D高斯属性在优化过程中的时变偏差；2) 扰动感知优化策略，生成每帧各向异性加权掩码指导优化。这两个组件共同在空间和时间维度上识别和抑制扰动。

Result: 在自建数据集和两个公共数据集上的实验表明，WildGHand达到最先进性能，相比基础模型在多个指标上显著提升（PSNR相对增益达15.8%，LPIPS相对减少达23.1%）。

Conclusion: WildGHand通过显式建模和抑制扰动，能够在野外视频中实现鲁棒的高保真3D手部化身重建，为真实世界应用提供了有效解决方案。

Abstract: Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\%$ relative gain in PSNR and a $23.1\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.

</details>


### [24] [AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents](https://arxiv.org/abs/2602.20569)
*Jiaqi Wu,Yuchen Zhou,Muduo Xu,Zisheng Liang,Simiao Ren,Jiayu Xue,Meige Yang,Siying Chen,Jingheng Huan*

Main category: cs.CV

TL;DR: AIForge-Doc是首个专门针对金融和表单文档中基于扩散模型的修复伪造的基准测试，包含像素级标注，揭示了现有检测器对AI伪造文档的严重盲区。


<details>
  <summary>Details</summary>
Motivation: 现有文档伪造数据集依赖传统数字编辑工具，而基于扩散模型的AI修复技术快速发展，导致现有检测器无法有效识别AI伪造的文档欺诈，存在严重安全漏洞。

Method: 使用Gemini 2.5 Flash Image和Ideogram v2 Edit两种AI修复API，在四个公开文档数据集（CORD、WildReceipt、SROIE、XFUND）上系统伪造数字字段，生成4,061张伪造图像，并提供像素级精确的篡改区域掩码。

Result: 现有检测方法性能大幅下降：TruFor的AUC从0.96降至0.751；DocTamper的AUC从0.98降至0.563，像素级IoU仅0.020；GPT-4o的准确率仅0.509，接近随机猜测水平。

Conclusion: AIForge-Doc代表了文档取证领域一个全新的、尚未解决的挑战，现有检测器对AI伪造的文档欺诈基本无效，需要开发新的检测方法。

Abstract: We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.

</details>


### [25] [An interactive enhanced driving dataset for autonomous driving](https://arxiv.org/abs/2602.20575)
*Haojie Feng,Peizhi Zhang,Mengjie Tian,Xinrui Zhang,Zhuoren Li,Junpeng Huang,Xiurong Wang,Junfan Zhu,Jianzhou Wang,Dongxiao Yin,Lu Xiong*

Main category: cs.CV

TL;DR: 该论文提出了交互增强驾驶数据集（IEDD），通过从自然驾驶数据中挖掘百万级交互片段，并构建严格对齐语义动作与结构化语言的BEV视频VQA数据集，以解决自动驾驶VLA模型发展中交互场景稀疏和多模态对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶向全自动化发展需要强大的交互能力，但现有数据中交互场景稀疏且多模态对齐不足，限制了视觉-语言-动作（VLA）模型的发展。

Method: 1. 开发可扩展的流水线从自然驾驶数据中基于交互轨迹挖掘百万级交互片段；2. 设计量化交互过程的指标；3. 构建IEDD-VQA数据集，生成合成鸟瞰图（BEV）视频，其中语义动作与结构化语言严格对齐。

Result: 提供了评估十个主流视觉语言模型（VLM）的基准结果，展示了该数据集在评估和微调自动驾驶模型推理能力方面的重用价值。

Conclusion: IEDD数据集为解决自动驾驶VLA模型发展中的交互场景稀疏和多模态对齐问题提供了有价值的资源，有助于提升自动驾驶模型的交互推理能力。

Abstract: The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.

</details>


### [26] [Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion](https://arxiv.org/abs/2602.20577)
*Jiaru Zhang,Manav Gagvani,Can Cui,Juntong Peng,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: MVLAD-AD是一种用于自动驾驶的新型掩码视觉-语言-动作扩散框架，通过离散动作标记化和几何感知嵌入学习，在保持语义可解释性的同时提高了规划效率和精度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM和VLM在自动驾驶应用中面临推理延迟、动作精度和可解释性挑战。现有自回归方法生成速度慢，而扩散方法使用通用语言标记缺乏明确的几何结构。

Method: 提出掩码视觉-语言-动作扩散模型，采用离散动作标记化策略从真实驾驶分布构建紧凑的动力学可行路径点码本，引入几何感知嵌入学习确保潜在空间嵌入近似物理几何度量，并使用动作优先解码策略优先生成轨迹。

Result: 在nuScenes及其衍生基准测试中，MVLAD-AD在规划精度上超越了最先进的自回归和扩散基线，同时实现了更高的效率和可解释性推理。

Conclusion: MVLAD-AD成功弥合了高效规划与语义可解释性之间的差距，为自动驾驶提供了一种既高效又具有高保真可解释推理的新框架。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.

</details>


### [27] [PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models](https://arxiv.org/abs/2602.20583)
*Wonyong Seo,Jaeho Moon,Jaehyup Lee,Soo Ye Kim,Munchurl Kim*

Main category: cs.CV

TL;DR: PropFly：无需配对视频数据集，利用预训练视频扩散模型实时生成监督信号，训练传播式视频编辑模型


<details>
  <summary>Details</summary>
Motivation: 传播式视频编辑需要大规模配对视频数据集进行训练，但这些数据集获取成本高且复杂。现有方法依赖现成或预计算的配对数据集，限制了模型的训练效率和可扩展性。

Method: 提出PropFly训练管道，利用预训练视频扩散模型实时生成监督信号。通过不同CFG尺度从中间噪声潜在空间生成"源"（低CFG）和"编辑"（高CFG）潜在对，源潜在提供视频结构信息，编辑潜在提供目标变换。使用Guidance-Modulated Flow Matching损失训练附加适配器，学习传播编辑。

Result: PropFly在多种视频编辑任务上显著优于现有最先进方法，能够生成高质量的编辑结果，同时学习到时间一致且动态的变换。

Conclusion: PropFly通过实时监督信号解决了传播式视频编辑的数据集依赖问题，提供了一种高效且可扩展的训练方法，在保持原始视频上下文的同时实现精确的用户控制。

Abstract: Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.

</details>


### [28] [Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change](https://arxiv.org/abs/2602.20584)
*Beverley Gorry,Tobias Fischer,Michael Milford,Alejandro Fontan*

Main category: cs.CV

TL;DR: 提出一种用于长期环境监测的联合SfM方法，通过跨会话对应关系直接重建单一连贯3D模型，解决了传统方法在时间间隔大、外观变化显著时的失效问题。


<details>
  <summary>Details</summary>
Motivation: 长期环境监测需要能够重建和对齐相隔数月或数年的3D模型。现有SfM流水线假设图像捕获时间相近且外观变化有限，在珊瑚礁调查等长期监测场景中，由于视觉和结构变化显著，传统方法会失效。

Method: 1. 在联合SfM重建中直接强制执行跨会话对应关系；2. 结合手工制作和学习视觉特征，在大的时间间隔下稳健建立对应关系；3. 通过视觉位置识别识别可能的跨会话图像对，限制昂贵的深度学习特征匹配计算成本。

Result: 在表现出显著真实世界变化的长期珊瑚礁数据集上评估，证明在现有方法无法产生连贯重建的情况下，能够实现跨会话的一致联合重建。计算成本降低且对齐鲁棒性提高。

Conclusion: 通过直接强制执行跨会话对应关系的联合SfM方法，解决了长期环境监测中传统SfM流水线的局限性，能够在相隔数年的图像捕获中重建单一连贯3D模型。

Abstract: Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.

</details>


### [29] [Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing](https://arxiv.org/abs/2602.20597)
*Yuejiao Su,Yi Wang,Lei Yao,Yawen Cui,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 提出InterFormer模型，通过动态查询生成器、双上下文特征选择器和条件共现损失，解决手-物交互解析中的查询初始化、特征噪声和物理一致性问题，在EgoHOS和mini-HOI4D数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的手-物交互解析方法存在三个主要问题：1) 查询初始化机制对变化场景适应性有限；2) 像素级语义特征可能引入交互无关内容；3) 模型易产生物理不一致的"交互幻觉"预测。

Method: 提出端到端的Interaction-aware Transformer (InterFormer)，包含三个核心组件：动态查询生成器(DQG)基于手-物接触空间动态初始化查询；双上下文特征选择器(DFS)融合粗略交互线索与语义特征，抑制噪声；条件共现(CoCo)损失加入手-物关系约束增强物理一致性。

Result: 在EgoHOS和具有挑战性的out-of-distribution mini-HOI4D数据集上均达到最先进性能，展示了模型的有效性和强大的泛化能力。

Conclusion: InterFormer通过整合空间动态感知的查询初始化、交互相关特征选择和物理关系约束，有效解决了手-物交互解析中的关键挑战，为下一代具身智能体提供了更精细的环境交互理解能力。

Abstract: A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability to changing active objects across varying input scenes; 2) previous transformer-based methods utilize pixel-level semantic features to iteratively refine queries during mask generation, which may introduce interaction-irrelevant content into the final embeddings; and 3) prevailing models are susceptible to "interaction illusion", producing physically inconsistent predictions. To address these issues, we propose an end-to-end Interaction-aware Transformer (InterFormer), which integrates three key components, i.e., a Dynamic Query Generator (DQG), a Dual-context Feature Selector (DFS), and the Conditional Co-occurrence (CoCo) loss. The DQG explicitly grounds query initialization in the spatial dynamics of hand-object contact, enabling targeted generation of interaction-aware queries for hands and various active objects. The DFS fuses coarse interactive cues with semantic features, thereby suppressing interaction-irrelevant noise and emphasizing the learning of interactive relationships. The CoCo loss incorporates hand-object relationship constraints to enhance physical consistency in prediction. Our model achieves state-of-the-art performance on both the EgoHOS and the challenging out-of-distribution mini-HOI4D datasets, demonstrating its effectiveness and strong generalization ability. Code and models are publicly available at https://github.com/yuggiehk/InterFormer.

</details>


### [30] [VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos](https://arxiv.org/abs/2602.20608)
*Aihua Mao,Kaihang Huang,Yong-Jin Liu,Chee Seng Chan,Ying He*

Main category: cs.CV

TL;DR: VAGNet：首个视频引导的3D物体可供性定位框架，利用动态交互序列解决静态方法无法处理的模糊性，在PVAD数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有3D物体可供性定位方法主要依赖静态视觉或文本线索，忽略了可供性本质上由动态动作定义，导致难以准确定位真实交互中的接触区域。人类通过观察和模仿动作学习使用物体，而不仅仅是检查形状。

Method: 提出视频引导的3D可供性定位新范式，引入VAGNet框架，将视频衍生的交互线索与3D结构对齐，解决静态线索无法处理的模糊性。创建首个HOI视频-3D配对可供性数据集PVAD，提供先前工作中缺乏的功能监督。

Result: 在PVAD数据集上的大量实验表明，VAGNet实现了最先进的性能，显著优于基于静态方法的基线模型。

Conclusion: 通过利用动态交互序列提供功能监督，视频引导的3D可供性定位能够更准确地识别支持人-物交互的区域，解决了静态方法固有的局限性。代码和数据集将公开。

Abstract: 3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.

</details>


### [31] [Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model](https://arxiv.org/abs/2602.20616)
*Xueqiang Lv,Shizhou Zhang,Yinghui Xing,Di Xu,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: IPOW框架通过概念分解模型使开放世界目标检测可解释化，将RoI特征分解为判别性、共享和背景概念，并利用概念引导修正解决已知-未知混淆问题


<details>
  <summary>Details</summary>
Motivation: 现有开放世界目标检测方法主要关注提高未知物体召回率，但忽视了可解释性，导致已知-未知混淆和预测可靠性降低。本文旨在使整个OWOD框架可解释，让检测器真正"知道未知"

Method: 提出概念驱动的可解释OWOD框架(IPOW)，引入概念分解模型(CDM)，将Faster R-CNN中的耦合RoI特征显式分解为判别性、共享和背景概念。判别性概念识别最具判别性的特征以扩大已知类别间距离，而共享和背景概念因其强泛化能力可轻松迁移到未知类别检测。提出概念引导修正(CGR)解决已知-未知混淆问题

Result: 大量实验表明，IPOW显著提高了未知物体召回率，同时缓解了已知-未知混淆问题，并为已知和未知预测提供了概念级别的可解释性

Conclusion: IPOW框架通过概念分解实现了开放世界目标检测的可解释性，使检测器能够真正理解未知物体，并通过概念引导修正有效解决了已知-未知混淆问题，为OWOD提供了新的可解释性解决方案

Abstract: Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly "knowing the unknown". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.

</details>


### [32] [RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces](https://arxiv.org/abs/2602.20618)
*Haonan An,Xiaohui Ye,Guang Hua,Yihang Tao,Hangcheng Cao,Xiangyu Yu,Yuguang Fang*

Main category: cs.CV

TL;DR: RecoverMark是一个水印框架，通过将受保护的人脸内容本身作为水印嵌入到周围背景中，同时实现鲁棒的篡改定位、内容恢复和所有权验证，解决了传统脆弱水印易受攻击的问题。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的泛滥导致复杂的面部篡改，严重破坏视觉完整性并带来知识产权挑战。现有脆弱水印方法假设攻击者不知道水印存在，忽略了它们对水印移除攻击的内在脆弱性。此外，常用的双水印策略中，鲁棒水印和脆弱水印相互干扰且嵌入容量有限，进一步降低了脆弱水印的有效性。

Method: RecoverMark采用两个关键洞察：1) 利用现实约束——攻击者必须保持背景语义一致性以避免视觉检测；2) 使用图像自身内容（人脸）作为水印增强提取鲁棒性。该方法将受保护的人脸内容本身作为水印嵌入到周围背景中，设计了鲁棒的两阶段训练范式，包含模拟全面潜在攻击的失真层和渐进训练策略。

Result: 大量实验表明，RecoverMark对已见和未见攻击都具有鲁棒性，并且在分布内和分布外数据上都具有良好的泛化能力，能够同时实现鲁棒的篡改定位、内容恢复和图像知识产权保护。

Conclusion: RecoverMark通过创新的水印框架解决了传统脆弱水印的局限性，利用图像自身内容作为水印并嵌入到背景中，实现了同时对抗篡改定位、内容恢复和所有权验证的鲁棒解决方案，为AI生成内容时代的图像完整性保护提供了有效方法。

Abstract: The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.

</details>


### [33] [Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection](https://arxiv.org/abs/2602.20627)
*Zhaonian Kuang,Rui Ding,Meng Yang,Xinhu Zheng,Gang Hua*

Main category: cs.CV

TL;DR: 提出在线对象-场景-相机分解与重组数据增强方案，解决单目3D目标检测中对象、场景、相机姿态三者紧密耦合导致的数据多样性不足问题


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测本质上是病态问题，需要大量标注数据。但由于人类偏见，训练数据中对象、场景和相机姿态总是紧密耦合，特定3D对象总是在特定场景中以固定相机姿态被捕获，缺乏必要的多样性，导致数据利用不足和过拟合

Method: 提出在线对象-场景-相机分解与重组方案：1）将训练图像高效分解为带纹理的3D对象点云模型和背景场景；2）在每个训练周期中，通过将3D对象插入背景场景的自由空间，并使用带纹理的3D点表示以扰动相机姿态渲染新训练图像，从而覆盖对象、场景和相机姿态的所有独立组合

Result: 该方法可作为即插即用组件提升M3OD模型性能，适用于全监督和稀疏监督设置。在稀疏监督设置中，仅标注最靠近自车的对象实例，可灵活控制标注成本。在KITTI和更复杂的Waymo数据集上对五个代表性M3OD模型进行了广泛验证

Conclusion: 提出的数据增强方案能有效缓解单目3D目标检测中对象、场景、相机姿态紧密耦合导致的数据多样性不足问题，提高训练数据利用效率，防止过拟合，提升模型性能

Abstract: Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.

</details>


### [34] [VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation](https://arxiv.org/abs/2602.21054)
*Seongheon Park,Changdae Oh,Hyeong Kyu Choi,Xuefeng Du,Sharon Li*

Main category: cs.CV

TL;DR: VAUQ是一个视觉感知的不确定性量化框架，用于大型视觉语言模型的自评估，通过图像信息分数和核心区域掩码策略来减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型经常产生幻觉，限制了其在现实应用中的安全部署。现有的LLM自评估方法主要依赖语言先验，不适合评估视觉条件预测。

Method: 提出VAUQ框架，引入图像信息分数来衡量模型输出对视觉证据的依赖程度，采用无监督核心区域掩码策略来增强显著区域的影响，结合预测熵得到无需训练的打分函数。

Result: 综合实验表明，VAUQ在多个数据集上始终优于现有的自评估方法。

Conclusion: VAUQ通过视觉感知的不确定性量化，为大型视觉语言模型提供了有效的自评估框架，能够可靠地反映答案正确性，减少幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.

</details>


### [35] [From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection](https://arxiv.org/abs/2602.20630)
*Yepeng Liu,Hao Li,Liwen Yang,Fangzhen Li,Xudi Ge,Yuliang Gu,kuang Gao,Bing Wang,Guang Chen,Hangjun Ye,Yongchao Xu*

Main category: cs.CV

TL;DR: TraqPoint：基于强化学习的序列关键点检测方法，通过轨迹质量奖励机制优化关键点的长期跟踪能力


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的关键点匹配方法主要在图像对上训练，无法显式优化关键点在序列中的长期跟踪能力，特别是在视角和光照变化等挑战性场景下。

Method: 将关键点检测重构为序列决策问题，提出TraqPoint框架，采用端到端强化学习方法，核心创新是轨迹感知奖励机制，通过策略梯度方法联合优化多视角下关键点的一致性和独特性。

Result: 在稀疏匹配基准测试（包括相对姿态估计和3D重建）中，TraqPoint显著优于一些最先进的关键点检测和描述方法。

Conclusion: 通过将关键点检测重构为序列决策问题并引入轨迹质量优化，TraqPoint在挑战性场景下实现了更好的关键点跟踪性能。

Abstract: Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \textbf{Tra}ck-\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.

</details>


### [36] [Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection](https://arxiv.org/abs/2602.20632)
*Xiaokai Bai,Lianqing Zheng,Si-Yuan Cao,Xiaohan Zhang,Zhe Wu,Beinan Yu,Fang Wang,Jie Bai,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SIFormer是一个场景-实例感知的Transformer模型，用于4D毫米波雷达和相机的3D目标检测，通过跨视图激活机制和Transformer融合模块，在弱雷达几何条件下实现可靠的实例感知。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达在自动驾驶中具有鲁棒性和经济性优势，但其稀疏和弱的几何线索使得可靠的实例激活困难。现有的雷达-相机融合范式存在局限性：BEV级融合提供全局场景理解但实例关注弱，透视级融合捕获实例细节但缺乏整体上下文。

Method: SIFormer首先通过分割和深度引导的定位在视图变换中抑制背景噪声；然后引入跨视图激活机制，将2D实例线索注入BEV空间，在弱雷达几何条件下实现可靠的实例感知；最后使用基于Transformer的融合模块聚合互补的图像语义和雷达几何信息。

Result: SIFormer在View-of-Delft、TJ4DRadSet和NuScenes数据集上实现了最先进的性能。

Conclusion: SIFormer通过桥接BEV级融合和透视级融合两种范式，结合它们的互补优势，解决了雷达固有的稀疏性问题，提高了检测精度，增强了实例感知能力。

Abstract: 4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.

</details>


### [37] [SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement](https://arxiv.org/abs/2602.20636)
*Rulin Zhou,Guankun Wang,An Wang,Yujie Ma,Lixin Ouyang,Bolin Cui,Junyan Li,Chaowei Zhu,Mingyang Li,Ming Chen,Xiaopin Zhong,Peng Lu,Jiankun Wang,Xianming Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: SurgAtt-Tracker：一种通过时空学习和密集注意力热图来跟踪手术注意力，实现连续可解释的视野引导的框架，并引入大规模基准数据集SurgAtt-1.16M


<details>
  <summary>Details</summary>
Motivation: 现有手术视野引导方法通常将视觉注意力估计与下游相机控制混为一谈，或依赖于直接的对象中心假设，缺乏连续、可解释的帧级视野引导能力

Method: 将手术注意力跟踪建模为时空学习问题，通过提案级重排序和运动感知细化来利用时间一致性，而不是直接回归，提出SurgAtt-Tracker框架

Result: 在多个手术数据集上的实验表明，SurgAtt-Tracker在遮挡、多器械干扰和跨域设置下均能实现最先进的性能和强大的鲁棒性

Conclusion: 该方法不仅提供了手术注意力跟踪的解决方案，还能直接支持下游机器人视野规划和自动相机控制，为手术视野引导提供了连续、可解释的帧级信号

Abstract: Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.

</details>


### [38] [Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression](https://arxiv.org/abs/2602.20650)
*Chenyue Yu,Lingao Xiao,Jinhong Deng,Ivor W. Tsang,Yang He*

Main category: cs.CV

TL;DR: DCQ通过减少颜色空间冗余来压缩视觉数据集，在保持模型训练所需关键信息的同时显著降低存储需求。


<details>
  <summary>Details</summary>
Motivation: 大规模图像数据集存储需求高，现有方法通过丢弃样本减少数据集大小，但忽略了每个图像内部（特别是颜色空间）的显著冗余。

Method: 提出数据集颜色量化（DCQ）框架：在相似图像间强制一致调色板表示，根据模型感知选择性保留语义重要颜色，保持有效特征学习所需的结构细节。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K上的广泛实验表明，DCQ在激进压缩下显著提升训练性能。

Conclusion: DCQ为数据集级存储减少提供了可扩展且稳健的解决方案，通过减少颜色空间冗余有效压缩视觉数据集。

Abstract: Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.

</details>


### [39] [SD4R: Sparse-to-Dense Learning for 3D Object Detection with 4D Radar](https://arxiv.org/abs/2602.20653)
*Xiaokai Bai,Jiahao Cheng,Songkai Wang,Yixuan Luo,Lianqing Zheng,Xiaohan Zhang,Si-Yuan Cao,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SD4R是一个将稀疏4D雷达点云转换为密集表示的新框架，通过前景点生成器和logit查询编码器实现降噪和前景点稠密化，在View-of-Delft数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 4D雷达测量提供经济且天气鲁棒的3D感知方案，但雷达点云固有的稀疏性和噪声给准确3D物体检测带来挑战，需要有效且鲁棒的点云稠密化方法。现有方法难以处理4D雷达点云的极端稀疏性，且在点数较少的场景中鲁棒性有限。

Method: 提出SD4R框架：1) 前景点生成器(FPG)减轻噪声传播并生成稠密点云；2) logit查询编码器(LQE)增强传统pillarization，产生鲁棒的特征表示。

Result: 在公开的View-of-Delft数据集上进行大量实验，SD4R在降噪和前景点稠密化方面表现出强大能力，实现了最先进的性能。

Conclusion: SD4R通过创新的前景点生成器和logit查询编码器，有效解决了4D雷达点云的稀疏性和噪声问题，为3D物体检测提供了鲁棒的稠密化解决方案。

Abstract: 4D radar measurements offer an affordable and weather-robust solution for 3D perception. However, the inherent sparsity and noise of radar point clouds present significant challenges for accurate 3D object detection, underscoring the need for effective and robust point clouds densification. Despite recent progress, existing densification methods often fail to address the extreme sparsity of 4D radar point clouds and exhibit limited robustness when processing scenes with a small number of points. In this paper, we propose SD4R, a novel framework that transforms sparse radar point clouds into dense representations. SD4R begins by utilizing a foreground point generator (FPG) to mitigate noise propagation and produce densified point clouds. Subsequently, a logit-query encoder (LQE) enhances conventional pillarization, resulting in robust feature representations. Through these innovations, our SD4R demonstrates strong capability in both noise reduction and foreground point densification. Extensive experiments conducted on the publicly available View-of-Delft dataset demonstrate that SD4R achieves state-of-the-art performance. Source code is available at https://github.com/lancelot0805/SD4R.

</details>


### [40] [Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video](https://arxiv.org/abs/2602.20658)
*Mohammad Sadra Rajabi,Aanuoluwapo Ojelade,Sunwook Kim,Maury A. Nussbaum*

Main category: cs.CV

TL;DR: 研究评估了使用视觉语言模型从RGB视频流中非侵入式估计NIOSH举升方程中水平和垂直手部距离的可行性，开发了两种多阶段VLM管道，分割+多视角管道表现最佳。


<details>
  <summary>Details</summary>
Motivation: 手动举升任务是工作相关肌肉骨骼疾病的主要原因，有效的工效学风险评估至关重要。修订版NIOSH举升方程是广泛使用的评估工具，但其所需的水平和垂直手部距离参数通常需要手动测量或专用传感系统，在实际环境中难以使用。

Method: 开发了两种多阶段视觉语言模型管道：文本引导的仅检测管道和检测+分割管道。两种管道都使用文本引导定位任务相关感兴趣区域，从这些区域提取视觉特征，并基于transformer的时间回归来估计举升开始和结束时的H和V距离。在多种举升任务和七种相机视角条件下进行了评估。

Result: 分割+多视角管道表现最佳，估计H的平均绝对误差约为6-8厘米，V约为5-8厘米。在所有管道和相机配置中，像素级分割相对于仅检测管道将H的估计误差降低了约20-30%，V降低了35-40%。

Conclusion: 研究结果支持基于视觉语言模型的管道用于视频估计RNLE距离参数的可行性，分割+多视角方法在准确性和鲁棒性方面表现最佳。

Abstract: Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.

</details>


### [41] [AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?](https://arxiv.org/abs/2602.20664)
*Hailong Yan,Shice Liu,Tao Wang,Xiangtao Zhang,Yijie Zhong,Jinwei Chen,Le Zhang,Bo Li*

Main category: cs.CV

TL;DR: 本文提出了AnimeAgent，首个基于图像到视频的多智能体框架，用于解决自定义故事板生成中的一致性、表达力和迭代优化问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态扩散模型的自定义故事板生成方法存在三个关键限制：1）静态模型缺乏动态表达力，常采用"复制粘贴"模式；2）一次性推理无法迭代修正缺失属性或提示词遵从度差的问题；3）多智能体方法依赖非鲁棒的评估器，不适合评估风格化、非现实动画。

Method: 提出AnimeAgent框架，受迪士尼"直接绘制与姿势到姿势结合"工作流程启发，利用图像到视频的隐式运动先验增强一致性和表达力，采用混合主观-客观评审机制实现可靠的迭代优化。

Result: 实验表明AnimeAgent在一致性、提示词遵从度和风格化方面达到了最先进的性能。研究还收集了带有人工标注真实值的故事板生成基准数据集。

Conclusion: AnimeAgent通过图像到视频的多智能体框架有效解决了自定义故事板生成中的关键挑战，在多个评估维度上优于现有方法。

Abstract: Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to "copy-paste" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's "Combination of Straight Ahead and Pose to Pose" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.

</details>


### [42] [BoxSplitGen: A Generative Model for 3D Part Bounding Boxes in Varying Granularity](https://arxiv.org/abs/2602.20666)
*Juil Koo,Wei-Tung Lin,Chanho Park,Chanhyeok Park,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出一个交互式3D形状生成框架，通过迭代分割边界框从粗到细生成3D部件，包含两个生成模型：BoxSplitGen用于边界框分割，box-to-shape模型用于将边界框转换为具体形状。


<details>
  <summary>Details</summary>
Motivation: 人类创造力遵循从抽象到细节的感知过程，但现有3D生成模型缺乏专门辅助人类想象力、支持从粗到细细节化的工具。需要一种能够直观交互地生成3D形状的框架。

Method: 提出包含两个生成模型的框架：1) BoxSplitGen模型通过迭代分割边界框生成多粒度3D部件边界框，学习聚合合并过程的逆过程；2) box-to-shape生成模型利用现有3D扩散模型先验，并加入边界框条件约束。

Result: 实验表明，box-splitting生成模型优于token预测模型和无条件扩散模型的修复方法；box-to-shape模型基于最先进的3D扩散模型，相比先前模型提供更优结果。

Conclusion: 该框架能够实现直观交互的3D形状生成，通过从粗到细的边界框分割过程辅助人类创造力，两个核心生成模型在各自任务上均表现出优越性能。

Abstract: Human creativity follows a perceptual process, moving from abstract ideas to finer details during creation. While 3D generative models have advanced dramatically, models specifically designed to assist human imagination in 3D creation -- particularly for detailing abstractions from coarse to fine -- have not been explored. We propose a framework that enables intuitive and interactive 3D shape generation by iteratively splitting bounding boxes to refine the set of bounding boxes. The main technical components of our framework are two generative models: the box-splitting generative model and the box-to-shape generative model. The first model, named BoxSplitGen, generates a collection of 3D part bounding boxes with varying granularity by iteratively splitting coarse bounding boxes. It utilizes part bounding boxes created through agglomerative merging and learns the reverse of the merging process -- the splitting sequences. The model consists of two main components: the first learns the categorical distribution of the box to be split, and the second learns the distribution of the two new boxes, given the set of boxes and the indication of which box to split. The second model, the box-to-shape generative model, is trained by leveraging the 3D shape priors learned by an existing 3D diffusion model while adapting the model to incorporate bounding box conditioning. In our experiments, we demonstrate that the box-splitting generative model outperforms token prediction models and the inpainting approach with an unconditional diffusion model. Also, we show that our box-to-shape model, based on a state-of-the-art 3D diffusion model, provides superior results compared to a previous model.

</details>


### [43] [GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio](https://arxiv.org/abs/2602.20673)
*Hao Zhang,Lue Fan,Qitai Wang,Wenbo Li,Zehuan Wu,Lewei Lu,Zhaoxiang Zhang,Hongsheng Li*

Main category: cs.CV

TL;DR: GA-Drive是一个创新的驾驶模拟框架，通过几何-外观解耦和扩散模型生成，能够沿用户指定轨迹生成高质量、可编辑的相机视图。


<details>
  <summary>Details</summary>
Motivation: 需要构建一个自由视点、可编辑且高保真的驾驶模拟器，用于训练和评估端到端自动驾驶系统。

Method: 采用几何-外观解耦方法：首先利用几何信息合成新的伪视图，然后使用训练好的视频扩散模型将其转换为逼真的视图。支持通过先进的视频到视频编辑技术进行外观编辑，同时保持底层几何结构的一致性。

Result: 在NTA-IoU、NTL-IoU和FID分数方面，GA-Drive显著优于现有方法。

Conclusion: GA-Drive提供了一个有效的驾驶模拟框架，实现了高质量视图生成和外观编辑功能，对自动驾驶系统的训练和评估具有重要意义。

Abstract: A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.

</details>


### [44] [RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation](https://arxiv.org/abs/2602.20685)
*Yichen Xie,Chensheng Peng,Mazen Abdelfattah,Yihan Hu,Jiezhi Yang,Eric Higgins,Ryan Brigden,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: RAYNOVA是一个几何无关的世界模型，采用双因果自回归框架，通过全局注意力进行统一的4D时空推理，在nuScenes数据集上实现了最先进的多视角视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常分别处理空间和时间相关性，或者强加3D几何先验，这限制了模型对不同相机设置和自运动的泛化能力。需要一种能够统一处理时空关系且不依赖显式3D几何表示的世界模型。

Method: 提出RAYNOVA模型：1）采用双因果自回归框架，遵循尺度级和时间拓扑顺序；2）基于相对Plücker射线位置编码构建各向同性的时空表示；3）引入循环训练范式缓解长时视频生成中的分布漂移问题。

Result: 在nuScenes数据集上实现了最先进的多视角视频生成结果，具有更高的吞吐量和强大的可控性，能够泛化到新视角和相机配置，无需显式3D场景表示。

Conclusion: RAYNOVA通过几何无关的双因果自回归框架和统一的4D时空推理，为世界建模提供了一种新方法，能够鲁棒地泛化到不同的相机设置和自运动，展示了在复杂场景中生成高质量多视角视频的能力。

Abstract: World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.

</details>


### [45] [MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision](https://arxiv.org/abs/2602.20689)
*Bedrettin Cetinkaya,Sinan Kalkan,Emre Akbas*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的匹配监督模块LPP，用于端到端学习单像素宽度的清晰边缘，无需依赖传统的非极大值抑制或骨架细化等后处理。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法为获得单像素宽度的清晰边缘，通常依赖非可微的后处理算法（如非极大值抑制和骨架细化），这阻碍了端到端优化。所有现有清晰边缘检测方法仍需此类后处理才能获得满意结果。

Method: 提出LPP模块，仅增加约21K参数，可附加到任何边缘检测模型上进行端到端联合学习。在每次训练迭代中，基于空间距离和置信度对预测边缘和真实边缘进行一对一匹配，确保训练和测试协议的一致性。

Result: 在四个流行数据集上的实验表明，集成LPP显著提升了现有边缘检测模型的性能。LPP将平均清晰度指标提升高达2-4倍。在强调清晰度的评估中，LPP将基线性能提升高达20-35%（ODS），在OIS和AP上也有类似提升，首次达到或超越标准后处理的SOTA性能。

Conclusion: LPP是一种轻量级、即插即用的匹配监督模块，能够实现端到端的清晰边缘学习，无需依赖传统后处理，首次使模型性能达到或超越标准后处理水平。

Abstract: Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \MethodLPP, a lightweight, only $\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \MethodLPP substantially improves the performance of existing edge detection models. In particular, \MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \MethodLPP further boosts baseline performance by up to 20--35\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.

</details>


### [46] [NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image](https://arxiv.org/abs/2602.20700)
*Anna Badalyan,Pratheba Selvaraju,Giorgio Becherini,Omid Taheri,Victoria Fernandez Abrevaya,Michael Black*

Main category: cs.CV

TL;DR: 提出NGL（自然服装语言）作为中间表示，通过NGL-Prompter训练免费流程从图像提取结构化服装参数，实现高质量3D服装缝纫图案重建


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖合成数据微调大模型，难以泛化到真实世界图像，无法捕捉服装部件间的真实关联，且通常限于单层服装。而大视觉语言模型擅长用自然语言描述服装，但在直接从图像回归GarmentCode参数时表现不佳

Method: 提出NGL将GarmentCode重构为语言模型更易理解的表示，设计NGL-Prompter训练免费流程，查询大视觉语言模型提取结构化服装参数，然后确定性映射到有效的GarmentCode

Result: 在Dress4D、CloSe和约5000张真实时尚图像数据集上评估，在标准几何指标上达到最先进性能，在人类和GPT感知评估中显著优于现有基线，能恢复多层服装而竞争方法主要关注单层

Conclusion: 通过NGL中间语言和训练免费流程，无需昂贵模型训练即可实现准确的缝纫图案重建，对真实世界图像（即使有遮挡部分）具有强泛化能力

Abstract: Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.

</details>


### [47] [Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization](https://arxiv.org/abs/2602.20718)
*Yangsen Chen,Hao Wang*

Main category: cs.CV

TL;DR: 提出基于3D高斯泼溅的柔性内窥镜组织重建方法，通过表面感知重建和物理约束实现平滑变形表面与实时渲染


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在组织表面重建一致性方面存在挑战，而NeRF方法缺乏实时渲染能力，需要同时实现平滑变形表面和实时渲染

Method: 1) 表面感知重建：使用符号距离场构建网格，用网格约束高斯泼溅重建过程；2) 物理变形约束：结合局部刚性和全局非刚性限制来指导高斯变形，适应软组织的可变形特性

Result: 方法实现了快速渲染过程和平滑表面外观，定量和定性分析表明在纹理和几何方面都获得了坚实的重建质量

Conclusion: 提出的基于3D高斯泼溅的方法成功解决了柔性内窥镜组织重建中表面一致性和实时渲染的挑战，为机器人辅助手术提供了有效解决方案

Abstract: Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.

</details>


### [48] [CleanStyle: Plug-and-Play Style Conditioning Purification for Text-to-Image Stylization](https://arxiv.org/abs/2602.20721)
*Xiaoman Feng,Mingkun Lei,Yang Wang,Dingwen Fu,Chi Zhang*

Main category: cs.CV

TL;DR: CleanStyle是一个即插即用的扩散模型风格迁移框架，通过SVD分解动态抑制风格嵌入中的尾部成分来减少内容泄漏，并引入风格特定的无分类器引导来提升提示对齐和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器的扩散模型风格迁移方法存在内容泄漏问题，即风格图像中的语义元素会不期望地出现在输出中，损害了提示保真度和风格一致性。

Method: 提出CleanStyle框架：1) CleanStyleSVD (CS-SVD)：通过奇异值分解分离风格嵌入的尾部成分，并使用时间感知指数调度动态抑制这些成分；2) Style-Specific Classifier-Free Guidance (SS-CFG)：重用被抑制的尾部成分构建风格感知的无条件输入，提供针对性的负信号来抑制与提示无关的视觉元素。

Result: 大量实验表明，CleanStyle显著减少了内容泄漏，提高了风格化质量和提示对齐，适用于广泛的风格参考和提示。

Conclusion: CleanStyle是一个轻量级、可解释的框架，无需重新训练即可无缝集成到现有的基于编码器的扩散模型中，有效解决了风格迁移中的内容泄漏问题。

Abstract: Style transfer in diffusion models enables controllable visual generation by injecting the style of a reference image. However, recent encoder-based methods, while efficient and tuning-free, often suffer from content leakage, where semantic elements from the style image undesirably appear in the output, impairing prompt fidelity and stylistic consistency. In this work, we introduce CleanStyle, a plug-and-play framework that filters out content-related noise from the style embedding without retraining. Motivated by empirical analysis, we observe that such leakage predominantly stems from the tail components of the style embedding, which are isolated via Singular Value Decomposition (SVD). To address this, we propose CleanStyleSVD (CS-SVD), which dynamically suppresses tail components using a time-aware exponential schedule, providing clean, style-preserving conditional embeddings throughout the denoising process. Furthermore, we present Style-Specific Classifier-Free Guidance (SS-CFG), which reuses the suppressed tail components to construct style-aware unconditional inputs. Unlike conventional methods that use generic negative embeddings (e.g., zero vectors), SS-CFG introduces targeted negative signals that reflect style-specific but prompt-irrelevant visual elements. This enables the model to effectively suppress these distracting patterns during generation, thereby improving prompt fidelity and enhancing the overall visual quality of stylized outputs. Our approach is lightweight, interpretable, and can be seamlessly integrated into existing encoder-based diffusion models without retraining. Extensive experiments demonstrate that CleanStyle substantially reduces content leakage, improves stylization quality and improves prompt alignment across a wide range of style references and prompts.

</details>


### [49] [Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation](https://arxiv.org/abs/2602.20725)
*Junwei Shu,Wenjie Liu,Changgu Chen,Hantang Liu,Yang Li,Changbo Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种统一随机公式，将蒙特卡洛渲染与基于扩散的生成建模相结合，实现对扩散生成结果的物理基础控制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本/图像条件生成方面表现出色，但缺乏对低级别、物理基础的着色和材质属性的显式控制；而基于物理的渲染（PBR）提供细粒度物理控制但缺乏提示驱动的灵活性。两者都遵循从噪声观察到干净图像的共同演化路径，需要建立统一框架。

Method: 1. 基于中心极限定理为蒙特卡洛积分建立通用随机微分方程（SDE）公式；2. 通过基于物理的路径追踪实例化，将其转换为物理基础的SDE表示；3. 从噪声方差角度分析路径追踪的物理特性如何扩展到现有扩散模型。

Result: 在多个任务上的广泛实验表明，该方法能够对扩散生成结果施加物理基础的控制，涵盖渲染和材质编辑等任务。

Conclusion: 提出的统一随机公式成功桥接了蒙特卡洛渲染和扩散生成建模，实现了物理精确控制与提示驱动灵活性的结合，为计算机图形学和计算机视觉领域提供了新的交叉方法。

Abstract: Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.

</details>


### [50] [Communication-Inspired Tokenization for Structured Image Representations](https://arxiv.org/abs/2602.20731)
*Aram Davtyan,Yusuf Sahin,Yasaman Haghighi,Sebastian Stapf,Pablo Acuaviva,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: COMiT是一种受人类交流启发的视觉tokenization框架，通过迭代观察图像局部区域并递归更新离散表示，生成结构化、面向对象的视觉token序列，显著提升了组合泛化和关系推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有离散图像tokenizer主要针对重建和压缩优化，产生的token往往捕捉局部纹理而非对象级语义结构。受人类交流的渐进性和组合性启发，需要开发能生成结构化语义token序列的方法。

Method: COMiT框架在固定token预算内构建潜在消息：迭代观察局部图像裁剪，递归更新离散表示；每一步整合新视觉信息同时精炼重组现有token序列；最终消息条件化流匹配解码器重建完整图像；编码解码在单一transformer中端到端训练，结合流匹配重建和语义表示对齐损失。

Result: 实验表明，语义对齐提供基础，而注意力顺序tokenization对诱导可解释、面向对象的token结构至关重要，相比先前方法显著提升了组合泛化和关系推理能力。

Conclusion: COMiT通过受人类交流启发的迭代tokenization框架，成功生成了结构化、语义丰富的视觉token序列，为视觉和多模态系统提供了更有效的序列接口。

Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.

</details>


### [51] [OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation](https://arxiv.org/abs/2602.20752)
*Tian Lan,Lei Xu,Zimu Yuan,Shanggui Liu,Jiajun Liu,Jiaxin Liu,Weilai Xiang,Hongyu Yang,Dong Jiang,Jianxin Yin,Dingyu Wang*

Main category: cs.CV

TL;DR: OrthoDiffusion是一个基于扩散模型的统一基础模型，用于多任务肌肉骨骼MRI解读，在膝关节MRI上表现出色，并能迁移到其他关节的诊断任务。


<details>
  <summary>Details</summary>
Motivation: 肌肉骨骼疾病是全球健康的重要负担，MRI诊断具有挑战性。放射科医生需要在复杂的解剖结构中识别多种异常，这个过程需要专业知识且存在变异性。因此需要开发能够提高肌肉骨骼MRI解读效率和准确性的AI模型。

Method: 开发了OrthoDiffusion，一个基于扩散的统一基础模型。使用三个方向特定的3D扩散模型，在15,948个未标记的膝关节MRI扫描上进行自监督预训练，学习矢状面、冠状面和轴状面的稳健解剖特征。这些视图特定表示被整合以支持多种临床任务，包括解剖分割和多标签诊断。

Result: OrthoDiffusion在11个膝关节结构分割和8个膝关节异常检测中表现出色。模型在不同临床中心和MRI场强下表现出显著稳健性，始终优于传统监督模型。在标记数据稀缺的情况下，仅使用10%的训练标签仍保持高诊断精度。从膝关节成像学到的解剖表示可高度迁移到其他关节，在踝关节和肩关节的11种疾病中实现强诊断性能。

Conclusion: 基于扩散的基础模型可以作为多疾病诊断和解剖分割的统一平台，有可能提高真实世界临床工作流程中肌肉骨骼MRI解读的效率和准确性。

Abstract: Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.

</details>


### [52] [Federated Learning for Cross-Modality Medical Image Segmentation via Augmentation-Driven Generalization](https://arxiv.org/abs/2602.20773)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: 该研究在联邦学习框架下，针对医疗影像跨模态泛化问题，系统评估了多种数据增强策略，发现全局强度非线性增强在保持解剖结构的同时模拟跨模态外观变化，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医疗影像分析中，由于数据分散在不同机构且受隐私保护限制，开发鲁棒且可泛化的分割模型面临挑战。联邦学习虽然支持协作训练而不集中数据，但跨模态域偏移问题突出，特别是当模型在一个模态上训练后无法泛化到另一模态时。现有解决方案通常需要患者的多模态配对数据或依赖复杂架构，这在真实临床环境中不切实际。

Method: 研究考虑了一个现实的联邦学习场景：每个客户端持有单模态数据（CT或MRI）。系统评估了多种增强策略用于跨模态泛化，包括基于卷积的空间增强、频域操作、域特定归一化以及全局强度非线性增强。使用腹部器官分割和全心分割作为代表性的多类和二值分割基准进行评估。

Result: 全局强度非线性增强在集中式和联邦式设置中均优于其他方法，通过模拟跨模态外观变化同时保持解剖结构。对于胰腺分割，Dice分数从0.073提升到0.437，增益达498%。联邦学习方法达到了集中式训练准确率的93-98%，在不损害数据隐私的情况下实现了强大的跨模态泛化能力。

Conclusion: 全局强度非线性增强是解决联邦学习框架下跨模态泛化问题的有效策略，能够在保持数据隐私的同时实现接近集中式训练的性能，为在不同医疗系统中部署可行的联邦AI解决方案指明了方向。

Abstract: Artificial intelligence has emerged as a transformative tool in medical image analysis, yet developing robust and generalizable segmentation models remains difficult due to fragmented, privacy-constrained imaging data siloed across institutions. While federated learning (FL) enables collaborative model training without centralizing data, cross-modality domain shifts pose a critical challenge, particularly when models trained on one modality fail to generalize to another. Many existing solutions require paired multimodal data per patient or rely on complex architectures, both of which are impractical in real clinical settings. In this work, we consider a realistic FL scenario where each client holds single-modality data (CT or MRI), and systematically investigate augmentation strategies for cross-modality generalization. Using abdominal organ segmentation and whole-heart segmentation as representative multi-class and binary segmentation benchmarks, we evaluate convolution-based spatial augmentation, frequency-domain manipulation, domain-specific normalization, and global intensity nonlinear (GIN) augmentation. Our results show that GIN consistently outperforms alternatives in both centralized and federated settings by simulating cross-modality appearance variations while preserving anatomical structure. For the pancreas, Dice score improved from 0.073 to 0.437, a 498% gain. Our federated approach achieves 93-98% of centralized training accuracy, demonstrating strong cross-modality generalization without compromising data privacy, pointing toward feasible federated AI deployment across diverse healthcare systems.

</details>


### [53] [Real-time Motion Segmentation with Event-based Normal Flow](https://arxiv.org/abs/2602.20790)
*Sheng Zhong,Zhongyang Ren,Xiya Zhu,Dehao Yuan,Cornelia Fermuller,Yi Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种基于法向流的事件相机运动分割框架，通过将运动分割建模为能量最小化问题，利用图割算法求解，实现了近800倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然具有微秒级分辨率的优势，但单个事件包含的信息稀疏，直接处理原始事件数据效率低下，限制了在实时任务（如运动分割）中的应用。需要更有效的中间表示来压缩运动信息。

Method: 提出基于法向流的运动分割框架：1）从事件邻域直接学习密集法向流作为输入；2）将运动分割建模为能量最小化问题，通过图割算法求解；3）通过法向流聚类和运动模型拟合进行迭代优化；4）使用基于法向流的运动模型初始化和拟合方法，仅需有限候选模型。

Result: 在多个公共数据集上的广泛评估表明，该框架在准确性和效率方面表现优异。与开源的最先进方法相比，实现了近800倍的速度提升，显著降低了计算复杂度，确保了实时性能。

Conclusion: 通过将法向流作为中间表示来压缩事件簇的运动信息，提出的框架能够高效估计独立运动物体的运动模型，仅需有限候选模型，大幅提升计算效率，为事件相机在实时视觉任务中的应用提供了有效解决方案。

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.

</details>


### [54] [SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking](https://arxiv.org/abs/2602.20792)
*Muhammad Saif Ullah Khan,Didier Stricker*

Main category: cs.CV

TL;DR: 提出SIMSPINE数据集和生物力学感知的关键点模拟框架，通过肌肉骨骼建模为现有姿态数据集添加解剖一致的3D脊柱关键点，解决脊柱运动建模中缺乏大规模3D标注的问题。


<details>
  <summary>Details</summary>
Motivation: 脊柱运动建模对于理解人体生物力学至关重要，但由于脊柱复杂的多关节运动学和缺乏大规模3D标注，在计算机视觉领域尚未得到充分探索。

Method: 开发生物力学感知的关键点模拟框架，从肌肉骨骼建模中推导解剖一致的3D脊柱关键点，增强现有人体姿态数据集。创建SIMSPINE数据集，包含214万帧室内多相机捕获的自然全身运动数据，提供稀疏椎骨级3D脊柱标注。

Result: 2D脊柱基线在受控环境中将AUC从0.63提升至0.80，在野外脊柱跟踪中将AP从0.91提升至0.93。发布了涵盖微调2D检测器、单目3D姿态提升模型和多视图重建管道的预训练基线。

Conclusion: 模拟框架和SIMSPINE数据集通过实现可重复、解剖基础的3D脊柱估计，推动了基于视觉的生物力学、运动分析和数字人体建模研究。

Abstract: Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.

</details>


### [55] [VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving](https://arxiv.org/abs/2602.20794)
*Jie Wang,Guang Li,Zhijian Huang,Chenxu Dang,Hangjun Ye,Yahong Han,Long Chen*

Main category: cs.CV

TL;DR: VGGDrive：一种通过跨视角3D几何赋能视觉语言模型的新架构，用于提升自动驾驶任务性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）缺乏跨视角3D几何建模能力，导致在自动驾驶任务中表现平庸。虽然有些方法尝试通过构建问答数据进行辅助训练，但仍无法从根本上让VLMs全面处理多样化的评估协议。

Method: 提出VGGDrive架构，通过跨视角3D几何赋能器（CVGE）将成熟的3D基础模型的跨视角几何信息注入到VLMs中。CVGE采用分层自适应注入机制，将冻结的3D视觉模型的跨视角3D几何特征与VLM的2D视觉特征相融合。

Result: 在五个自动驾驶基准测试中，VGGDrive显著提升了基础VLM的性能，包括跨视角风险感知、运动预测和轨迹规划等任务。

Conclusion: 成熟的3D基础模型可以通过有效集成来赋能自动驾驶任务，这项初步探索展示了该范式在自动驾驶领域的潜力。

Abstract: The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.

</details>


### [56] [RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction](https://arxiv.org/abs/2602.20807)
*Yangfan Zhao,Hanwei Zhang,Ke Huang,Qiufeng Wang,Zhenzhou Shao,Dengyu Wu*

Main category: cs.CV

TL;DR: RU4D-SLAM是一个结合4D高斯溅射与SLAM的框架，通过引入时间因素、不确定性感知和语义引导重加权机制，在动态环境中显著提升了轨迹精度和4D场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射与SLAM结合的方法在动态环境中表现不佳，移动物体会干扰3D重建和可靠跟踪。4D重建特别是4D高斯溅射为解决这些挑战提供了有前景的方向，但其在4D感知SLAM中的应用尚未充分探索。

Method: 提出RU4D-SLAM框架，将时间因素引入空间3D表示，结合不确定性感知的场景变化、模糊图像合成和动态场景重建。通过运动模糊渲染增强动态场景表示，扩展每像素不确定性建模以处理模糊图像，提出语义引导的重加权机制进行动态场景的不确定性估计，并引入可学习的透明度权重支持自适应4D映射。

Result: 在标准基准测试上的广泛实验表明，该方法在轨迹精度和4D场景重建方面显著优于最先进的方法，特别是在具有移动物体和低质量输入的动态环境中表现突出。

Conclusion: RU4D-SLAM通过整合4D高斯溅射与不确定性感知机制，成功解决了动态环境中的SLAM挑战，为4D感知的SLAM系统提供了有效的解决方案。

Abstract: Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io

</details>


### [57] [GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection](https://arxiv.org/abs/2602.20818)
*Yingying Guo,Ke Zhang,Zirong Zeng*

Main category: cs.CV

TL;DR: GatedCLIP模型通过改进CLIP架构，专门用于检测仇恨表情包，在Hateful Memes数据集上AUROC达到0.66，显著优于CLIP基线（0.49）。


<details>
  <summary>Details</summary>
Motivation: 检测多模态表情包中的仇恨内容具有独特挑战，因为有害信息往往来自良性图像和文本的复杂交互，需要专门的多模态能力来识别这种微妙关系。

Method: 提出GatedCLIP模型，包含三个关键改进：1）学习投影头将CLIP嵌入映射到任务优化的语义空间；2）动态门控融合机制自适应加权视觉和文本特征；3）对比学习目标保持跨模态语义对齐。

Result: 在Hateful Memes数据集上，GatedCLIP达到AUROC 0.66，显著优于CLIP基线（AUROC 0.49），同时保持计算效率，仅需35万可训练参数。

Conclusion: GatedCLIP通过专门的架构改进有效增强了CLIP的多模态能力，为仇恨表情包检测提供了高效且性能优越的解决方案。

Abstract: Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.

</details>


### [58] [Training-Free Multi-Concept Image Editing](https://arxiv.org/abs/2602.20839)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: 提出无需训练的基于概念的图像编辑框架，结合优化DDS与LoRA驱动概念组合，实现多视觉概念的控制与编辑


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像编辑方法存在局限性：优化方法虽能实现零样本文本编辑，但难以保持图像身份或捕捉无法用纯文本表达的视觉概念（如面部结构、材质纹理、物体几何等）

Method: 提出训练免费的概念图像编辑框架，将优化的DDS与LoRA驱动的概念组合统一，其中LoRA的训练数据代表概念。通过有序时间步、正则化和负提示引导来改进DDS的稳定性和可控性，在扩散过程中直接结合和控制多个视觉概念

Result: 在InstructPix2Pix和ComposLoRA基准测试中，定量和定性结果均显示相比现有训练免费扩散编辑方法有持续改进

Conclusion: 提出的框架成功解决了纯文本无法表达的视觉概念编辑问题，通过概念组合实现了更精确的图像编辑，代码将公开可用

Abstract: Editing images with diffusion models without training remains challenging. While recent optimisation-based methods achieve strong zero-shot edits from text, they struggle to preserve identity or capture details that language alone cannot express. Many visual concepts such as facial structure, material texture, or object geometry are impossible to express purely through text prompts alone. To address this gap, we introduce a training-free framework for concept-based image editing, which unifies Optimised DDS with LoRA-driven concept composition, where the training data of the LoRA represent the concept. Our approach enables combining and controlling multiple visual concepts directly within the diffusion process, integrating semantic guidance from text with low-level cues from pretrained concept adapters. We further refine DDS for stability and controllability through ordered timesteps, regularisation, and negative-prompt guidance. Quantitative and qualitative results demonstrate consistent improvements over existing training-free diffusion editing methods on InstructPix2Pix and ComposLoRA benchmarks. Code will be made publicly available.

</details>


### [59] [Hybrid Fusion: One-Minute Efficient Training for Zero-Shot Cross-Domain Image Fusion](https://arxiv.org/abs/2602.20851)
*Ran Zhang,Xuanhua He,Liu Liu*

Main category: cs.CV

TL;DR: 提出一种混合图像融合框架，结合深度学习与传统方法，通过可学习的U-Net生成动态引导图指导固定拉普拉斯金字塔融合核，实现高效全分辨率训练，消除训练-推理差距。


<details>
  <summary>Details</summary>
Motivation: 传统图像融合方法速度快但缺乏适应性和性能，而深度学习方法虽能达到SOTA效果，但存在训练效率低、资源消耗大、基于patch的训练与全分辨率推理存在显著差距的问题。

Method: 提出混合框架：使用可学习的U-Net生成动态引导图，指导固定的经典拉普拉斯金字塔融合核。这种策略学习与像素合成的解耦设计实现了高效的全分辨率训练。

Result: 在RTX 4090上约1分钟或在消费级笔记本GPU上约2分钟即可从头训练达到SOTA可比性能，无需外部模型。在红外-可见光到医学成像等多种任务上展现出强大的零样本泛化能力。

Conclusion: 该混合框架解决了传统方法与深度学习之间的权衡问题，通过设计确保融合输出仅从源信息线性构建，为关键应用提供高保真度，同时实现高效训练和强大泛化能力。

Abstract: Image fusion seeks to integrate complementary information from multiple sources into a single, superior image. While traditional methods are fast, they lack adaptability and performance. Conversely, deep learning approaches achieve state-of-the-art (SOTA) results but suffer from critical inefficiencies: their reliance on slow, resource-intensive, patch-based training introduces a significant gap with full-resolution inference. We propose a novel hybrid framework that resolves this trade-off. Our method utilizes a learnable U-Net to generate a dynamic guidance map that directs a classic, fixed Laplacian pyramid fusion kernel. This decoupling of policy learning from pixel synthesis enables remarkably efficient full-resolution training, eliminating the train-inference gap. Consequently, our model achieves SOTA-comparable performance in about one minute on a RTX 4090 or two minutes on a consumer laptop GPU from scratch without any external model and demonstrates powerful zero-shot generalization across diverse tasks, from infrared-visible to medical imaging. By design, the fused output is linearly constructed solely from source information, ensuring high faithfulness for critical applications. The codes are available at https://github.com/Zirconium233/HybridFusion

</details>


### [60] [On the Explainability of Vision-Language Models in Art History](https://arxiv.org/abs/2602.20853)
*Stefanie Schneider*

Main category: cs.CV

TL;DR: 评估7种XAI方法在艺术史背景下解释CLIP视觉推理的能力，发现这些方法能捕捉部分人类解释，但效果取决于概念稳定性和表征可用性


<details>
  <summary>Details</summary>
Motivation: 研究如何通过可解释人工智能方法使视觉语言模型（特别是CLIP）在艺术史背景下的视觉推理过程变得可理解，探讨机器"理解"的本质

Method: 评估7种XAI方法，结合零样本定位实验和人类可解释性研究，在艺术史背景下分析CLIP的视觉推理

Result: XAI方法能够捕捉人类解释的某些方面，但其有效性取决于所检查类别的概念稳定性和表征可用性

Conclusion: 虽然XAI方法有助于理解VLM的视觉推理过程，但其解释能力受到概念特征的影响，在艺术史等复杂领域需要更精细的方法

Abstract: Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.

</details>


### [61] [When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance](https://arxiv.org/abs/2602.20880)
*Yongli Xiang,Ziming Hong,Zhaoqing Wang,Xiangyu Zhao,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文提出CASG框架，通过冲突感知自适应安全引导解决T2I扩散模型中多类别有害内容生成的安全冲突问题


<details>
  <summary>Details</summary>
Motivation: 现有基于安全引导的方法在处理多类别有害内容时存在"有害冲突"问题，即抑制一种有害类别可能无意中放大另一种有害类别，导致总体有害率上升

Method: 提出冲突感知自适应安全引导(CASG)框架，包含两个组件：1)冲突感知类别识别(CaCI)，识别与模型生成状态最匹配的有害类别；2)冲突解决引导应用(CrGA)，仅沿识别出的类别应用安全引导以避免多类别干扰

Result: 在T2I安全基准测试中，CASG实现了最先进的性能，相比现有方法将有害率降低了高达15.4%

Conclusion: CASG框架有效解决了多类别有害内容生成中的冲突问题，可应用于潜在空间和文本空间的安全保障，显著提升了T2I模型的安全性

Abstract: Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to "harmful conflicts" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.

</details>


### [62] [SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models](https://arxiv.org/abs/2602.20901)
*Yuechen Xie,Xiaoyan Zhang,Yicheng Shan,Hao Zhu,Rui Tang,Rong Wei,Mingli Song,Yuanyu Wan,Jie Song*

Main category: cs.CV

TL;DR: 该研究提出了SpatiaLQA基准，用于评估视觉语言模型的空间逻辑推理能力，并提出了递归场景图辅助推理方法来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在常见视觉问答和逻辑推理方面表现出色，但在复杂现实环境中仍缺乏合理的空间逻辑推理能力，这需要同时理解复杂场景中物体的空间关系和多步任务中步骤间的逻辑依赖。

Method: 提出了递归场景图辅助推理方法，利用视觉基础模型逐步将复杂场景分解为任务相关的场景图，从而增强视觉语言模型的空间逻辑推理能力。

Result: 在41个主流视觉语言模型上进行了广泛实验，结果显示即使最先进的模型在空间逻辑推理方面仍然存在困难。提出的递归场景图辅助推理方法超越了所有先前的方法。

Conclusion: SpatiaLQA基准有效评估了视觉语言模型的空间逻辑推理能力，提出的递归场景图辅助推理方法能够显著提升模型在此类任务上的表现，为复杂现实环境中的智能决策提供了新的解决方案。

Abstract: Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.

</details>


### [63] [TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering](https://arxiv.org/abs/2602.20903)
*Hanshen Zhu,Yuliang Liu,Xuecheng Wu,An-Lan Wang,Hao Feng,Dingkang Yang,Chao Feng,Can Huang,Jingqun Tang,Xiang Bai*

Main category: cs.CV

TL;DR: TextPecker：一种即插即用的结构异常感知强化学习策略，通过构建字符级结构异常标注数据集和笔画编辑合成引擎，显著提升文本到图像生成中的结构保真度和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 当前视觉文本渲染（VTR）面临结构异常问题（如扭曲、模糊、错位），而主流多模态大模型和OCR模型无法有效感知这些结构异常，这成为VTR评估和基于强化学习的优化的关键瓶颈。

Method: 提出TextPecker策略：1）构建带有字符级结构异常标注的识别数据集；2）开发笔画编辑合成引擎以扩展结构错误覆盖；3）采用即插即用的结构异常感知强化学习方法，可与任何文本到图像生成器配合使用。

Result: TextPecker能持续改进多种文本到图像模型，即使在已优化的Qwen-Image上，中文文本渲染的结构保真度平均提升4%，语义对齐提升8.7%，在高保真VTR方面达到新的最先进水平。

Conclusion: TextPecker填补了VTR优化的空白，为可靠且结构保真的视觉文本生成提供了基础性步骤，解决了当前模型难以感知结构异常的关键瓶颈问题。

Abstract: Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.

</details>


### [64] [Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting](https://arxiv.org/abs/2602.20933)
*Shuangkang Fang,I-Chao Shen,Xuanyang Zhang,Zesheng Wang,Yufeng Wang,Wenrui Ding,Gang Yu,Takeo Igarashi*

Main category: cs.CV

TL;DR: DropAnSH-GS提出了一种基于锚点的Dropout策略，通过同时移除锚点高斯及其空间邻居来打破局部冗余，并扩展Dropout到高阶球谐系数以缓解过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射Dropout方法存在邻居补偿效应（被丢弃的高斯常被邻居补偿，削弱正则化效果），且忽视了高阶球谐系数对过拟合的贡献。

Method: 提出锚点Dropout策略：随机选择高斯作为锚点，同时移除其空间邻居以打破局部冗余；扩展Dropout到颜色属性，随机丢弃高阶球谐系数，将外观信息集中到低阶系数。

Result: DropAnSH-GS显著优于现有Dropout方法，计算开销可忽略，可轻松集成到各种3DGS变体中提升性能，并支持通过球谐截断实现灵活的模型后压缩。

Conclusion: DropAnSH-GS通过锚点Dropout策略有效解决了邻居补偿效应和高阶球谐过拟合问题，为稀疏视图条件下的3D高斯溅射提供了更鲁棒的正则化方法。

Abstract: Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS

</details>


### [65] [UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling](https://arxiv.org/abs/2602.20943)
*Kaiyuan Tan,Yingying Shen,Mingfei Tu,Haohui Zhu,Bing Wang,Guang Chen,Hangjun Ye,Haiyang Sun*

Main category: cs.CV

TL;DR: UFO提出了一种新颖的循环范式，用于高效的长距离4D驾驶场景重建，结合了基于优化和前馈方法的优点，能在0.5秒内重建16秒驾驶日志。


<details>
  <summary>Details</summary>
Motivation: 动态驾驶场景重建对自动驾驶仿真和闭环学习至关重要。现有前馈方法在处理长距离驾驶序列时面临序列长度的二次复杂度挑战，以及在长时间内建模动态对象的困难。

Method: 提出UFO循环范式，维护4D场景表示并随着新观测迭代优化。使用基于可见性的过滤机制选择信息丰富的场景标记，实现长序列高效处理。对动态对象引入基于物体姿态引导的建模方法，支持准确的长距离运动捕捉。

Result: 在Waymo Open Dataset上的实验表明，该方法在各种序列长度上显著优于基于场景的优化和现有前馈方法。能够在0.5秒内重建16秒驾驶日志，同时保持优越的视觉质量和几何精度。

Conclusion: UFO通过循环范式有效解决了长距离4D驾驶场景重建的效率和精度问题，为自动驾驶仿真和闭环学习提供了高效可靠的解决方案。

Abstract: Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.

</details>


### [66] [See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis](https://arxiv.org/abs/2602.20951)
*Jaehyun Park,Minyoung Ahn,Minkyu Kim,Jonghyun Lee,Jae-Gil Lee,Dongmin Park*

Main category: cs.CV

TL;DR: ArtiAgent是一个自动化系统，通过三个智能体协作生成带有丰富伪影标注的图像对（真实图像与注入伪影的图像），用于训练伪影检测和修复模型。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型有所进展，AI生成图像仍常包含视觉伪影，影响真实感。现有方法依赖人工标注的伪影数据集，成本高且难以扩展，需要自动化方法来获取可靠的伪影标注数据。

Method: 提出ArtiAgent系统，包含三个智能体：感知智能体识别和定位真实图像中的实体和子实体；合成智能体通过扩散变换器中的新颖patch-wise嵌入操作，使用伪影注入工具引入伪影；策展智能体过滤合成伪影，并为每个实例生成局部和全局解释。

Result: 使用ArtiAgent合成了10万张带有丰富伪影标注的图像，并在多种应用中展示了其有效性和多功能性。

Conclusion: ArtiAgent提供了一种自动化、可扩展的方法来创建伪影标注数据集，解决了人工标注成本高的问题，为伪影检测和修复研究提供了有价值的资源。

Abstract: Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.

</details>


### [67] [Are Multimodal Large Language Models Good Annotators for Image Tagging?](https://arxiv.org/abs/2602.20972)
*Ming-Kun Xie,Jia-Hao Xiao,Zhiqiang Kou,Zhongnian Li,Gang Niu,Masashi Sugiyama*

Main category: cs.CV

TL;DR: TagLLM框架通过结构化分组提示生成候选标签和交互式标签消歧，显著缩小了MLLM生成标注与人工标注之间的差距，使MLLM标注成本降至人工的千分之一，下游任务性能差距缩小60-80%。


<details>
  <summary>Details</summary>
Motivation: 传统图像标注依赖人工标注成本高昂，MLLMs有潜力自动化标注但能力未被充分探索。本文旨在分析MLLM生成标注与人工标注的差距，并提出解决方案使MLLM标注能替代人工标注。

Method: 提出TagLLM框架，包含两个组件：1) 候选标签生成：使用结构化分组提示高效生成紧凑的候选集，尽可能覆盖真实标签同时减少后续标注工作量；2) 标签消歧：交互式校准提示中的语义概念，有效细化候选标签。

Result: MLLM标注成本可降至人工成本的千分之一，标注质量达到人工的50-80%，下游训练任务性能超过90%。TagLLM显著缩小了MLLM生成标注与人工标注的差距，在下游训练性能方面缩小了约60-80%的差异。

Conclusion: TagLLM框架有效缩小了MLLM生成标注与人工标注的差距，使MLLM能够以极低成本替代人工标注，为图像标注任务提供了高效实用的解决方案。

Abstract: Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\% to 80\% of human performance, while achieving over 90\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\% to 80\% of the difference.

</details>


### [68] [CrystaL: Spontaneous Emergence of Visual Latents in MLLMs](https://arxiv.org/abs/2602.20980)
*Yang Zhang,Danyang Li,Yuxuan Li,Xin Zhang,Tianyu Xie,Mingming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: CrystaL是一个单阶段框架，通过处理完整和损坏图像的双路径设计，将潜在表示"结晶"为任务相关的视觉语义，提升多模态大语言模型的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有潜在思维链方法中启发式预定义的监督信号对中间潜在状态中关键视觉信息的保留指导有限，需要更有效的监督机制来提升视觉语义的保持能力。

Method: 提出CrystaL框架，采用双路径设计分别处理完整图像和损坏图像，通过显式对齐两个路径的注意力模式和预测分布，将潜在表示结晶为任务相关的视觉语义，无需额外注释或外部模块。

Result: 在感知密集型基准测试中，CrystaL持续优于最先进的基线方法，在细粒度视觉理解方面取得显著提升，同时保持强大的推理能力。

Conclusion: CrystaL通过双路径对齐机制有效解决了潜在思维链中视觉信息保留不足的问题，为多模态大语言模型提供了更强大的视觉语义理解能力。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.

</details>


### [69] [EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer](https://arxiv.org/abs/2602.20985)
*Munish Monga,Vishal Chudasama,Pankaj Wasnik,C. V. Jawahar*

Main category: cs.CV

TL;DR: EW-DETR框架解决现实世界物体检测在动态环境中的挑战，通过增量学习、领域适应和未知检测的协同，在无示例约束下实现先进性能


<details>
  <summary>Details</summary>
Motivation: 现实世界物体检测需要在动态环境中运行，新类别不断出现、领域发生偏移、未知物体需要被识别为"未知"，且不能访问先前数据

Method: 提出EW-DETR框架，包含三个协同模块：增量LoRA适配器用于无示例增量学习；查询规范物体性适配器从DETR解码器查询中解耦物体性感知特征；熵感知未知混合用于校准未知检测

Result: 在Pascal Series和Diverse Weather基准测试中，EW-DETR优于其他方法，将FOGS分数提高了57.24%

Conclusion: EW-DETR框架有效解决了动态世界物体检测的挑战，通过协同增量学习、领域适应和未知检测，在无示例约束下实现了最先进的性能

Abstract: Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as "unknown": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.

</details>


### [70] [Cycle-Consistent Tuning for Layered Image Decomposition](https://arxiv.org/abs/2602.20989)
*Zheng Gu,Min Lu,Zhida Sun,Dani Lischinski,Daniel Cohen-O,Hui Huang*

Main category: cs.CV

TL;DR: 提出基于大扩散模型的上下文图像分解框架，专注于logo-物体分解任务，通过轻量级LoRA微调和循环一致性训练实现高质量分层分离。


<details>
  <summary>Details</summary>
Motivation: 真实世界图像中的视觉层分离是一个长期挑战，因为层间存在非线性、全局耦合的交互（如阴影、反射、透视变形）。现有方法难以处理logo与物体表面这种复杂交互的分解问题。

Method: 1) 基于预训练扩散模型进行轻量级LoRA微调；2) 引入循环一致性训练策略，联合训练分解和合成模型，确保分解后重新合成的图像与原图一致；3) 采用渐进式自我改进过程，迭代增加高质量模型生成样本到训练集以优化性能。

Result: 实验表明该方法能实现准确、连贯的分解效果，在logo-物体分解任务上表现优异，并能有效泛化到其他类型的图像分解任务，显示出作为统一分层图像分解框架的潜力。

Conclusion: 提出的上下文图像分解框架通过利用大扩散模型、循环一致性训练和渐进式自我改进，成功解决了复杂视觉层分离问题，为分层图像分解提供了有效的统一解决方案。

Abstract: Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.

</details>


### [71] [VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models](https://arxiv.org/abs/2602.20999)
*Bowen Zheng,Yongli Xiang,Ziming Hong,Zerong Lin,Chaojian Yu,Tongliang Liu,Xinge You*

Main category: cs.CV

TL;DR: 本文提出了一种名为视觉指令注入（VII）的训练免费、可迁移的越狱框架，通过将恶意文本意图伪装成参考图像中的良性视觉指令，来攻击图像到视频生成模型的安全机制。


<details>
  <summary>Details</summary>
Motivation: 图像到视频生成模型具有视觉指令跟随能力，但这一能力带来了被攻击者利用的风险。攻击者可能通过图像模态注入恶意意图，绕过文本内容的安全过滤机制。

Method: VII框架包含两个模块：恶意意图重编程模块从恶意文本提示中提取意图并降低其静态危害性；视觉指令接地模块将提取的意图渲染到安全输入图像上，保持与原始恶意文本的语义一致性，从而在视频生成时诱导有害内容。

Result: 在四个最先进的商业I2V模型上的实验表明，VII攻击成功率最高达83.5%，同时将拒绝率降低到接近零，显著优于现有基线方法。

Conclusion: 该研究揭示了图像到视频生成模型中视觉指令跟随能力的安全漏洞，提出了有效的攻击方法，为未来开发更鲁棒的安全防御机制提供了重要参考。

Abstract: Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.

</details>


### [72] [From Perception to Action: An Interactive Benchmark for Vision Reasoning](https://arxiv.org/abs/2602.21015)
*Yuhao Wu,Maojia Song,Yihuai Lan,Lei Wang,Zhiqiang Hu,Yao Xiao,Heng Zhou,Weihua Zheng,Dylan Raharja,Soujanya Poria,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: CHAIN基准测试：一个交互式3D物理驱动测试平台，用于评估模型在物理约束下理解、规划和执行结构化动作序列的能力，填补了现有VLM评估在物理结构理解方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型评估主要关注结构无关的单轮设置（如VQA），无法评估智能体在动态环境中理解几何、接触和支撑关系如何共同约束可能动作的能力，这对于具身智能体、交互设计和长时程操作等实际应用至关重要。

Method: 提出CHAIN基准测试，这是一个交互式3D物理驱动测试平台，包含机械拼图互锁、3D堆叠和包装等任务，将评估从被动感知转向主动问题解决，并在统一交互设置下对最先进的VLMs和扩散模型进行全面研究。

Result: 研究显示，表现最佳的模型仍然难以内化物理结构和因果约束，经常无法产生可靠的长时程计划，也不能稳健地将感知到的结构转化为有效动作。

Conclusion: CHAIN基准测试揭示了当前模型在物理结构理解和因果推理方面的局限性，为评估模型在物理约束下的交互能力提供了重要工具，有助于推动具身智能和交互系统的发展。

Abstract: Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.

</details>


### [73] [MIP Candy: A Modular PyTorch Framework for Medical Image Processing](https://arxiv.org/abs/2602.21033)
*Tianhao Fu,Yucheng Chen*

Main category: cs.CV

TL;DR: MIPCandy是一个基于PyTorch的医学图像处理框架，提供完整的模块化流程，通过实现单个方法即可获得完整工作流，同时保持对每个组件的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 医学图像处理需要处理高维体积数据、异构文件格式和领域特定训练流程的专用软件。现有框架要么提供需要大量集成工作的低级组件，要么强加僵化的单体管道难以修改。

Method: 基于PyTorch的模块化框架，核心是LayerT延迟配置机制，支持运行时替换卷积、归一化和激活模块而无需子类化。提供完整的数据加载、训练、推理和评估流程，只需实现build_network方法即可获得完整工作流。

Result: 框架内置k折交叉验证、自动ROI检测的数据集检查、深度监督、指数移动平均、多前端实验跟踪、训练状态恢复和商回归验证分数预测。扩展包生态系统提供预建模型实现，遵循一致的训练器-预测器模式。

Conclusion: MIPCandy是一个开源、免费的医学图像处理框架，提供灵活性和易用性的平衡，允许研究人员快速构建工作流同时保持对组件的控制，采用Apache-2.0许可证，需要Python 3.12+。

Abstract: Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.

</details>


### [74] [Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning](https://arxiv.org/abs/2602.21035)
*Junhao Xiao,Zhiyu Wu,Hao Lin,Yi Chen,Yahui Liu,Xiaoran Zhao,Zixu Wang,Zejiang He*

Main category: cs.CV

TL;DR: CLIPGlasses：一个即插即用框架，通过双阶段设计增强CLIP对否定视觉描述的理解能力，解决CLIP在否定理解上的局限性


<details>
  <summary>Details</summary>
Motivation: 像CLIP这样的视觉语言模型在理解否定概念时存在困难，经常将肯定和否定嵌入到相似的表示中（例如将"没有狗"与狗的图像匹配）。现有方法通过微调CLIP的文本编码器来改进否定理解，但存在过拟合风险。

Method: 提出CLIPGlasses框架，采用双阶段设计：1) Lens模块从文本嵌入中解耦否定语义；2) Frame模块预测上下文感知的排斥强度，将其整合到修改后的相似度计算中，以惩罚与否定语义的对齐，从而减少误匹配。

Result: 实验表明，配备CLIPGlasses的CLIP在域内性能上具有竞争力，并在跨域泛化方面优于最先进方法。在低资源条件下的优势尤为明显，表明其在不同领域间具有更强的鲁棒性。

Conclusion: CLIPGlasses是一个有效的即插即用框架，能够显著增强CLIP对否定视觉描述的理解能力，同时避免了微调带来的过拟合风险，在跨域泛化和低资源条件下表现出色。

Abstract: Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching "no dog" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.

</details>


### [75] [OmniOCR: Generalist OCR for Ethnic Minority Languages](https://arxiv.org/abs/2602.21042)
*Bonan Liu,Zeyu Zhang,Bingbing Meng,Han Wang,Hanshuo Zhang,Chengping Wang,Daji Ergu,Ying Cai*

Main category: cs.CV

TL;DR: OmniOCR是一个针对少数民族文字的通用OCR框架，通过动态低秩适应和稀疏正则化技术，在低资源/零样本设置下显著提升了少数民族文字的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前OCR技术主要关注拉丁文、中文等资源丰富的文字，而少数民族文字由于书写系统复杂、标注数据稀缺、历史与现代形式多样等问题，在低资源或零样本设置下面临泛化挑战。

Method: 提出OmniOCR框架，引入动态低秩适应（Dynamic LoRA）技术，在模型层和文字之间动态分配模型容量，同时使用稀疏正则化修剪冗余更新，实现紧凑高效的适应而不增加推理成本。

Result: 在藏文MNIST、水书、古彝文和东巴文四个数据集上的评估显示，OmniOCR在零样本基础模型和标准后训练方法中表现最优，相比最先进的基线模型，准确率提升了39%-66%。

Conclusion: OmniOCR为少数民族文字识别提供了一个有效的通用框架，通过动态适应和稀疏正则化技术，在保持参数效率的同时显著提升了识别准确率，解决了少数民族文字OCR的低资源挑战。

Abstract: Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.

</details>


### [76] [OCR-Agent: Agentic OCR with Capability and Memory Reflection](https://arxiv.org/abs/2602.21053)
*Shimin Wen,Zeyu Zhang,Xingdou Bian,Hongjie Zhu,Lulu He,Layi Shama,Daji Ergu,Ying Cai*

Main category: cs.CV

TL;DR: 提出一种新型迭代自校正框架，通过能力反思和记忆反思机制，让视觉语言模型能够自我诊断错误、制定修正计划，并避免重复尝试，显著提升推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型缺乏有效的自校正机制，难以独立纠正认知偏差，在多轮修正中常陷入重复无效尝试，无法稳定提升答案质量。

Method: 提出包含能力反思和记忆反思的自校正框架：能力反思用于诊断错误并生成修正计划；记忆反思用于回顾过去尝试以避免重复并探索新解决方案；最后通过严格重新推理优化答案。

Result: 在OCRBench v2基准测试中，OCR-Agent在英文子集上比当前开源SOTA模型InternVL3-8B提升+2.0分，中文子集提升+1.2分，在视觉理解(79.9)和推理(66.5)方面达到最先进水平，甚至超过更大的微调模型。

Conclusion: 结构化、自我感知的反思机制能够显著增强视觉语言模型的推理鲁棒性，无需额外训练即可实现性能提升。

Abstract: Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.

</details>


### [77] [Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction](https://arxiv.org/abs/2602.21100)
*Noé Artru,Rukhshanda Hussain,Emeline Got,Alexandre Messier,David B. Lindell,Abdallah Dib*

Main category: cs.CV

TL;DR: 提出了一种结合基础模型和优化方法优势的混合方法，从图像重建高保真3D头部几何，在减少相机需求和计算成本的同时达到密集视图摄影测量的质量


<details>
  <summary>Details</summary>
Motivation: 现有3D头部重建方法存在根本性限制：传统摄影测量需要大量相机阵列和计算；基础模型虽能单图重建但缺乏几何细节；优化方法保真度高但需要密集视图和昂贵计算。需要一种能结合两者优势的方法。

Method: 提出混合方法：1) 多视角表面法线预测模型，通过跨视角注意力扩展单目基础模型，前向传播产生几何一致的法线；2) 将这些预测作为强几何先验，在逆渲染优化框架中恢复高频表面细节。

Result: 方法优于现有单图和多视角方法，在减少相机需求和计算成本的同时，达到与密集视图摄影测量相当的高保真重建质量。

Conclusion: 提出的混合方法成功结合了基础模型和优化方法的优势，实现了高质量3D头部重建，在保真度、相机需求和计算成本之间取得了良好平衡。

Abstract: Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while optimization-based methods achieve higher fidelity but require dense views and expensive computation. We bridge this gap with a hybrid approach that combines the strengths of both paradigms. Our method introduces a multi-view surface normal prediction model that extends monocular foundation models with cross-view attention to produce geometrically consistent normals in a feed-forward pass. We then leverage these predictions as strong geometric priors within an inverse rendering optimization framework to recover high-frequency surface details. Our approach outperforms state-of-the-art single-image and multi-view methods, achieving high-fidelity reconstruction on par with dense-view photogrammetry while reducing camera requirements and computational cost. The code and model will be released.

</details>


### [78] [Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones](https://arxiv.org/abs/2602.21101)
*Rong Zou,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 该论文提出了一种融合异步事件流与运动模糊帧的统一框架，用于从高速无人机飞行中重建高保真辐射场，解决了高速飞行导致的图像模糊和位姿估计漂移问题。


<details>
  <summary>Details</summary>
Motivation: 高速飞行的无人机在电池有限条件下能快速完成基础设施检查、地形探索和搜救等任务，但高速会导致图像严重运动模糊，位姿估计产生显著漂移和噪声，使得对这类退化敏感的神经辐射场（NeRF）重建特别困难。

Method: 提出统一框架，将事件-图像融合嵌入NeRF优化过程，同时利用事件和帧模态联合优化基于事件的视觉惯性里程计先验，无需地面真值监督即可恢复清晰辐射场和准确相机轨迹。

Result: 在合成数据和真实世界高速无人机采集的序列上验证了方法有效性。即使在RGB帧严重运动模糊、位姿先验不可靠的高度动态无人机飞行条件下，仍能重建高保真辐射场并保留精细场景细节，在真实数据上相比最先进方法性能提升超过50%。

Conclusion: 该方法通过融合事件流与模糊帧，成功解决了高速无人机飞行中NeRF重建的挑战，实现了无需地面真值监督的高质量辐射场重建和相机轨迹估计，为快速无人机应用提供了有效的3D重建解决方案。

Abstract: Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.

</details>


### [79] [BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting](https://arxiv.org/abs/2602.21105)
*Jiaxing Yu,Dongyang Ren,Hangyu Xu,Zhouyuxiao Yang,Yuanqi Li,Jie Guo,Zhengkang Zhou,Yanwen Guo*

Main category: cs.CV

TL;DR: BrepGaussian：从2D图像学习3D参数化表示的框架，使用高斯泼溅渲染器和两阶段学习策略，实现几何重建和特征解耦


<details>
  <summary>Details</summary>
Motivation: 从非结构化数据恢复B-rep表示是计算机视觉和图形学中的重要挑战任务。现有深度学习方法依赖密集干净的点云，且难以泛化到新形状，需要从2D图像直接学习3D参数化表示的新方法

Method: 提出B-rep高斯泼溅框架，使用带可学习特征的高斯泼溅渲染器，配合特定拟合策略。采用两阶段学习：第一阶段捕获几何和边缘，第二阶段精炼面片特征，实现几何重建和特征学习的解耦

Result: 大量实验证明该方法在性能上优于现有最先进方法，能够实现干净的几何和一致的实例表示

Conclusion: BrepGaussian框架能够从2D图像有效学习3D参数化表示，解决了现有方法对点云数据的依赖和泛化能力不足的问题，为B-rep恢复提供了新思路

Abstract: The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.

</details>


### [80] [SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception](https://arxiv.org/abs/2602.21141)
*Jose Moises Araya-Martinez,Thushar Tom,Adrián Sanchis Reig,Pablo Rey Valiente,Jens Lambrecht,Jörg Krüger*

Main category: cs.CV

TL;DR: SynthRender是一个开源合成图像生成框架，结合领域随机化和Reality-to-Simulation技术，为缺乏3D模型的工业零件提供低成本、可迁移的训练数据，在多个基准测试中达到高精度。


<details>
  <summary>Details</summary>
Motivation: 工业物体感知需要大量标注数据，但获取和标注专有零件的真实数据成本高昂，成为广泛部署的主要障碍。现有监督深度学习模型在半非受控条件下需要大规模数据集才能实现稳健自动化。

Method: 开发了SynthRender开源框架，具有引导领域随机化能力；结合Reality-to-Simulation技术从2D图像创建3D资产；引入IRIS工业真实-模拟图像集，包含32个类别、约20,000个标签；通过消融实验确定高效数据生成指南。

Result: 在多个基准测试中表现优异：公共机器人数据集上达到99.1% mAP@50，汽车基准测试上达到98.3% mAP@50，IRIS数据集上达到95.3% mAP@50，超越了现有方法。

Conclusion: SynthRender框架结合领域随机化和Reality-to-Simulation技术，为缺乏3D文件的工业零件提供了低开销、可迁移的数据生成解决方案，显著降低了工业物体感知的数据获取成本。

Abstract: Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. Furthermore, we benchmark recent Reality-to-Simulation techniques for 3D asset creation from 2D images of real parts. Combined with Domain Randomization, these synthetic assets provide low-overhead, transferable data even for parts lacking 3D files. We also introduce IRIS, the Industrial Real-Sim Imagery Set, containing 32 categories with diverse textures, intra-class variation, strong inter-class similarities and about 20,000 labels. Ablations on multiple benchmarks outline guidelines for efficient data generation with SynthRender. Our method surpasses existing approaches, achieving 99.1% mAP@50 on a public robotics dataset, 98.3% mAP@50 on an automotive benchmark, and 95.3% mAP@50 on IRIS.

</details>


### [81] [LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis](https://arxiv.org/abs/2602.21142)
*Zhifan Jiang,Dong Yang,Vishwesh Nath,Abhijeet Parida,Nishad P. Kulkarni,Ziyue Xu,Daguang Xu,Syed Muhammad Anwar,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: LUMEN是一个针对纵向胸部X光图像解释优化的训练框架，通过多图像多任务指令微调提升预后和诊断性能


<details>
  <summary>Details</summary>
Motivation: 放射科医生分析纵向影像变化是耗时过程，需要开发能够提供预后能力的训练框架，以辅助临床决策支持

Method: 提出LUMEN训练框架，采用多图像和多任务指令微调，构建包含纵向研究的指令跟随数据集，支持预后视觉问答任务

Result: 在MIMIC-CXR和Medical-Diff-VQA数据集上，相比基线模型在诊断VQA任务中有显著改进，并展现出有前景的预后能力

Conclusion: 精心设计的指令微调视觉语言模型能够实现更准确、更具临床意义的纵向放射影像解释，在放射学决策支持中具有重要价值

Abstract: Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.

</details>


### [82] [Seeing Through Words: Controlling Visual Retrieval Quality with Language Models](https://arxiv.org/abs/2602.21175)
*Jianglin Lu,Simon Jenni,Kushal Kafle,Jing Shi,Handong Zhao,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了一种质量可控的检索新范式，通过生成语言模型将简短查询扩展为包含视觉属性和质量控制的描述性查询，从而改善文本到图像检索效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的文本到图像检索经常面临简短且不明确的用户查询挑战。这些查询通常只有一两个词，导致语义模糊、容易产生多种视觉解释，并且缺乏对检索图像质量的明确控制。

Method: 提出质量可控检索框架，利用生成语言模型作为查询补全函数，将不明确的查询扩展为包含姿态、场景、美学等细粒度视觉属性的描述性形式。通过基于相关性和美学评分模型导出的离散质量级别来条件化查询补全，使查询扩展既语义丰富又质量感知。

Result: 大量实验表明，该方法显著改善了检索结果，提供了有效的质量控制，弥合了现代视觉语言模型的表达能力与简短用户查询不明确性之间的差距。

Conclusion: 提出的质量可控检索框架具有三个关键优势：1) 灵活性，与任何预训练视觉语言模型兼容；2) 透明度，扩展后的查询对用户明确可解释；3) 可控性，能够将检索结果导向用户偏好的质量级别。

Abstract: Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.

</details>


### [83] [Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision](https://arxiv.org/abs/2602.21179)
*Nicolás Gaggion,Maria J. Ledesma-Carbayo,Stergios Christodoulidis,Maria Vakalopoulou,Enzo Ferrante*

Main category: cs.CV

TL;DR: Mask-HybridGNet：无需手动标注地标，直接使用标准像素级掩码训练基于图的医学图像分割模型，通过Chamfer距离监督和边缘正则化实现可变长度边界与固定长度地标预测的对齐，具有隐式图谱学习能力。


<details>
  <summary>Details</summary>
Motivation: 基于图的医学图像分割方法需要具有点对点对应关系的手动标注地标数据集，这在临床实践中很少存在，限制了其临床应用。需要一种能够利用现有标准像素级分割掩码训练图模型的方法。

Method: 结合Chamfer距离监督和边缘正则化，将可变长度的真实边界与固定长度的地标预测对齐，通过可微分光栅化进行细化。使用固定图邻接矩阵确保解剖结构的连通性和拓扑完整性。

Result: 在胸部X光、心脏超声、心脏MRI和胎儿成像等多个医学影像模态上，模型达到了与最先进像素级方法竞争的结果，同时确保了解剖合理性。预测的地标位置在不同患者间保持一致的解剖位置对应关系。

Conclusion: Mask-HybridGNet框架能够利用大量可用的标准分割掩码构建结构化模型，保持拓扑完整性并提供隐式对应关系，支持时间追踪、跨切片重建和形态学群体分析等临床应用。

Abstract: Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.

</details>


### [84] [Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning](https://arxiv.org/abs/2602.21186)
*Haoyi Jiang,Liu Liu,Xinjie Wang,Yonghao He,Wei Sui,Zhizhong Su,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Spa3R是一个自监督框架，通过预测性空间场建模从无姿态多视角图像学习统一、视角不变的空间表示，无需显式3D模态或空间指令调优，显著提升VLM的3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在3D空间理解方面表现肤浅，现有方法要么依赖显式3D模态，要么通过部分视角条件几何先验增强VLM，这些方法限制了可扩展性，并让语言模型承担从稀疏线索隐式重建整体3D几何的困难任务。作者认为空间智能应该从2D视觉中自然涌现，而不是通过显式空间指令调优强加。

Method: 提出Spa3R自监督框架，基于预测性空间场建模范式，从无姿态多视角图像学习紧凑的潜在表示，能够合成任意未见视角的特征场，从而内化对底层3D场景的整体连贯理解。通过轻量级适配器将预训练的Spa3R编码器集成到现有VLM中，形成Spa3-VLM，使语言推理基于全局空间上下文。

Result: 在具有挑战性的VSI-Bench上，Spa3-VLM在3D视觉问答任务上达到了58.6%的最新准确率，显著优于先前方法。

Conclusion: 预测性空间场建模为推进空间智能提供了一条可扩展的路径，证明空间智能可以从2D视觉中自然涌现，无需依赖显式3D模态或空间指令调优。

Abstract: While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.

</details>


### [85] [Human Video Generation from a Single Image with 3D Pose and View Control](https://arxiv.org/abs/2602.21188)
*Tiantian Wang,Chun-Han Yao,Tao Hu,Mallikarjun Byrasandra Ramalinga Reddy,Ming-Hsuan Yang,Varun Jampani*

Main category: cs.CV

TL;DR: HVG是一个从单图像生成高质量4D人体视频的潜在视频扩散模型，通过三维姿态和视角控制实现多视角时空一致的人体视频生成


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频合成方法在人体视频生成方面存在挑战，特别是从单图像推断视角一致、运动相关的服装褶皱仍然是一个难题

Method: 提出三个关键设计：1) 关节姿态调制，通过双维骨骼图捕捉三维关节的解剖关系，引入三维信息解决跨视角自遮挡；2) 视角和时间对齐，确保多视角一致性和参考图像与姿态序列之间的对齐；3) 渐进时空采样与时间对齐，保持长多视角动画的平滑过渡

Result: 在图像到视频任务上的大量实验表明，HVG在从多样化人体图像和姿态输入生成高质量4D人体视频方面优于现有方法

Conclusion: HVG能够从单图像生成高质量、多视角、时空一致的人体视频，解决了人体视频生成中的关键挑战

Abstract: Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.

</details>


### [86] [Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography](https://arxiv.org/abs/2602.21195)
*Xingyi Cheng,Julien Maufront,Aurélie Di Cicco,Daniël M. Pelt,Manuela Dezi,Daniel Lévy*

Main category: cs.CV

TL;DR: TomoROIS-SurfORA是一个用于冷冻电子断层扫描图像中直接、形状无关的感兴趣区域分割和表面形态分析的两步框架


<details>
  <summary>Details</summary>
Motivation: 目前冷冻电子断层扫描中感兴趣区域通常通过全结构分割后分析间接获得，这种方法对于连续复杂结构（如膜结构）效果有限，需要更直接的ROI分割和形态分析方法

Method: 开发了TomoROIS-SurfORA两步框架：TomoROIS使用深度学习进行ROI分割，可用小标注数据集训练；SurfORA将分割结构处理为点云和表面网格，提取形态特征如膜间距离、曲率和表面粗糙度

Result: 在体外重建的膜系统上验证了工具的有效性，能够自动定量分析膜接触位点和重塑事件（如内陷），支持开放和封闭表面分析

Conclusion: 该框架为冷冻电子断层扫描膜数据提供了直接的ROI分割和表面形态分析工具，虽然主要针对膜数据开发，但可广泛应用于科学成像中的ROI检测和表面分析

Abstract: Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [87] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: SA-SFT是一种轻量级自增强方法，通过让LLM在微调前生成自我对话数据并与任务数据混合，有效缓解灾难性遗忘问题，同时提升领域内性能。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）针对特定任务数据进行微调时，灾难性遗忘问题严重，会降低模型的通用知识和推理能力。现有方法如层冻结或外部数据混合存在局限性，需要更简单有效的解决方案。

Method: 提出SA-SFT（自增强监督微调）方法：1）让LLM在微调前生成自我对话数据；2）将自生成数据与任务数据混合；3）不改变优化或训练计划，保持训练流程简单。

Result: 在50个评估场景中，SA-SFT保持与原始模型相当的性能，在40个案例中取得最佳结果，优于层冻结和外部数据混合等基线方法。同时缓解了灾难性遗忘问题并提升了领域内性能。

Conclusion: 自增强为LLM适应提供了一种简单有效的机制，无需外部数据或额外调优即可缓解灾难性遗忘。理论分析表明遗忘部分源于风格诱导的参数漂移，而自生成数据的自对齐能有效抵消这种效应。

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [88] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: 知识蒸馏能创建比传统训练方法计算效率高2000倍以上的小型语言模型，其性能可与10倍大小的标准模型相媲美甚至超越


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中开发强大而高效的小型语言模型，需要验证知识蒸馏作为主要策略而非仅仅是压缩技术的有效性

Method: 对蒸馏模型与其原始版本和专有模型进行性能和计算成本的基准测试，提供效率的定量分析

Result: 蒸馏创建了优越的性能-计算曲线，8B蒸馏模型比其原始版本计算效率高2000倍以上，推理能力与10倍大小的标准模型相当甚至超越

Conclusion: 知识蒸馏不仅是压缩技术，更是构建最先进、可访问AI的主要策略

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [89] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 研究发现查询语句的特定结构特征（如从句嵌套深度、指代模糊等）与LLM产生幻觉的概率相关，为通过查询改写降低幻觉风险提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 传统上LLM幻觉被视为模型或解码策略的缺陷，但本文从语言学角度出发，认为查询语句的形式特征也会影响模型响应，需要系统研究查询特征与幻觉风险的关系。

Method: 基于古典语言学理论构建22维查询特征向量，涵盖从句复杂度、词汇稀有度、指代、否定、可回答性和意图明确性等维度，使用369,837个真实世界查询进行大规模分析。

Result: 分析揭示了稳定的"风险图谱"：深层从句嵌套和指代模糊等特征与更高的幻觉倾向相关，而明确的意图基础和可回答性则与较低的幻觉率相关；领域特异性等特征则表现出混合的、数据集和模型依赖的效应。

Conclusion: 研究建立了与幻觉风险相关的可观测查询特征表示，为引导性查询改写和未来干预研究铺平了道路，表明通过优化查询结构可以降低LLM幻觉风险。

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [90] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits是一个模型无关的上下文bandit框架，通过在线学习选择最优查询重写策略来减少大语言模型的幻觉问题，特别针对闭源模型设计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能力增强导致更多幻觉问题，但现有缓解工作主要关注开源模型的后检测和参数编辑。闭源模型在机构部署中占绝大多数，却缺乏相关研究，这尤其令人担忧。

Method: 提出QueryBandits框架，这是一个模型无关的上下文bandit系统，通过在线学习自适应选择最优查询重写策略。利用经验验证和校准的奖励函数，基于语义特征学习在线策略，仅通过前向传播机制改变模型行为。

Result: 在16个QA场景中，最佳QueryBandit（Thompson Sampling）相比No-Rewrite基线获得87.5%的胜率，分别比零样本静态策略（如Paraphrase或Expand）高出42.6%和60.3%。所有上下文bandit在所有数据集上都优于普通bandit，特征方差越大，臂选择方差也越大。

Conclusion: 没有单一的重写策略适用于所有查询，某些静态策略甚至比不重写产生更高的累积遗憾。QueryBandits通过在线学习语义特征策略，仅通过前向传播机制就能改变模型行为，使其适用于闭源模型，无需重新训练或基于梯度的适应。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [91] [How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity](https://arxiv.org/abs/2602.20372)
*Chundra Cathcart,Arne Rubehn,Katja Bocklage,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Jekaterina Mažara,Carlo Y. Meloni,David Snee,Johann-Mattis List*

Main category: cs.CL

TL;DR: 研究发现许多语言的数字系统在交流效率上并不如预期那样高效，挑战了先前关于精确递归数字系统优化效率的观点。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为精确递归数字系统通过平衡数字词汇库大小和数字术语的平均形态句法复杂性来优化交流效率，但作者认为这些研究未能充分解释语言所展现的复杂程度。

Method: 使用来自52种遗传多样语言的数据，采用区分可预测和不可预测异形（形式变异）的注释方案进行分析。

Result: 研究表明，世界上许多语言的数字系统在效率上明显低于预期水平。

Conclusion: 研究结果对数字系统研究和更广泛的语言演化研究具有重要意义，挑战了先前关于数字系统效率优化的假设。

Abstract: Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.

</details>


### [92] [Disentangling Geometry, Performance, and Training in Language Models](https://arxiv.org/abs/2602.20433)
*Atharva Kulkarni,Jacob Mitchell Springer,Arjun Subramonian,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 研究探索了Transformer权重几何特性（特别是unembedding矩阵的有效秩）与模型性能的关系，发现现有几何指标主要反映训练选择而非性能预测


<details>
  <summary>Details</summary>
Motivation: Transformer权重的几何特性在可解释性研究中很有用，但其对下游性能估计的效用尚不清楚。本文旨在系统研究模型性能与unembedding矩阵几何（特别是有效秩）之间的关系

Method: 使用108个OLMo风格语言模型进行受控实验，系统分析unembedding矩阵的有效秩与模型性能的关系，并扩展到其他几何指标和最终层表示

Result: 1) 性能最佳模型通常具有高有效秩，但这一趋势在不同任务和训练设置中并不普遍；2) 低有效秩不会导致小模型后期性能下降，而是与之共存；3) 有效秩受预训练超参数（如批次大小和权重衰减）强烈影响；4) 现有几何指标无法可靠预测下游性能

Conclusion: 模型几何特性（通过现有指标捕获）主要反映训练选择而非性能，现有几何指标不能可靠预测下游性能

Abstract: Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.

</details>


### [93] [Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning](https://arxiv.org/abs/2602.20528)
*Justin Lovelace,Christian Belardi,Sofian Zalouk,Adhitya Polavaram,Srivatsa Kundurthy,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: STAR-LDM结合潜在扩散规划与自回归生成，通过"思考"阶段在连续空间进行全局规划，显著提升语言理解和推理能力


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型局限于逐个token的决策，缺乏全局规划能力。STAR-LDM旨在通过引入扩散规划机制，在生成前进行语义层面的全局规划，提升语言理解和推理的质量

Method: STAR-LDM集成潜在扩散规划与自回归生成，包含"思考"阶段：暂停生成，通过扩散过程在连续空间精炼语义计划，然后再继续自回归生成。架构支持通过轻量级分类器进行直接控制

Result: 在语言理解基准测试中显著优于相似规模模型；在LLM作为评判者的比较中，叙事连贯性和常识推理方面获得超过70%的胜率；通过轻量级分类器实现细粒度控制，保持更好的流畅性-控制权衡

Conclusion: STAR-LDM通过结合扩散规划和自回归生成，实现了更好的全局语义规划能力，在语言理解和推理任务上表现优异，同时支持高效的控制机制而无需重新训练模型

Abstract: The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a "thinking" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.

</details>


### [94] [Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books](https://arxiv.org/abs/2602.20647)
*W. Frederick Zimmerman*

Main category: cs.CL

TL;DR: 该研究提出语义新颖性作为衡量叙事结构的信息论指标，通过分析28,606本PG19书籍发现8种叙事形状原型，其中新颖性轨迹的方差最能预测读者参与度，并揭示了体裁和历史时期对叙事结构的显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种基于信息论的量化方法来分析大规模语料库中的叙事结构，超越传统的情感或主题分析，探索语义新颖性动态如何影响读者参与度。

Method: 使用768维SBERT嵌入计算段落级语义新颖性（当前段落与前面所有段落质心的余弦距离），将新颖性曲线简化为16段PAA表示，通过Ward-linkage聚类识别叙事形状原型，并分析多种统计特征与读者参与度的关系。

Result: 发现了8种典型的叙事形状原型（从陡降到陡升），新颖性轨迹的方差（Volume）是长度无关的最强读者参与度预测因子（部分rho=0.32），体裁对叙事形状有极强约束（p<10^{-242}），历史分析显示1840-1910年间书籍变得越来越可预测。

Conclusion: 信息密度动态是叙事结构的一个基本维度，与情感或主题不同，对读者参与度有可测量的影响，研究还揭示了语料库研究中长度混杂效应的重要性。

Abstract: I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty

</details>


### [95] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: CARE是一个基于LLM的框架，能够从心理咨询对话记录中自动预测多维度的治疗联盟评分并生成可解释的理由，在预测准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统心理咨询后问卷负担重且延迟，现有计算方法只能产生粗略评分、缺乏可解释理由且无法建模完整的会话上下文，需要更准确、可解释的治疗联盟评估工具。

Method: 基于CounselingWAI数据集，使用9,516个专家标注的理由进行增强监督微调，以LLaMA-3.1-8B-Instruct为骨干模型，开发了CARE框架来预测联盟评分并生成解释性理由。

Result: CARE在预测准确性上优于主流LLM，将咨询师评估与客户感知联盟之间的差距显著缩小，与客户评分的Pearson相关性提高了70%以上，同时能生成高质量、上下文相关的解释理由。

Conclusion: CARE框架展示了作为AI辅助工具支持心理健康的潜力，能够识别联盟建设中的常见挑战，分析互动模式如何影响联盟发展，并提供可操作的见解。

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [96] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA是一种新型参数高效微调框架，通过从预训练权重矩阵中提取和重用聚类参数组，形成多个共享单个可训练低秩矩阵的低秩组件，在减少可训练参数的同时保持模型能力


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，即使是LoRA的最新变体也会引入大量可训练参数开销，而过度降低秩数又会在复杂多任务场景中显著降低性能，需要打破这种权衡

Method: 从预训练权重矩阵中提取和重用聚类参数组，用这些组形成多个低秩组件，所有组件共享单个初始化的可训练低秩矩阵，从而减少可训练参数数量

Result: 在数学推理、代码生成、MMLU、常识问答和安全对齐五个基准测试中，ID-LoRA优于全微调和现有PEFT基线，比标准LoRA减少高达46%的可训练参数；在多任务场景中，在代码和MMLU任务上超越LoRA及其变体，仅需传统LoRA 54%的参数

Conclusion: ID-LoRA成功打破了参数效率与性能之间的权衡，在显著减少可训练参数的同时保持甚至提升了模型性能，为大规模语言模型的高效微调提供了新方案

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [97] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: 提出自适应文本匿名化新任务，通过任务特定提示优化框架自动构建匿名化指令，适应不同隐私目标、领域和下游使用模式，在多个数据集上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有匿名化方法依赖静态手动设计策略，缺乏适应不同需求的灵活性，且难以跨领域泛化。文本匿名化是高度上下文敏感的问题，需要在隐私保护和效用保持之间找到适当平衡。

Method: 提出自适应文本匿名化任务框架，通过任务特定提示优化自动为语言模型构建匿名化指令，能够适应不同的隐私目标、数据领域和下游使用模式。

Result: 在涵盖五个不同领域、隐私约束和效用目标的数据集上评估，框架在所有设置中始终实现比现有基线更好的隐私-效用权衡，计算效率高，开源语言模型性能与大型闭源模型相当，并能发现新的匿名化策略。

Conclusion: 自适应文本匿名化框架通过自动优化提示，能够灵活适应多样化的隐私-效用需求，在多个领域和设置中优于现有方法，为文本匿名化提供了更灵活有效的解决方案。

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [98] [Explicit Grammar Semantic Feature Fusion for Robust Text Classification](https://arxiv.org/abs/2602.20749)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CL

TL;DR: 提出一种轻量级文本分类模型，通过显式编码句子级语法结构与冻结的上下文嵌入融合，在资源受限环境下优于基线模型2%-15%


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的NLP模型计算密集，不适合资源受限环境，需要一种轻量级但能同时捕捉语法和语义特征的分类模型

Method: 将句子级语法结构（句法组合、短语模式、复杂度指标）编码为紧凑的语法向量，与冻结的上下文嵌入融合，形成统一表示；使用DBNs、LSTMs、BiLSTMs、BERT、XLNET等模型进行训练评估

Result: 统一特征表示模型能同时捕捉文本的语义和结构特性，在异构领域表现优于基线模型2%-15%，在边缘设备上提供更好性能

Conclusion: 将语法作为显式归纳偏置而非可学习模块，构建了轻量级模型，在资源受限环境下实现了更好的文本分类性能

Abstract: Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices

</details>


### [99] [SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing](https://arxiv.org/abs/2602.20751)
*Yifei Xu,Guilherme Potje,Shivam Shandilya,Tiancheng Yuan,Leonardo de Oliveira Nunes,Rakshanda Agarwal,Saeid Asgari,Adam Atkinson,Emre Kıcıman,Songwu Lu,Ranveer Chandra,Tusher Chakraborty*

Main category: cs.CL

TL;DR: SibylSense：一种通过可调记忆库自适应生成评估标准的推理时学习方法，用于提升开放生成任务的强化学习后训练效果


<details>
  <summary>Details</summary>
Motivation: 开放生成任务的RL后训练中，设计对齐且鲁棒的奖励函数是关键挑战。现有评估标准方法存在成本高、表面化、不一致、易饱和和奖励黑客等问题，需要更有效的自适应评估标准生成方法。

Method: SibylSense采用推理时学习方法，通过可调记忆库自适应冻结的评估标准生成器。使用基于验证器的项目奖励更新记忆，通过参考-候选答案判别性差距衡量。交替进行记忆调优和评估标准对抗性策略更新，生成满足评估标准的候选答案。

Result: 在两个开放生成任务上的实验表明，SibylSense能产生更具判别性的评估标准，并在下游RL性能上优于静态和非自适应基线方法。

Conclusion: SibylSense通过自适应评估标准生成解决了开放生成RL后训练中的奖励设计问题，提供了一种可扩展且鲁棒的解决方案。

Abstract: Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.

</details>


### [100] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: OP-GRPO是一个强化学习框架，让单个大语言模型能够生成多元视角的回复，无需显式提示或模块化编排，实现了"小模型，大视角覆盖"的效果。


<details>
  <summary>Details</summary>
Motivation: 现有对齐范式在捕捉人类价值观的多元性方面存在局限，需要一种能够从单一查询生成多样化视角回复的方法。

Method: OP-GRPO框架包含两个主要步骤：1）相似性估计器训练，微调Sentence Transformer用于Overton Pluralism任务；2）OP-GRPO训练，将相似性估计器整合到双奖励系统中，确保既覆盖真实人类视角又保持每个视角的独特性。

Result: 训练后的Qwen2.5-3B-Instruct模型在自然语言推理基准上相对20B GPT-OSS基线提升了37.4%的准确率，相对模块化架构基线提升了19.1%。使用GPT-4.1作为大语言模型评判的额外评估进一步证实了方法的鲁棒性。

Conclusion: OP-GRPO框架成功实现了隐式Overton Pluralism，使单个大语言模型能够生成多元视角的回复，在保持模型规模较小的同时显著提升了视角覆盖能力。

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [101] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出一种新的尾部感知散度方法，用于语言模型蒸馏，通过解耦教师模型前K个高概率预测与低概率预测的贡献，减少教师模式主导，增强分布尾部信息的影响。


<details>
  <summary>Details</summary>
Motivation: 传统KL散度在语言模型蒸馏中主要受教师模型最高概率的token（教师模式）主导，降低了低概率但可能信息丰富的分布尾部成分的影响。

Method: 提出一种新的尾部感知散度方法，将教师模型前K个高概率预测与低概率预测的贡献解耦，同时保持与KL散度相同的计算特性。

Result: 实验结果表明，改进的蒸馏方法在解码器模型的预训练和监督蒸馏中，在各种数据集上都表现出竞争性能。蒸馏过程高效，可以在学术预算下处理大型数据集，无需工业级计算资源。

Conclusion: 尾部感知散度方法有效解决了传统KL散度中教师模式主导的问题，增强了分布尾部信息在蒸馏过程中的贡献，为语言模型蒸馏提供了更高效且性能优异的替代方案。

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


### [102] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: FinAnchor是一个轻量级框架，通过锚定表示方法整合多个LLM的嵌入，无需微调底层模型，在金融NLP任务中优于单模型基线和标准集成方法。


<details>
  <summary>Details</summary>
Motivation: 金融长文档预测面临挑战：可操作信号稀疏且被噪声掩盖，不同任务和时间段的最优LLM各不相同。需要一种能够有效整合多个LLM嵌入的方法。

Method: FinAnchor框架选择锚定嵌入空间，学习线性映射将其他模型的表示对齐到该锚定空间，然后聚合这些对齐特征形成统一表示用于下游预测。

Result: 在多个金融NLP任务中，FinAnchor始终优于强大的单模型基线和标准集成方法，证明了锚定异构表示对稳健金融预测的有效性。

Conclusion: FinAnchor通过锚定异构表示的方法，提供了一种轻量级且有效的框架，能够整合多个LLM的嵌入，显著提升金融预测性能。

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [103] [Exa-PSD: a new Persian sentiment analysis dataset on Twitter](https://arxiv.org/abs/2602.20892)
*Seyed Himan Ghaderi,Saeed Sarbazi Azad,Mohammad Mehdi Jaziriyan,Ahmad Akbari*

Main category: cs.CL

TL;DR: 本文介绍了Exa波斯语情感分析数据集，包含12,000条波斯语推文，标注为积极、中性和消极三类，使用预训练模型评估达到79.87的Macro F-score。


<details>
  <summary>Details</summary>
Motivation: 波斯语自然语言处理面临挑战，现有数据集多集中在特定主题（如产品、食品、酒店），而社交媒体中用户常使用讽刺、口语化表达，因此需要专门的波斯语Twitter情感分析数据集。

Method: 收集波斯语推文构建Exa数据集，包含12,000条推文，由5名波斯语母语标注者标注为积极、中性和消极三类。使用预训练的Pars Bert和Roberta作为基础模型进行评估。

Result: 评估达到79.87的Macro F-score，表明模型和数据集对于情感分析系统具有足够价值。

Conclusion: Exa波斯语情感分析数据集填补了波斯语社交媒体情感分析数据的空白，为波斯语NLP研究提供了有价值的资源。

Abstract: Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.

</details>


### [104] [The Art of Efficient Reasoning: Data, Reward, and Optimization](https://arxiv.org/abs/2602.20945)
*Taiqiang Wu,Zenan Zu,Bo Zhou,Ngai Wong*

Main category: cs.CL

TL;DR: 本文系统研究了LLMs的高效推理机制，提出了更细粒度的评估指标，揭示了训练过程的两阶段范式，并通过大量实验发现了避免长度崩溃的关键方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能从扩展的思维链推理中获益，但也面临沉重的计算开销。当前的高效推理方法通常通过强化学习进行奖励塑造，但缺乏对其机制的深入理解。

Method: 采用统一的实验协议，对训练提示和回滚、奖励塑造、优化策略进行解构分析。提出在相对简单的提示上进行训练，确保正奖励信号的密度，避免长度崩溃。使用Qwen3系列模型（0.6B到30B）进行验证。

Result: 揭示了训练过程遵循两阶段范式：长度适应和推理精炼。发现学习到的长度偏差可以跨领域泛化。通过约20万GPU小时的实验验证了方法的有效性，并在不同规模的模型上展示了鲁棒性和泛化能力。

Conclusion: 研究为LLMs的高效推理提供了有价值的见解和实践指南，证明了在相对简单的提示上训练可以避免长度崩溃，且学习到的长度偏差具有跨领域泛化能力。

Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.

</details>


### [105] [Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models](https://arxiv.org/abs/2602.20966)
*Paola Merlo,Chunyang Jiang,Giuseppe Samo,Vivi Nastase*

Main category: cs.CL

TL;DR: 本文提出了一种受智力测试启发的Blackbird语言矩阵任务，构建了结构化多选数据集，用于探究大语言模型的语言对象识别、系统性模式检测等核心能力。


<details>
  <summary>Details</summary>
Motivation: 为了深入探究当前大语言模型的核心能力：能否检测语言对象及其属性？能否识别和使用跨句子的系统性模式？更容易出现语言错误还是推理错误？这些错误如何相互作用？需要构建具有丰富结构的自然数据集来支持多方面的语言模型能力研究。

Method: 设计了Blackbird语言矩阵任务，构建了多层级结构的多选问题数据集：句子内部、输入序列之间、候选答案内部。数据集经过精心策划但保持自然性，部分手工构建，包含学习上下文和预期答案。

Result: BLM任务虽然具有挑战性，但可以通过简单基线模型或在多种语言中达到良好性能水平，更定制化的模型表现更好。模型表示中包含解决语言任务所需的语法对象和属性，这些解决方案是通过检测跨句子的系统性模式实现的。

Conclusion: 精心策划的结构化数据集支持对语言和大语言模型特性的多方面研究。BLM数据集因其精心设计的结构、包含学习上下文和预期答案、部分手工构建等特点，属于能够支持可解释性研究的类别，有助于理解大语言模型的行为机制。

Abstract: This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?
  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.
  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.

</details>


### [106] [Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving](https://arxiv.org/abs/2602.20973)
*Yuliang Ji,Fuchen Shen,Jian Wu,Qiujie Xie,Yue Zhang*

Main category: cs.CL

TL;DR: 论文提出了PC-FOL数据集，专注于基于案例的推理问题，以评估大语言模型在数学推理中的能力，发现线性推理与案例推理之间存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集主要关注线性推理，忽略了反证法和分情况证明等其他重要推理形式，这些形式对于全面评估大语言模型的推理能力至关重要。

Method: 1. 引入由专业数学家标注的新型一阶逻辑数据集PC-FOL，专注于基于案例的推理问题；2. 所有实例都配有手动编写的自然语言证明；3. 对领先的大语言模型进行实验评估；4. 基于图模型进行理论分析。

Result: 实验结果显示，大语言模型在线性推理和基于案例的推理问题之间存在显著的性能差距。理论分析为观察到的两种推理类型之间的差异提供了解释。

Conclusion: 这项工作揭示了自动自然语言数学证明生成领域的核心挑战，为未来研究铺平了道路，强调了需要更全面地评估大语言模型的数学推理能力。

Abstract: To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.

</details>


### [107] [Evaluating Proactive Risk Awareness of Large Language Models](https://arxiv.org/abs/2602.20976)
*Xuan Luo,Yubin Chen,Zhiyu Hou,Linpu Yu,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 论文提出了一种主动风险意识评估框架，用于衡量LLM能否在损害发生前预测潜在危害并提供警告，并在环境生态领域构建了Butterfly数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地嵌入日常决策，其安全责任需要从应对明确有害意图扩展到预测非故意但具有后果的风险，特别是在环境生态领域。

Method: 提出了主动风险意识评估框架，构建了包含1,094个查询的Butterfly数据集，模拟可能引发潜在生态影响的普通解决方案寻求活动，并在五个广泛使用的LLM上进行实验。

Result: 实验结果显示：在长度受限的响应下主动意识显著下降，跨语言表现相似，在（多模态）物种保护方面存在持续盲点，突显了当前安全对齐与现实生态责任要求之间的关键差距。

Conclusion: 当前LLM的安全对齐与真实世界生态责任要求存在差距，需要在LLM部署中建立主动安全防护机制。

Abstract: As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.

</details>


### [108] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: 该研究提出了一种混合方法，使用大语言模型进行方面识别，同时采用传统机器学习方法进行大规模情感分类，成功分析了470万条餐厅评论，证明了该方法在酒店业和其他服务行业大规模客户反馈分析中的实用性。


<details>
  <summary>Details</summary>
Motivation: 客户评论已成为企业和顾客的重要信息来源，但有效分析数百万条非结构化评论仍然具有挑战性。虽然大语言模型在自然语言理解方面表现出潜力，但由于计算成本和可扩展性问题，它们在大规模评论分析中的应用受到限制。

Method: 采用混合方法：使用ChatGPT分析抽样餐厅评论以识别关键方面，然后基于人工标注的评论开发情感分类器，最后将该方法应用于从主要在线平台收集的17年间的470万条评论，并进行回归分析。

Result: 回归分析显示，机器标注的方面在不同餐饮体验、菜系和地理区域中显著解释了餐厅总体评分的方差。该方法成功实现了大规模客户反馈的基于方面的情感分析自动化。

Conclusion: 将大语言模型与传统机器学习方法相结合，可以有效自动化大规模客户反馈的基于方面的情感分析，为酒店业和其他服务行业的研究者和从业者提供了一个实用的分析框架。

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


### [109] [Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.21103)
*Sanket Badhe,Deep Shah*

Main category: cs.CL

TL;DR: 论文提出Prompt-Level Distillation（PLD）方法，通过从教师模型中提取显式推理模式，组织成结构化指令列表放入学生模型的系统提示中，使小型模型在保持低延迟的同时达到前沿性能，并保持决策过程透明可验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在明显缺陷：Chain-of-Thought提示准确但延迟高、推理成本大；微调小模型牺牲可解释性且引入显著资源开销。需要一种既能保持高性能、低延迟，又能维持透明可验证推理过程的方法。

Method: 提出Prompt-Level Distillation（PLD）方法：1）从教师模型中提取显式推理模式；2）将这些模式组织成结构化、表达性强的指令列表；3）将指令列表作为学生模型的系统提示；4）使小型模型能够模仿教师模型的推理过程。

Result: 在StereoSet和Contract-NLI数据集上使用Gemma-3 4B模型评估：Macro F1分数分别从57%提升到90.0%，从67%提升到83%。紧凑模型能够匹配前沿性能，同时保持可忽略的延迟开销。

Conclusion: PLD方法成功解决了现有方法的局限性，使小型模型在保持低延迟的同时达到高性能，且决策过程完全透明可验证。该方法特别适用于法律、金融、内容审核等受监管行业，以及高吞吐量用例和边缘设备。

Abstract: Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\% to 90.0\% and 67\% to 83\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.

</details>


### [110] [PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data](https://arxiv.org/abs/2602.21165)
*Samah Fodeh,Linhai Ma,Yan Wang,Srivani Talakokkul,Ganesh Puthiaraju,Afshan Khan,Ashley Hagaman,Sarah Lowe,Aimee Roundtree*

Main category: cs.CL

TL;DR: PVminer是一个专门用于分析患者声音的NLP框架，通过多标签多分类预测任务，结合患者特定BERT编码器和主题建模，在患者-提供者安全通信中有效识别患者中心沟通和社会健康决定因素。


<details>
  <summary>Details</summary>
Motivation: 患者生成的文本（如安全消息、调查、访谈）包含丰富的患者声音，反映了沟通行为和社会健康决定因素。传统定性编码方法劳动密集且难以扩展到大规模患者消息，现有ML/NLP方法通常将患者中心沟通和社会健康决定因素作为独立任务处理，或使用不适合患者语言的模型。

Method: PVminer将患者声音检测构建为多标签多分类预测任务，整合患者特定BERT编码器（PV-BERT-base和PV-BERT-large）、用于主题增强的无监督主题建模（PV-Topic-BERT），以及针对Code、Subcode和Combo级别标签的微调分类器。在微调和推理过程中融入主题表示以丰富语义输入。

Result: PVminer在分层任务中表现优异，优于生物医学和临床预训练基线，F1分数达到82.25%（Code）、80.14%（Subcode）和最高77.87%（Combo）。消融研究表明作者身份和基于主题的增强各自带来显著性能提升。

Conclusion: PVminer提供了一个有效的领域适应NLP框架，用于结构化患者-提供者安全通信中的患者声音，解决了现有方法的局限性，并在多个层次上实现了高性能的患者声音检测。

Abstract: Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.

</details>
