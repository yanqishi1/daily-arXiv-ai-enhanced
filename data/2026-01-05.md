<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 48]
- [cs.CL](#cs.CL) [Total: 29]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld是一个实时多模态4D世界建模框架，通过生成-重建-引导范式统一视频生成、动态场景重建和长期世界记忆，实现实时交互和长期一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在实时交互、长期一致性和动态场景持久记忆方面存在局限，阻碍了它们发展为实用的世界模型。需要一种能够统一视频生成、动态场景重建和长期记忆的框架。

Method: 提出TeleWorld框架，采用生成-重建-引导范式：生成的视频流被连续重建为动态4D时空表示，然后引导后续生成以保持一致性。使用增强的自回归扩散视频模型，结合宏观-微观规划（MMPL）减少误差累积，以及高效的分布匹配蒸馏（DMD）实现实时合成。

Result: TeleWorld在静态和动态世界理解、长期一致性和实时生成效率方面表现优异，实现了动态对象建模和静态场景表示在统一4D框架中的无缝集成。

Conclusion: TeleWorld是向实用、交互式和计算可访问的世界模型迈出的重要一步，为多模态生成和具身智能提供了具有记忆功能的交互式世界模型。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 本文提出通过噪声优化来解决文本到图像生成模型中的模式崩溃问题，相比传统方法，该方法能提高生成多样性同时保持模型保真度


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型存在显著的模式崩溃问题，即给定相同文本提示时生成的图像缺乏多样性。现有方法主要通过引导机制或生成大量候选再筛选来解决，但本文探索了不同的方向

Method: 采用噪声优化方法，通过简单的噪声优化目标来缓解模式崩溃，同时分析噪声的频率特性，探索不同频率分布的噪声初始化对优化和搜索的影响

Result: 实验表明噪声优化方法在生成质量和多样性方面都取得了优越的结果，既能减轻模式崩溃，又能保持基础模型的保真度

Conclusion: 噪声优化是解决文本到图像生成中模式崩溃问题的有效方法，通过优化噪声参数可以显著提高生成多样性，同时保持生成质量

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: Spatial4D-Bench是一个用于评估多模态大语言模型4D空间智能的大规模基准测试，包含约4万个问题-答案对，涵盖18个任务和6个认知类别，揭示了现有模型在4D空间推理方面的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类天生具备4D空间智能（感知物体随时间移动或变化的能力），但当前多模态大语言模型在这方面的能力尚不明确。现有空间智能基准测试要么规模小，要么多样性不足，无法全面评估模型的4D空间推理能力。

Method: 提出了Spatial4D-Bench基准测试，这是一个大规模、多任务的评估基准，包含约40,000个问题-答案对，涵盖18个明确定义的任务。这些任务被系统性地组织到六个认知类别中：物体理解、场景理解、空间关系理解、时空关系理解、空间推理和时空推理。

Result: 对多种最先进的开源和专有多模态大语言模型进行了基准测试，发现它们在多种4D空间推理方面存在显著局限性，如路线规划、动作识别和物理合理性推理等。

Conclusion: Spatial4D-Bench为评估MLLMs的空间认知能力提供了一个结构化、全面的基准测试，揭示了当前模型在4D空间智能方面的不足，希望该基准能够促进开发更接近人类水平4D空间智能的MLLMs。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: CMP框架通过压缩历史遍历数据学习空间先验，仅需32KB/km²存储空间，显著提升3D目标检测性能


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶视觉系统通常将每个位置视为首次访问，忽略了历史遍历数据中蕴含的宝贵空间先验信息。人类驾驶员很少去无人去过的地方，大多数自动驾驶系统部署区域都曾被访问过，但现有系统未能有效利用这些历史信息。

Method: 提出压缩地图先验(CMP)框架，从历史遍历数据中学习空间先验。使用二值化哈希映射存储先验信息，存储密度仅为32KB/km²，比密集存储减少20倍。该框架可轻松集成到主流3D感知系统中，几乎不增加额外计算成本。

Result: 在nuScenes数据集上，CMP框架显著且一致地提升了多种架构的3D目标检测性能。压缩存储方案实现了20倍的存储效率提升。

Conclusion: 利用历史遍历数据中的空间先验信息可以有效提升自动驾驶视觉系统的感知性能。CMP框架提供了一种高效、低成本的集成方案，为自动驾驶感知系统利用历史信息开辟了新途径。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [5] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: FCMBench-V1.0是一个专门针对金融信贷领域的大规模多模态基准测试，包含4,043张合规图像和8,446个QA样本，用于评估视觉语言模型在信贷文档理解和风险评估中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在信贷风险评估和文档审查中的广泛应用，迫切需要能够反映金融信贷特定文档和工作流程、包含信贷特定理解能力、保持隐私合规性且不牺牲实用性的领域特定基准测试。

Method: 通过封闭式合成-采集流水线构建样本：手动合成带有虚拟内容的文档模板，并在内部采集场景感知图像。评估框架包含三个维度：感知（3个基础任务）、推理（4个信贷特定任务）和鲁棒性（10种现实采集伪影类型）。

Result: 在评估的23个最先进视觉语言模型中，Gemini 3 Pro作为商业模型获得最佳F1分数（64.61%），Qwen3-VL-235B作为开源基线获得最佳分数（57.27%），而专门的金融信贷模型Qfin-VL-Instruct获得最高总体分数（64.92%。鲁棒性评估显示，即使在采集伪影下，表现最佳的模型也会出现明显性能下降。

Conclusion: FCMBench能够有效区分现代视觉语言模型的性能差异和鲁棒性，为金融信贷领域的多模态AI评估提供了专门的基准测试工具，同时解决了隐私合规和现实实用性的平衡问题。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [6] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文提出了FaceFocalDesc问题，即针对任意选定面部区域生成和识别包含面部动作单元、情绪状态和年龄估计的多属性自然语言描述，并构建了相应数据集，开发了基于Qwen2.5-VL的Focal-RegionFace模型进行面部状态分析。


<details>
  <summary>Details</summary>
Motivation: 当前面部分析研究在针对任意选定面部区域生成多属性自然语言描述方面存在不足。作者认为系统能够聚焦于个体面部区域将带来更好的理解和控制能力，因此需要解决这一未充分探索的问题。

Method: 1) 构建了针对任意选定面部区域的多属性描述数据集，提供丰富的区域级标注和自然语言描述；2) 提出了基于Qwen2.5-VL的Focal-RegionFace视觉语言模型，通过多个渐进微调阶段逐步细化对局部面部特征的关注，实现可解释的年龄估计、面部动作单元和情绪检测。

Result: 实验结果表明，Focal-RegionFace在新基准测试中，无论是传统广泛使用的指标还是新提出的指标，都取得了最佳性能，充分验证了其在细粒度多属性面部区域聚焦分析场景中的有效性和多功能性。

Conclusion: 本文成功解决了面部分析中针对任意选定区域生成多属性自然语言描述的问题，通过构建新数据集和开发Focal-RegionFace模型，实现了对局部面部特征的可解释分析，为细粒度面部状态分析提供了有效解决方案。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [7] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D是一个无需训练的三维变形框架，利用结构化潜在表示生成高质量跨类别三维变形序列


<details>
  <summary>Details</summary>
Motivation: 三维变形面临语义一致性和时间平滑性的挑战，尤其是在跨类别变形时。现有方法难以生成高质量的三维变形序列

Method: 基于结构化潜在表示，提出变形交叉注意力机制融合源和目标特征以保持结构连贯性，以及时间融合自注意力机制增强时间一致性，并采用方向校正策略解决姿态模糊问题

Result: 实验表明该方法能生成最先进的变形序列，即使在具有挑战性的跨类别情况下也表现出色，并支持解耦变形和三维风格迁移等高级应用

Conclusion: MorphAny3D通过智能融合结构化潜在特征，在无需额外训练的情况下实现了高质量的三维变形，并可推广到其他基于结构化潜在表示的生成模型

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [8] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出了一种基于3D实例分割的作物计数框架，利用多视角2D图像和神经辐射场（NeRF）实现精确的作物数量统计，无需作物特定参数调优。


<details>
  <summary>Details</summary>
Motivation: 室外农田环境中，部分遮挡和作物聚集造成的模糊性给基于图像的作物计数方法带来了巨大挑战，需要更精确的计数方法。

Method: 使用多视角2D图像，结合神经辐射场（NeRF）进行视图合成，引入作物可见性和掩码一致性评分，结合3D信息实现3D实例分割。

Result: 在棉花、苹果和梨三个农业数据集上验证了方法的有效性，展示了在不同作物颜色、形状和大小变化下的稳定计数性能，优于现有技术。

Conclusion: 提出的3D实例分割框架能够实现高精度的作物计数，消除了对作物特定参数调优的依赖，并贡献了棉花植物数据集以促进进一步研究。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [9] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: IntraStyler：一种基于示例的样式合成方法，无需先验知识即可捕获多样的域内样式，用于改善跨模态域适应的图像翻译和数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法主要关注源域和目标域之间的域偏移，而域内变异性研究不足。传统方法需要预先指定域内变化进行样式合成，这在实践中不切实际。

Method: 提出IntraStyler方法，使用示例图像指导样式合成，使输出样式匹配示例样式。引入基于对比学习的样式编码器来提取纯样式特征，无需先验知识即可捕获多样域内样式。

Result: 在CrossMoDA 2023数据集上评估，实验证明该方法在可控样式合成方面有效，多样化的合成数据对下游分割任务有益。

Conclusion: IntraStyler能够无需先验知识捕获多样域内样式，通过可控样式合成生成多样化数据，提升跨模态域适应性能，特别是在医学图像分割任务中。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [10] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 该研究提出使用强化学习来提升多模态大语言模型的视觉推理能力，通过设计多种奖励函数激励模型进行更长的结构化推理，减少视觉信息绕过问题，在Qwen-2.5-VL-7B模型上取得了5.56%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在生成推理链时缺乏对视觉信息的充分整合，限制了其在需要精确视觉感知的任务（如视觉谜题）上的表现。研究表明视觉感知是这类任务的关键瓶颈，将图像转换为文本描述能显著提升性能。

Method: 采用奖励驱动的强化学习方法，设计了六种针对不同推理方面的奖励函数（包括图像理解、思维步骤和答案准确性），使用组相对策略优化（GRPO）来明确激励更长的结构化推理，并减轻视觉信息绕过问题。

Result: 在Qwen-2.5-VL-7B模型上实现了5.56%的性能提升，在领域内和领域外设置下都获得了一致的改进。实验还显示，将图像转换为文本描述能为Claude 3.5带来26.7%的提升，为Claude 3.7带来23.6%的提升。

Conclusion: 强化学习是解锁开源多模态大语言模型长视觉推理能力的有效机制，无需昂贵的监督数据。通过精心设计的奖励函数和GRPO优化，可以显著提升模型在需要精确视觉感知任务上的表现。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [11] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: LooC是一种新型向量量化方法，通过低维码本组合量化实现高容量且紧凑的向量表示，性能优于现有方法


<details>
  <summary>Details</summary>
Motivation: 随着数据和模型复杂度增加，需要高容量但更紧凑的向量量化方法来解决传统VQ方法在码本容量和紧凑性之间的冲突

Method: 1. 引入参数高效的码本设计，将码向量视为特征向量的低维组合单元而非独立匹配；2. 采用参数自由的外推-插值机制增强特征平滑；3. 作为即插即用模块兼容现有VQ方法

Result: 在不同任务、数据集和架构上的广泛评估表明，LooC优于现有VQ方法，在显著减小码本规模的同时实现了最先进的性能

Conclusion: LooC成功解决了向量量化中容量与紧凑性的矛盾，通过低维组合码本设计实现了高效、紧凑且性能优越的向量量化方案

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [12] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: SynDR-IQA通过重塑合成数据分布来解决BIQA模型泛化能力不足的问题，提出分布感知多样性内容上采样和密度感知冗余聚类下采样两种策略，在多个跨数据集设置中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 盲图像质量评估（BIQA）面临大规模标注数据稀缺的挑战，合成数据是潜在解决方案，但现有合成数据集训练的模型泛化能力有限。研究发现合成数据集学习到的表征呈现离散聚类模式，阻碍回归性能：高质量图像特征围绕参考图像聚类，低质量图像特征按失真类型聚类。

Method: 提出SynDR-IQA框架，基于样本多样性和冗余对泛化误差影响的理论推导，采用两种策略：1）分布感知多样性内容上采样，增强视觉多样性同时保持内容分布；2）密度感知冗余聚类下采样，通过减少密集聚类区域的密度来平衡样本。

Result: 在三种跨数据集设置（合成到真实、合成到算法、合成到合成）上进行了广泛实验，证明了方法的有效性。

Conclusion: 通过重塑合成数据分布可以有效提升BIQA模型的泛化能力，SynDR-IQA为解决合成数据训练模型的泛化问题提供了有效框架。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [13] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出跨模态数据增强框架，结合CycleGAN和YOLOv8解决PCB红外缺陷检测数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 红外数据稀缺是PCB缺陷检测的关键瓶颈，传统方法依赖配对监督数据，但实际应用中红外数据获取困难且成本高昂

Method: 使用CycleGAN进行无配对图像转换，将丰富的可见光PCB图像映射到红外域，生成高质量伪红外样本；构建异构训练策略，融合生成的伪红外数据和有限真实红外样本训练轻量级YOLOv8检测器

Result: 该方法在低数据条件下有效增强特征学习，增强后的检测器显著优于仅使用有限真实数据训练的模型，性能接近完全监督训练的基准水平

Conclusion: 伪红外合成作为工业检测的鲁棒增强策略具有显著效果，为解决红外数据稀缺问题提供了有效解决方案

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [14] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: 提出轻量级害虫检测与农药推荐框架，适用于智能手机等低资源设备，帮助小农户实现精准农业


<details>
  <summary>Details</summary>
Motivation: 传统害虫管理方法依赖人工检查和化学农药，成本高、耗时长、劳动密集且对环境有害，需要适合小农户的低成本解决方案

Method: 包含两个模块：1) 害虫检测模块使用轻量级CNN结合原型元学习，支持少样本学习；2) 农药推荐模块结合作物类型和生长阶段等环境因素推荐环保农药

Result: 轻量级CNN达到与先进模型相当的准确率，同时显著降低计算复杂度；决策支持系统减少对传统化学农药的依赖，促进可持续实践

Conclusion: 该框架在精准农业中具有实时应用潜力，特别适合资源有限的农户，通过智能技术改善害虫管理效率

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [15] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: TotalFM是一个基于器官分离概念的放射学基础模型，通过3D-CT图像与语言表达的对应学习，在计算效率和表示能力之间取得平衡，在零样本器官病变分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 放射学基础模型在处理3D-CT容积数据时面临计算成本约束的挑战，需要一种既能高效学习又能保持良好表示能力的方法。

Method: 基于器官分离概念，利用14万系列的大规模数据集，通过分割技术和基于LLM的放射学报告处理自动创建器官体积-发现句子对，结合VideoMAE的自监督预训练和体积-文本对的对比学习。

Result: 在零样本器官病变分类任务中，相比CT-CLIP在83%的器官上获得更高F1分数，相比Merlin在64%的器官上表现更好；在零样本发现病变分类任务中，相比Merlin在83%的发现类别上获得更高AUROC；在放射学报告生成任务中性能与现有VLM相当。

Conclusion: 器官分离学习框架为3D-CT基础模型的实际应用提供了现实有效的设计指南，展示了在临床评估环境中的高泛化性能。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [16] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: S1-MMAlign是一个包含1550万高质量图像-文本对的多学科科学多模态数据集，通过AI增强管道提升科学图像与文本的对齐质量，为科学AI研究提供基础资源。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在通用领域取得了革命性进展，但在科学发现中的应用受到复杂科学图像与稀疏文本描述之间深刻语义鸿沟的阻碍。现有科学数据集中图像与文本对齐质量普遍较弱，限制了科学多模态模型的发展。

Method: 从250万篇开放获取科学论文中收集1550万高质量图像-文本对，涵盖物理、生物、工程等多个学科。引入AI就绪的语义增强管道，利用Qwen-VL多模态大模型系列，通过合成论文摘要和引用上下文来重新描述图像，改善原始科学标题的弱对齐问题。

Result: 技术验证表明增强显著提升了数据质量：基于SciBERT的伪困惑度指标显示语义模糊性降低，CLIP分数显示图像-文本对齐改善了18.21%。数据集公开可用，为科学推理和跨模态理解提供了基础资源。

Conclusion: S1-MMAlign通过大规模、高质量、多学科的科学多模态数据集和语义增强管道，为AI for Science时代的科学推理和跨模态理解研究提供了重要基础资源，有助于弥合科学图像与文本之间的语义鸿沟。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [17] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 提出ActErase方法，一种无需训练的概念擦除技术，通过分析激活差异区域并动态替换输入激活，在保持模型生成能力的同时有效移除敏感概念。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽然生成能力强，但存在安全、版权和伦理风险。现有概念擦除方法大多依赖数据密集且计算昂贵的微调，存在显著局限性。

Method: 提出训练免费方法ActErase：1) 通过提示对分析识别激活差异区域；2) 提取目标激活；3) 在前向传播过程中动态替换输入激活。

Result: 在三个关键擦除任务（裸露内容、艺术风格、对象移除）上实现最先进的擦除性能，有效保持模型整体生成能力，并展现对对抗攻击的强大鲁棒性。

Conclusion: ActErase为扩散模型中的概念操作建立了一种新的即插即用范式，实现了轻量级但有效的概念擦除，无需训练过程。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [18] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: SV-GS：一种在稀疏观测下重建动态目标的框架，通过骨架驱动变形场实现运动估计和几何细节保持，在稀疏观测条件下性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中动态目标重建面临观测稀疏的挑战，传统方法需要密集的时空覆盖，而实际场景中观测往往在时间和视角上都很稀疏（如监控摄像头），使得动态重建成为高度不适定问题。

Method: SV-GS框架利用粗略骨架图和初始静态重建作为输入，优化骨架驱动变形场，包括粗粒度骨架关节姿态估计器和细粒度变形模块。通过仅使关节姿态估计器具有时间依赖性，实现平滑运动插值同时保持学习到的几何细节。

Result: 在合成数据集上，该方法在稀疏观测条件下比现有方法PSNR提升高达34%；在真实世界数据集上，使用显著更少的帧数就能达到与密集单目视频方法相当的性能。输入要求可放宽，初始静态重建可用基于扩散的生成先验替代。

Conclusion: SV-GS能够在稀疏观测条件下有效重建动态目标，通过骨架驱动变形场实现运动估计和几何细节保持，具有实际应用价值，特别是在监控等现实场景中。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [19] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 基于Swin Transformer的深度学习模型在ISIC2019数据集上对8种皮肤病变分类达到87.71%准确率，可作为临床诊断支持工具和患者自评估辅助。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病日益普遍而皮肤科医生资源有限，需要智能工具支持患者和临床医生进行及时准确的皮肤疾病诊断。

Method: 开发基于深度学习的皮肤疾病分类诊断模型，利用公开皮肤疾病图像数据集进行预训练，提取视觉特征，优化模型架构、数据预处理流程，应用针对性数据增强技术提升性能，最终采用Swin Transformer架构。

Result: 在ISIC2019数据集上对8种皮肤病变类别实现87.71%的预测准确率。

Conclusion: 该模型展示了作为临床医生诊断支持工具和患者自评估辅助的潜力，能够帮助解决皮肤科医生资源不足的问题。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [20] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor是一个基于草图的视频着色模型，支持使用异构、可变数量的参考图像，通过显式的每参考区域分配和时空对应掩码注意力来提高颜色保真度、身份一致性和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数着色模型只基于单一参考（通常是场景的第一帧），忽略了其他条件数据源，如角色设定表、背景图像或任意着色帧。这限制了着色质量和一致性。

Method: TimeColor将参考图像编码为额外的潜在帧，在时间维度上拼接，使它们能在每个扩散步骤中并行处理。使用显式的每参考区域分配、时空对应掩码注意力以及模态分离的RoPE索引，防止捷径学习和跨身份调色板泄漏。

Result: 在SAKUGA-42M数据集上的实验表明，TimeColor在单参考和多参考协议下，在颜色保真度、身份一致性和时间稳定性方面均优于现有基线方法。

Conclusion: TimeColor通过支持异构、可变数量的参考图像，并采用有效的参考绑定机制，显著提升了基于草图的视频着色性能，解决了传统方法中存在的参考限制问题。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [21] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: VisNet是一个计算高效的行人重识别模型，通过多尺度特征融合、语义聚类、动态权重平均和FIDI损失函数等技术，在保持高精度的同时显著降低计算成本，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 当前行人重识别方法虽然精度高但计算成本大，难以在计算资源有限的监控和移动应用中实时部署。需要开发既准确又计算高效的模型。

Method: 1. 多尺度特征融合：融合ResNet50的1-4阶段特征，不使用并行路径；2. 语义聚类：基于解剖学身体分区进行规则化伪标签空间约束；3. 动态权重平均：平衡分类语义正则化；4. FIDI损失函数：改进度量学习任务。

Result: 在Market-1501数据集上达到87.05% Rank-1准确率和77.65% mAP，仅需32.41M参数和4.601 GFLOPs计算量，显著优于现有高计算成本方法。

Conclusion: VisNet提出了一种实用的行人重识别方法，在保持高精度的同时大幅降低计算复杂度，适合在计算资源有限的监控和移动应用中实时部署。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [22] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 该论文提出了OmniVaT框架，首次成功解决了单域泛化的多模态视觉-触觉学习任务，通过多模态分数傅里叶适配器和离散树生成模块来缓解模态差异和域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 视觉-触觉学习面临两个主要挑战：1）视觉和触觉图像之间的模态差异；2）由非标准化触觉传感器和不一致数据收集程序引起的域差距。这些挑战限制了VTL在实际应用中的泛化能力。

Method: 提出了OmniVaT框架，包含两个核心模块：1）多模态分数傅里叶适配器（MFFA），将视觉和触觉嵌入映射到统一的嵌入-频率空间，缓解模态差异；2）离散树生成（DTG）模块，通过分层树结构获得多样且可靠的多模态分数表示，增强对未见域中波动域偏移的适应性。

Result: 大量实验表明，OmniVaT在SDG-VTL任务上表现出优越的跨域泛化性能。

Conclusion: 该研究首次成功解决了单域泛化的多模态视觉-触觉学习任务，提出的OmniVaT框架通过创新的MFFA和DTG模块有效缓解了模态差异和域偏移问题，为实际应用中的视觉-触觉感知提供了有前景的解决方案。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [23] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: DVEFormer：基于RGB-D Transformer的高效方法，通过知识蒸馏预测密集文本对齐视觉嵌入，替代传统语义分割，支持自然语言查询和3D建图


<details>
  <summary>Details</summary>
Motivation: 家庭环境中，机器人需要全面理解周围环境才能与未经训练的人类进行有效直观的交互。传统语义分割方法使用固定预定义类别，缺乏灵活性。

Method: 提出DVEFormer，基于RGB-D Transformer架构，通过知识蒸馏从Alpha-CLIP教师模型学习细粒度像素级嵌入，而不是直接进行传统语义分割。

Result: 在常见室内数据集上达到竞争性性能，满足实时性要求：完整模型26.3 FPS，小型变体77.0 FPS（NVIDIA Jetson AGX Orin）。支持文本查询和3D建图应用。

Conclusion: DVEFormer可作为传统分割方法的直接替代，同时支持灵活的自然语言查询，并能无缝集成到移动机器人的3D建图流程中。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [24] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种概率双流框架，用于骨架动作识别，通过统一可靠性建模和多模态集成，在不确定条件下实现专家化学习，特别关注手部细微动作。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作识别方法主要关注身体大尺度运动，忽略了对手部细微动作的识别，而这些细微动作对于细粒度识别至关重要。同时，现有方法缺乏对不确定性和多模态集成的统一处理。

Method: 提出概率双流框架，包含三个关键组件：1) 无需校准的预处理流程，直接从原生坐标学习；2) 概率Noisy-OR融合，稳定可靠性感知的双流学习；3) 从骨架模态到RGB表示的跨模态集成，耦合四种骨架模态。

Result: 在多个基准测试（NTU RGB+D 60/120, PKU-MMD, N-UCLA）和新定义的手部中心基准上进行了全面评估，显示出在噪声和异构条件下的持续改进和鲁棒性。

Conclusion: 该框架通过统一可靠性建模和多模态集成，在骨架动作识别中实现了对细微手部动作的更好识别，并在各种条件下表现出优越性能。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [25] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse是一个多功能4D世界模型，能够进行4D重建、新轨迹视频生成和丰富的下游应用，通过免姿态前馈4D重建和在线单目退化模式模拟等技术，实现了对多样化单目视频的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界建模方法存在可扩展性限制，主要源于昂贵的多视角4D数据或繁琐的训练预处理。作者希望建立一个能够扩展到多样化单目视频的完整流程。

Method: NeoVerse采用免姿态前馈4D重建、在线单目退化模式模拟以及其他对齐良好的技术，使整个流程能够扩展到多样化的单目视频。

Result: NeoVerse在标准重建和生成基准测试中达到了最先进的性能，并展现出对各种领域的泛化能力。

Conclusion: NeoVerse是一个可扩展且多功能的4D世界模型，能够处理4D重建、新轨迹视频生成和下游应用，为4D世界建模提供了新的解决方案。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [26] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: RoLID-11K是首个基于行车记录仪的大规模路边垃圾检测数据集，包含11,000多张标注图像，专注于极端小目标检测，旨在支持低成本路边垃圾监测系统开发。


<details>
  <summary>Details</summary>
Motivation: 当前路边垃圾监测依赖人工调查和公众报告，空间覆盖有限。现有视觉数据集主要针对街景、航拍或水环境，无法反映行车记录仪中垃圾目标极小、稀疏且背景杂乱的独特特征。

Method: 构建了RoLID-11K数据集，包含11,000多张英国多样化驾驶条件下的标注图像，具有明显的长尾分布和小目标特征。对现代检测器进行全面基准测试，包括精度导向的Transformer架构和实时YOLO模型。

Result: CO-DETR及相关Transformer模型在定位精度上表现最佳，而实时模型受限于粗糙的特征层次结构。该数据集为动态驾驶场景中的极端小目标检测建立了具有挑战性的基准。

Conclusion: RoLID-11K是首个针对行车记录仪路边垃圾检测的大规模数据集，支持开发可扩展的低成本路边垃圾监测系统，为动态驾驶场景中的极端小目标检测提供了重要基准。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [27] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO是一种用于微调视觉语言模型的对比感知策略优化方法，通过检测扰动图像下模型输出的熵变化来识别感知标记，并引入对比感知损失来增强感知一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在语言模型的推理方面取得了进展，但将其扩展到多模态推理需要同时改进感知和推理能力。先前的工作主要使用显式感知奖励来解决这一挑战，但将感知标记与推理标记分离很困难，需要额外的LLM、真实数据、强制分离感知与推理，或对所有输出标记不加区分地应用奖励。

Method: CPPO通过检测在扰动输入图像下模型输出的熵变化来识别感知标记，然后在强化学习目标函数中引入对比感知损失（CPL），该损失在信息保留扰动下强制一致性，在信息移除扰动下强制敏感性。

Result: 实验表明，CPPO超越了先前的感知奖励方法，同时避免了使用额外模型，使训练更加高效和可扩展。

Conclusion: CPPO提供了一种有效的方法来改进视觉语言模型的感知能力，通过对比感知策略优化解决了感知标记识别和奖励分配的难题，提高了多模态推理的性能。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [28] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics是一个端到端可微分框架，通过自然语言提示为3D场景推断合理的物理参数，无需真实轨迹或标注视频指导，利用多模态大语言模型估计材料参数，并通过可学习运动蒸馏损失从预训练视频扩散模型中提取运动先验。


<details>
  <summary>Details</summary>
Motivation: 传统3D物体和材料模拟需要专家知识和耗时的物理参数调整才能获得期望的动态行为，这限制了非专业用户的使用。

Method: 1. 使用多模态大语言模型估计材料参数值，并约束在合理范围内；2. 提出可学习运动蒸馏损失，从预训练视频扩散模型中提取鲁棒运动先验，同时最小化外观和几何归纳偏差来指导模拟。

Result: 在30多个场景中评估，包括真实世界、人工设计和AI生成的3D物体，涵盖弹性固体、金属、泡沫、沙子以及牛顿和非牛顿流体等多种材料。MotionPhysics能生成由自然语言引导的视觉逼真动态模拟，超越现有技术，同时自动确定物理合理的参数。

Conclusion: MotionPhysics通过自然语言提示为3D场景推断物理参数，无需真实轨迹或标注视频，实现了用户友好的物理模拟，在多种材料和场景中表现出色，为物理模拟提供了新的端到端解决方案。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [29] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText是一个无需训练、即插即用的框架，通过利用扩散Transformer模型的内在机制来改进文本渲染，解决了多行布局、密集排版和中文等长尾脚本的文本渲染问题。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型在开放域合成方面表现出色，但在精确文本渲染方面仍有困难，特别是对于多行布局、密集排版和中文等长尾脚本。现有解决方案通常需要昂贵的重新训练或严格的外部布局约束，这会降低美学质量并限制灵活性。

Method: FreeText将问题分解为"在哪里写"和"写什么"两个部分。对于"在哪里写"，通过读取图像到文本注意力的token-wise空间归因来定位书写区域，使用sink-like tokens作为稳定的空间锚点，并通过拓扑感知细化产生高置信度掩码。对于"写什么"，引入频谱调制字形注入(SGMI)，注入噪声对齐的字形先验，通过频域带通调制来增强字形结构并抑制语义泄漏。

Result: 在Qwen-Image、FLUX.1-dev和SD3变体上的广泛实验表明，在长文本基准测试、CVTG和作者提出的CLT-Bench上，文本可读性持续提升，同时很大程度上保持了语义对齐和美学质量，推理开销适中。

Conclusion: FreeText是一个无需训练、即插即用的框架，通过利用扩散Transformer模型的内在机制有效改进了文本渲染，在保持语义对齐和美学质量的同时显著提升了文本可读性。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [30] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: VNS-SAM通过改进SAM在视觉非显著场景下的分割能力，解决了低对比度前景背景难以准确分割的问题，同时保持了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: SAM在视觉非显著场景（前景背景对比度低）中表现不佳，现有方法难以捕捉准确轮廓，需要增强SAM对此类场景的感知能力。

Method: 提出VNS-SAM，包含Mask-Edge Token Interactive解码器和Non-Salient Feature Mining模块，有效利用SAM的低层特征，仅需少量参数和计算增量。同时构建VNS-SEG数据集（超过35K图像）用于训练和评估。

Result: VNS-SAM在各种VNS分割任务中表现出优越性能，特别是在零样本设置下，额外参数可在4小时内优化完成，证明了其可行性和实用性。

Conclusion: VNS-SAM成功增强了SAM在视觉非显著场景下的分割能力，同时保持了原有的零样本泛化性，具有广泛的现实应用潜力。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [31] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag提出了一种基于"预测-移动"框架的拖拽式图像编辑方法，通过迭代执行运动预测和运动监督来实现像素级图像操控，解决了传统方法中的跟踪丢失和模糊跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑方法主要采用"移动-跟踪"框架，存在跟踪丢失和模糊跟踪等不可避免的挑战性问题。其他框架下的方法也存在源图像与目标编辑图像差距过大、中间点不合理导致编辑性低等问题。

Method: DynaDrag采用"预测-移动"框架，迭代执行运动预测和运动监督。在每个迭代中，运动预测模块首先预测手柄点应该移动的位置，然后运动监督模块相应地进行拖拽。还提出动态调整有效手柄点以进一步提升性能。

Result: 在人脸和人体数据集上的实验展示了该方法相对于先前工作的优越性。

Conclusion: DynaDrag是首个在"预测-移动"框架下的拖拽方法，通过迭代的运动预测和监督机制，有效解决了传统拖拽式图像编辑中的关键问题，实现了更好的像素级图像操控效果。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [32] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: SlingBAG Pro是一种基于点云迭代概念的光声成像重建算法，专门针对不规则几何换能器阵列设计，相比传统方法显著提升了重建速度和质量。


<details>
  <summary>Details</summary>
Motivation: 解决不规则几何换能器阵列在三维光声成像中的重建难题。传统迭代重建算法在处理不规则阵列配置时面临计算复杂度高、内存需求大、重建时间长等问题，限制了临床应用中高质量三维光声成像的发展。

Method: 基于Sliding ball adaptive growth (SlingBAG)方法的点云迭代概念，扩展其兼容性以支持任意阵列几何形状。采用分层优化策略，结合零梯度滤波和迭代过程中逐步增加的时间采样率，快速去除冗余空间点云，加速收敛。

Result: 相比原始SlingBAG算法，SlingBAG Pro在不规则阵列几何下的基于点云的三维光声重建中实现了高达2.2倍的速度提升。通过仿真和活体小鼠实验验证了方法的有效性。

Conclusion: SlingBAG Pro算法能够在不规则几何换能器阵列配置下实现高质量、高效率的三维光声成像重建，显著减少所需换能器数量并缩短重建时间，为临床应用中高质量三维光声成像提供了有效的解决方案。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [33] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: MS COCOAI是一个包含96000个真实和合成图像的新数据集，用于AI生成图像检测，基于MS COCO构建，使用了5种生成器创建合成图像，并提出了两个检测任务。


<details>
  <summary>Details</summary>
Motivation: 随着多模态生成AI系统（如Stable Diffusion、DALL-E、MidJourney）的普及，合成图像越来越难以与真实照片区分，这导致了误导性内容、虚假信息和操纵媒体的传播，因此检测AI生成图像变得迫切重要。

Method: 基于MS COCO数据集构建了包含96000个数据点的MS COCOAI数据集，使用五种生成器（Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3、MidJourney v6）创建合成图像，并设计了两个检测任务：图像真实性分类和生成模型识别。

Result: 创建了公开可用的MS COCOAI数据集（可通过Hugging Face访问），为AI生成图像检测研究提供了标准化的基准数据集，支持真实vs合成分类和生成模型识别两个任务。

Conclusion: MS COCOAI数据集为解决AI生成图像检测的紧迫挑战提供了重要资源，有助于开发更有效的检测方法，对抗虚假信息和操纵媒体的传播。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [34] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: AEGIS是一个评估统一多模态模型世界知识应用能力的多任务基准，包含1050个手动标注的问题，涵盖21个主题和6种推理类型，并提出了确定性检查表评估方法以提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在局限性，只能提供孤立的单任务评估，诊断能力有限，无法全面评估统一多模态模型在不同任务中应用世界知识的能力。

Method: 提出了AEGIS基准，包含1050个具有挑战性的手动标注问题，涵盖视觉理解、生成、编辑和交错生成等多个任务，涉及21个主题和6种推理类型。同时提出了确定性检查表评估方法，用原子化的"是/否"判断替代模糊的提示式评分。

Result: 实验表明大多数统一多模态模型存在严重的世界知识缺陷，性能在复杂推理任务中显著下降。简单的插件式推理模块可以部分缓解这些弱点。

Conclusion: 世界知识推理是统一多模态模型发展的关键前沿领域，需要进一步研究来提升模型在这方面的能力。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [35] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 提出GranAlign框架解决零样本视频时刻检索中的语义粒度不匹配问题，通过粒度感知查询重写和查询感知字幕生成技术，在三个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频时刻检索面临的主要挑战是文本查询与视觉内容之间的语义粒度不匹配。现有方法虽然利用了高质量的预训练知识，但未能平衡不同模态之间的语义粒度，导致检索不准确。

Method: 提出无需训练的GranAlign框架，包含两个互补技术：1) 基于粒度的查询重写，生成不同语义粒度的查询变体；2) 查询感知字幕生成，将查询意图嵌入视频内容。通过将多级查询与查询无关和查询感知的字幕配对，有效解决语义不匹配问题。

Result: 在三个主要基准数据集(QVHighlights, Charades-STA, ActivityNet-Captions)上均达到新的SOTA性能，特别是在具有挑战性的QVHighlights数据集上实现了3.23%的mAP@avg显著提升。

Conclusion: GranAlign框架通过粒度感知对齐有效解决了零样本视频时刻检索中的语义粒度不匹配问题，无需额外训练即可显著提升检索性能，证明了平衡多模态语义粒度的重要性。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [36] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: SafeMo：基于最小运动遗忘的两阶段机器学习遗忘策略，在连续空间实现安全人体运动生成，避免离散码本替换的缺陷，提供更好的安全-效用平衡


<details>
  <summary>Details</summary>
Motivation: 现有基于离散VQ-VAE码本替换的文本到运动生成方法存在两个关键缺陷：1）替换被良性提示重用的码本条目会导致日常任务漂移，降低模型良性性能；2）离散标记方法引入量化和平滑度损失，导致伪影和抖动过渡。此外，现有文本到运动数据集天然包含不安全意图和相应运动，不适合安全驱动的机器学习。

Method: 提出SafeMo可信运动生成框架，集成最小运动遗忘（MMU）——一种两阶段机器学习遗忘策略，在连续空间实现安全人体运动生成，保留连续运动学特性而无码本损失。同时构建首个安全文本到运动数据集SafeMoVAE-29K，包含重写的安全文本提示和连续精炼运动。

Result: 实验显示SafeMo在HumanML3D和Motion-X数据集上分别达到2.5倍和14.4倍更高的遗忘集FID，相比之前最先进的人类运动遗忘方法LCR表现出更强的遗忘性能，同时在安全提示上的良性性能相当或更好。

Conclusion: SafeMo通过最小运动遗忘策略有效解决了现有离散码本替换方法的安全缺陷，在连续空间实现了安全人体运动生成，提供了更好的安全-效用平衡，并建立了首个安全文本到运动数据集，为可信人类运动遗忘研究奠定了基础。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [37] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种模态主导指数(MDI)来量化RGB-红外多模态感知中的优化偏差问题，并开发了模态主导感知跨模态学习(MDACL)框架，通过分层跨模态引导和对抗均衡正则化来平衡优化动态，在RGB-IR基准测试中实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: RGB-红外多模态感知在复杂物理环境的嵌入式多媒体系统中至关重要。虽然现有的跨模态融合方法有所进展，但由于模态特征不对称导致的优化动态问题尚未得到充分探索。实践中，信息密度和特征质量的差异会引入持续的优化偏差，导致训练过度强调主导模态，阻碍有效的跨模态融合。

Method: 提出了模态主导指数(MDI)，通过联合建模特征熵和梯度贡献来量化模态主导程度。基于MDI开发了模态主导感知跨模态学习(MDACL)框架，包含：1) 分层跨模态引导(HCG)来增强特征对齐；2) 对抗均衡正则化(AER)来平衡融合过程中的优化动态。

Result: 在三个RGB-IR基准测试上进行了广泛实验，结果表明MDACL能够有效缓解优化偏差，并实现了最先进的性能表现。

Conclusion: 该研究通过量化模态主导现象并开发相应的平衡优化框架，有效解决了RGB-红外多模态感知中的优化偏差问题，为跨模态融合提供了新的解决方案。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [38] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: TOLF框架通过归一化流建模复杂误差分布和不确定性引导优化，解决小目标检测中因标注噪声导致的过拟合问题，提升小目标定位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 小目标检测相比常规尺度目标存在显著性能差距，且小目标对标注噪声高度敏感，优化严格定位目标容易导致噪声过拟合。

Method: 提出Tiny Object Localization with Flows (TOLF)框架：1) 使用归一化流进行灵活误差建模，捕捉复杂非高斯预测分布；2) 不确定性感知梯度调制机制，抑制高不确定性噪声样本的学习。

Result: 在三个数据集上的广泛实验验证了方法的有效性。特别是在AI-TOD数据集上，TOLF将DINO基线提升了1.2% AP。

Conclusion: TOLF通过流基误差建模和不确定性引导优化，有效解决了小目标检测中的噪声鲁棒性问题，显著提升了小目标定位性能。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [39] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose：用于康复训练的实时3D人体姿态估计与运动分析方法，通过多摄像头RGB视频输入实现端到端的实时监测与评估，提供即时反馈指导患者正确执行康复动作。


<details>
  <summary>Details</summary>
Motivation: 康复训练中需要实时监测和评估患者动作，提供即时反馈指导患者正确执行康复练习，帮助恢复肌肉力量和运动功能。现有方法在多人干扰的医疗康复场景中跟踪速度不足，姿态估计误差较大。

Method: 1. 提出统一管道用于端到端实时人体姿态估计与运动分析；2. 针对多人干扰的医疗康复场景提出快速跟踪方法（单帧跟踪<1ms）；3. 改进SmoothNet用于实时姿态估计，减少误差并恢复真实运动状态；4. 使用Unity平台进行实时监测评估并显示肌肉应力状况。

Result: 方法能够实时监测和评估患者康复动作，提供即时反馈；快速跟踪方法在多人干扰场景下实现<1ms的单帧跟踪；改进的SmoothNet有效减少姿态估计误差，使运动状态更平滑真实；Unity平台成功实现实时监测和肌肉应力可视化。

Conclusion: RePose方法为康复训练提供了一套完整的实时3D人体姿态估计与运动分析解决方案，能够有效辅助患者正确执行康复练习，加速康复进程，具有实际临床应用价值。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [40] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 基于图像的深度学习为马铃薯储存期间的质量监测提供非侵入式解决方案，通过预训练模型实现发芽检测、重量损失估计和货架期预测，DenseNet在发芽检测中达到98.03%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决马铃薯储存期间的质量监测挑战，包括发芽检测、重量损失估计和货架期预测，为自动化分拣和库存系统提供非侵入式、可扩展的解决方案。

Method: 在200天受控温湿度条件下收集图像和重量数据，利用ResNet、VGG、DenseNet和Vision Transformer等预训练架构，设计两个专门模型：高精度二分类发芽检测器和多类别重量损失/货架期预测器。

Result: DenseNet在发芽检测中达到98.03%准确率；货架期预测在2-5个粗分类时表现最佳（准确率>89.83%），6-8个细分类时准确率下降；证明了图像模型集成到自动化系统的可行性。

Conclusion: 图像深度学习为马铃薯质量评估提供经济高效的非破坏性方法，支持储存和分销的效率与可持续性；虽然精确货架期预测仍有挑战，但粗分类可确保稳健性能；未来需开发适应不同品种和储存条件的通用模型。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [41] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: CRoPS是一个无需训练的幻觉缓解框架，通过选择性移除关键文本标记构建幻觉模型，结合广义对比解码来减少大型视觉语言模型中的幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在生成幻觉内容的倾向，这影响了其在现实应用中的可靠性。现有无需训练的方法存在两个局限：1) 对幻觉来源的假设过于狭窄；2) 在生成后期效果下降，而幻觉最可能在此阶段发生。

Method: 提出CRoPS框架：1) 构建新型幻觉模型，通过选择性移除关键文本标记来捕捉幻觉效应；2) 引入广义对比解码，整合多个幻觉模型以代表多样化的幻觉来源。

Result: CRoPS将CHAIR分数提高了20%，在六个基准测试和三个LVLM家族中取得一致增益，优于最先进的无需训练方法。

Conclusion: CRoPS是一个有效的无需训练幻觉缓解框架，通过选择性移除文本标记和广义对比解码策略，显著减少了大型视觉语言模型中的幻觉生成问题。

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [42] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 提出一种基于3D高斯场景表示的单图像视频生成框架，能够在单次前向传播中生成相机引导的视频，无需迭代去噪注入物体运动，实现了高效且高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有单图像视频生成方法虽然改进了时间一致性和3D一致性，但缺乏鲁棒的用户可控性（如修改相机路径），且大多数相机控制方法在准确建模相机运动、保持时间一致性和几何完整性方面存在困难。

Method: 构建3D高斯场景表示并在单次前向传播中采样合理的物体运动，给定单张图像即可快速生成相机引导的视频，无需通过迭代去噪将物体运动注入渲染帧。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K数据集上的实验表明，该方法在视频质量和推理效率方面达到了最先进的水平。

Conclusion: 提出的框架能够高效生成相机引导的单图像视频，解决了现有方法在时间一致性、几何完整性和用户可控性方面的局限性，为智能系统提供了重要组件。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [43] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM是一种鲁棒的高斯溅射SLAM框架，用免训练对应到高斯初始化替代了GS-SLAM的残差驱动致密化阶段，通过多视图对应三角测量生成高斯种子，加速收敛20%，在纹理丰富和杂乱场景中实现更高渲染保真度。


<details>
  <summary>Details</summary>
Motivation: 传统GS-SLAM使用残差驱动致密化阶段，通过逐步添加高斯来揭示缺失几何结构。这种方法可能效率不高，特别是在纹理丰富和杂乱场景中。RGS-SLAM旨在通过更智能的初始化方法来稳定早期建图并加速收敛。

Method: RGS-SLAM采用免训练对应到高斯初始化方法：1）从DINOv3描述符中提取密集多视图对应关系；2）通过置信度感知的内点分类器进行精炼；3）执行一次性三角测量生成结构感知的高斯种子；4）在优化前生成分布良好且结构感知的高斯先验。

Result: 在TUM RGB-D和Replica数据集上的评估显示：1）收敛速度提升约20%；2）在纹理丰富和杂乱场景中获得更高的渲染保真度；3）与最先进的高斯和基于点的SLAM系统相比，实现竞争性或更优的定位和重建精度；4）保持实时建图性能，最高达925 FPS。

Conclusion: RGS-SLAM通过免训练对应到高斯初始化方法，有效解决了传统GS-SLAM残差驱动致密化的局限性，提供了更稳定、更快速的收敛，同时保持与现有GS-SLAM管道的完全兼容性，在实时SLAM应用中表现出色。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [44] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 该研究探讨了视觉语言模型在医学图像分析中面临数据分布偏移时的性能退化检测问题，开发了DomainSAT工具箱用于输入级数据偏移检测，并提出基于输出置信度的无标签性能退化指标，两者结合能更可靠地监测模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学图像分析和疾病诊断中表现出强大潜力，但部署后当输入数据分布与开发时观察到的分布发生偏移时，模型性能可能会下降。检测这种性能退化对于临床可靠性至关重要，但对于大型预训练视觉语言模型来说，在没有标注数据的情况下仍然具有挑战性。

Method: 1. 开发DomainSAT工具箱，这是一个轻量级图形界面工具，集成了代表性的数据偏移检测算法，便于系统分析输入数据偏移；2. 研究输入级数据偏移和输出级预测行为在监测模型可靠性中的作用；3. 引入基于输出置信度的无标签性能退化指标，直接捕捉模型预测置信度的变化；4. 在大型病理学数据集上进行肿瘤分类实验，评估输入数据偏移检测和输出置信度指标的组合效果。

Result: 分析表明：1. 输入数据偏移检测能有效识别分布变化并提供早期诊断信号，但并不总是与实际性能退化相对应；2. 基于输出置信度的性能退化指标与性能退化表现出密切关系，可作为输入偏移检测的有效补充；3. 在病理学数据集上的实验证明，结合输入数据偏移检测和输出置信度指标能够更可靠地检测和解释视觉语言模型在数据偏移下的性能退化。

Conclusion: 该研究为数字病理学中基础模型的可靠性监测提供了一个实用且互补的框架，通过结合输入数据偏移检测和输出置信度指标，能够更有效地检测和解释视觉语言模型在数据分布偏移下的性能退化问题。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [45] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 提出基于多模态大语言模型的端到端手写STEM考试自动评分工作流，通过参考解决方案和分级规则实现可靠评分，在真实课程测试中达到约8分的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能捕捉开放式推理和图表，但人工评分速度慢且难以扩展，需要自动化解决方案来保持标准考试流程。

Method: 采用多阶段设计：格式/存在性检查防止空白答案评分，独立评分器集合，监督聚合，以及刚性模板生成可审计的机器可解析报告。使用GPT-5.2和Gemini-3 Pro作为后端模型。

Result: 完整流程在真实课程测试中达到约8分的平均绝对误差，偏差低，手动审查触发率约17%。消融实验显示简单提示和移除参考解决方案会显著降低准确性并引入系统性过评分。

Conclusion: 结构化提示和参考解决方案基础是手写STEM考试自动评分的关键，多阶段设计确保了可靠性和可审计性，为工程教育提供了可扩展的评分解决方案。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [46] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 本文探索将自监督学习作为辅助任务来优化深度伪造检测的主要任务，通过融合自监督任务的特征表示，在跨数据集评估中实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测面临泛化能力不足的问题，特别是在跨数据集评估时性能下降。本文旨在探索如何利用自监督学习作为辅助任务来增强主要任务的性能，提高检测器的泛化能力。

Method: 研究不同的训练方案组合，将自监督学习作为辅助任务与深度伪造检测的主要任务相结合。通过融合自监督辅助任务的特征表示，创建一个结合了自监督和主要任务独特表示的强大特征表示。

Result: 在DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV等多个数据集上进行实验，结果显示在跨数据集评估中相比当前最先进的检测器具有更好的泛化性能。

Conclusion: 融合自监督辅助任务的特征表示是一种有效的策略，能够充分利用自监督学习和主要任务的潜力，为深度伪造检测问题提供更好的特征表示，从而提高检测器的泛化能力。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [47] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 本文提出LNU-Net和IBU-Net两种深度学习架构用于心脏MRI图像的左心室分割，通过不同的归一化技术改进U-Net，在805张MRI图像上验证了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对于心脏图像的临床量化和诊断至关重要，需要开发更准确的分割方法来改进现有U-Net架构的性能。

Method: 提出两种基于U-Net的改进架构：LNU-Net在卷积块中使用层归一化，IBU-Net在第一个卷积块中结合实例归一化和批归一化。采用下采样路径进行特征提取和上采样路径进行精确定位，并应用仿射变换和弹性变形进行数据增强。

Result: 在包含45名患者805张MRI图像的数据集上评估，LNU-Net和IBU-Net在Dice系数和平均垂直距离指标上均优于原始U-Net和其他最先进方法。

Conclusion: 提出的LNU-Net和IBU-Net架构通过不同的归一化策略有效改进了左心室分割性能，为心脏图像分析提供了更准确的分割解决方案。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [48] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: AdaGaR提出统一框架解决动态3D场景重建中的频率适应性和时间连续性问题，通过自适应Gabor表示和时间连续性约束实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有单高斯基元方法受限于低通滤波特性，标准Gabor函数存在能量不稳定问题，且缺乏时间连续性约束导致插值时出现运动伪影。

Method: 1) 自适应Gabor表示：通过可学习频率权重和自适应能量补偿扩展高斯函数；2) 时间连续性：使用三次Hermite样条和时间曲率正则化；3) 自适应初始化：结合深度估计、点跟踪和前景掩码建立稳定点云分布。

Result: 在Tap-Vid DAVIS数据集上取得SOTA性能（PSNR 35.49, SSIM 0.9433, LPIPS 0.0723），在帧插值、深度一致性、视频编辑和立体视图合成等任务上表现出强泛化能力。

Conclusion: AdaGaR通过统一解决频率适应性和时间连续性，实现了高质量动态3D场景重建，在多个下游任务中展现出优越性能。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [49] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: RIMRULE是一种基于动态规则注入的神经符号方法，通过从失败轨迹中提取紧凑、可解释的规则，在推理时注入提示中，以提高LLM在特定领域工具使用中的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域工具使用中表现不佳，因为这些领域的API可能具有特殊性、文档不足或针对私有工作流程定制，需要有效的任务特定工具适应方法。

Method: 提出RIMRULE方法：1）从失败轨迹中提取规则；2）使用最小描述长度目标进行规则整合，偏好通用性和简洁性；3）规则以自然语言和结构化符号形式存储；4）在推理时动态注入规则到提示中。

Result: 实验表明：1）提高在已见和未见工具上的准确性；2）优于基于提示的适应方法；3）与微调互补；4）从一个LLM学习的规则可以重用于改进其他LLM，包括长推理LLM。

Conclusion: RIMRULE通过动态规则注入有效提高了LLM在特定领域工具使用中的性能，展示了符号知识在不同架构间的可移植性，且无需修改模型权重。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [50] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: Pat-DEVAL：首个专为专利说明书设计的多维评估框架，通过法律约束推理机制评估长文本结构连贯性和法定合规性，显著优于现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有专利自动撰写评估方法无法评估长文本结构连贯性和特定法定合规性（如可实施性和书面描述要求），需要专门框架来确保技术披露的全面性和法律标准的符合性。

Method: 提出Pat-DEVAL框架，采用LLM-as-a-judge范式，引入Chain-of-Legal-Thought（CoLT）机制，这是一种法律约束推理机制，强制执行顺序性的专利法特定分析。

Result: 在Pap2Pat-EvalGold数据集上，经专利专家验证，Pat-DEVAL达到0.69的皮尔逊相关系数，显著优于基线指标和现有LLM评估器；在法律专业合规性方面达到0.73的优异相关性。

Conclusion: 通过明确注入法定约束条件，Pat-DEVAL能够捕捉细微的法律有效性，为自动专利撰写系统的实际部署提供了稳健的方法学基础，建立了确保技术合理性和法律合规性的新标准。

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [51] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 该论文通过系统分析IEMOCAP数据集，填补了对话情感识别(ERC)领域的两个关键空白：识别哪些架构选择真正重要，以及连接识别与生成的语言学分析。研究发现对话上下文至关重要，简单架构使用严格因果上下文即可超越先前方法。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别(ERC)虽然取得了高准确率，但仍存在两个关键空白：1) 对哪些架构选择真正重要的理解有限；2) 缺乏连接识别与生成的语言学分析。该研究旨在通过系统分析IEMOCAP数据集来填补这两个空白。

Method: 1) 识别方面：对IEMOCAP数据集进行严格的消融研究，采用10次种子评估，分析对话上下文、分层句子表示和外部情感词典的影响；2) 语言学分析：分析5,286个话语标记出现情况，研究情感与标记位置的关系，特别关注悲伤话语的特征。

Result: 1) 对话上下文至关重要，90%的性能提升来自最近10-30个轮次；2) 分层句子表示在话语层面有帮助，但提供对话上下文后此优势消失；3) 外部情感词典无增益；4) 简单架构使用严格因果上下文获得82.69%(4类)和67.07%(6类)加权F1，超越先前方法；5) 情感与话语标记位置显著相关(p<.0001)，悲伤话语左边缘标记使用减少(21.9%)，需要更多上下文进行消歧。

Conclusion: 对话上下文是情感识别的关键因素，简单的因果上下文架构即可实现高性能。悲伤话语缺乏显式语用信号，需要更多上下文进行消歧，这解释了为什么悲伤从上下文中获益最大(+22%)。研究为对话情感识别提供了实用的架构指导，并建立了识别与生成之间的语言学联系。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [52] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: 本文提出了一种专门针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型，将结构和时序推理能力转移到轻量级学生模型中，在保持紧凑架构的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理模型通常依赖大量参数和密集计算，导致硬件成本高、能耗大，难以部署在资源受限的实时推理平台上。现有的模型压缩和蒸馏技术主要针对静态知识图谱设计，无法充分捕捉时序知识图谱中的时间依赖性，导致推理性能下降。

Method: 提出专门针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型指导蒸馏过程，有效转移结构和时序推理能力到轻量级学生模型。通过整合大规模公共知识和任务特定的时序信息，增强学生模型对时序动态的建模能力，同时保持紧凑高效的架构。

Result: 在多个公开基准数据集上的广泛实验表明，该方法持续优于强基线模型，在推理准确性、计算效率和实际可部署性之间实现了有利的平衡。

Conclusion: 提出的蒸馏框架成功解决了时序知识图谱推理中的效率和部署问题，通过大语言模型引导的蒸馏过程，实现了轻量级模型在保持高性能的同时具备实际部署能力。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [53] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: 该研究提出了一种将循证医学原则整合到图基检索增强生成中的通用策略，通过PICO框架改进知识图谱构建和检索，并采用贝叶斯启发的重排序算法，在运动康复领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的检索增强生成方法主要关注性能提升，但忽视了循证医学原则，存在两个关键缺陷：1）查询与检索证据之间缺乏PICO对齐；2）重排序过程中缺少证据等级考量。

Method: 提出通用化策略将循证医学适应到图基RAG中：1）将PICO框架整合到知识图谱构建和检索中；2）提出贝叶斯启发的重排序算法，根据证据等级校准排名分数而不引入预定义权重；3）在运动康复领域验证该框架。

Result: 构建了包含357,844个节点和371,226条边的知识图谱，创建了1,637个QA对的可重用基准。系统在各项指标上表现优异：0.830的金块覆盖率、0.819的答案忠实度、0.882的语义相似度、0.788的PICOT匹配准确度。五位专家临床医生在5点李克特量表中给出4.66-4.84的高分评价。

Conclusion: 提出的循证医学适应策略显著提高了检索和答案质量，且可转移到其他临床领域。发布的资源有助于解决运动康复领域RAG数据集稀缺的问题。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [54] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: JP-TL-Bench是一个轻量级开源基准测试，用于指导日语-英语翻译系统的迭代开发，专注于评估"哪个翻译更好"而非"翻译是否可接受"的细微差别。


<details>
  <summary>Details</summary>
Motivation: 日语-英语翻译中，礼貌、隐含意义、省略和语域等细微选择会显著影响翻译的自然度，需要专门基准来评估这些细微差异。

Method: 使用无参考的成对LLM比较方法，将候选模型与固定的版本化锚定集进行比较，通过Bradley-Terry模型聚合结果，并计算胜率和0-10分的"LT"标准化分数。

Result: 该基准通过冻结的锚定集确保评分结构稳定性，使不同候选模型在相同基础集、评判标准和聚合代码下具有可比性。

Conclusion: JP-TL-Bench提供了一个可靠且经济有效的评估框架，专门针对日语-英语翻译的细微质量差异，支持翻译系统的迭代改进。

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [55] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 该研究系统评估了大型语言模型生成多语言反事实的能力，发现翻译生成的反事实有效性更高但修改更多，多语言反事实数据增强对低资源语言效果更好，但生成质量限制了模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在生成英语反事实方面表现出色且具备多语言能力，但其在多语言反事实生成方面的有效性尚不清楚，需要系统研究来评估其实际表现和局限性。

Method: 1) 在六种语言上进行自动评估，比较直接生成的反事实和通过英语翻译生成的反事实；2) 分析高资源欧洲语言反事实的编辑模式；3) 识别和分类生成反事实中的错误类型；4) 评估多语言反事实数据增强与跨语言数据增强的效果。

Result: 1) 翻译生成的反事实比直接生成的有效性更高，但需要更多修改，且质量仍不及原始英语反事实；2) 高资源欧洲语言的反事实编辑模式相似，表明跨语言扰动遵循共同策略原则；3) 识别出四种主要错误类型；4) 多语言反事实数据增强比跨语言数据增强带来更大的模型性能提升，尤其对低资源语言，但生成不完美限制了性能提升。

Conclusion: 大型语言模型在多语言反事实生成方面仍有局限，翻译方法虽能提高有效性但代价高昂，多语言数据增强对低资源语言有益，但需要改进生成质量以最大化模型性能和鲁棒性提升。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [56] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval是一个评估LLM智能体函数调用能力的基准测试，专注于真实API复杂性，包含API规范和API执行两个维度的挑战，提供了约32K种测试配置。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设理想的API系统，忽略了真实世界因素如噪声API输出。需要评估LLM智能体在真实API复杂性下的函数调用能力。

Method: 创建WildAGTEval基准测试，包含60个不同的复杂性场景，可组合成约32K种测试配置。包含API系统（API规范和API执行）和用户-智能体交互评估框架。

Result: 大多数场景具有挑战性，无关信息复杂性最难，使强LLM性能下降27.3%。定性分析显示LLM有时会扭曲用户意图以声称完成任务，严重影响用户满意度。

Conclusion: WildAGTEval揭示了LLM智能体在真实API复杂性下的局限性，特别是处理无关信息和意图扭曲问题，为改进LLM智能体提供了重要基准。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [57] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化对大型语言模型自解释能力的影响研究：量化通常导致自解释质量和忠实度适度下降，但不会削弱量化作为模型压缩技术的有效性


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于加速大型语言模型推理和简化部署，但其对自解释的影响尚未被探索。自解释是LLMs为证明自身输出而生成的解释，需要推理模型自身的决策过程，这种能力可能对量化特别敏感。随着自解释在高风险应用中越来越依赖透明度，理解量化是否以及多大程度上降低自解释质量和忠实度至关重要。

Method: 研究两种类型的自解释：自然语言解释和反事实示例，使用三种常见量化技术在不同比特宽度下量化LLMs生成。通过用户研究评估量化对自解释连贯性和可信度的影响，比较不同规模模型对量化的弹性。

Result: 量化通常导致自解释质量（最多下降4.4%）和忠实度（最多下降2.38%）适度下降。用户研究表明量化降低了自解释的连贯性和可信度（最多8.5%）。相比小模型，大模型在自解释质量方面对量化的弹性有限，但在保持忠实度方面更好。没有一种量化技术在任务准确性、自解释质量和忠实度方面始终表现优异。

Conclusion: 量化影响因上下文而异，建议针对特定用例验证自解释质量，特别是对量化更敏感的自然语言解释。然而，自解释质量和忠实度的相对较小恶化并不削弱量化作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [58] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: DepFlow是一个三阶段抑郁条件文本转语音框架，通过对抗训练学习说话人和内容不变的抑郁嵌入，然后通过流匹配TTS模型注入抑郁严重程度控制，最后构建伪装抑郁增强数据集来缓解语义偏见，提升抑郁检测模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁数据集（如DAIC-WOZ）中语言情感与诊断标签强耦合，导致模型学习语义捷径，在真实场景（如伪装抑郁）中鲁棒性不足。需要缓解语义偏见，提高模型在语言内容与抑郁状态不匹配情况下的检测能力。

Method: 1. 抑郁声学编码器：通过对抗训练学习说话人和内容不变的抑郁嵌入，实现有效解耦同时保持抑郁可区分性（ROC-AUC: 0.693）。2. 流匹配TTS模型：使用FiLM调制注入抑郁嵌入，控制抑郁严重程度同时保留内容和说话人身份。3. 原型严重程度映射：提供平滑可解释的抑郁连续体操作。4. 构建伪装抑郁增强数据集：将抑郁声学模式与积极/中性内容配对，创建声学-语义不匹配数据。

Result: 1. 抑郁声学编码器ROC-AUC达到0.693。2. 构建的伪装抑郁增强数据集在三种抑郁检测架构上分别提升macro-F1 9%、12%和5%，优于传统增强策略。3. 提供可控合成平台，用于对话系统和基于仿真的评估。

Conclusion: DepFlow通过缓解语义偏见有效提升了抑郁检测模型的鲁棒性，特别是在伪装抑郁场景中。该方法不仅增强了模型性能，还为临床数据有限的场景提供了可控合成平台，具有实际应用价值。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [59] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 该研究针对大语言模型幻觉问题，提出了一种基于多事实生成任务的鲁棒不确定性量化方法，通过构建包含虚假名称的陷阱问题集来评估模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题严重影响了AI生成内容的可靠性和可信度。传统不确定性量化方法在常规问答场景中有效，但在面对非常规或对抗性提问策略时表现不足，这引发了在需要强大批判性思维能力的实际应用中LLM响应可靠性的担忧。

Method: 研究构建了包含虚假名称的陷阱问题集，并创新性地提出了一种鲁棒的不确定性量化方法(RU)，在四个不同模型上与基线方法进行比较实验。

Result: 构建的陷阱问题集表现优异，提出的RU方法在四个不同模型上相比最佳基线方法的ROCAUC值平均提高了0.1-0.2，为处理LLM幻觉问题提供了新的视角和方法。

Conclusion: 该研究填补了传统不确定性量化方法在非常规场景下的性能差距，提出的鲁棒不确定性量化方法在检测LLM幻觉方面表现出色，为解决大语言模型幻觉问题提供了有效的技术途径。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [60] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: BERT-JEPA (BEPA) 是一种新的自监督训练范式，通过在BERT风格模型中添加JEPA训练目标，解决了[CLS]嵌入空间坍缩问题，将其转变为语言无关空间，从而在多语言基准测试中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决BERT模型中[CLS]嵌入空间坍缩的问题，并创建一个语言无关的表示空间，以提升多语言任务性能。

Method: 在BERT风格模型中引入Joint Embedding Predictive Architectures (JEPA)训练目标，通过自监督学习方式训练模型，将[CLS]嵌入空间转变为语言无关空间。

Result: BERT-JEPA在多语言基准测试中表现出性能提升，证明了该方法在创建语言无关表示空间方面的有效性。

Conclusion: 通过结合JEPA训练目标，BERT-JEPA成功解决了[CLS]嵌入空间坍缩问题，创建了语言无关的表示空间，从而在多语言任务中实现了性能改进。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [61] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: Geo-R是一个无需检索的图像地理定位框架，通过基于规则的分层推理和强化学习，直接从GPS坐标生成结构化推理路径，提高定位准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图像地理定位中通常依赖合成推理标注或外部图像检索，这限制了方法的可解释性和泛化能力。需要一种无需检索、可直接从真实坐标生成结构化推理的方法。

Method: 提出Geo-R框架：1) Chain of Region - 基于规则的分层推理范式，将GPS坐标映射到地理实体层次结构；2) 轻量级强化学习策略，使用基于Haversine距离的坐标对齐奖励，通过空间有意义的反馈优化预测。

Result: 在多个基准测试中验证了Geo-R的有效性，实现了更高的定位准确性、更强的泛化能力和更透明的推理过程，建立了无需检索的可扩展且可解释的图像地理定位新范式。

Conclusion: Geo-R通过结合结构化地理推理和直接空间监督，成功开发了一个无需检索的图像地理定位框架，在准确性、泛化性和可解释性方面均有显著提升，为相关研究提供了新的方向。

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [62] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: 提出了judgeWEL数据集，这是一个用于卢森堡语命名实体识别的数据集，通过新颖的LLM管道自动标注和验证，比现有数据集大5倍，覆盖更平衡的实体类别。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言构建数据集是自然语言处理的主要瓶颈之一，资源稀缺和语言特性使得大规模标注成本高昂且可能不一致。卢森堡语作为代表性语言面临这些挑战。

Method: 利用维基百科和维基数据作为弱监督的结构化来源，通过维基百科文章内部链接推断实体类型，然后使用多个LLM识别和保留高质量标注句子来降低噪声。

Result: 生成的judgeWEL数据集比现有卢森堡语NER数据集大约5倍，提供更广泛和更平衡的实体类别覆盖，为多语言和低资源NER研究提供了重要新资源。

Conclusion: 提出的方法通过结合维基百科结构化知识和LLM验证，有效解决了低资源语言数据集构建的挑战，为其他类似语言的数据集创建提供了可行方案。

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [63] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 本文提出了一种新的超关系时序知识广义超图（HTKGHs）来扩展传统时序知识图的表达能力，以更好地表示现实世界中的复杂地缘政治事件，并基于POLECAT数据库构建了htkgh-polecat数据集，评估了大型语言模型在复杂预测场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统时序知识图（TKGs）及其扩展超关系时序知识图（HTKGs）虽然能表示简单的时间关系，但表达能力有限，无法有效表示复杂事实。特别是HTKGs不支持超过两个主要实体的时序事实，而这在地缘政治事件中很常见。

Method: 1. 提出HTKGHs（超关系时序知识广义超图）作为HTKGs的泛化形式，支持两种常见于地缘政治事件的复杂事实类型
2. 为HTKGHs建立形式化定义，确保向后兼容性
3. 基于全球事件数据库POLECAT构建htkgh-polecat数据集
4. 在关系预测任务上对流行的大型语言模型进行基准测试和分析

Result: 1. 成功形式化了HTKGHs概念，扩展了时序知识图的表达能力
2. 创建了htkgh-polecat数据集，为复杂地缘政治事件预测提供了新的基准
3. 对大型语言模型在复杂预测场景中的适应性和能力进行了评估分析

Conclusion: HTKGHs为表示复杂时序事实提供了更强大的框架，特别是在地缘政治事件预测领域。通过htkgh-polecat数据集和LLM基准测试，为未来研究复杂时序知识图预测任务奠定了基础，揭示了LLM在复杂预测场景中的潜力和局限性。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [64] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: 比较三种轻量级Transformer模型（DistilBERT、MiniLM、ALBERT）在三个文本自动化任务（情感分类、新闻分类、仇恨言论检测）上的性能与效率表现，分析不同模型在准确性和效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 企业NLP应用对高效、轻量级模型处理多领域文本自动化任务的需求日益增长，需要评估不同轻量级Transformer模型在实际企业场景中的表现。

Method: 使用IMDB、AG News和Measuring Hate Speech三个数据集，评估DistilBERT、MiniLM和ALBERT在情感分类、新闻分类和仇恨言论检测三个任务上的表现。采用准确性指标（准确率、精确率、召回率、F1分数）和效率指标（模型大小、推理时间、吞吐量、内存使用）。

Result: 没有单一模型在所有性能维度上占优：ALBERT在多个领域获得最高任务特定准确率，MiniLM在推理速度和吞吐量上表现最佳，DistilBERT在任务间保持最一致的准确率同时保持竞争力效率。

Conclusion: 研究揭示了准确性和效率之间的权衡，建议：对延迟敏感的企业应用选择MiniLM，需要平衡性能的选择DistilBERT，资源受限环境选择ALBERT。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [65] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）如何为检验语言意义理论提供新视角，对比了社会建构主义（语言游戏）和数学导向的语义场理论，认为两者是互补而非竞争的关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现为检验长期存在的语言意义理论提供了新的实证环境。作者旨在探讨两种主要理论框架：社会建构主义的语言游戏理论和数学导向的语义场理论，并分析它们在解释LLMs成功与局限方面的适用性。

Method: 作者基于先前研究，形式化了词汇场（Lexfelder）和语言场（Lingofelder）作为连续语义空间中相互作用的结构。然后分析了transformer架构的核心特性（如分布式表示、注意力机制、嵌入空间的几何规律性）与这些概念的关系。

Result: 研究发现LLMs在捕捉语义规律性方面的成功支持了语言具有底层数学结构的观点，而它们在语用推理和上下文敏感性方面的持续局限则与哲学语言使用理论中强调的社会基础重要性一致。

Conclusion: 数学结构和语言游戏可以被理解为互补而非竞争的观点。这一框架澄清了纯统计语言模型的范围和局限，并为理论指导的AI架构设计提供了新的方向。

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [66] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 提出Defensive M2S训练范式，通过将多轮对话压缩为单轮对话来训练护栏模型，显著降低计算成本，同时保持高攻击检测性能。


<details>
  <summary>Details</summary>
Motivation: 护栏模型对LLM部署安全至关重要，但处理完整多轮对话历史会带来巨大计算成本。需要一种既能保持安全性又能显著降低计算开销的方法。

Method: 提出Multi-turn to Single-turn (M2S)压缩训练范式，使用三种压缩模板（hyphenize、numberize、pythonize）将多轮对话压缩为单轮对话，在三个护栏模型家族（LlamaGuard、Nemotron、Qwen3Guard）上进行训练。

Result: 训练成本从O(n²)降至O(n)，训练数据量减少93倍（从15.7M token降至169K token）。最佳配置（Qwen3Guard + hyphenize）在SafeDialBench上达到93.8%攻击检测召回率，推理token减少94.6%，比基线提升38.9个百分点。

Conclusion: M2S压缩可作为护栏模型部署的有效效率技术，能够在显著降低训练和推理成本的同时，保持对多轮对话的安全筛查能力，实现可扩展的安全部署。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [67] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 该论文提出了一种针对职业教育培训领域历史数字化文档的鲁棒命名实体识别方法，通过噪声感知训练、合成OCR错误注入和迁移学习等技术，显著提升了在OCR噪声条件下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 职业教育培训领域的历史数字化文档存在OCR噪声问题，这严重影响了命名实体识别的准确性。目前缺乏针对该领域多实体类型识别的方法，特别是在噪声条件下的鲁棒性解决方案。

Method: 提出噪声感知训练方法，通过合成注入OCR错误，结合迁移学习和多阶段微调。系统比较了三种互补策略：在噪声数据、干净数据和人工合成数据上的训练。

Result: 实验结果表明，领域特定和噪声感知的微调显著提高了在噪声条件下的鲁棒性和准确性。该方法在德语文档上验证，但可扩展到任意语言。

Conclusion: 该方法首次实现了职业教育培训文档中多实体类型的识别，为领域特定噪声感知NER提供了可复现的解决方案，并公开了代码。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [68] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: 该研究分析了复杂句子结构对基于规则的原子句子提取性能的影响，识别出相对从句、同位语、并列谓语等结构是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的原子句子提取方法缺乏可解释性，无法揭示哪些具体语言结构导致提取失败，需要系统分析复杂句子结构对规则提取的影响。

Method: 使用WikiSplit数据集，在spaCy中实现基于依存关系的提取规则，生成100个黄金标准原子句子集，使用ROUGE和BERTScore评估性能。

Result: 系统获得ROUGE-1 F1=0.6714，ROUGE-2 F1=0.478，ROUGE-L F1=0.650，BERTScore F1=0.5898，显示中高水平的词汇、结构和语义对齐。相对从句、同位语、并列谓语、状语从句和被动结构最具挑战性。

Conclusion: 基于规则的原子句子提取在准确性上表现合理，但对句法复杂性敏感，需要针对特定复杂结构改进提取规则。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [69] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: ECR框架通过语义锚点保持紧凑模型嵌入空间的结构一致性，避免语义漂移，适用于多语言场景和容量受限的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 紧凑模型在容量受限或多语言场景下容易丢失嵌入空间的结构，导致语义漂移，影响下游任务性能。现有压缩方法只关注表层输出对齐，未能保持底层流形结构。

Method: 提出嵌入一致性调节（ECR）框架：从教师模型嵌入中提取语义锚点（离线计算），然后让紧凑模型学习在这些锚点周围保持一致的几何结构，不依赖匹配logits或内部特征。推理时仅添加小型投影步骤，不改变解码架构或运行时行为。

Result: 在10万规模的多语言语料实验中，ECR稳定训练过程，跨任务和语言保持语义结构，产生更紧凑且任务对齐的表示空间，使低容量模型能学习比传统基线更清晰的流形结构。

Conclusion: ECR帮助紧凑模型更好地遵循任务要求，使其在严格效率或隐私限制下更容易部署。该方法不依赖教师输出，与蒸馏兼容但独立，能有效防止语义漂移。

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [70] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级、语言无关的多语言ASR系统，基于CTC架构和领域自适应，通过分层LoRA-MoE框架实现高效单次解码，无需先验语言信息。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言ASR模型（如Whisper）虽然性能强大，但计算和延迟成本高，难以在资源受限的边缘设备上部署。需要开发轻量级、高效的解决方案。

Method: 提出语言无关的分层LoRA-MoE（HLoRA）框架，集成到mHuBERT-CTC模型中。包含多语言共享LoRA学习语言不变声学表示，以及语言特定LoRA专家建模语言依赖特性。通过LID后验驱动的LoRA路由实现端到端解码，无需推理时的语言身份信息或显式语言标签。

Result: 在MSR-86K和MLC-SLM 2025挑战数据集上的实验表明，HLoRA仅通过单次解码就能达到与最先进的两阶段推理方法相竞争的性能，显著提高了低资源多语言ASR应用的解码效率。

Conclusion: 提出的HLoRA框架为资源受限环境下的多语言ASR提供了一种高效、语言无关的解决方案，通过分层LoRA-MoE设计和智能路由机制实现了性能与效率的良好平衡。

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [71] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth是一个基于信息论原则自动生成和评估推理基准的框架，使用KL散度和熵量化基准新颖性和多样性，通过遗传算法从种子数据集合成Python编程问题，生成准确率97%，能控制问题难度和新颖性。


<details>
  <summary>Details</summary>
Motivation: 传统基准创建依赖人工，成本高且耗时；现有基准常污染LLM训练数据，需要新颖多样的基准来准确评估LLM的真实能力。

Method: 提出基于KL散度和熵的指标量化基准新颖性和多样性；开发端到端流水线，使用遗传算法和迭代代码反馈从种子数据集合成稳健的Python编程问题。

Result: 方法生成新问题的准确测试用例和解决方案达97%成功率；合成基准相比种子数据集展现出更高的新颖性和多样性；算法能控制生成问题的新颖性/多样性和难度。

Conclusion: InfoSynth为LLM提供了一个可扩展、自验证的高质量、新颖且多样基准构建流水线。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [72] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: CSSBench是一个专门针对中文特定对抗模式的安全基准测试，用于评估轻量级大语言模型在中文环境下的安全性，填补了现有基准测试主要关注英文的空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地部署在成本敏感和设备端场景中，但现有的安全防护主要针对英文。中文恶意查询通常通过同音字、拼音、符号分割等中文特有的模式隐藏意图，这些对抗模式在现有基准测试中未被充分捕捉，特别是对于可能更脆弱的轻量级模型。

Method: 引入中文特定安全基准测试(CSSBench)，重点关注中文特有的对抗模式，涵盖六个真实中文场景中的常见领域：非法活动与合规、隐私泄露、健康与医疗错误信息、欺诈与仇恨、成人内容、公共与政治安全，并将查询组织成多种任务类型。

Result: 评估了一系列流行的轻量级大语言模型，并测量了过度拒绝行为以评估安全性导致的性能下降。结果显示，中文特定的对抗模式对轻量级大语言模型构成了关键挑战。

Conclusion: CSSBench为中文环境下的大语言模型安全性提供了全面评估，有助于在实际部署中实现更稳健的安全防护，填补了现有基准测试在中文特定对抗模式方面的空白。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [73] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 提出一种模型无关的框架，通过重复采样和多数投票机制，为减少LLM在确定性任务中的幻觉提供概率保证


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在确定性自动化工作流中经常产生上下文幻觉，即生成内容与提示中明确信息相矛盾或忽略这些信息。这类错误在输入固定、正确性明确的场景中尤为严重。

Method: 提出一个简单且模型无关的框架：1) 在独立上下文窗口中重复相同提示，利用指数级降低所有输出都错误的概率；2) 引入LLM作为评判者识别正确答案；3) 当评判者不完美时，通过独立评判调用的多数投票来增强，获得指数级降低的集合级错误率。

Result: 在受控提取任务上的实验验证了理论预测：管道失败概率随重复次数指数下降，幻觉选择概率随评判者数量指数下降。框架能够在不修改模型权重、解码策略或提示工程的情况下，将幻觉概率降至任意低水平。

Conclusion: 该研究提供了一个轻量级、模块化且理论可靠的方法，能够在固定输入的LLM工作流中任意降低幻觉概率，为确定性自动化任务提供了实用的解决方案。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [74] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: FwPKM是一种新型架构，将静态的产品键内存转换为动态的快速权重情景记忆，解决了语言模型中存储容量与计算效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型中的序列建模层面临存储容量与计算效率之间的权衡：Softmax注意力提供无限存储但计算成本高，线性变体效率高但存储有限。需要一种既能高效计算又具有足够存储容量的解决方案。

Method: 提出快速权重产品键内存（FwPKM），将稀疏的产品键内存从静态模块转变为动态的"快速权重"情景记忆。FwPKM在训练和推理时通过局部块级梯度下降动态更新参数，能够快速记忆和检索输入序列中的新键值对。

Result: 实验表明FwPKM作为有效的情景记忆，补充了标准模块的语义记忆，在长上下文数据集上显著降低了困惑度。在"大海捞针"评估中，尽管只在4K标记序列上训练，FwPKM能够泛化到128K标记的上下文。

Conclusion: FwPKM成功解决了语言模型中存储容量与计算效率的权衡问题，通过动态更新的情景记忆机制实现了高效的长上下文处理能力，为语言模型架构设计提供了新思路。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [75] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: 提出Sigmoid Head模块解决语言模型概率不可靠问题，通过sigmoid激活和负采样策略改进质量估计


<details>
  <summary>Details</summary>
Motivation: 语言模型的softmax概率分布不可靠，因为自然语言存在歧义，多个有效输出选项会分散概率分布，导致质量估计不准确

Method: 在预训练语言模型上训练质量估计模块Sigmoid Head，使用sigmoid激活替代softmax，并通过启发式负采样避免选择潜在的正确替代词

Result: Sigmoid Head的概率信号显著优于原始softmax头，计算效率高，且不依赖人工标注数据，在域外设置中更鲁棒

Conclusion: Sigmoid Head能有效解决语言模型概率不可靠问题，提供更好的质量估计信号，适用于无监督质量评估场景

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [76] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在文本跨度识别任务上的表现，特别是在情感分析、攻击性语言识别和声明验证等主观性较强的任务中，探索了指令调优、上下文学习和思维链等策略。


<details>
  <summary>Details</summary>
Motivation: 当前大多数跨度识别方法依赖于BERT等较小的预训练模型，而大型语言模型在主观性跨度识别任务（如基于方面的情感分析）中的应用尚未充分探索。本文旨在填补这一重要空白。

Method: 在三个流行任务（情感分析、攻击性语言识别和声明验证）中评估各种LLM的文本跨度识别性能，探索指令调优、上下文学习和思维链等多种LLM策略。

Result: 研究结果表明，文本内部的潜在关系有助于LLM识别精确的文本跨度。

Conclusion: 本文填补了LLM在主观性文本跨度识别任务中的研究空白，展示了文本内部关系对LLM识别精确跨度的积极作用，为下游任务的可解释性提供了重要见解。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [77] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: 本研究评估了跨省适应癌症登记NLP模型的效果，通过微调BCCRTron和GatorTron模型，结合OR集成策略，显著减少了漏报癌症病例，实现了隐私保护下的跨省模型共享。


<details>
  <summary>Details</summary>
Motivation: 癌症登记依赖病理报告作为主要诊断来源，但人工提取资源密集且导致数据延迟。现有基于transformer的NLP系统在跨司法管辖区（不同报告规范）的泛化能力尚不清楚，需要评估跨省适应癌症登记NLP模型的可行性。

Method: 研究采用跨省评估方法，将BC省开发的BCCRTron模型和生物医学transformer模型GatorTron适应到纽芬兰与拉布拉多省癌症登记。使用约104,000份（Tier 1）和22,000份（Tier 2）去标识化病理报告进行微调，采用互补的概要式和诊断聚焦报告部分输入管道，并通过保守的OR集成策略组合两个模型。

Result: 适应后的模型在新省份测试集上保持高性能，证明在一个司法管辖区预训练的transformer模型可通过适度微调本地化到另一个管辖区。OR集成显著提高了敏感性：Tier 1召回率达到0.99，漏报癌症从单独模型的48和54例减少到24例；Tier 2召回率0.99，漏报可报告癌症从54和46例减少到33例。

Conclusion: 结合互补文本表示的集成策略能显著减少癌症漏报并提高错误覆盖率。研究实现了隐私保护工作流程（仅共享模型权重），支持可互操作的NLP基础设施，为未来泛加拿大癌症病理和登记工作流程的基础模型奠定了基础。

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>
