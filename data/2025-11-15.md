<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同参数规模的SmolVLM2模型（500M和2.2B）在盲人和低视力用户视频描述任务中的性能，提出了两个专门针对BLV可访问性评估的新框架，并系统评估了提示设计策略和移动设备部署性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成高质量视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对于依赖详细上下文感知描述的盲人和低视力用户。

Method: 在两个多样化数据集（AVCaps户外和Charades室内）上评估不同参数规模的SmolVLM2变体；引入两个新的BLV可访问性评估框架：多上下文BLV框架和导航辅助框架；系统评估四种提示设计策略；在智能手机上部署模型并评估FP32和INT8精度变体。

Result: 通过评估不同规模的模型和精度变体，分析了模型大小对可访问性描述质量的影响，以及在资源受限移动设备上的实际性能约束。

Conclusion: 研究为开发更实用的BLV辅助技术提供了重要见解，特别是在模型规模优化和移动部署方面的指导。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 提出自一致性采样（SCS）方法，通过引入视觉扰动和轨迹重采样来纠正多模态大语言模型中结果奖励强化学习的不忠实轨迹问题，在多个基准测试中显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试的多选题设置中，结果奖励强化学习面临一个被忽视的障碍：即使推理链存在错误但最终猜对正确答案的轨迹，与真正正确推理的轨迹获得相同奖励，这影响了学习效果。

Method: 提出自一致性采样（SCS）：对每个问题引入小的视觉扰动，并对初始轨迹进行重复截断和重采样；通过结果轨迹的一致性得出可微的一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 基于Qwen2.5-VL-7B-Instruct模型，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率提升高达7.7个百分点，且额外计算成本可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也获得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单通用的解决方案，有效纠正了不忠实轨迹问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文提出了编码器增强的因果解码器模型架构，在有限硬件上实现更高压缩率，通过基于token的熵估计来指导模型训练，证明接近但不超出估计熵值的模型具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于因果（下一个token预测）的大语言模型虽然是最有效的语言压缩算法，但用它们准确估计语言熵在计算上不可行。需要找到更高效的方法来估计语言熵并利用这种估计来提升模型性能。

Method: 引入编码器增强的因果解码器模型架构，这种架构具有更好的训练效率特性；开发基于token的熵估计方法；训练模型使其接近但不超出估计的熵值。

Result: 新架构在有限硬件上实现了比因果transformer更高的压缩率；能够获得基于token的熵估计；接近估计熵值的模型比单纯最小化损失的模型具有更好的泛化能力。

Conclusion: 语言预测受语言内在信息熵的限制，存在准确性的上限；通过熵指导的模型训练可以提升泛化能力；编码器增强的因果解码器架构在效率和压缩性能上优于传统因果transformer。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [4] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 提出Socratic Self-Refine (SSR)框架，通过细粒度评估和精确精炼来改进LLM推理能力，将模型响应分解为可验证的子问题-子答案对，实现步骤级置信度估计和迭代精炼。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗粒度的自我验证和自我校正，在复杂任务上效果有限，需要更精细的方法来评估和改进LLM推理过程。

Method: SSR框架将模型响应分解为可验证的(子问题,子答案)对，通过受控重解和自一致性检查进行步骤级置信度估计，精确定位不可靠步骤并迭代精炼。

Result: 在五个推理基准和三个LLM上的实验结果表明，SSR持续优于最先进的迭代自我精炼基线方法。

Conclusion: SSR不仅提升了性能，还提供了一种原则性的黑盒方法来评估和理解LLM的内部推理过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [5] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开源的30亿参数语言模型家族，使用公开数据和AMD MI300X GPU训练，在完全开源模型中达到最先进性能，并发布了支持128K上下文长度的Instella-Long和数学推理优化的Instella-Math变体。


<details>
  <summary>Details</summary>
Motivation: 解决高性能语言模型大多闭源或部分开源的问题，提供透明且可复现的替代方案，推动开放和可复现的语言建模研究。

Method: 使用公开可用数据和代码库，通过大规模预训练、通用指令微调以及与人类偏好对齐来开发模型，并针对长上下文和数学推理任务进行专门优化。

Result: Instella在使用较少预训练token的情况下，在完全开源模型中达到最先进结果，并与同规模领先的开源权重模型具有竞争力。

Conclusion: Instella系列模型为社区提供了透明、高性能且多功能的替代方案，推动了开放和可复现语言建模研究的发展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [6] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种仅权重的训练后量化方法，通过成对旋转量化和通道级缩放来减少量化误差，特别针对推理大语言模型中的异常值问题，在保持低开销的同时显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有训练后量化方法无法有效抑制权重和激活中的异常值，导致量化误差大和精度严重下降，特别是在推理大语言模型中误差会在长思维链中累积。

Method: 结合硬件高效的独立Givens旋转和通道级缩放，均衡通道间幅度并缩小量化组内动态范围，同时协同设计推理内核以充分利用GPU并行性。

Result: 在推理任务上比AWQ平均提升2.4%的准确率，同时保持低于10%的开销。

Conclusion: 该方法为推理大语言模型的高效准确部署铺平了道路。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>
