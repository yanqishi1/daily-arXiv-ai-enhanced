{"id": "2509.13480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u6027\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u610f\u5927\u5229\u8bed\u6027\u522b\u4e2d\u6027\u6539\u5199\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5f00\u653e\u6e90\u6a21\u578b\u8d85\u8fc7\u4e13\u95e8\u6a21\u578b\uff0c\u7ec6\u8c03\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u4e0b\u8fbe\u5230\u76f8\u4f3c\u6027\u80fd", "motivation": "\u89e3\u51b3\u8bed\u6cd5\u6027\u522b\u8bed\u8a00(\u5982\u610f\u5927\u5229\u8bed)\u4e2d\u6027\u522b\u4e2d\u6027\u6539\u5199\u7684\u6311\u6218\uff0c\u7f3a\u4e4f\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30", "method": "\u6784\u5efa\u4e8c\u7ef4\u8bc4\u4f30\u6846\u67b6(\u4e2d\u6027\u6027\u548c\u8bed\u4e49\u5fe0\u5b9e\u5ea6)\uff0c\u6bd4\u8f83\u5c11\u6837\u672c\u63d0\u793a\u3001\u7ec6\u8c03\u9009\u62e9\u6a21\u578b\u5e76\u5e94\u7528\u76ee\u6807\u6e05\u7406\u6280\u672f", "result": "\u5f00\u653e\u6743\u91cdLLM\u5728\u610f\u5927\u5229\u8bedGNR\u4e2d\u8d85\u8fc7\u4e13\u95e8\u6a21\u578b\uff0c\u7ec6\u8c03\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u4e0b\u8fbe\u5230\u76f8\u4f3c\u6216\u66f4\u597d\u6027\u80fd", "conclusion": "\u5c55\u793a\u4e86\u5728\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u4e2d\u6027\u6027\u548c\u610f\u4e49\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u8bed\u6cd5\u6027\u522b\u8bed\u8a00\u7684\u6027\u522b\u4e2d\u6027\u6539\u5199\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "Op-Fed\u6570\u636e\u96c6\u5305\u542b1044\u6761\u4eba\u5de5\u6807\u6ce8\u7684FOMC\u4f1a\u8bae\u8bb0\u5f55\u53e5\u5b50\uff0c\u7528\u4e8e\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u53e5\u5b50\u95f4\u4f9d\u8d56\u7684\u6280\u672f\u6311\u6218\u3002", "motivation": "FOMC\u8d27\u5e01\u653f\u7b56\u51b3\u7b56\u5f71\u54cd\u6570\u767e\u4e07\u4eba\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u96c6\u6765\u5206\u6790\u653f\u7b56\u7acb\u573a\u8868\u8fbe\u3002", "method": "\u5f00\u53d1\u4e94\u9636\u6bb5\u5206\u5c42\u6807\u6ce8\u6846\u67b6\uff0c\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u6807\u6ce8\u5b9e\u4f8b\uff0c\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u9876\u7ea7\u95ed\u6e90LLM\u5728\u89c2\u70b9\u5206\u7c7b\u4e0a\u8fbe\u52300.80\u51c6\u786e\u7387\uff0c\u4f46\u5728\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u7c7b\u4e0a\u4ec50.61\uff0c\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c60.89\u3002", "conclusion": "Op-Fed\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u4f5c\u4e3a\u672a\u6765\u6807\u6ce8\u5de5\u4f5c\u7684\u79cd\u5b50\u6570\u636e\u96c6\u3002"}}
{"id": "2509.13569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendon\u00e7a", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "Jo\u00e3o Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "DSTC12 Track 1 \u9488\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u548c\u591a\u8bed\u8a00\u6587\u5316\u5b89\u5168\u68c0\u6d4b\uff0c\u5efa\u7acb\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u65b9\u6cd5\u5728\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u5728\u6587\u5316\u5b89\u5168\u610f\u8bc6\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u51f8\u663e\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u4e0d\u8db3\uff0c\u5b89\u5168\u8003\u8651\u5f80\u5f80\u5b9a\u4e49\u72ed\u7a84\u6216\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u3001\u591a\u7ef4\u5ea6\u3001\u8de8\u6587\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7DSTC12 Track 1\u7ade\u8d5b\u8bbe\u7f6e\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a1\uff09\u5bf9\u8bdd\u7ea7\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0810\u4e2a\u7ef4\u5ea6\uff09\uff0c2\uff09\u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u5b89\u5168\u68c0\u6d4b\u3002\u63d0\u4f9b\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff08Llama-3-8B\u548cLlama-Guard-3-1B\uff09\uff0c\u6536\u96c6\u53c2\u8d5b\u56e2\u961f\u63d0\u4ea4\u7684\u65b9\u6848\u8fdb\u884c\u8bc4\u4f30\u6bd4\u8f83\u3002", "result": "\u4efb\u52a11\u4e2dLlama-3-8B\u57fa\u7ebf\u6a21\u578b\u83b7\u5f97\u6700\u9ad8\u5e73\u5747Spearman\u76f8\u5173\u7cfb\u65700.1681\uff0c\u8868\u660e\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u4efb\u52a12\u4e2d\u53c2\u8d5b\u56e2\u961f\u5728\u591a\u8bed\u8a00\u5b89\u5168\u5b50\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u6700\u4f73ROC-AUC 0.9648\uff09\uff0c\u4f46\u5728\u6587\u5316\u5b50\u96c6\u4e0a\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\uff080.5126 ROC-AUC\uff09\uff0c\u51f8\u663e\u6587\u5316\u5b89\u5168\u610f\u8bc6\u7684\u5173\u952e\u9700\u6c42\u3002", "conclusion": "\u5f53\u524d\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u5728\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u548c\u6587\u5316\u5b89\u5168\u610f\u8bc6\u65b9\u9762\u90fd\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u66f4\u5168\u9762\u3001\u6587\u5316\u654f\u611f\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0cDSTC12 Track 1\u4e3a\u6b64\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u548c\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u8fc1\u79fb\u5b66\u4e60\u77e9\u9635\u548c\u964d\u7ef4\u6280\u672f\u6765\u63a2\u7d22\u8de8\u4efb\u52a1\u8f6c\u79fb\u5b66\u4e60\u7684\u6f5c\u5728\u80fd\u529b\u548c\u526f\u4f5c\u7528\uff0c\u53d1\u73b0\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\u6bd4\u8868\u9762\u6570\u636e\u76f8\u4f3c\u6027\u66f4\u5173\u952e\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7ecf\u5e38\u9047\u5230\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\uff0c\u9700\u8981\u4f9d\u9760\u8f83\u5dee\u8d28\u91cf\u6570\u636e\u8fdb\u884c\u8f6c\u79fb\u5b66\u4e60\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u8de8\u4efb\u52a1\u8f6c\u79fb\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\u3002", "method": "\u6784\u5efa\u8f6c\u79fb\u5b66\u4e60\u77e9\u9635\u5e76\u4f7f\u7528\u964d\u7ef4\u6280\u672f\uff0c\u8bad\u7ec3\u5206\u679010\u4e2a\u6a21\u578b\u6765\u8bc6\u522b\u6f5c\u5728\u80fd\u529b\uff08\u5982\u63a8\u7406\u3001\u60c5\u611f\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7b97\u672f\u7b49\uff09\u548c\u8f6c\u79fb\u5b66\u4e60\u7684\u526f\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u6027\u80fd\u6539\u5584\u5e76\u4e0d\u53d6\u51b3\u4e8e\u8868\u9762\u6570\u636e\u76f8\u4f3c\u6027\u6216\u6e90\u6570\u636e\u8d28\u91cf\uff0c\u800c\u662f\u7531\u6e90\u6570\u636e\u96c6\u7684\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\uff08\u5982\u7c7b\u5206\u5e03\u3001\u751f\u6210\u957f\u5ea6\u504f\u597d\uff09\u548c\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u66f4\u5177\u5f71\u54cd\u529b\u3002", "conclusion": "\u8fd9\u4e2a\u5de5\u4f5c\u63ed\u793a\u4e86\u8f6c\u79fb\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\uff0c\u4e3a\u66f4\u53ef\u9884\u6d4b\u548c\u6709\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u6293\u57fa\u7840\u3002"}}
{"id": "2509.13338", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc1\u636e\u68c0\u7d22\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u673a\u5236\uff0c\u901a\u8fc7\u67e5\u627e\u8fd1\u4f3c\u793a\u4f8b\u5e76\u878d\u5408\u5176\u9884\u6d4b\u5206\u5e03\u6765\u5b9e\u73b0\u4f9b\u4f9b\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u9002\u5e94\u6027\u51b3\u7b56\u9605\u523b\u3002", "motivation": "\u66ff\u4ee3\u5355\u4e00\u5168\u5c40\u9605\u523b\u6807\u51c6\uff0c\u63d0\u4f9b\u4e00\u79cd\u57fa\u4e8e\u8bc1\u636e\u7684\u9002\u5e94\u6027\u51b3\u7b56\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u3002", "method": "\u4e3a\u6bcf\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u67e5\u627e\u8fd1\u4f3c\u793a\u4f8b\uff0c\u901a\u8fc7Dempster-Shafer\u7406\u8bba\u878d\u5408\u5176\u9884\u6d4b\u5206\u5e03\uff0c\u5f62\u6210\u878d\u5408\u4fe1\u5ff5\u4f5c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u9605\u523b\u673a\u5236\u3002", "result": "\u5728CIFAR-10/100\u4e0a\u4f7f\u7528BiT\u548cViT\u80cc\u699c\u8fdb\u884c\u5b9e\u9a8c\uff0c\u663e\u793a\u4e86\u66f4\u9ad8\u6216\u76f8\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u81ea\u4fe1\u9519\u8bef\u7684\u7ed3\u679c\uff0c\u5e76\u4fdd\u6301\u53ef\u6301\u7eed\u7684\u5ba1\u67e5\u8d1f\u8377\u3002\u4ec5\u9700\u5c11\u91cf\u8bc1\u636e\u5373\u53ef\u5b9e\u73b0\u8fd9\u4e9b\u6536\u76ca\u3002", "conclusion": "\u8bc1\u636e\u6761\u4ef6\u5316\u6807\u8bb0\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u9884\u6d4b\u71b5\u9605\u523b\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u64cd\u4f5c\u6027\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u3002"}}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "LLMs\u5185\u90e8\u795e\u7ecf\u5143\u7ebf\u6027\u7f16\u7801\u95ee\u9898\u6b67\u4e49\u6027\uff0c\u5c11\u6570\u795e\u7ecf\u5143\u5373\u53ef\u68c0\u6d4b\u548c\u63a7\u5236\u6b67\u4e49\u6027\uff0c\u4f7f\u6a21\u578b\u4ece\u76f4\u63a5\u56de\u7b54\u8f6c\u4e3a\u5f03\u6743", "motivation": "\u73b0\u5b9e\u95ee\u9898\u666e\u904d\u5b58\u5728\u6b67\u4e49\u6027\uff0c\u4f46LLMs\u5f80\u5f80\u81ea\u4fe1\u56de\u7b54\u800c\u975e\u5bfb\u6c42\u6f84\u6e05\uff0c\u9700\u8981\u7814\u7a76\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u6b67\u4e49\u6027", "method": "\u5728\u6a21\u578b\u9884\u586b\u5145\u9636\u6bb5\u8bc6\u522b\u6b67\u4e49\u7f16\u7801\u795e\u7ecf\u5143(AENs)\uff0c\u8bad\u7ec3\u63a2\u9488\u8fdb\u884c\u6b67\u4e49\u68c0\u6d4b\uff0c\u901a\u8fc7\u64cd\u7eb5AENs\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "result": "AENs\u63a2\u9488\u5728\u6b67\u4e49\u68c0\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u6b67\u4e49\u4fe1\u53f7\u5728\u6d45\u5c42\u65e9\u671f\u7f16\u7801\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u64cd\u7eb5\u53ef\u63a7\u5236\u6a21\u578b\u56de\u7b54\u884c\u4e3a", "conclusion": "LLMs\u5f62\u6210\u7d27\u51d1\u7684\u5185\u90e8\u6b67\u4e49\u6027\u8868\u793a\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u884c\u4e3a\uff0c\u4e3a\u6a21\u578b\u6b67\u4e49\u5904\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3"}}
{"id": "2509.13353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u7eaf\u7ecf\u5178\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa", "motivation": "\u7cfb\u7edf\u6bd4\u8f83\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u7eaf\u7ecf\u5178\u6a21\u578b\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u8bc4\u4f30\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c", "method": "\u5728MNIST\u3001CIFAR100\u548cSTL10\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5c06\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\u4e0e\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bad\u7ec350\u4e2aepoch\uff0c\u8bc4\u4f30\u9a8c\u8bc1\u51c6\u786e\u7387\u3001\u6d4b\u8bd5\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u3001\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u548c\u5bf9\u6297\u9c81\u68d2\u6027", "result": "\u6df7\u5408\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u5747\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff08MNIST: 99.38% vs 98.21%\uff1bCIFAR100: 41.69% vs 32.25%\uff1bSTL10: 74.05% vs 63.76%\uff09\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb5-12\u500d\uff0c\u53c2\u6570\u51cf\u5c116-32%\uff0c\u5185\u5b58\u4f7f\u7528\u66f4\u4f4e\uff084-5GB vs 5-6GB\uff09\uff0cCPU\u5229\u7528\u7387\u66f4\u4f4e\uff089.5% vs 23.2%\uff09\uff0c\u5728\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u5bf9\u6297\u9c81\u68d2\u6027\u663e\u8457\u66f4\u597d", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u5728\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u89c6\u89c9\u4efb\u52a1\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e"}}
{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e2d\u6587\u6587\u732e\u8bed\u6cd5\u7ea0\u9519\u6301\u7eed\u5b66\u4e60\u57fa\u51c6CL\u00b2GEC\uff0c\u5305\u542b10\u4e2a\u5b66\u79d1\u768410,000\u6761\u6807\u6ce8\u53e5\u5b50\uff0c\u8bc4\u4f30LLM\u5728\u8de8\u5b66\u79d1\u8bed\u6cd5\u7ea0\u9519\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u7f3a\u4e4f\u591a\u5b66\u79d1\u5b66\u672f\u5199\u4f5c\u7684\u4e13\u95e8\u57fa\u51c6\uff0c\u5ffd\u89c6\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53d8\u5f02\u548c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b10\u4e2a\u5b66\u79d110,000\u53e5\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u987a\u5e8f\u8c03\u4f18\u3001\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u548c\u56db\u79cd\u4ee3\u8868\u6027CL\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u56de\u653e\u6216\u7b80\u5355\u987a\u5e8f\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8de8\u5b66\u79d1\u5b66\u672f\u9886\u57df\u7684\u81ea\u9002\u5e94\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2509.13361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316YOLOv11-DIoU\u548cDeepSort\u7b97\u6cd5\u63d0\u9ad8\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8f66\u8f86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u4f7f\u7528GRU-Attention\u6a21\u578b\u8fdb\u884c\u62d2\u585e\u9884\u8b66\uff0c\u5728\u9ad8\u901f\u516c\u8def\u62d2\u585e\u7ba1\u7406\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\"\u68c0\u6d4b-\u9884\u6d4b\"\u7cfb\u7edf\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u8f66\u8f86\u611f\u77e5\u51c6\u786e\u6027\u4f4e\u548c\u62d2\u585e\u9884\u6d4b\u4e2d\u957f\u5e8f\u5217\u4f9d\u8d56\u5931\u5931\u7684\u5173\u952e\u7f3a\u9677\u3002", "method": "1\uff09\u4f18\u5316YOLOv11\u4e3aYOLOv11-DIoU\uff08\u66ff\u6362GIoU Loss\u4e3aDIoU Loss\uff09\u548cDeepSort\uff08\u878d\u5408\u9a6c\u6d0b\u6d77\u65af\u8ddd\u79bb\u548c\u4f59\u5f26\u8ddd\u79bb\uff09\u63d0\u9ad8\u8f66\u8f86\u68c0\u6d4b\u6027\u80fd\n2\uff09\u4f7f\u7528Greenberg\u6a21\u578b\u5206\u6790\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\u901f\u5ea6\u4e0e\u5bc6\u5ea6\u5173\u7cfb\n3\uff09\u6784\u5efaGRU-Attention\u6a21\u578b\u8fdb\u884c\u62d2\u585e\u9884\u8b66\uff0c\u8bad\u7ec3300\u8fde\u7eed\u65f6\u95f4\u6b65\u957f", "result": "1\uff09YOLOv11-DIoU\u8fbe\u523095.7% mAP\uff08\u6bd4\u57fa\u7ebf\u63d0\u9ad86.5%\uff09\uff0c906e\u6321\u6f0f\u68c0\u7387\u4ec55.3%\n2\uff09DeepSort\u8fbe\u523093.8% MOTA\uff08\u6bd4SORT\u63d0\u9ad811.3%\uff09\uff0c\u4ec54\u6b21ID\u5207\u6362\n3\uff09\u901f\u5ea6\u4e0e\u5bc6\u5ea6\u5448\u5f3a\u8d1f\u76f8\u5173\uff08r=-0.97\uff09\n4\uff09GRU-Attention\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe99.7%\uff08\u6bd4\u4f20\u7edfGRU\u63d0\u9ad87-9%\uff09\uff0c10\u5206\u949f\u9884\u8b62\u9519\u5dee\u22641\u5206\u949f\n5\uff09\u72ec\u7acb\u89c6\u9891\u9a8c\u8bc195%\u9884\u8b66\u51c6\u786e\u7387\uff0c>90%\u62d2\u585e\u70b9\u7a7a\u95f4\u91cd\u53e0\u7387", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u4e3a\u9ad8\u901f\u516c\u8def\u62d2\u585e\u63a7\u5236\u63d0\u4f9b\u4e86\u91cf\u5316\u652f\u6491\uff0c\u5728\u667a\u80fd\u4ea4\u901a\u9884\u8b66\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13677", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "AgentCTG\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u6587\u672c\u751f\u6210\u7684\u7cbe\u786e\u590d\u6742\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u89d2\u8272\u9a71\u52a8\u91cd\u5199\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1NLP\u9886\u57df\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u53d7\u63a7\u6587\u672c\u751f\u6210(CTG)\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6761\u4ef6\u63a7\u5236\u65b9\u9762\u3002\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u8fd8\u9700\u8981\u8003\u8651\u6210\u672c\u3001\u53ef\u6269\u5c55\u6027\u3001\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\u548c\u66f4\u7cbe\u786e\u7684\u63a7\u5236\u3002", "method": "\u63d0\u51faAgentCTG\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\u6765\u589e\u5f3a\u6587\u672c\u751f\u6210\u7684\u7cbe\u786e\u63a7\u5236\u3002\u63a2\u7d22\u4e0d\u540c\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u63d0\u793a\u6a21\u5757\u8fdb\u4e00\u6b65\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u63d0\u51fa\u7684\u89d2\u8272\u9a71\u52a8\u91cd\u5199\u4efb\u52a1\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5728\u5728\u7ebf\u5bfc\u822a\u89d2\u8272\u626e\u6f14\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u4f53\u9a8c\u548c\u5185\u5bb9\u4ea4\u4ed8\u6548\u679c\u3002", "conclusion": "AgentCTG\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u63a7\u6587\u672c\u751f\u6210\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\u6311\u6218\uff0c\u4e3a\u5728\u7ebf\u793e\u533a\u63d0\u4f9b\u4e86\u66f4\u6c89\u6d78\u5f0f\u7684\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u4e86\u4e2a\u6027\u5316\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2509.13366", "categories": ["cs.CV", "68U99", "J.2"], "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u81ea\u52a8\u5316\u5730\u9762\u771f\u5b9e\u6d4b\u8bd5\u8fc7\u7a0b\uff0c\u5c06\u4eba\u529b\u82af\u8017\u51cf\u5c1199.58%\uff0c\u63d0\u5347\u4e86\u57fa\u4e8e\u7fa4\u6676\u6570\u636e\u7684\u5b9e\u65f6\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u8d28\u91cf", "motivation": "\u4f18\u5316\u73b0\u6709\u7684\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u8d28\u91cf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5730\u9762\u771f\u5b9e\u6d4b\u8bd5\u8fc7\u7a0b\u6765\u66ff\u4ee3\u4eba\u5de5\u5de5\u4f5c\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51cf\u5c11\u4eba\u529b\u82af\u8017", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u548c\u56fe\u50cf\u6a21\u5f0f\u8bc6\u522b\u6280\u672f\uff0c\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\uff0c\u6765\u5b8c\u5584\u6570\u636e\u5e93\u5e76\u81ea\u52a8\u5316\u5206\u6790\u8fc7\u7a0b", "result": "\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4eba\u529b\u8d44\u6e90\u65f6\u95f4\u6d88\u8017\u51cf\u5c1199.58%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u6548\u7387", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u81ea\u52a8\u5316\u7684\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u7684\u670d\u52a1\u4f18\u5316\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u663e\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b"}}
{"id": "2509.13683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "CARE\u6846\u67b6\u901a\u8fc7\u6559\u5bfcLLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u751f\u6210\u6027\u80fd\uff0c\u65e0\u9700\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03", "motivation": "\u89e3\u51b3LLM\u5728\u57fa\u4e8e\u7ed9\u5b9a\u4fe1\u606f\u56de\u7b54\u95ee\u9898\u65f6\u5b58\u5728\u7684\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u5229\u7528\u7ed9\u5b9a\u4e0a\u4e0b\u6587", "method": "\u63d0\u51faCARE\u6846\u67b6\uff0c\u6559\u5bfcLLM\u5229\u7528\u81ea\u8eab\u68c0\u7d22\u80fd\u529b\u5728\u63a8\u7406\u94fe\u4e2d\u663e\u5f0f\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u8bc1\u636e\u6570\u636e", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u53cd\u4e8b\u5b9eQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u3001\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u548c\u5916\u90e8\u68c0\u7d22\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u4f7fLLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u66f4\u52a0\u51c6\u786e\u3001\u53ef\u9760\u548c\u9ad8\u6548\u7684\u6839\u672c\u6027\u8fdb\u6b65"}}
{"id": "2509.13375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u4e2d\u7684\u673a\u5236\u3001\u4f18\u52bf\u548c\u654f\u611f\u6027\uff0c\u63ed\u793a\u4e86\u5176\u5229\u7528\u8bed\u4e49\u65b0\u9896\u6027\u7684\u4f18\u52bf\uff0c\u4f46\u5bf9\u63d0\u793a\u8bcd\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\u7684\u5173\u952e\u8106\u5f31\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(\u5982CLIP)\u5728\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u754c\u5bf9\u5176\u5de5\u4f5c\u673a\u5236\u3001\u76f8\u5bf9\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\u4ee5\u53ca\u884c\u4e3a\u9c81\u68d2\u6027\u7f3a\u4e4f\u5168\u9762\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4f7f\u7528\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u63d0\u793a\u8bcd\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u7814\u7a76VLM-based OOD\u68c0\u6d4b\uff1a\u673a\u5236\u7279\u6027\u5f62\u5f0f\u5316\u3001\u4f18\u52bf\u91cf\u5316\u6bd4\u8f83\u3001\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u53d1\u73b0VLM\u80fd\u591f\u5229\u7528\u4e30\u5bcc\u7684\u8bed\u4e49\u65b0\u9896\u6027\uff0c\u5728OOD\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u5bf9\u79f0\u7684\u9c81\u68d2\u6027\u7279\u5f81\uff1a\u5bf9\u56fe\u50cf\u566a\u58f0\u5177\u6709\u97e7\u6027\uff0c\u4f46\u5bf9\u63d0\u793a\u8bcd\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9VLM-based OOD\u68c0\u6d4b\u4f18\u52bf\u548c\u5173\u952e\u8106\u5f31\u6027\u7684\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2509.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6bd4\u8f83\u7ea7\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5404\u79cdLLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u63d0\u793a\u683c\u5f0f\u654f\u611f\uff0c\u4e14\u5728\u5904\u7406\u65e5\u8bed\u7279\u6709\u8bed\u8a00\u73b0\u8c61\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4f46\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u80fd\u5e2e\u52a9\u6a21\u578b\u89e3\u51b3\u56f0\u96be\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6d89\u53ca\u6570\u503c\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\u7684\u63a8\u7406\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6bd4\u8f83\u7ea7\u662f\u4e0e\u6b64\u7c7b\u63a8\u7406\u76f8\u5173\u7684\u5173\u952e\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46LLM\u5728\u5904\u7406\u6bd4\u8f83\u7ea7\u65b9\u9762\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u975e\u4e3b\u5bfc\u8bed\u8a00\uff08\u5982\u65e5\u8bed\uff09\u4e2d\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6bd4\u8f83\u7ea7\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u5404\u79cdLLM\u3002\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u683c\u5f0f\u7684\u5f71\u54cd\u4ee5\u53ca\u5c11\u6837\u672c\u793a\u4f8b\u4e2d\u9ec4\u91d1\u6807\u7b7e\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5bf9\u63d0\u793a\u683c\u5f0f\u654f\u611f\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u53d7\u5230\u9ec4\u91d1\u6807\u7b7e\u7684\u5f71\u54cd\u3002LLM\u5728\u5904\u7406\u65e5\u8bed\u7279\u6709\u8bed\u8a00\u73b0\u8c61\u65f6\u5b58\u5728\u56f0\u96be\u3002\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u80fd\u5e2e\u52a9\u6a21\u578b\u9884\u6d4b\u90a3\u4e9b\u5373\u4f7f\u5728\u5c11\u6837\u672c\u793a\u4f8b\u4e0b\u4e5f\u96be\u4ee5\u89e3\u51b3\u7684\u63a8\u7406\u95ee\u9898\u7684\u6b63\u786e\u6807\u7b7e\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLM\u5728\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u7ea7\u63a8\u7406\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u901a\u8fc7\u9002\u5f53\u7684\u63d0\u793a\u5de5\u7a0b\uff08\u7279\u522b\u662f\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8fd9\u4e3a\u6539\u8fdb\u591a\u8bed\u8a00NLI\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.13385", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "J\u00fcrgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u622a\u9762\u66f2\u7387\u6982\u5ff5\u7684\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u6784\u5efa\u66f2\u7387\u5256\u9762\u6765\u8bc4\u4f30\u6570\u636e\u8868\u793a\u6548\u679c\u548c\u4f30\u8ba1\u6570\u636e\u96c6\u672c\u5f81\u7ef4\u5ea6", "motivation": "\u5229\u7528\u65b0\u53d1\u5c55\u7684\u622a\u9762\u66f2\u7387\u62bd\u8c61\u6982\u5ff5\u6765\u5206\u6790\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u4e3a\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u6548\u679c\u548c\u63a2\u7d22\u7ecf\u9a8c\u7f51\u7edc\u7684\u5927\u89c4\u6a21\u51e0\u4f55\u7ed3\u6784\u63d0\u4f9b\u5b9a\u91cf\u5de5\u5177", "method": "\u57fa\u4e8e\u6355\u83b7\u70b9\u4e09\u5143\u7ec4\u4e0e\u5176\u4ed6\u70b9\u4e4b\u95f4\u5ea6\u91cf\u5173\u7cfb\u7684\u66f2\u7387\u6982\u5ff5\uff0c\u6784\u5efa\u66f2\u7387\u5256\u9762\u6765\u5206\u6790\u6570\u636e\u8868\u793a", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u672c\u5f81\u7ef4\u5ea6\uff0c\u63a2\u7d22\u7ecf\u9a8c\u7f51\u7edc\u7684\u5927\u89c4\u6a21\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u7684\u6709\u6548\u6027", "conclusion": "\u66f2\u7387\u5206\u6790\u65b9\u6cd5\u4e3a\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u7684\u51e0\u4f55\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u5728\u8bc4\u4f30\u6570\u636e\u8868\u793a\u8d28\u91cf\u548c\u7ef4\u5ea6\u4f30\u8ba1\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2509.13696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "\u4f7f\u7528DSPy\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u5c06\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e34\u5e8a\u6587\u672c\u548c\u7ed3\u6784\u5316EHR\u6570\u636e\uff0c\u6027\u80fd\u5ab2\u7f8e\u4e13\u4e1a\u591a\u6a21\u6001\u7cfb\u7edf\u4e14\u66f4\u7b80\u5355\u7075\u6d3b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u5305\u542b\u65f6\u95f4\u5e8f\u5217\u7b49\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22", "method": "\u91c7\u7528\u57fa\u4e8eDSPy\u7684\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u5bf9\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9002\u914d\uff0c\u4f7f\u5176\u80fd\u591f\u8054\u5408\u5904\u7406\u4e34\u5e8a\u7b14\u8bb0\u548c\u7ed3\u6784\u5316EHR\u8f93\u5165", "result": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u4e13\u4e1a\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u6709\u66f4\u4f4e\u7684\u590d\u6742\u5ea6\u548c\u66f4\u5f3a\u7684\u8de8\u4efb\u52a1\u9002\u5e94\u6027", "conclusion": "\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5904\u7406\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\uff0c\u4e3a\u533b\u7597AI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13388", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u9065\u611f\u6280\u672f\u5206\u6790\u659c\u8482\u6cbb\u7eb3\u8fea\u5e022013-2024\u5e74\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u4e3a\u57ce\u5e02\u5316\u76d1\u6d4b\u63d0\u4f9b\u6280\u672f\u652f\u6301", "motivation": "\u659c\u8482\u7eb3\u8fea\u4f5c\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u6b63\u7ecf\u5386\u5feb\u901f\u57ce\u5e02\u5316\uff0c\u9700\u8981\u76d1\u6d4b\u571f\u5730\u5229\u7528\u53d8\u5316\u4ee5\u652f\u6301\u57ce\u5e02\u89c4\u5212\u548c\u53d1\u5c55\u9879\u76ee", "method": "\u4f7f\u7528Landsat-8\u536b\u661f\u56fe\u50cf\uff0c\u901a\u8fc7Google Earth Engine\u548ck-means\u805a\u7c7b\u8fdb\u884c\u65e0\u76d1\u7763\u5206\u7c7b\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6709\u76d1\u7763\u5206\u7c7b\uff0c\u751f\u6210\u571f\u5730\u8986\u76d6\u56fe", "result": "\u5efa\u7acb\u4e86\u571f\u5730\u5229\u7528\u53d8\u5316\u76d1\u6d4b\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u663e\u793a\u4e86\u57ce\u5e02\u533a\u57df\u968f\u65f6\u95f4\u7684\u53d8\u5316\u8d8b\u52bf", "conclusion": "\u7814\u7a76\u4e3a\u659c\u8482\u7eb3\u8fea\u7684\u571f\u5730\u8986\u76d6\u6a21\u578b\u548c\u53d8\u5316\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u652f\u6301\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u57ce\u5e02\u53d1\u5c55\u89c4\u5212\u548c\u73af\u5883\u7ba1\u7406"}}
{"id": "2509.13702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "DSCC-HS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e3b\u52a8\u5f0f\u5e7b\u89c9\u6291\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u5b9e\u65f6\u8f6c\u5411\u5411\u91cf\u6765\u52a8\u6001\u6821\u51c6\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u65e0\u9700\u4fee\u6539\u76ee\u6807\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u662f\u53ef\u9760\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u5982RAG\u591a\u4e3a\u88ab\u52a8\u53cd\u5e94\u5f0f\uff0c\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u5e72\u9884\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u8ba4\u77e5\u7406\u8bba\uff0c\u4f7f\u7528\u7d27\u51d1\u4ee3\u7406\u6a21\u578b\u5206\u522b\u4f5c\u4e3a\u4e8b\u5b9e\u5bf9\u9f50\u4ee3\u7406(FAP)\u548c\u5e7b\u89c9\u68c0\u6d4b\u4ee3\u7406(HDP)\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8ba1\u7b97\u4e24\u8005logits\u5dee\u5f02\u751f\u6210\u5b9e\u65f6\u8f6c\u5411\u5411\u91cf\uff0c\u52a8\u6001\u5f15\u5bfc\u76ee\u6807\u6a21\u578b\u3002", "result": "\u5728TruthfulQA\u4e0a\u8fbe\u523099.2%\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u7387\uff0c\u5728BioGEN\u957f\u6587\u672c\u57fa\u51c6\u4e0a\u83b7\u5f97\u6700\u9ad8FActScore 46.50\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DSCC-HS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.13396", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "\u57fa\u4e8eYOLOv7\u5206\u5272\u548cConvNeXt\u7279\u5f81\u63d0\u53d6\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u5f02\u7269\u5165\u4fb5\u7684\u5b9e\u65f6\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u548c\u589e\u91cf\u66f4\u65b0\u3002", "motivation": "\u89e3\u51b3\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u7684\u5b9e\u65f6\u6027\u3001\u51c6\u786e\u6027\u548c\u9065\u9510\u6027\u95ee\u9898\uff0c\u9002\u5e94\u590d\u6742\u573a\u666f\u4e0b\u7684\u906e\u634f\u548c\u8fd0\u52a8\u60c5\u51b5\u3002", "method": "1\uff09YOLOv7\u5206\u5272\u6a21\u578b\u8fdb\u884c\u5feb\u901f\u7269\u4f53\u5b9a\u4f4d 2\uff09ConvNeXt\u57fa\u4e8e\u4e09\u5143\u7ec4\u635f\u5931\u8bad\u7ec3\u7684\u7279\u5f81\u63d0\u53d6\u5668 3\uff09\u7279\u5f81\u8f85\u52a9IoU\u8ddf\u8e2a\u5668\u5904\u7406\u906e\u634f\u548c\u591a\u7269\u4f53\u8ddf\u8e2a", "result": "\u5728\u771f\u5b9e\u76d1\u63a7\u548c\u65e0\u4eba\u673a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u9ad8\u51c6\u786e\u6027\u548c\u9065\u9510\u6027\uff0cNVIDIA Jetson\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7535\u529b\u4f20\u8f93\u5b89\u5168\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u5927\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "\u4f7f\u7528NLP\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u8bc6\u522b\u653e\u5c04\u6cbb\u7597\u4e2d\u9ad8\u4e25\u91cd\u5ea6\u4e8b\u6545\u62a5\u544a\u7684\u8de8\u673a\u6784\u6a21\u578b\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73", "motivation": "\u624b\u52a8\u5ba1\u67e5\u533b\u7597\u4e8b\u6545\u62a5\u544a\u8017\u65f6\u8017\u529b\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u9ad8\u6548\u7387", "method": "\u4f7f\u7528\u4e24\u4e2a\u673a\u6784\u76847,094\u4efd\u62a5\u544a\u8bad\u7ec3NLP\u6a21\u578b\uff0c\u5305\u62ecSVM\u548cBlueBERT\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u8de8\u673a\u6784\u8f6c\u79fb\u5b66\u4e60\u6280\u672f", "result": "\u8de8\u673a\u6784\u8f6c\u79fb\u5b66\u4e60\u540e\u7684BlueBERT\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230AUROC 0.78\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u7684AUROC 0.81", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u80fd\u591f\u8de8\u673a\u6784\u8bc6\u522b\u9ad8\u4e25\u91cd\u5ea6\u4e8b\u6545\u62a5\u544a\u7684NLP\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u4f3c"}}
{"id": "2509.13399", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "EdiVal-Agent\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u591a\u8f6e\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89d2\u548c\u4e13\u5bb6\u5de5\u5177\u5957\u4ef6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(i)\u4f9d\u8d56\u914d\u5bf9\u53c2\u8003\u56fe\u50cf\u5bfc\u81f4\u8986\u76d6\u8303\u56f4\u6709\u9650\u4e14\u7ee7\u627f\u751f\u6210\u6a21\u578b\u504f\u89c1\uff0c(ii)\u4ec5\u4f9d\u8d56\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5f80\u5f80\u4e0d\u7cbe\u786e\u3002\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "EdiVal-Agent\u9996\u5148\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u8bed\u4e49\u5bf9\u8c61\uff0c\u7136\u540e\u5408\u6210\u591a\u6837\u5316\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7f16\u8f91\u6307\u4ee4\u3002\u8bc4\u4f30\u65f6\u7ed3\u5408VLM\u4e0e\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u5668\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\uff0c\u4f7f\u7528\u8bed\u4e49\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u8bc4\u4f30\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u5224\u65ad\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c06VLM\u4e0e\u5bf9\u8c61\u68c0\u6d4b\u5668\u7ed3\u5408\u4f7f\u7528\u5728\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u4e2d\u6bd4\u5355\u72ec\u4f7f\u7528VLM\u548c\u57fa\u4e8eCLIP\u7684\u6307\u6807\u66f4\u80fd\u4e0e\u4eba\u7c7b\u5224\u65ad\u8fbe\u6210\u4e00\u81f4\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u672a\u6765\u5de5\u5177\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "EdiVal-Agent\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u73b0\u6709\u7f16\u8f91\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f16\u8f91\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4fe1\u606f\uff0c\u5e76\u5efa\u7acb\u4e86EdiVal-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d69\u79cd\u6307\u4ee4\u7c7b\u578b\u548c11\u79cd\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u6a21\u578b\u3002"}}
{"id": "2509.13723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "\u57fa\u4e8e\u8fc7\u6ee4\u548c\u526a\u679d\u7684\u4e24\u9636\u6bb5\u65e0\u8bad\u7ec3\u63d0\u793a\u538b\u7f29\u65b9\u6cd5DSPC\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u7684\u524d\u63d0\u4e0b\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u8fc5\u901f\u589e\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u591a\u9700\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u53cc\u9636\u6bb5\u6e10\u8fdb\u5f0f\u538b\u7f29\u65b9\u6cd5DSPC\uff1a\u7c97\u7c92\u5ea6\u9636\u6bb5\u4f7f\u7528TF-IDF\u8fc7\u6ee4\u4f4e\u8bed\u4e49\u503c\u53e5\u5b50\uff1b\u7ec6\u7c92\u5ea6\u9636\u6bb5\u7edf\u8003\u5173\u6ce8\u8d21\u732e\u5ea6\u3001\u8de8\u6a21\u578b\u635f\u5931\u5dee\u5f02\u548c\u4f4d\u7f6e\u91cd\u8981\u6027\u6765\u526a\u679d\u4f4e\u6548\u7528token\u3002", "result": "\u5728LLaMA-3.1-8B-Instruct\u548cGPT-3.5-Turbo\u4e0a\u9a8c\u8bc1\uff0c\u5728\u4ec5\u4f7f\u75283\u500d\u66f4\u5c11token\u7684\u60c5\u51b5\u4e0b\uff0cLongbench\u6570\u636e\u96c6FewShot\u4efb\u52a1\u4e2d\u8fbe\u523049.17\u6027\u80fd\uff0c\u8d85\u8fc7\u6700\u4f73\u73b0\u6709\u65b9\u6cd5LongLLMLingua 7.76\u3002", "conclusion": "DSPC\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u3001\u6548\u679c\u663e\u8457\uff0c\u80fd\u5728\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u4e3a\u63d0\u793a\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8eTransformer\u7684\u524d\u9988\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u5355\u5f20\u6216\u591a\u5f20\u56fe\u50cf\u4ee5\u53ca\u53ef\u9009\u51e0\u4f55\u8f93\u5165\uff0c\u76f4\u63a5\u56de\u5f52\u5ea6\u91cf3D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\uff0c\u5728\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33D\u89c6\u89c9\u4efb\u52a1\u4e2d\u9700\u8981\u4e13\u95e8\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\u6765\u5904\u7406\u591a\u79cd3D\u91cd\u5efa\u4efb\u52a1\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u524d\u9988\u67b6\u6784\uff0c\u8f93\u5165\u56fe\u50cf\u548c\u53ef\u9009\u51e0\u4f55\u4fe1\u606f\uff0c\u91c7\u7528\u5206\u89e3\u7684\u591a\u89c6\u89d2\u573a\u666f\u51e0\u4f55\u8868\u793a\uff08\u6df1\u5ea6\u56fe\u3001\u5c40\u90e8\u5c04\u7ebf\u56fe\u3001\u76f8\u673a\u4f4d\u59ff\u548c\u5ea6\u91cf\u5c3a\u5ea6\u56e0\u5b50\uff09\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u76d1\u7763\u548c\u7075\u6d3b\u8f93\u5165\u589e\u5f3a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a3D\u89c6\u89c9\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u95e8\u7684\u524d\u9988\u6a21\u578b\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u8054\u5408\u8bad\u7ec3\u6027\u80fd\uff0c\u5305\u62ec\u672a\u6807\u5b9aSfM\u3001\u6807\u5b9a\u591a\u89c6\u89d2\u7acb\u4f53\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u76f8\u673a\u5b9a\u4f4d\u7b49\u4efb\u52a1\u3002", "conclusion": "MapAnything\u4e3a\u901a\u75283D\u91cd\u5efa\u4e3b\u5e72\u7f51\u7edc\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u7edf\u4e00\u6a21\u578b\u5728\u591a\u6837\u53163D\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u7684\u65e5\u8bed\u6bd4\u8f83\u53e5\u903b\u8f91\u63a8\u7406\u7cfb\u7edfccg-jcomp\uff0c\u4ee5\u89e3\u51b3\u65e5\u8bed\u6bd4\u8f83\u53e5\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u65e5\u8bed\u548c\u82f1\u8bed\u6bd4\u8f83\u53e5\u5728\u8bed\u6cd5\u548c\u8bed\u4e49\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f7f\u5f97\u73b0\u6709\u7684\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u65e5\u8bed\u6bd4\u8f83\u53e5\u3002\u9700\u8981\u4e13\u95e8\u4e3a\u65e5\u8bed\u8bbe\u8ba1\u4e00\u79cd\u7a33\u5065\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u5b66\u7684\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\uff0c\u4e13\u95e8\u4e3a\u65e5\u8bed\u6bd4\u8f83\u53e5\u8bbe\u8ba1\u3002\u901a\u8fc7\u5bf9\u6bd4\u8bed\u4e49\u8868\u8fbe\u548c\u6570\u91cf\u5173\u7cfb\u7684\u6df1\u5ea6\u7406\u89e3\u6765\u5b9e\u73b0\u63a8\u7406\u3002", "result": "\u5728\u65e5\u8bed\u6bd4\u8f83\u53e5NLI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u51c6\u786e\u7387\u5bf9\u6bd4\u3002\u7ed3\u679c\u663e\u793a\u8be5\u7cfb\u7edf\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ec4\u5408\u8bed\u4e49\u65b9\u6cd5\u6784\u5efa\u7684\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\uff0c\u4e3a\u65e5\u8bed\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "\u63d0\u51faSCM-PR\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408RGB\u56fe\u50cf\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u63d0\u5347\u5728LiDAR\u5730\u56fe\u4e2d\u7684\u8de8\u6a21\u6001\u5b9a\u4f4d\u6027\u80fd\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u3001\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709RGB-based\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u5bf9\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u53d8\u5316\u654f\u611f\uff0c\u800c\u73b0\u6709\u8de8\u6a21\u6001\u5b9a\u4f4d\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u3001\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528VMamba\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6RGB\u56fe\u50cf\u7279\u5f81\uff1b\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u6a21\u5757(SAFF)\u7ed3\u5408\u5730\u70b9\u63cf\u8ff0\u7b26\u548c\u5206\u5272\u63a9\u7801\uff1b\u8bbe\u8ba1\u5305\u542b\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u7684LiDAR\u63cf\u8ff0\u7b26\uff1b\u5728NetVLAD\u4e2d\u5f15\u5165\u8de8\u6a21\u6001\u8bed\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff1b\u5728\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\u8bbe\u8ba1\u591a\u89c6\u89d2\u8bed\u4e49-\u51e0\u4f55\u5339\u914d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728KITTI\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCM-PR\u76f8\u6bd4\u5176\u4ed6\u8de8\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff0cSCM-PR\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u548c\u89c6\u89d2\u53d8\u5316\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2509.13775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u7684\u6570\u636e\u6548\u7387\u548c\u53c2\u6570\u6548\u7387\u65b9\u6cd5\uff0c\u5305\u62ec\u8f6f\u63d0\u793a\u7b56\u7565\u548cLoRA\u91cd\u53c2\u6570\u5316\uff0c\u53d1\u73b0LoRA\u7cbe\u8c03\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u6548\u7387\u548c\u53c2\u6570\u6548\u7387\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65b9\u8a00\u533a\u5206\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u79cd\u8f6f\u63d0\u793a\u7b56\u7565\uff08\u524d\u7f00\u8c03\u6574\u3001\u63d0\u793a\u8c03\u6574\u3001P\u8c03\u6574\u7b49\uff09\u548cLoRA\u91cd\u53c2\u6570\u5316\uff0c\u5728\u963f\u62c9\u4f2f\u7279\u5b9a\u7f16\u7801\u5668\u6a21\u578b\u548c\u89e3\u7801\u5668\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u5c0f\u6837\u672c\u63a8\u7406\u4e2d\u533a\u5206\u65b9\u8a00\u7ec6\u5fae\u5dee\u5f02\u80fd\u529b\u4e0d\u4f73\uff0c\u8f6f\u63d0\u793a\u7f16\u7801\u5668\u8868\u73b0\u66f4\u597d\uff0cLoRA\u7cbe\u8c03\u6a21\u578b\u8868\u73b0\u6700\u597d\u4e14\u8d85\u8d8a\u5168\u91cf\u7cbe\u8c03\u3002", "conclusion": "LoRA\u57fa\u4e8e\u7684\u53c2\u6570\u6548\u7387\u7cbe\u8c03\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5bf9\u65b9\u8a00\u7ec6\u5fae\u5dee\u5f02\u7684\u533a\u5206\u80fd\u529b\u6709\u9650\u3002"}}
{"id": "2509.13482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "\u63d0\u51fa\u573a\u666f\u81ea\u9002\u5e94\u683c\u70b9\u5411\u91cf\u91cf\u5316(SALVQ)\u65b9\u6cd5\u66ff\u4ee3\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u6027\u80fd\uff0c\u652f\u6301\u591a\u7801\u7387\u76ee\u6807\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3", "motivation": "3DGS\u751f\u6210\u5927\u91cf\u6570\u636e\u9700\u8981\u538b\u7f29\uff0c\u73b0\u6709\u57fa\u4e8e\u951a\u70b9\u7684\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u90fd\u4f7f\u7528\u7b80\u5355\u7684\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u5148\u8fdb\u7684\u91cf\u5316\u5668\u6765\u63d0\u5347\u538b\u7f29\u6027\u80fd", "method": "\u4f7f\u7528\u683c\u70b9\u5411\u91cf\u91cf\u5316(LVQ)\u66ff\u4ee3\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u573a\u666f\u4f18\u5316\u683c\u70b9\u57fa\u5411\u91cf\uff0c\u5b9e\u73b0\u573a\u666f\u81ea\u9002\u5e94LVQ(SALVQ)\uff0c\u901a\u8fc7\u7f29\u653e\u683c\u70b9\u57fa\u5411\u91cf\u52a8\u6001\u8c03\u6574\u683c\u70b9\u5bc6\u5ea6", "result": "SALVQ\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7387\u5931\u771f\u6548\u7387\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u67093DGS\u538b\u7f29\u67b6\u6784\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017", "conclusion": "SALVQ\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u5411\u91cf\u91cf\u5316\u7684\u7387\u5931\u771f\u6548\u7387\u548c\u5747\u5300\u6807\u91cf\u91cf\u5316\u7684\u4f4e\u590d\u6742\u5ea6\uff0c\u4e3a3DGS\u538b\u7f29\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "CAMPUS\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5b50\u8bfe\u7a0b\u3001\u80fd\u529b\u611f\u77e5\u7684\u8bfe\u7a0b\u8fdb\u5ea6\u8c03\u6574\u548c\u591a\u7ef4\u5ea6\u96be\u5ea6\u8c03\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u4e2d\u8bfe\u7a0b\u521a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u8c03\u4f18\u6548\u679c", "motivation": "\u5f53\u524d\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u96be\u5ea6\u6307\u6807\uff0c\u5b58\u5728\u8bfe\u7a0b\u521a\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u6f14\u8fdb\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u56fa\u5b9a\u4e14\u53ef\u80fd\u6b21\u4f18\u7684\u5b66\u4e60\u8f68\u8ff9", "method": "\u63d0\u51faCAMPUS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u4f18\u52bf\uff1a1\uff09\u52a8\u6001\u9009\u62e9\u5b50\u8bfe\u7a0b\uff1b2\uff09\u57fa\u4e8e\u80fd\u529b\u611f\u77e5\u8c03\u6574\u8bfe\u7a0b\u8fdb\u5ea6\uff1b3\uff09\u591a\u7ef4\u5ea6\u96be\u5ea6\u8c03\u5ea6\u7b56\u7565", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eCAMPUS\u5728\u9ad8\u6548\u6307\u4ee4\u8c03\u4f18\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "CAMPUS\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u6a21\u578b\u80fd\u529b\u6f14\u8fdb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bfe\u7a0b\u521a\u6027\u95ee\u9898\uff0c\u4e3a\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5b66\u4e60\u8f68\u8ff9"}}
{"id": "2509.13484", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86MINGLE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7ba1\u9053\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\uff0c\u5305\u62ec\u4eba\u4f53\u68c0\u6d4b\u3001VLM\u793e\u4ea4\u5173\u7cfb\u5206\u7c7b\u548c\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b10\u4e07\u5f20\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6", "motivation": "\u7406\u89e3\u516c\u5171\u573a\u6240\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u5bf9\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7269\u4f53\u68c0\u6d4b\u6765\u89e3\u8bfb\u590d\u6742\u7684\u793e\u4f1a\u4fe1\u53f7", "method": "\u4e09\u9636\u6bb5\u6a21\u5757\u5316\u7ba1\u9053\uff1a1)\u73b0\u6210\u7684\u4eba\u4f53\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1 2)VLM\u57fa\u7840\u7684\u6210\u5bf9\u793e\u4ea4\u5173\u7cfb\u5206\u7c7b 3)\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\u5b9a\u4f4d\u793e\u4ea4\u8fde\u63a5\u7fa4\u4f53", "result": "\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u5f20\u57ce\u5e02\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e2a\u4eba\u548c\u793e\u4ea4\u7fa4\u4f53\u7684\u8fb9\u754c\u6846\u548c\u6807\u7b7e\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548cMINGLE\u8f93\u51fa", "conclusion": "\u63d0\u51fa\u4e86\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\u68c0\u6d4b\u65b0\u4efb\u52a1\u548cMINGLE\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u57fa\u7840"}}
{"id": "2509.13803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura Garc\u00eda-Sardi\u00f1a", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u8bed\u6cd5\u6027\u522b\u5728\u804c\u4e1a\u540d\u79f0\u4e2d\u7684\u663e\u5f0f\u5206\u914d\u5982\u4f55\u5f71\u54cd\u81ea\u52a8\u804c\u4e1a\u6392\u540d\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u7528RBO\u6307\u6807\u8bc4\u4f30\u6027\u522b\u504f\u89c1\uff0c\u5e76\u57284\u79cd\u8bed\u8a00\u4e2d\u521b\u5efa\u4e86\u5305\u542b\u9633\u6027/\u9634\u6027\u5f62\u5f0f\u804c\u4e1a\u540d\u79f0\u7684\u6d4b\u8bd5\u96c6\u3002", "motivation": "\u7814\u7a76\u8bed\u6cd5\u6027\u522b\u5728\u804c\u4e1a\u540d\u79f0\u4e2d\u7684\u663e\u5f0f\u5206\u914d\u5bf9\u81ea\u52a8\u804c\u4e1a\u6392\u540d\u7cfb\u7edf\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u5728\u804c\u4e1a\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528RBO(Rank-Biased Overlap)\u6307\u6807\u6765\u8bc4\u4f30\u804c\u4e1a\u540d\u79f0\u6392\u540d\u7cfb\u7edf\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u57284\u79cd\u5177\u6709\u8bed\u6cd5\u6027\u522b\u7684\u8bed\u8a00\u4e2d\u521b\u5efa\u5305\u542b\u9633\u6027\u548c\u9634\u6027\u5f62\u5f0f\u804c\u4e1a\u540d\u79f0\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u8bc4\u4f30\u591a\u4e2a\u5916\u90e8\u591a\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684\u5916\u90e8\u591a\u8bed\u8a00\u6a21\u578b\u90fd\u4e0d\u540c\u7a0b\u5ea6\u5730\u5c55\u73b0\u51fa\u6027\u522b\u504f\u89c1\uff0c\u8bc1\u660e\u4e86\u8bed\u6cd5\u6027\u522b\u5bf9\u81ea\u52a8\u804c\u4e1a\u6392\u540d\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u804c\u4e1a\u540d\u79f0\u6392\u540d\u7cfb\u7edf\u4e2d\u7684\u6027\u522b\u504f\u89c1\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5e76\u8bc1\u5b9e\u4e86\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u95ee\u9898\u4e0a\u7684\u5b58\u5728\u95ee\u9898\uff0c\u4e3a\u4ee5\u540e\u7684\u504f\u89c1\u6d88\u9664\u5de5\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13496", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "BiasMap\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u6982\u5ff5\u7ea7\u8868\u5f81\u504f\u89c1\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u63ed\u793a\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4e0e\u8bed\u4e49\u6982\u5ff5\u7684\u7ed3\u6784\u6027\u7ea0\u7f20\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\u7684\u4eba\u53e3\u7edf\u8ba1\u5206\u5e03\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u504f\u89c1\u7f13\u89e3\u540e\u6982\u5ff5\u8868\u5f81\u7684\u89e3\u8026\u3002\u9700\u8981\u66f4\u6df1\u5165\u5730\u63ed\u793a\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8868\u5f81\u504f\u89c1\u3002", "method": "\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u91cf\u5316\u4eba\u53e3\u7edf\u8ba1-\u8bed\u4e49\u6982\u5ff5\u7684\u7a7a\u95f4\u7ea0\u7f20\uff08\u901a\u8fc7IoU\u6307\u6807\uff09\uff0c\u5e76\u91c7\u7528\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4fee\u6539\u6f5c\u5728\u566a\u58f0\u7a7a\u95f4\u4ee5\u6700\u5c0f\u5316SoftIoU\u671f\u671b\u503c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u516c\u5e73\u6027\u5e72\u9884\u53ef\u80fd\u51cf\u5c11\u8f93\u51fa\u5206\u5e03\u5dee\u8ddd\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e3\u8026\u6982\u5ff5\u7ea7\u8026\u5408\uff0c\u800cBiasMap\u7684\u7f13\u89e3\u65b9\u6cd5\u80fd\u591f\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u51cf\u8f7b\u6982\u5ff5\u7ea0\u7f20\uff0c\u540c\u65f6\u8865\u5145\u5206\u5e03\u504f\u89c1\u7f13\u89e3\u3002", "conclusion": "BiasMap\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d1\u73b0\u548c\u7f13\u89e3\u6f5c\u5728\u6982\u5ff5\u7ea7\u8868\u5f81\u504f\u89c1\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u63ed\u793a\u73b0\u6709\u516c\u5e73\u6027\u53d1\u73b0\u65b9\u6cd5\u4e2d\u9690\u85cf\u7684\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u64cd\u4f5c\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u504f\u89c1\u89e3\u8026\u3002"}}
{"id": "2509.13813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "\u901a\u8fc7\u51e0\u4f55\u6846\u67b6\u63d0\u51fa\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ed1\u76d2\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u7684\u95ee\u9898", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u4f46\u73b0\u6709\u9ed1\u76d2\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1", "method": "\u57fa\u4e8e\u54cd\u5e94\u5d4c\u5165\u7684\u539f\u578b\u5206\u6790\uff0c\u63d0\u51faGeometric Volume\uff08\u51e0\u4f55\u4f53\u79ef\uff09\u6d4b\u91cf\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\uff0cGeometric Suspicion\uff08\u51e0\u4f55\u7591\u60d1\uff09\u6392\u5e8f\u5e94\u54cd\u53ef\u9760\u6027", "result": "\u5728\u77ed\u5f62\u5f0f\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7c7b\u4f3c\u6216\u66f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u66f4\u4f18\u5f02\u7684\u7ed3\u679c", "conclusion": "\u8be5\u51e0\u4f55\u6846\u67b6\u4e3a\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u4e86\u5408\u6210\u7684\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u68c0\u6d4b\u5e7b\u89c9\u73b0\u8c61"}}
{"id": "2509.13504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hern\u00e1ndez"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "LivePixel\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u8fde\u63a5\u6210\u50cf\u8bbe\u5907\u8fdb\u884c\u5b9e\u65f6\u6807\u6ce8\uff0c\u652f\u6301\u8d1d\u585e\u5c14\u66f2\u7ebf\u548c\u4e8c\u8fdb\u5236\u63a9\u7801\u7b49\u4e13\u4e1a\u6807\u6ce8\u529f\u80fd", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\u9700\u8981\u5148\u4e0a\u4f20\u9884\u6536\u96c6\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u652f\u6301\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\u548c\u6309\u9700\u6807\u6ce8\uff0c\u8fd9\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u65f6\u4eea\u5668\u6570\u636e\u91c7\u96c6\u573a\u666f\u4e2d\u5c24\u4e3a\u4e0d\u4fbf", "method": "\u5f00\u53d1Python\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u96c6\u6210OpenCV\u548cNumpy\u7b49\u9ad8\u6027\u80fd\u5e93\uff0c\u652f\u6301\u591a\u79cd\u89c6\u9891\u8bbe\u5907\u8fde\u63a5\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u5546\u4e1a\u56fe\u5f62\u8f6f\u4ef6\u7684\u6807\u6ce8\u5de5\u5177\u548c\u975e\u7834\u574f\u6027\u56fe\u5c42\u7f16\u8f91", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u529f\u80fd\uff0c\u652f\u6301\u8d1d\u585e\u5c14\u66f2\u7ebf\u3001\u4e8c\u8fdb\u5236\u63a9\u7801\u7b49\u4e13\u4e1a\u6807\u6ce8\u5de5\u5177\uff0c\u4e0e\u663e\u5fae\u955c\u3001\u7f51\u7edc\u6444\u50cf\u5934\u7b49\u6210\u50cf\u8bbe\u5907\u65e0\u7f1d\u96c6\u6210", "conclusion": "LivePixel\u89e3\u51b3\u4e86\u79d1\u5b66\u9886\u57dfAI\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u6807\u6ce8\u5de5\u5177\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u6807\u6ce8\u52a0\u901f\u5b9e\u9a8c\u5de5\u4f5c\u6d41\u4e2d\u7684AI\u6a21\u578b\u5f00\u53d1"}}
{"id": "2509.13814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "AutoMin 2025\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u542b\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u95ee\u7b54\u4e24\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u82f1\u8bed\u548c\u6377\u514b\u8bed\uff0c\u53c2\u4e0e\u56e2\u961f\u8f83\u5c11\u4f46\u5305\u542b\u591a\u4e2aLLM\u57fa\u7ebf\u7cfb\u7edf\u8bc4\u4f30", "motivation": "\u63a8\u8fdb\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u6280\u672f\uff0c\u63a2\u7d22\u7ed3\u6784\u5316\u4f1a\u8bae\u8bb0\u5f55\u548c\u57fa\u4e8e\u4f1a\u8bae\u8f6c\u5f55\u7684\u95ee\u7b54\u80fd\u529b\uff0c\u8bc4\u4f30\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0", "method": "\u7ec4\u7ec7\u5171\u4eab\u4efb\u52a1\uff0c\u8bbe\u7f6e\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\uff08\u6db5\u76d6\u82f1\u8bed\u548c\u6377\u514b\u8bed\u7684\u9879\u76ee\u4f1a\u8bae\u548c\u6b27\u6d32\u8bae\u4f1a\u4f1a\u8bae\uff09\u548c\u95ee\u7b54\u4efb\u52a1\uff08\u5355\u8bed\u82f1\u8bed\u95ee\u7b54\u548c\u8de8\u8bed\u8a00\u6377\u514b\u8bed\u95ee\u7b54\uff09\u3002\u5305\u542b\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30", "result": "2025\u5e74\u53c2\u4e0e\u5ea6\u8f83\u4f4e\uff0c\u53ea\u67091\u4e2a\u56e2\u961f\u53c2\u52a0\u7eaa\u8981\u751f\u6210\u4efb\u52a1\uff0c2\u4e2a\u56e2\u961f\u53c2\u52a0\u95ee\u7b54\u4efb\u52a1\u3002\u4f46\u901a\u8fc7\u7ec4\u7ec7\u8005\u63d0\u4f9b\u7684\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30", "conclusion": "\u5c3d\u7ba1\u53c2\u4e0e\u56e2\u961f\u6709\u9650\uff0c\u4f46AutoMin 2025\u6210\u529f\u8bc4\u4f30\u4e86\u5f53\u524dLLM\u5728\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6"}}
{"id": "2509.13506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "\u63d0\u51faDEFT-VTON\u65b9\u6cd5\uff0c\u901a\u8fc7Doob's h-transform\u9ad8\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u4ec5\u8bad\u7ec31.42%\u53c2\u6570\u5b9e\u73b0\u865a\u62df\u8bd5\u7a7f\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\u5c06\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\u81f315\u6b65\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u9700\u8981\u6709\u9650\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8d44\u6e90\u9884\u7b97\u3002\u9700\u8981\u5f00\u53d1\u53c2\u6570\u9ad8\u6548\u4e14\u63a8\u7406\u5feb\u901f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528DEFT\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\uff0c\u8bad\u7ec3\u5c0f\u578bh-transform\u7f51\u7edc\u5b66\u4e60\u6761\u4ef6\u53d8\u6362\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\uff0c\u7ed3\u5408\u4e00\u81f4\u6027\u635f\u5931\u548c\u53bb\u566a\u5206\u6570\u5339\u914d\u635f\u5931\u8fdb\u884c\u4f4e\u6210\u672c\u5fae\u8c03\u3002", "result": "DEFT-VTON\u5728\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ec5\u970015\u4e2a\u53bb\u566a\u6b65\u9aa4\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a\u4f20\u7edfPEFT\u65b9\u6cd5\u76841.42%\uff08\u5bf9\u6bd45.52%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u865a\u62df\u8bd5\u7a7f\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u548c\u5feb\u901f\u63a8\u7406\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5fb7\u56fd\u65b9\u8a00\u8bed\u8005\u7684\u504f\u89c1\uff0c\u53d1\u73b0LLMs\u5728\u8054\u60f3\u4efb\u52a1\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u90fd\u5b58\u5728\u663e\u8457\u7684\u65b9\u8a00\u540d\u79f0\u548c\u4f7f\u7528\u504f\u89c1\uff0c\u4e14\u660e\u786e\u6807\u6ce8\u8bed\u8a00\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4f1a\u66f4\u5927\u7a0b\u5ea6\u653e\u5927\u504f\u89c1\u3002", "motivation": "\u65b9\u8a00\u4f5c\u4e3a\u4eba\u7c7b\u6587\u5316\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u8bed\u8a00\u8005\u5374\u9762\u4e34\u8d1f\u9762\u793e\u4f1a\u5b9a\u578b\u3002\u7814\u7a76\u8003\u5bdf\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u65b9\u8a00\u504f\u89c1\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u793e\u4f1a\u8bed\u8a00\u5b66\u6587\u732e\u6784\u5efa\u65b9\u8a00\u8bed\u8005\u7279\u5f81\u5206\u6790\uff0c\u901a\u8fc7\u8054\u60f3\u4efb\u52a1\u548c\u51b3\u7b56\u4efb\u52a1\u8bc4\u4f30LLMs\u7684\u65b9\u8a00\u540d\u79f0\u504f\u89c1\u548c\u65b9\u8a00\u4f7f\u7528\u504f\u89c1\uff0c\u6784\u5efa\u4e86\u5305\u542b\u4e03\u79cd\u5fb7\u56fd\u5730\u533a\u65b9\u8a00\u7684\u65b0\u9898\u8bc4\u4f30\u8bed\u6599\u5e93\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u7684LLMs\u90fd\u663e\u793a\u51fa\u663e\u8457\u7684\u65b9\u8a00\u540d\u79f0\u548c\u4f7f\u7528\u504f\u89c1\uff08\u8d1f\u9762\u5f62\u5bb9\u8bcd\u8054\u60f3\uff09\uff0c\u6240\u6709\u6a21\u578b\u90fd\u5728\u51b3\u7b56\u4e2d\u590d\u73b0\u4e86\u8fd9\u4e9b\u504f\u89c1\uff0c\u660e\u786e\u6807\u6ce8\u8bed\u8a00\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u6bd4\u9690\u542b\u7ebf\u7d22\u66f4\u5927\u7a0b\u5ea6\u653e\u5927\u504f\u89c1\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5bf9\u5fb7\u56fd\u65b9\u8a00\u8bed\u8005\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u660e\u786e\u6807\u6ce8\u8bed\u8a00\u4eba\u53e3\u7279\u5f81\u65f6\u4f1a\u66f4\u52a0\u4e25\u91cd\uff0c\u5448\u73b0\u51fa\u4e0e\u793e\u4f1a\u5b9a\u578b\u76f8\u4f3c\u7684\u8d1f\u9762\u504f\u89c1\u6a21\u5f0f\u3002"}}
{"id": "2509.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u548c\u5bf9\u6297\u5b66\u4e60\u751f\u6210\u5408\u6210\u4ea4\u901a\u573a\u666f\uff0c\u63d0\u9ad8\u884c\u4eba\u8bc6\u522b\u6027\u80fd", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981\u5408\u6210\u6570\u636e\u8986\u76d6\u7279\u5b9a\u4ea4\u901a\u573a\u666f\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u9700\u8981\u63d0\u9ad8\u884c\u4eba\u8bc6\u522b\u7684\u51c6\u786e\u6027", "method": "\u5f00\u53d1\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u865a\u62df\u884c\u4eba\uff1b\u63d0\u51fa\u65b0\u9896\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\uff0c\u5b66\u4e60\u6570\u636e\u96c6\u5149\u7167\u6761\u4ef6\u4ee5\u63d0\u9ad8\u589e\u5f3a\u771f\u5b9e\u6027", "result": "\u5728\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5bf9\u6297\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u5408\u6210\u4ea4\u901a\u573a\u666f\uff0c\u6709\u6548\u6539\u5584\u884c\u4eba\u8bc6\u522b\u6027\u80fd"}}
{"id": "2509.13869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u504f\u89c1\u65b9\u9762\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5b58\u5728\u9519\u4f4d\uff0c\u6a21\u578b\u89c4\u6a21\u5927\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5728\u7279\u5b9a\u573a\u666f\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u5bf9\u9f50\u504f\u597d\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u504f\u89c1\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u7279\u522b\u662f\u8d1f\u9762\u4e0e\u975e\u8d1f\u9762\u95ee\u9898\u573a\u666f\u4e0b\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u6a21\u578b\u5bf9\u793e\u4ea4\u504f\u89c1\u4ef7\u503c\u89c2\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5bf94\u4e2a\u6a21\u578b\u5bb6\u65cf\u768412\u4e2aLLM\u8fdb\u884c\u5e7f\u6cdb\u5206\u6790\uff0c\u4f7f\u75284\u4e2a\u6570\u636e\u96c6\u8bc4\u4f30\u6a21\u578b\u5728\u504f\u89c1\u573a\u666f\u4e2d\u7684\u5bf9\u9f50\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u6a21\u578b\u5bf9HVSB\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u5fae\u8c03\u8d4b\u4e88\u5c0f\u6a21\u578b\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u5927\u53c2\u6570\u89c4\u6a21\u7684LLM\u4e0d\u4e00\u5b9a\u5177\u6709\u66f4\u4f4e\u7684\u9519\u4f4d\u7387\u548c\u653b\u51fb\u6210\u529f\u7387\uff1b\u6a21\u578b\u5bf9\u7279\u5b9a\u7c7b\u578b\u573a\u666f\u8868\u73b0\u51fa\u5bf9\u9f50\u504f\u597d\uff1b\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u7684\u6a21\u578b\u5224\u65ad\u4e00\u81f4\u6027\u66f4\u9ad8\uff1b\u4e0d\u540cLLM\u5bf9HVSB\u7684\u7406\u89e3\u65e0\u663e\u8457\u5dee\u5f02\uff1b\u6a21\u578b\u504f\u597d\u81ea\u8eab\u751f\u6210\u7684\u89e3\u91ca\uff1b\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u751f\u6210\u66f4\u6613\u8bfb\u4f46\u6a21\u578b\u8ba4\u540c\u5ea6\u8f83\u4f4e\u7684\u89e3\u91ca\u3002", "conclusion": "LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u6548\u679c\u53d7\u573a\u666f\u7c7b\u578b\u5f71\u54cd\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7c7b\u578b\u8fdb\u884c\u4e13\u95e8\u7684\u5bf9\u9f50\u4f18\u5316\uff0c\u540c\u65f6\u5c0f\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u83b7\u5f97\u89e3\u91ca\u80fd\u529b\u4f46\u9700\u8981\u63d0\u5347\u6a21\u578b\u8ba4\u540c\u5ea6\u3002"}}
{"id": "2509.13508", "categories": ["cs.CV", "I.4.3; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86FunKAN\u7f51\u7edc\uff0c\u4e00\u79cd\u4e13\u95e8\u4e3a\u56fe\u50cf\u5904\u7406\u8bbe\u8ba1\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u67b6\u6784\u590d\u6742\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u800cKolmogorov-Arnold\u7f51\u7edc\u867d\u7136\u53ef\u89e3\u91ca\u4f46\u4f1a\u7834\u574f\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u7279\u5f81", "method": "\u57fa\u4e8e\u51fd\u6570\u7a7a\u95f4\u63a8\u5e7fKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\uff0c\u4f7f\u7528Hermite\u51fd\u6570\u7684\u5085\u91cc\u53f6\u5206\u89e3\u5b66\u4e60\u5185\u90e8\u51fd\u6570\uff0c\u63d0\u51fa\u4e86FunKAN\u548cU-FunKAN\u6a21\u578b", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5305\u62ecIXI\uff08MRI\u53bb\u4f2a\u5f71\uff09\u3001BUSI\uff08\u4e73\u817a\u764c\u68c0\u6d4b\uff09\u3001GlaS\uff08\u817a\u4f53\u5206\u5272\uff09\u3001CVC-ClinicDB\uff08\u606f\u8089\u68c0\u6d4b\uff09\uff0c\u5728PSNR\u3001TV\u3001IoU\u3001F1\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6KAN\u65b9\u6cd5", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u7406\u8bba\u51fd\u6570\u903c\u8fd1\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "CER\u6846\u67b6\u7ed3\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u9886\u57df\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\uff08\u5982\u75ab\u82d7\u72b9\u8c6b\u548c\u672a\u7ecf\u8bc1\u5b9e\u7684\u6cbb\u7597\u65b9\u6cd5\uff09\u5bf9\u516c\u5171\u536b\u751f\u548c\u533b\u7597\u7cfb\u7edf\u4fe1\u4efb\u6784\u6210\u98ce\u9669\uff0c\u800c\u751f\u7269\u533b\u5b66\u58f0\u660e\u9a8c\u8bc1\u56e0\u590d\u6742\u672f\u8bed\u3001\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u79d1\u5b66\u8bc1\u636e\u57fa\u7840\u9700\u6c42\u800c\u5177\u6709\u72ec\u7279\u6311\u6218\u6027\u3002", "method": "CER\u6846\u67b6\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u53ca\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u4e0e\u9ad8\u8d28\u91cf\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\u7684\u5148\u8fdb\u68c0\u7d22\u6280\u672f\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u901a\u8fc7\u5c06\u751f\u6210\u8f93\u51fa\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u6765\u6e90\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u5e7b\u89c9\u98ce\u9669\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2509.13515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5b9e\u4f8b\u56fe\u548c\u8865\u5145\u6743\u91cd\u56fe\u6765\u7cbe\u786e\u68c0\u6d4b\u5077\u6076\u89c6\u9891\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8f7b\u5fae\u7684\u5077\u6076\u5185\u5bb9\u4e5f\u80fd\u5b9a\u4e49\u89c6\u9891\u7c7b\u522b\uff0c\u4e14\u65e0\u6cd5\u7cfb\u7edf\u5730\u6293\u53d6\u89c6\u9891\u4e2d\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u591a\u4e2a\u5b9e\u4f8b\u6765\u6784\u5efa\u5b9e\u4f8b\u56fe\u63d0\u53d6\u5b9e\u4f8b\u7ea7\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u8865\u5145\u6743\u91cd\u56fe\u4e3a\u8fd9\u4e9b\u7279\u5f81\u8d4b\u4e88\u91cd\u8981\u6027\u6743\u91cd\uff0c\u5f3a\u8c03\u5077\u6076\u5b9e\u4f8b\uff0c\u6700\u540e\u7ed3\u5408\u6743\u91cd\u548c\u7279\u5f81\u751f\u6210\u89c6\u9891\u6807\u7b7e\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5077\u6076\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u56fe\u57fa\u6846\u67b6\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u5077\u6076\u89c6\u9891\uff0c\u901a\u8fc7\u5f3a\u8c03\u5077\u6076\u5185\u5bb9\u548c\u7cfb\u7edf\u5730\u6a21\u5f0f\u5316\u591a\u6a21\u6001\u5173\u7cfb\uff0c\u4e3a\u5728\u7ebf\u5b89\u5168\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13888", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "CER\u6846\u67b6\u7ed3\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5728\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u533b\u7597\u9886\u57df\u9519\u8bef\u4fe1\u606f\uff08\u5982\u75ab\u82d7\u72b9\u8c6b\u548c\u672a\u7ecf\u8bc1\u5b9e\u7684\u6cbb\u7597\u65b9\u6cd5\uff09\u5bf9\u516c\u5171\u536b\u751f\u548c\u533b\u7597\u7cfb\u7edf\u4fe1\u4efb\u6784\u6210\u5a01\u80c1\u3002\u751f\u7269\u533b\u5b66\u58f0\u660e\u9a8c\u8bc1\u5177\u6709\u672f\u8bed\u590d\u6742\u3001\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3001\u5fc5\u987b\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51faCER\u6846\u67b6\uff0c\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\u3002\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\u7684\u68c0\u7d22\u6280\u672f\uff0c\u786e\u4fdd\u8f93\u51fa\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u6765\u6e90\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u4e3a\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bc1\u636e\u68c0\u7d22\u548c\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u73b0\u6027\u3002"}}
{"id": "2509.13525", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "ColonCrafter\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u76ee\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u6df1\u5ea6\u56fe\uff0c\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u89c6\u9891\u5e8f\u5217\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5176\u57283D\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u4ece\u5408\u6210\u7ed3\u80a0\u955c\u5e8f\u5217\u5b66\u4e60\u51e0\u4f55\u5148\u9a8c\u6765\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5f15\u5165\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u5c06\u771f\u5b9e\u4e34\u5e8a\u89c6\u9891\u9002\u914d\u5230\u5408\u6210\u8bad\u7ec3\u57df\u3002", "result": "\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u901a\u7528\u548c\u7ed3\u80a0\u955c\u4e13\u7528\u65b9\u6cd5\uff0c\u5c55\u793a\u4e863D\u70b9\u4e91\u751f\u6210\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u7b49\u4e34\u5e8a\u5e94\u7528\u3002", "conclusion": "\u867d\u7136\u5b8c\u6574\u8f68\u8ff9\u76843D\u91cd\u5efa\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46ColonCrafter\u5728\u4e34\u5e8a\u76f8\u5173\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7ed3\u80a0\u955c\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u548cDeepSeek-V3\u7b49\u9886\u5148\u6a21\u578b\u4e0e\u4e13\u95e8WSD\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u80fd\u8fbe\u523098%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u8bc4\u4f30\u5de5\u4f5c\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u8bcd\u4e49\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u8bcd\u4e49\u6d88\u6b67\u548c\u8bcd\u4e49\u7406\u89e3\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18LLMs\u7684\u8bcd\u4e49\u6d88\u6b67\u80fd\u529b\uff0c\u5e76\u4e0e\u4e13\u95e8WSD\u7cfb\u7edf\u6bd4\u8f83\uff1b\u8bc4\u4f30\u4e24\u4e2a\u9876\u7ea7\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u5728\u4e09\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1a\u5b9a\u4e49\u751f\u6210\u3001\u81ea\u7531\u5f62\u5f0f\u89e3\u91ca\u548c\u793a\u4f8b\u751f\u6210\u3002", "result": "\u5728WSD\u4efb\u52a1\u4e2d\uff0cGPT-4o\u548cDeepSeek-V3\u4e0e\u4e13\u95e8WSD\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u5728\u4e0d\u540c\u9886\u57df\u548c\u96be\u5ea6\u7ea7\u522b\u4e0a\u8868\u73b0\u66f4\u7a33\u5065\uff1b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLLMs\u80fd\u4ee5\u524d\u540e\u6587\u51c6\u786e\u89e3\u91ca\u8bcd\u4e49\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe98%\uff0c\u81ea\u7531\u5f62\u5f0f\u89e3\u91ca\u4efb\u52a1\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u4e49\u7406\u89e3\u548c\u6d88\u6b67\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u8fbe\u5230\u4e0e\u4e13\u95e8\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u751f\u6210\u89e3\u91ca\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u81ea\u7531\u5f62\u5f0f\u7684\u89e3\u91ca\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2509.13536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u51e0\u4f55\u76f8\u4f3c\u6027\u7684\u6c47\u7f16\u7b97\u6cd5\u548c\u8868\u9762\u6837\u672c\u91c7\u96c6\u65b9\u6cd5\uff0c\u5728\u4e0d\u5f71\u54cd\u6e32\u67d3\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u964d\u4f4e\u4e863D\u9ad8\u65af\u5206\u6563GPU\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf", "motivation": "\u89e3\u51b3\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u5982\u5fae\u578b\u98de\u884c\u5668\uff09\u57283D\u9ad8\u65af\u5206\u6563\u6280\u672f\u4e2d\u9047\u5230\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7cfb\u7edf\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u91cd\u5efa\u8d28\u91cf", "method": "1\uff09\u5728\u6c47\u7f16\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u51e0\u4f55\u76f8\u4f3c\u6027\u5408\u5e76\u5197\u4f59\u76843D\u9ad8\u65af\u539f\u8bed\uff1b2\uff09\u901a\u8fc7\u8868\u9762\u6837\u672c\u91c7\u96c6\u65b9\u6cd5\u521d\u59cb\u53163D\u9ad8\u65af\u539f\u8bed\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u6574\u4e2a\u573a\u666f", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4eGPU\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u6e32\u67d3\u8d28\u91cf", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5d4c\u5165\u5f0f\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D\u9ad8\u65af\u5206\u6563\u65b9\u6848\uff0c\u5728\u4e0d\u7f29\u51cf\u8fd0\u884c\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u5185\u5b58\u4f18\u5316\u548c\u8d28\u91cf\u63d0\u5347"}}
{"id": "2509.13930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00RAG\u7cfb\u7edf\u4e2d\u5b58\u5728\u82f1\u8bed\u504f\u597d\u504f\u89c1\uff0c\u6a21\u578b\u4f1a\u4f18\u5148\u5f15\u7528\u82f1\u8bed\u6587\u6863\u800c\u975e\u6700\u76f8\u5173\u7684\u6587\u6863\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u4e2d\u95f4\u4f4d\u7f6e\u65f6\u66f4\u660e\u663e", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u4e0d\u540c\u8bed\u8a00\u6587\u6863\u6df7\u5408\u662f\u5426\u4f1a\u5bf9\u751f\u6210\u548c\u5f15\u7528\u884c\u4e3a\u4ea7\u751f\u610f\u5916\u5f71\u54cd\uff0c\u7279\u522b\u662f\u8bed\u8a00\u504f\u597d\u662f\u5426\u4f1a\u5f71\u54cd\u5f15\u7528\u9009\u62e9", "method": "\u4f7f\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6587\u6863\u76f8\u5173\u6027\u7b49\u5176\u4ed6\u56e0\u7d20\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u6d4b\u91cf\u8bed\u8a00\u504f\u597d\uff0c\u6db5\u76d68\u79cd\u8bed\u8a00\u548c6\u4e2a\u5f00\u6e90\u6a21\u578b", "result": "\u6a21\u578b\u5728\u82f1\u8bed\u67e5\u8be2\u65f6\u4f18\u5148\u5f15\u7528\u82f1\u8bed\u6765\u6e90\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4f4d\u4e8e\u4e0a\u4e0b\u6587\u4e2d\u95f4\u7684\u6587\u6863\u4e2d\u66f4\u52a0\u660e\u663e\uff1b\u6a21\u578b\u6709\u65f6\u4f1a\u727a\u7272\u6587\u6863\u76f8\u5173\u6027\u6765\u6ee1\u8db3\u8bed\u8a00\u504f\u597d", "conclusion": "\u5f15\u7528\u9009\u62e9\u5e76\u975e\u603b\u662f\u7531\u4fe1\u606f\u91cf\u9a71\u52a8\uff0c\u8bed\u8a00\u504f\u597d\u663e\u8457\u5f71\u54cd\u591a\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e2d\u7684\u5f15\u7528\u884c\u4e3a\uff0c\u8fd9\u5bf9\u591a\u8bed\u8a00RAG\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2509.13577", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u81ea\u9002\u5e94OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u6a21\u5f0f\uff0c\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u90e8\u7f72\u4e2d\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u4e0e\u73b0\u5b9e\u6761\u4ef6\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u73b0\u6709OOD\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u8f68\u8ff9\u7ea7\u522b\u7684OOD\u68c0\u6d4b\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u5feb\u901f\u53d8\u5316\u68c0\u6d4b(QCD)\u4efb\u52a1\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u7684\u6a21\u5f0f\u4f9d\u8d56\u6027\u5206\u5e03\u53ca\u5176\u968f\u65f6\u95f4\u6f14\u5316\u7684\u7279\u6027\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684UQ\u548c\u57fa\u4e8e\u89c6\u89c9\u7684OOD\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u9760\u3001\u9a7e\u9a76\u611f\u77e5\u7684\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002"}}
{"id": "2509.13980", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "\u57fa\u4e8eCOMET\u6846\u67b6\u6784\u5efa\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\uff0c\u9884\u6d4b\u9519\u8bef\u8de8\u5ea6\u6807\u6ce8\u5206\u6570\uff0c\u5728WMT25\u8bc4\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u77ed\u7247\u6bb5\u6a21\u578b", "motivation": "\u73b0\u6709\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u77ed\u7247\u6bb5\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5229\u7528\uff0c\u800c\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027", "method": "1) \u4f7f\u7528\u9886\u57df\u5185\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\u6784\u5efa\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\uff0c\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u5206\u6570\uff1b2) \u6574\u5408\u591a\u79cd\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6(MQM\u3001SQM\u3001DA)\uff0c\u901a\u8fc7\u5c3a\u5ea6\u5f52\u4e00\u5316\u5904\u7406\uff1b3) \u8bad\u7ec3\u591a\u8bed\u8a00\u56de\u5f52\u6a21\u578b\uff0c\u57fa\u4e8e\u6e90\u6587\u672c\u3001\u5047\u8bbe\u7ffb\u8bd1\u548c\u53c2\u8003\u7ffb\u8bd1\u9884\u6d4b\u8d28\u91cf\u5206\u6570", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6a21\u578b\u76f8\u6bd4\u4ec5\u4f7f\u7528\u77ed\u7247\u6bb5\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u66f4\u9ad8", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u57fa\u4e8eCOMET\u6846\u67b6\u7684\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u8bc4\u4f30\u6027\u80fd"}}
{"id": "2509.13586", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e9a\u9a6c\u900a\u96e8\u6797\u780d\u4f10\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u536b\u661f\u56fe\u50cf\u5bf9\u6bd4\u548c\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u53d8\u5316\u533a\u57df", "motivation": "\u4e9a\u9a6c\u900a\u96e8\u6797\u780d\u4f10\u5bf9\u5168\u7403\u78b3\u6392\u653e\u548c\u751f\u7269\u591a\u6837\u6027\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u6d4b\u5de5\u5177\u6765\u7814\u7a76\u548c\u5e94\u5bf9\u8fd9\u4e00\u73af\u5883\u95ee\u9898", "method": "\u5229\u7528\u5730\u7403\u89c2\u6d4b\u536b\u661f\u56fe\u50cf\u5bf9\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6bd4\u8f83\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u540c\u4e00\u533a\u57df\u56fe\u50cf\uff0c\u8bc6\u522b\u68ee\u6797\u8986\u76d6\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u79d1\u5b66\u6587\u732e\u63d0\u53d6\u5173\u952e\u8bcd\u6784\u5efa\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8", "result": "\u5728\u4e9a\u9a6c\u900a\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u780d\u4f10\u5e76\u751f\u6210\u76f8\u5173\u6807\u6ce8", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u6d4b\u548c\u7814\u7a76\u4e9a\u9a6c\u900a\u780d\u4f10\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u7528\u5de5\u5177\uff0c\u867d\u7136\u4ee5\u73af\u5883\u5e94\u7528\u4e3a\u91cd\u70b9\u5c55\u793a\uff0c\u4f46\u5177\u6709\u901a\u7528\u6027\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df"}}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "Slim-SC\u662f\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\uff0c\u901a\u8fc7\u9010\u6b65\u526a\u679d\u7b56\u7565\u51cf\u5c11Self-Consistency\u4e2d\u7684\u5197\u4f59\u63a8\u7406\u94fe\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "Self-Consistency\u867d\u7136\u6709\u6548\u4f46\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5206\u6570\u6216\u7f3a\u4e4f\u5b9e\u8bc1\u652f\u6301\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faSlim-SC\u65b9\u6cd5\uff0c\u5728\u601d\u7ef4\u5c42\u9762\u4f7f\u7528\u94fe\u95f4\u76f8\u4f3c\u6027\u8bc6\u522b\u548c\u79fb\u9664\u5197\u4f59\u63a8\u7406\u94fe\uff0c\u91c7\u7528\u9010\u6b65\u526a\u679d\u7b56\u7565", "result": "\u5728\u4e09\u4e2aSTEM\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e24\u79cdLLM\u67b6\u6784\u4e0a\uff0cSlim-SC\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e45%\uff0cKVC\u4f7f\u7528\u964d\u4f4e26%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027", "conclusion": "Slim-SC\u4e3aSelf-Consistency\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\u7ef4\u6301\u4e86\u63a8\u7406\u6027\u80fd"}}
