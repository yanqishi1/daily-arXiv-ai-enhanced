<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 12]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: DPCache提出了一种基于动态规划的全局路径规划方法，通过构建路径感知成本张量来优化扩散模型采样过程中的关键时间步选择，实现训练免费加速且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面表现出色，但多步迭代采样的计算开销阻碍了实际部署。现有的基于缓存的加速方法采用固定或局部自适应调度，没有考虑去噪轨迹的全局结构，容易导致误差累积和视觉伪影。

Method: DPCache将扩散采样加速问题形式化为全局路径规划问题。首先从小型校准集构建路径感知成本张量，量化在给定前一个关键时间步条件下跳过时间步的路径依赖误差。然后使用动态规划算法选择最优的关键时间步序列，最小化总路径成本同时保持轨迹保真度。在推理时，模型只在关键时间步进行完整计算，中间输出则使用缓存特征进行高效预测。

Result: 在DiT、FLUX和HunyuanVideo上的广泛实验表明，DPCache在保持最小质量损失的情况下实现了显著加速。在FLUX上，4.87倍加速时ImageReward提升+0.031，3.54倍加速时甚至超过完整步数基线+0.028 ImageReward，优于先前的加速方法。

Conclusion: DPCache通过路径感知的全局调度框架，有效解决了扩散模型加速中的误差累积问题，实现了高质量的训练免费加速，为扩散模型的实用部署提供了有效解决方案。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [2] [Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval](https://arxiv.org/abs/2602.22759)
*Yuan-Chih Chen,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出统一的隐藏代码恢复框架，支持从后处理和生成中水印两种范式进行图像内容检索与恢复，在ImageNet-S基准上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前图像真实性研究主要集中在深度伪造检测和定位，而针对篡改内容的事实检索恢复相对缺乏探索。需要建立超越检测和定位的通用图像恢复基础。

Method: 提出统一隐藏代码恢复框架，将语义和感知信息编码为紧凑的隐藏代码表示，通过多尺度向量量化进行精炼，并利用条件Transformer模块增强上下文推理能力。

Result: 在ImageNet-S基准上的大量实验表明，该方法展现出有前景的检索和重建性能，同时保持与多种水印管道的完全兼容性。

Conclusion: 该框架为超越检测和定位的通用图像恢复建立了基础，实现了从水印中检索和恢复篡改内容的能力。

Abstract: Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.

</details>


### [3] [Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins](https://arxiv.org/abs/2602.22919)
*Haofan Wu,Nay Aung,Theodoros N. Arvanitis,Joao A. C. Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: Chain of Flow (COF) 是一种基于心电图的生成框架，能够从单次心动周期重建完整的4D心脏结构和运动，将心脏数字孪生从特定任务预测模型转变为完全生成的患者特异性虚拟心脏。


<details>
  <summary>Details</summary>
Motivation: 现有心脏数字孪生框架局限于特定任务预测，而非构建患者特异性、可操作的虚拟心脏。临床可操作的心脏数字孪生需要重建个体化心脏解剖结构和生理功能，从多模态信号更新内部状态，并支持广泛的下游模拟任务。

Method: COF是一个基础性的心电图驱动生成框架，在训练过程中整合电影磁共振成像和12导联心电图，学习心脏几何结构、电生理和运动动力学的统一表示，能够从单个心动周期重建完整的4D心脏结构和运动。

Result: 在多样化队列上评估显示，COF能够准确恢复心脏解剖结构、各腔室功能和动态运动模式。重建的4D心脏进一步支持下游心脏数字孪生任务，如容积测量、区域功能分析和虚拟电影合成。

Conclusion: 通过直接从心电图实现完整的4D器官重建，COF将心脏数字孪生从狭窄的预测模型转变为完全生成的患者特异性虚拟心脏，为临床可操作的心脏数字孪生提供了重要进展。

Abstract: A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.

</details>


### [4] [Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy](https://arxiv.org/abs/2602.23088)
*Matthew Sutton,Katrin Amunts,Timo Dickscheid,Christian Schiffer*

Main category: cs.CV

TL;DR: 提出了一种基于标签中介的方法，通过标签而非成对的图像-文本数据，将细胞构筑学视觉基础模型连接到语言模型，实现显微镜图像的自然语言描述。


<details>
  <summary>Details</summary>
Motivation: 在研究和临床环境中，获取成对的图像-文本数据非常困难，特别是在细胞构筑学分析领域。需要一种方法能够在不依赖精细配对的图像-文本数据的情况下，将视觉基础模型与语言模型连接起来，提供自然语言界面。

Method: 使用标签作为中介，自动从相关文献中挖掘区域描述作为合成标题，反映典型的细胞构筑学属性。然后将现有的细胞构筑学视觉基础模型（CytoNet）通过图像到文本的训练目标连接到大型语言模型。

Result: 在57个脑区上，该方法生成了合理的区域级描述，并支持通过明确拒绝未见区域实现开放集使用。在范围内补丁上匹配细胞构筑学参考标签的准确率达到90.6%，在标签被掩蔽的情况下，其描述仍具有足够区分度，在8路测试中恢复区域的准确率达到68.6%。

Conclusion: 弱标签中介配对足以将现有的生物医学视觉基础模型连接到语言，为在精细配对注释稀缺的领域集成自然语言提供了实用的方法。

Abstract: Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.

</details>


### [5] [DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation](https://arxiv.org/abs/2602.23165)
*Yichen Peng,Jyun-Ting Song,Siyeol Jung,Ruofan Liu,Haiyang Liu,Xuangeng Chu,Ruicong Liu,Erwin Wu,Hideki Koike,Kris Kitani*

Main category: cs.CV

TL;DR: DyaDiT是一个多模态扩散变换器，能够从双人对话音频生成上下文适当的人类手势动作，考虑了社交互动动态，超越了传统单音频到单动作的映射方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将单一音频流映射到单一说话者的动作，忽略了社交上下文和对话双方之间的相互动态关系，这限制了生成手势的自然性和社交互动性。

Method: DyaDiT采用多模态扩散变换器架构，输入双人对话音频和可选的社交上下文标记，融合双方信息捕捉互动动态，使用动作字典编码动作先验，并可选择性地利用对话伙伴的手势生成更具响应性的动作。

Result: 在标准动作生成指标评估和定量用户研究中，DyaDiT不仅在客观指标上超越了现有方法，而且在用户偏好方面获得强烈认可，证明了其鲁棒性和社交友好的动作生成能力。

Conclusion: DyaDiT通过考虑社交上下文和对话双方的互动动态，能够生成更加自然、社交参与度更高的对话手势，为数字人类交互提供了更先进的解决方案。

Abstract: Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.

</details>


### [6] [Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration](https://arxiv.org/abs/2602.23169)
*Xiaole Tang,Xiaoyi He,Jiayi Xu,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: BaryIR是一个通过Wasserstein重心空间对齐多源退化特征的表征学习框架，旨在解决现有全合一图像恢复方法对分布外退化泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有全合一图像恢复方法虽然能处理多种退化类型，但对分布外退化场景泛化能力有限。作者观察到多源退化特征分布是由不同退化特定偏移从底层退化无关分布中诱导产生的，因此恢复这种共享分布对于实现跨退化泛化至关重要。

Method: 提出BaryIR框架：1）在Wasserstein重心空间中对齐多源退化特征，该空间通过最小化到多源退化分布的Wasserstein距离平均值来建模退化无关分布；2）引入残差子空间，其嵌入相互对比同时保持与WB嵌入正交；3）显式解耦两个正交空间：WB空间编码跨退化共享的退化无关不变内容，残差子空间自适应保留退化特定知识。

Result: BaryIR在性能上可与最先进的全合一方法竞争，在未见退化类型和级别上表现出良好的泛化能力，即使在有限退化类型上训练并在混合退化的真实世界数据上评估时，也能学习到泛化性强的特征。

Conclusion: 通过Wasserstein重心空间对齐和正交空间解耦，BaryIR有效缓解了对分布内退化的过拟合，实现了基于退化无关共享不变性的自适应恢复，显著提升了全合一图像恢复方法在真实世界场景中的泛化能力。

Abstract: Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.

</details>


### [7] [Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms](https://arxiv.org/abs/2602.23177)
*Bin Zeng,Johannes Künzel,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: 提出一种基于物理约束的实时人群计数方法，利用安装在列车上的单摄像头扫描站台，通过物理约束的卡尔曼滤波模型和虚拟计数带实现动态条件下的准确计数。


<details>
  <summary>Details</summary>
Motivation: 铁路站台实时人群计数对安全和容量管理至关重要。现有跟踪检测方法大多假设静态摄像头或忽略运动建模的物理一致性，在列车进站时的动态条件下（密集遮挡、摄像头运动、透视畸变）计数不可靠。

Method: 提出物理约束跟踪框架，将检测、外观和3D运动推理统一到实时流程中。集成迁移学习的YOLOv11m检测器和EfficientNet-B0外观编码到DeepSORT中，引入物理约束卡尔曼模型（Phys-3D），通过针孔几何强制物理上合理的3D运动动力学。为应对遮挡下的计数脆弱性，实现带持久性的虚拟计数带。

Result: 在平台基准测试MOT-RailwayPlatformCrowdHead数据集上，该方法将计数误差降低到2.97%，在运动和遮挡条件下表现出鲁棒性能。

Conclusion: 结合第一原理几何和运动先验，能够在安全关键的交通场景中实现可靠的人群计数，有助于有效的列车调度和站台安全管理。

Abstract: Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.

</details>


### [8] [Uni-Animator: Towards Unified Visual Colorization](https://arxiv.org/abs/2602.23191)
*Xinyuan Chen,Yao Xu,Shaowen Wang,Pengjie Song,Bowen Deng*

Main category: cs.CV

TL;DR: Uni-Animator是一个基于Diffusion Transformer的统一图像和视频草图着色框架，解决了现有方法在颜色转移不精确、物理细节保留不足和时间一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有草图着色方法难以统一图像和视频任务，存在以下问题：1）使用单个或多个参考时颜色转移不精确；2）高频物理细节保留不足；3）大运动场景中时间一致性差，存在运动伪影。

Method: 提出三个关键技术：1）通过实例补丁嵌入实现视觉参考增强，精确对齐和融合参考颜色信息；2）使用物理特征进行物理细节强化，有效捕捉和保留高频纹理；3）基于草图的动态RoPE编码，自适应建模运动感知的时空依赖关系。

Result: 大量实验结果表明，Uni-Animator在图像和视频草图着色任务上均取得有竞争力的性能，与任务特定方法相当，同时解锁了统一的跨域能力，具有高细节保真度和鲁棒的时间一致性。

Conclusion: Uni-Animator成功解决了现有草图着色方法的局限性，实现了图像和视频任务的统一处理，在颜色转移精度、细节保留和时间一致性方面表现出色。

Abstract: We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.

</details>


### [9] [Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks](https://arxiv.org/abs/2602.23217)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.CV

TL;DR: 本文提出了多维任务学习（MTL）框架，基于广义爱因斯坦MLP（GE-MLP），通过爱因斯坦积直接在张量上操作，为计算机视觉任务提供了统一的数学基础。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉任务表述受限于基于矩阵的思维：标准架构依赖矩阵值权重和向量值偏置，需要进行结构扁平化，这限制了自然可表达任务的空间。需要一种能够避免信息损失、更灵活地控制维度保持或收缩的框架。

Method: 提出基于广义爱因斯坦MLP（GE-MLP）的多维任务学习（MTL）框架，使用张量值参数，通过爱因斯坦积直接在张量上操作，能够显式控制哪些维度被保留或收缩，而不需要信息损失。

Result: 通过严格的数学推导证明，分类、分割和检测都是MTL的特殊情况，仅在形式化定义的任务空间中的维度配置上有所不同。证明该任务空间严格大于基于矩阵的表述所能原生表达的空间，支持时空或跨模态预测等需要破坏性扁平化的任务配置。

Conclusion: 这项工作为通过张量代数视角理解、比较和设计计算机视觉任务提供了数学基础，突破了传统矩阵思维的限制，实现了更灵活和表达力更强的任务表述。

Abstract: This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.

</details>


### [10] [PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM](https://arxiv.org/abs/2602.23297)
*Yiqing Wang,Chunming He,Ming-Chen Lu,Mercy Pawar,Leslie Niziol,Maria Woodward,Sina Farsiu*

Main category: cs.CV

TL;DR: PRIMA是一个多模态医学诊断框架，通过整合视觉特征和临床元数据，利用风险-疾病相关性知识和四种互补损失函数进行预训练，显著提升了疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学诊断方法通常将临床元数据视为孤立标签，未能充分利用临床描述中丰富的语义知识。需要一种能够有效整合视觉表现和临床元数据的框架。

Method: 1. 通过RAG构建风险-疾病相关性专家语料库，优化Clinical ModernBERT文本编码器；2. 采用DINOv3和优化BERT的双编码器预训练策略；3. 设计四种互补损失函数实现多粒度语义对齐；4. 使用Qwen-3融合对齐特征进行疾病分类。

Result: PRIMA在实验中显著优于现有最先进方法，有效协调像素级特征和抽象临床专业知识，具有优越的鲁棒性，且无需大规模数据收集或大量计算资源。

Conclusion: PRIMA框架成功将领域特定知识整合到多模态表示学习中，通过风险集成图像-元数据对齐，实现了医学诊断中视觉特征和临床元数据的有效融合。

Abstract: Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.

</details>


### [11] [ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding](https://arxiv.org/abs/2602.23306)
*Yiran Guan,Sifan Tu,Dingkang Liang,Linghao Zhu,Jianzhong Ju,Zhenbo Luo,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: ThinkOmni是一个无需训练和数据的框架，通过利用现成的大推理模型指导全模态大语言模型的解码过程，将文本推理能力提升到全模态场景。


<details>
  <summary>Details</summary>
Motivation: 现有的全模态大语言模型虽然能感知多种模态，但缺乏复杂推理能力，而通过额外训练增强推理能力面临高质量数据需求、任务特定适应和计算成本高等挑战。

Method: 提出ThinkOmni框架，包含两个关键组件：1) LRM-as-a-Guide：利用现成的大推理模型指导全模态大语言模型的解码过程；2) Stepwise Contrastive Scaling：自适应平衡感知和推理信号，无需手动超参数调优。

Result: 在六个多模态推理基准测试中，ThinkOmni持续带来性能提升，主要结果在MathVista上达到70.2分，在MMAU上达到75.5分。

Conclusion: ThinkOmni为全模态推理提供了一个灵活且可泛化的解决方案，并为推理能力的泛化和应用提供了新的见解。

Abstract: Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.

</details>


### [12] [Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?](https://arxiv.org/abs/2602.23339)
*Tilemachos Aravanis,Vladan Stojnić,Bill Psomas,Nikos Komodakis,Giorgos Tolias*

Main category: cs.CV

TL;DR: 该论文提出了一种检索增强的测试时适配器，通过融合文本和视觉支持特征来学习轻量级的每图像分类器，显著缩小了零样本与全监督分割之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 开放词汇分割（OVS）虽然扩展了视觉语言模型的零样本识别能力，但仍落后于全监督方法，主要面临两个挑战：训练VLM时使用的粗粒度图像级监督，以及自然语言的语义模糊性。

Method: 引入少样本设置，通过像素标注图像的支持集增强文本提示；提出检索增强的测试时适配器，通过学习每图像分类器，融合文本和视觉支持特征，实现模态间的更强协同作用。

Result: 实验表明，该方法显著缩小了零样本与监督分割之间的性能差距，同时保持了开放词汇能力，支持持续扩展的支持集，并适用于细粒度任务如个性化分割。

Conclusion: 通过融合文本和视觉支持特征的检索增强测试时适配器，有效解决了开放词汇分割中的语义模糊性和监督不足问题，实现了接近监督方法的性能。

Abstract: Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference](https://arxiv.org/abs/2602.22868)
*Yushi Ye,Feng Hong,Huangjie Zheng,Xu Chen,Zhiyong Chen,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: ReMix框架通过引入连续混合状态解决扩散大语言模型并行解码中的组合矛盾问题，实现2-8倍推理加速且无质量损失


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽然承诺快速非自回归推理，但在并行解码中存在严重的质量-速度权衡问题，这源于"组合矛盾"现象——并行生成的令牌会形成语义不一致的组合

Method: 提出ReMix框架，引入连续混合状态作为初始掩码状态和最终解码令牌状态之间的中间状态，允许令牌表示在连续空间中迭代细化，解决与其他令牌的相互冲突；同时采用拒绝规则将不确定表示从连续状态回退到掩码状态重新处理

Result: ReMix作为无需训练的方法，在广泛实验中实现了2-8倍的推理加速，且没有任何质量下降

Conclusion: ReMix通过将连续表示整合到离散解码过程中，有效缓解了扩散大语言模型中的组合矛盾问题，在保持生成质量的同时显著提升了推理速度

Abstract: Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \times$ inference speedup without any quality degradation.

</details>


### [14] [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871)
*Roy Miles,Aysim Toker,Andreea-Maria Oncescu,Songcen Xu,Jiankang Deng,Ismail Elezi*

Main category: cs.CL

TL;DR: 提出Stitching Noisy Diffusion Thoughts框架，通过扩散模型采样多推理轨迹，用过程奖励模型评分步骤，跨轨迹拼接高质量步骤形成复合推理，再用自回归模型生成最终答案，实现训练免费的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有聚合策略通常是轨迹级的（如选择最佳轨迹或投票最终答案），丢弃了部分或"接近正确"尝试中有用的中间工作。需要一种能重用步骤级候选的自我一致性框架。

Method: 1) 使用掩码扩散语言模型采样多样、低成本的推理轨迹；2) 用现成过程奖励模型（PRM）评分每个中间步骤；3) 跨轨迹拼接最高质量步骤形成复合推理；4) 用自回归模型基于复合推理重新计算最终答案。

Result: 在数学推理基准测试中，步骤级重组对更难问题最有益，最终自回归求解器能将拼接但不完美的推理转换为准确答案。训练免费框架在六个数学和编程任务中平均准确率提升达23.8%，同时相比传统扩散模型和统一架构实现1.8倍延迟降低。

Conclusion: 该模块化管道将探索（扩散）与评估和解决方案合成分离，避免单一统一混合架构，同时保持广泛搜索。步骤级重组特别适用于困难问题，自回归求解器对确保最终答案准确性至关重要。

Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.

</details>


### [15] [Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models](https://arxiv.org/abs/2602.22918)
*Jonathan Steinberg,Oren Gal*

Main category: cs.CL

TL;DR: 研究通过因果干预分析三种视觉语言模型中OCR信息的处理路径，发现不同架构的OCR瓶颈位置不同，OCR信号具有低维特性，且在模块化架构中移除OCR反而能提升计数性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型能够从图像中读取文本，但OCR信息具体在语言处理流程的哪个环节进入系统尚不清楚。本研究旨在探究不同架构的VLMs中OCR信息的处理路径和机制。

Method: 使用因果干预方法，通过计算原始图像与文本修复版本之间的激活差异，分析三种架构家族（Qwen3-VL、Phi-4、InternVL3.5）的OCR处理机制。采用主成分分析（PCA）研究OCR信号的维度特性，并测试不同数据集间的可迁移性。

Result: 1. 不同架构的OCR瓶颈位置不同：DeepStack模型（Qwen）在中间层（约50%）对场景文本最敏感；单阶段投影模型（Phi-4、InternVL）在早期层（6-25%）最敏感
2. OCR信号具有低维特性：第一主成分（PC1）能解释72.9%的方差
3. PCA方向在不同数据集间可迁移，表明存在共享的文本处理通路
4. 在模块化OCR电路架构中（如Qwen3-VL-4B），移除OCR反而能提升计数性能达6.9个百分点

Conclusion: 不同视觉语言模型架构采用不同的OCR信息处理策略，OCR信号具有低维和可迁移的特性。特别值得注意的是，在模块化架构中，OCR处理可能干扰其他视觉处理任务，这表明模型设计需要在文本识别和其他视觉能力之间取得平衡。

Abstract: Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.

</details>


### [16] [Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention](https://arxiv.org/abs/2602.23057)
*Jeongin Bae,Baeseong Park,Gunho Park,Minsub Kim,Joonhyung Lee,Junhee Yoo,Sunghyeon Woo,Jiwon Ryu,Se Jung Kwon,Dongsoo Lee*

Main category: cs.CL

TL;DR: 本文提出Affine-Scaled Attention，一种通过引入输入相关的缩放和偏置项来放松softmax严格归一化约束的注意力机制，在大型语言模型预训练中表现出更好的训练稳定性、优化行为和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer注意力使用softmax归一化，强制注意力权重具有单位总和归一化。这种约束限制了注意力幅度的灵活性，可能导致训练过程中注意力模式过于集中或不稳定。现有的改进方法如注意力汇聚或门控机制只能提供有限或间接的注意力重加权控制。

Method: 提出Affine-Scaled Attention，在标准注意力基础上引入输入相关的缩放和相应的偏置项，应用于softmax归一化的注意力权重。这种设计放松了严格的归一化约束，同时保持了值表示的聚合，允许模型以可控方式调整注意力的相对分布和尺度。

Result: 在多种模型规模的大规模语言模型预训练中进行了实证评估。实验结果显示，与标准softmax注意力和注意力汇聚基线相比，Affine-Scaled Attention在训练稳定性、优化行为和下游任务性能方面都取得了持续改进。

Conclusion: 适度的注意力输出重加权为改进Transformer模型中的注意力行为提供了一种实用有效的方法。Affine-Scaled Attention通过放松softmax的严格归一化约束，使模型能够更好地控制注意力分布和尺度。

Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.
  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.

</details>
