<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 202]
- [cs.CL](#cs.CL) [Total: 150]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 该研究发布了EDU-CIRCUIT-HW数据集，包含1300+份大学生手写STEM作业，用于评估多模态大语言模型在手写内容识别和自动评分中的表现，发现模型存在大量潜在错误，并提出通过识别错误模式来提升系统鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在教育领域有巨大潜力，但现有评估方法主要关注下游任务（如自动评分），无法全面评估模型对复杂手写逻辑的整体理解。缺乏真实、领域特定的基准数据集来评估模型对学生手写解决方案的准确解释能力。

Method: 发布EDU-CIRCUIT-HW数据集，包含1300+份大学STEM课程的真实学生手写解决方案，包含数学公式、图表和文本推理。利用专家验证的逐字转录和评分报告，同时评估各种MLLM的上游识别保真度和下游自动评分性能。通过案例研究展示如何利用识别出的错误模式来预先检测和纠正识别错误。

Result: 评估揭示了MLLM识别学生手写内容中存在惊人的潜在错误规模，表明模型在高风险教育环境中的自动评分和其他理解导向应用中的可靠性不足。通过识别错误模式并仅需约4%的人工干预，可以显著提升AI评分系统在未见学生解决方案上的鲁棒性。

Conclusion: 当前MLLM对学生手写STEM解决方案的识别和理解能力仍不足，需要更全面的评估方法和改进策略。通过主动检测和纠正识别错误，可以显著提升AI教育系统的可靠性，但需要进一步研究来确保模型在高风险教育环境中的安全部署。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [2] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: Simulate Anything是一个图形驱动的世界建模和仿真框架，仅使用多视角环境视频和现成资产就能高效生成高保真度的具身训练数据，其训练的VLA模型在下游任务中达到与真实数据相当甚至更好的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 具身智能的可扩展性受到真实世界交互数据稀缺的根本限制。现有仿真平台存在视觉和物理差距，依赖昂贵传感器、精确机器人校准或深度测量，限制了大规模实用性。

Method: 使用3D高斯溅射(3DGS)从视频重建真实环境为照片级场景表示，利用生成模型恢复物理真实表示，通过精度校准目标集成到仿真环境中，实现重建场景与真实世界的精确尺度对齐。

Result: 基于该框架模拟数据训练的视觉语言动作(VLA)模型在下游任务中表现出强大的零样本性能，匹配甚至超越了使用真实世界数据获得的结果。

Conclusion: 重建驱动的世界建模为可扩展和实用的具身智能训练提供了潜力，能够生成高质量仿真数据替代稀缺的真实交互数据。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [3] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE是一个用于训练和评估视觉语言模型在空间约束计数任务上的新数据集，填补了简单2D数据集和模糊真实数据集之间的空白，能提升模型在分布外图像上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有计数数据集存在局限性：VLMCountBench等简单2D数据集缺乏真实复杂性，而TallyQA等真实数据集在遮挡和空间构成方面缺乏控制，导致模型难以处理空间约束的计数任务。

Method: 创建SITUATE数据集，专门针对空间约束计数任务设计。通过微调Qwen VL 2.5 7B模型在该数据集上，并与Pixmo count数据集进行交叉验证，比较模型在不同计数基准上的性能。

Result: 在SITUATE上微调的Qwen VL 2.5 7B模型在Pixmo count测试数据上准确率提升，但反之不成立。这表明SITUATE能有效提升模型在分布外图像上的泛化能力，优于同等规模的Pixmo count微调集。

Conclusion: SITUATE数据集成功填补了现有计数数据集的空白，为训练和评估视觉语言模型在空间约束计数任务上提供了有效工具，能显著提升模型在未见数据上的泛化性能。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [4] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: 该研究评估了商业呈现攻击检测系统在低光照和自动采集条件下的性能表现，发现大多数系统在这些场景下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 远程身份验证系统中的呈现攻击检测子系统需要在各种环境条件下保持鲁棒性，但低光照条件和自动化图像采集对现有商业系统的影响尚未得到充分研究。

Method: 通过远程身份验证的场景测试，评估商业PAD系统在低光照条件和自动采集工作流程下的性能表现，分析不同环境条件对系统鲁棒性的影响。

Result: 研究发现，在低光照条件下，PAD系统的错误率增加了约四倍；在自动采集场景下，错误率翻倍。只有一个测试系统在所有场景下保持鲁棒性，其真实呈现分类错误率始终低于3%。

Conclusion: 为确保PAD系统在真实应用中的鲁棒性和可靠性，必须在多样化环境条件下进行测试，特别是要考虑低光照和自动化采集等实际场景。

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [5] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 提出一种新型遥感图像处理模型，通过地理空间嵌入机制和引导注意力模块，将辅助地理空间信息与视觉特征融合，提升多模态地理空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉转换器和多模态模型主要关注视觉与文本内容的语义对齐，缺乏对结构化地理空间层的理解和推理能力，无法有效处理遥感图像中的地理空间信息。

Method: 1) 地理空间嵌入机制：将多样化的地理空间数据转换为与图像块空间对齐的嵌入块；2) 引导注意力模块：基于与辅助数据的相关性动态计算注意力权重，引导模型关注最相关区域；3) 为不同注意力头分配不同角色，捕捉辅助信息的互补方面。

Result: 实验结果表明，该框架在预测疾病流行率方面优于现有的预训练地理空间基础模型，证明了其在多模态地理空间理解方面的有效性。

Conclusion: 提出的模型通过有效整合地理空间指导信息，显著提升了遥感图像处理的地理空间理解能力，为多模态地理空间分析提供了新的解决方案。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [6] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: 研究探索了商业条件下空间分配对犊牛玩耍行为的影响，发现8-10平方米/犊牛是最佳空间范围，并开发了高精度的计算机视觉监测系统。


<details>
  <summary>Details</summary>
Motivation: 玩耍行为是犊牛福利的重要指标，但商业条件下空间分配（特别是6-20平方米/犊牛的中高水平）对玩耍行为的影响尚不明确，需要量化研究为福利标准提供依据。

Method: 在荷兰14个商业农场对60头群养犊牛进行研究，空间范围2.66-17.98平方米/犊牛。使用详细行为谱分析视频观察，以观察期百分比表示玩耍行为。采用线性混合模型进行统计分析，农场作为随机效应。开发了基于108小时手动标注数据的计算机视觉管道，并在测试数据上验证。

Result: 计算机视觉分类器在主动玩耍检测上达到97.6%准确率和99.4%召回率。犊牛平均花费1.0%观察时间玩耍（约17小时中的10分钟）。空间与玩耍关系呈非线性，8-10平方米/犊牛时玩耍水平最高（1.6%），6-8平方米和12-14平方米时最低（<0.6%）。控制年龄、健康和群体大小后，空间影响仍显著。

Conclusion: 8-10平方米/犊牛是平衡福利效益与经济可行性的实用目标，自动化监测系统可将小规模标注项目扩展为连续福利评估系统，为规模化商业应用提供技术支持。

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [7] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 1S-DAug是一种单样本生成增强算子，通过结合几何扰动、受控噪声注入和去噪扩散过程，从单个测试图像生成多样且忠实的变化，无需训练即可提升少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统测试时增强在少样本学习中效果不佳，需要一种能够从极少样本生成多样且忠实变体的增强方法，以提升模型对新类别的泛化能力。

Method: 1S-DAug结合几何扰动、受控噪声注入和基于原始图像的去噪扩散过程，生成多样图像变体，然后将生成图像编码并与原始图像聚合为组合表示用于预测。

Result: 在4个标准数据集的少样本学习基准测试中，1S-DAug无需更新模型参数即可持续提升性能，在miniImagenet 5-way-1-shot基准上实现了超过10%的比例准确率提升。

Conclusion: 1S-DAug作为一种免训练、模型无关的插件，能够有效提升少样本学习性能，通过生成多样且忠实的图像变体增强模型泛化能力。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [8] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 提出一种新颖的异步事件驱动算法，用于实时检测事件相机数据中的小型事件簇，具有线性复杂度O(n)且运行时间与像素阵列维度无关。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生异步事件流数据，传统聚类算法难以实时处理这种特殊数据结构，需要开发专门针对事件相机特性的高效实时聚类算法。

Method: 采用异步事件驱动架构，基于事件簇的时空距离进行分层凝聚聚类，利用事件相机的特殊异步数据结构，通过精巧、高效且简单的决策机制实现。

Result: 算法具有线性复杂度O(n)，其中n为事件数量，且运行时间与像素阵列维度无关，能够实时检测小型事件簇。

Conclusion: 该算法为事件相机数据处理提供了一种高效、实时的聚类解决方案，特别适合需要实时检测小型事件簇的应用场景。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [9] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: VDE Bench是一个专门用于评估多语言复杂视觉文档图像编辑模型性能的基准测试，包含英中双语密集文本数据集和解耦评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图像编辑模型在视觉文档图像编辑方面研究不足，现有方法主要针对英文和稀疏文本布局，无法有效处理密集、结构复杂的文档或非拉丁文字（如中文）。

Method: 提出VDE Bench基准，包含高质量英中双语密集文本数据集（学术论文、海报、演示文稿、考试材料、报纸等），并设计了解耦评估框架，在OCR解析层面系统量化编辑性能。

Result: 对代表性最先进图像编辑模型进行了全面评估，人工验证显示人工判断与自动评估指标具有强一致性。VDE Bench是首个针对多语言密集文本视觉文档图像编辑模型的系统基准。

Conclusion: VDE Bench填补了视觉文档图像编辑评估的空白，为多语言复杂文档编辑模型提供了标准化评估工具，有助于推动该领域的发展。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [10] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: 提出上下文感知自编码器，通过整合上下文特定阈值来改进海上船舶交通监控中的异常检测，特别是针对集体和上下文异常。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在检测集体和上下文异常方面效果有限，尤其是在海上监控领域，异常检测依赖于船舶特定的上下文信息（来自AIS消息）。需要一种能更好处理上下文相关异常的方法。

Method: 提出上下文感知自编码器，通过整合上下文特定阈值来改进检测。比较了四种上下文感知自编码器变体和传统自编码器，以捕鱼状态异常为案例进行研究。

Result: 结果显示上下文对重构损失和异常检测有显著影响。上下文感知自编码器在时间序列数据异常检测方面优于其他方法，提高了检测准确性并降低了计算成本。

Conclusion: 通过整合上下文特定阈值并认识到上下文在异常检测中的重要性，该方法为提高海上船舶交通监控系统的准确性提供了有前景的解决方案。

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [11] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: POVNet+：首个用于社交辅助机器人的多模态深度学习架构，通过ADL和运动嵌入空间识别已知/未知ADL及异常执行，主动发起辅助交互


<details>
  <summary>Details</summary>
Motivation: 当前社交辅助机器人无法同时感知和辅助多项日常生活活动（ADL），限制了其长期部署。需要一种能够识别多种ADL并主动发起辅助的感知系统。

Method: 提出POVNet+多模态深度学习架构，引入ADL嵌入空间和运动嵌入空间，区分已知ADL、未知ADL及异常执行ADL。采用新颖的用户状态估计方法在运动嵌入空间中识别新ADL并监测用户表现。

Result: 与最先进的人类活动识别方法相比，POVNet+具有更高的ADL分类准确率。在杂乱生活环境中与社交辅助机器人Leia进行人机交互实验，成功识别已知/未知ADL及异常执行ADL，并主动发起适当的辅助交互。

Conclusion: POVNet+架构能够有效识别多种ADL活动，包括已知、未知及异常执行情况，使社交辅助机器人能够主动发起适当的辅助交互，为长期部署提供了技术基础。

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [12] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: SCANNER是首个针对仇恨视频检测的测试时自适应框架，通过利用仇恨内容中稳定的核心特征作为源域和目标域之间的桥梁，解决语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 仇恨内容经常演变为不规则和模糊的形式以逃避审查，导致严重的语义漂移，使现有训练模型失效。传统测试时自适应方法针对轻度分布偏移，难以处理仇恨视频检测中的严重语义漂移。

Method: SCANNER通过质心引导对齐机制从模糊布局中揭示稳定核心特征，采用样本级自适应质心对齐策略减少异常样本影响，并引入簇内多样性正则化防止语义塌陷。

Result: 实验表明SCANNER在所有基线方法中表现最佳，平均Macro-F1得分比最佳基线高出4.69%。

Conclusion: SCANNER通过利用仇恨内容中稳定的核心特征作为桥梁，有效解决了仇恨视频检测中的语义漂移问题，为测试时自适应在仇恨内容检测领域的应用提供了新思路。

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [13] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: LLaVA-FA：一种在频域进行联合低秩加量化近似的高效大型多模态模型，通过傅里叶变换特性和极坐标量化实现更紧凑准确的权重表示。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型虽然性能优异，但计算和内存成本高昂，阻碍实际部署。现有压缩方法通常将低秩分解和量化解耦，导致重建误差累积，特别是在存在跨模态冗余的多模态架构中。

Method: 提出LLaVA-FA，在频域进行联合低秩加量化近似；利用傅里叶变换的去相关和共轭对称特性；引入PolarQuant（针对复数矩阵的极坐标量化方法）和可选对角校准方案（无需大规模校准数据）。

Result: LLaVA-FA在多个基准测试中优于现有高效多模态模型，同时保持最少的激活参数和低计算成本，验证了其作为LMM压缩方案的有效性。

Conclusion: LLaVA-FA通过频域联合近似和专门设计的量化方法，为大型多模态模型压缩提供了强大解决方案，平衡了性能与效率。

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [14] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: 该研究开发了DensiThAI深度学习框架，利用红外热成像技术非电离性地评估乳腺密度，作为X线乳腺摄影的替代方案。


<details>
  <summary>Details</summary>
Motivation: 目前乳腺密度评估主要依赖X线乳腺摄影这种电离辐射成像方式，需要开发非电离、更安全的替代方法。乳腺密度是乳腺癌风险的重要生物标志物，也是影响乳腺摄影敏感性的主要因素。

Method: 提出DensiThAI多视角深度学习框架，利用乳腺纤维腺体和脂肪组织不同的热物理和生理特性导致的表面温度变化，通过五个标准热成像视角进行乳腺密度分类。

Result: 在包含3,500名女性的多中心数据集上，以乳腺摄影密度标签为参考，DensiThAI在10次随机分割中平均AUROC达到0.73，所有分割中密度类别间均有统计学显著差异(p << 0.05)。

Conclusion: 热成像技术作为非电离性乳腺密度评估方法具有潜力，在不同年龄组中表现一致，有望改善患者体验和工作流程优化。

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [15] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: NGFF是一个端到端神经框架，将3D高斯感知与基于物理的动态建模相结合，从多视角RGB输入生成交互式、物理真实的4D视频，速度比现有高斯模拟器快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然视觉质量高，但缺乏物理定律建模，无法一致生成物理合理的视频。而结合3D高斯溅射和物理引擎的方法计算成本高，在复杂现实场景中缺乏鲁棒性。

Method: 提出Neural Gaussian Force Field (NGFF)框架，集成3D高斯感知与物理动态建模。同时创建GSCollision数据集，包含超过64万渲染物理视频（约4TB），涵盖多种材料、多物体交互和复杂场景。

Result: 在合成和真实3D场景评估中，NGFF展现出强大的泛化能力和物理推理鲁棒性，速度比现有高斯模拟器快两个数量级。

Conclusion: NGFF通过整合3D感知与物理建模，推动了视频预测向物理基础世界模型的发展，为生成交互式、物理真实的4D视频提供了高效解决方案。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [16] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: 提出SDCM框架，通过模拟密度化、雷达补偿映射和Mamba建模交互融合，解决4D雷达点云稀疏和视觉数据在恶劣条件下退化的问题，提升车联网中的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 车联网中基于4D雷达-视觉的3D目标检测面临两个主要挑战：1）4D雷达点云稀疏导致3D表示能力差；2）视觉数据在低光照、远距离检测和密集遮挡场景下出现表示退化，在融合阶段提供不可靠的纹理信息。

Method: 提出SDCM框架，包含三个模块：1）SimDen模块：基于3D核密度估计的关键点高斯模拟生成点云，基于曲率模拟生成轮廓，实现雷达点云密度化；2）RCM模块：利用4D雷达的全天候特性，通过雷达补偿映射减少视觉数据表示退化的影响；3）MMIF模块：提取特征张量差异值中的有效信息，通过Mamba建模实现异质性减少和模态交互融合。

Result: 在VoD、TJ4DRadSet和Astyx HiRes 2019数据集上的实验结果表明，SDCM在参数数量更少、推理速度更快的情况下取得了最佳性能。

Conclusion: SDCM框架通过模拟密度化雷达点云、雷达补偿视觉退化以及Mamba建模交互融合，有效解决了4D雷达-视觉3D目标检测中的关键问题，为车联网应用提供了高性能的解决方案。

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [17] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 该研究系统评估了组织病理学基础模型在回归任务中的表现，特别是用于预测同源重组缺陷（HRD）评分这一关键生物标志物，结果显示基础模型特征相比对比学习基线在预测准确性和泛化能力方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管在大规模组织病理学数据上预训练的基础模型在计算病理学多个领域取得了成功，但它们在回归性生物标志物预测方面的影响尚未得到充分探索。本研究旨在系统评估这些基础模型在回归任务中的表现，特别是用于预测HRD评分这一对个性化癌症治疗至关重要的生物标志物。

Method: 研究采用多实例学习框架，使用五种最先进的基础模型从全切片图像中提取斑块级特征，并与基于对比学习的特征进行比较。模型在来自两个公共医学数据集的乳腺癌、子宫内膜癌和肺癌队列上进行训练，以预测连续的HRD评分。此外，提出了基于分布的上采样策略来缓解数据集中的目标不平衡问题，并通过消融研究探讨了不同采样策略和实例包大小的影响。

Result: 实验表明，基于基础模型特征训练的模型在预测准确性和泛化能力方面始终优于基线模型，同时不同基础模型之间存在系统性差异。提出的分布上采样策略显著提高了代表性不足但临床重要的患者群体的召回率和平衡准确率。

Conclusion: 大规模组织病理学预训练能够实现更精确和可转移的回归性生物标志物预测，展示了其在推进AI驱动的精准肿瘤学方面的潜力。基础模型特征在回归任务中表现出优越性能，为精准医疗提供了有力工具。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [18] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 提出一种轻量级压缩域跟踪模型，直接在视频流上操作，无需完全解码RGB视频，利用压缩数据中的运动矢量和变换系数，在MOTS数据集上实现最高3.7倍计算加速，仅损失4% mAP@0.5精度


<details>
  <summary>Details</summary>
Motivation: 针对大规模监控系统中实时分析的需求，传统RGB视频跟踪需要完全解码，计算开销大。压缩域包含丰富的运动信息（运动矢量和变换系数），可直接利用这些信息进行目标跟踪，避免完全解码的开销

Method: 提出轻量级压缩域跟踪模型，直接从压缩视频流中提取运动矢量和变换系数作为输入，使用深度学习模型在压缩域中传播目标边界框，无需完全解码RGB视频

Result: 在MOTS15/17/20数据集上，相比RGB基线方法，计算速度提升最高达3.7倍，仅损失4%的mAP@0.5精度，证明了压缩域运动建模在实时分析中的效率

Conclusion: 压缩域跟踪模型通过直接利用视频压缩数据中的运动信息，在保持较高跟踪精度的同时显著提升计算效率，特别适合大规模监控系统的实时分析应用

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [19] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: 开发基于姿态的机器学习框架，将临床视频转换为关键点时间序列，提取多维度运动特征，用于客观识别和监测超运动障碍


<details>
  <summary>Details</summary>
Motivation: 超运动障碍（如肌张力障碍、震颤、舞蹈症等）的临床表现波动、间歇且常重叠，临床识别和长期监测主要依赖主观评估，缺乏客观、可扩展的方法来区分这些重叠的表型

Method: 开发基于姿态的机器学习框架，将标准门诊视频转换为解剖学有意义的关键点时间序列，计算涵盖统计、时间、频谱和高阶不规则性-复杂性特征的运动学描述符

Result: 论文摘要中未提供具体结果数据，但方法描述表明该框架能够从常规临床视频中提取多维度运动特征

Conclusion: 该研究提出了一种客观、可扩展的方法，有望改善超运动障碍的临床识别和纵向监测，减少主观评估的变异性

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [20] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: 提出SPCL框架，通过引入"无关样本"概念和边界对比损失，解决心脏图像分割中边界区域表示污染问题，提升分割质量和边界精度。


<details>
  <summary>Details</summary>
Motivation: 心脏图像分割中，同一类别内部（如心肌组织）的边界区域像素表示容易受到污染，导致分割精度下降，特别是边界区域的分割质量不佳。

Method: 提出SPCL框架：1) 引入"无关样本"概念，区分同一类别内部区域和边界区域的像素表示；2) 设计边界对比损失，增强跨边界表示的可区分性；3) 对无关样本和边界对比损失进行理论分析。

Result: 在公开心脏数据集上的实验表明，SPCL显著提升了分割性能，在分割质量和边界精度方面优于现有方法。

Conclusion: SPCL框架通过解决边界表示污染问题，有效提升了心脏图像分割的精度，特别是在边界区域，为医学图像分割提供了新的解决方案。

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [21] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出噪声-频率延续框架，通过构建中间后验分布，在噪声依赖频带内实施测量一致性，解决了扩散后验采样中的细节恢复问题。


<details>
  <summary>Details</summary>
Motivation: 扩散后验采样在解决逆问题时，由于测量项与扩散噪声水平的弱耦合，往往无法恢复精细细节。在高噪声下，从不准确估计计算的数据一致性梯度可能与后验几何不一致，导致早期漂移、伪高频伪影以及对调度和病态算子的敏感性。

Method: 提出噪声-频率延续框架，构建中间后验分布族，其似然仅在噪声依赖频带内强制执行测量一致性。实现了一个稳定的后验采样器，结合扩散预测器、带限似然引导和多分辨率一致性策略，积极采用可靠的低频校正，保守地采用高频细节。

Result: 在超分辨率、修复和去模糊任务中达到最先进性能，运动去模糊PSNR比强基线提升高达5 dB。

Conclusion: 噪声-频率延续框架通过频带受限的测量一致性，有效解决了扩散后验采样中的细节恢复问题，显著提升了逆问题求解的性能。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [22] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: CamReasoner框架将相机运动理解重构为结构化推理过程，采用观察-思考-回答范式，通过大规模推理轨迹数据集和强化学习实现几何基础的相机动态理解，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型将相机动态理解视为黑盒分类任务，主要依赖表面视觉模式而非几何线索，容易混淆物理上不同的运动。需要弥合感知与电影逻辑之间的差距。

Method: 提出CamReasoner框架，采用观察-思考-回答结构化推理范式，构建大规模推理轨迹数据集（18k SFT推理链和38k RL反馈样本），首次在该领域使用强化学习进行逻辑对齐，确保运动推理基于物理几何而非上下文猜测。

Result: CamReasoner有效抑制幻觉，在多个基准测试中实现了最先进的性能。

Conclusion: 通过将相机运动理解重构为结构化推理过程，并采用强化学习进行逻辑对齐，CamReasoner能够基于几何线索而非表面模式理解相机动态，为视频空间智能提供了更可靠的方法。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [23] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 该论文发现当前深度伪造检测器主要依赖全局伪影而非局部合成内容，通过引入INP-X操作和数据集，揭示了VAE重建导致的频谱偏移问题，使现有检测器准确率大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现代基于深度学习的图像修复技术能够实现逼真的局部图像编辑，这对可靠检测提出了严峻挑战。研究者观察到当前检测器主要依赖作为修复副作用的全局伪影，而非局部合成内容本身，这导致检测存在根本性缺陷。

Method: 提出了Inpainting Exchange (INP-X)操作，该操作在保留所有合成内容的同时，恢复编辑区域外的原始像素。创建了包含9万张真实、修复和交换图像的测试数据集。通过理论分析将检测器失效行为与VAE信息瓶颈导致的高频衰减联系起来。

Result: 在INP-X干预下，预训练的最先进检测器（包括商业检测器）准确率急剧下降（例如从91%降至55%），经常接近随机猜测水平。使用该数据集训练能够获得比标准修复检测更好的泛化能力和定位能力。

Conclusion: 研究揭示了当前检测器对全局伪影而非局部合成内容的依赖问题，强调了内容感知检测的必要性。提出的INP-X操作和数据集为开发更鲁棒的检测方法提供了重要基础。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [24] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 提出多跳视觉推理链框架，将无监督医学图像配准重新定义为渐进推理过程，通过局部空间细化和交叉参考注意力机制实现高精度、可解释的配准。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在无监督医学图像配准中虽然精度较高，但缺乏透明度，导致误差漂移和临床信任度降低。需要一种既准确又具有可解释性的配准方法。

Method: 提出多跳视觉推理链框架，将配准视为渐进推理过程。每个视觉推理跳包含局部空间细化模块和交叉参考注意力机制，通过多跳策略处理大变形，生成透明中间预测序列。

Result: 在DIR-Lab 4D CT（肺部）和IXI T1加权MRI（脑部）两个挑战性公共数据集上的评估表明，VCoR实现了竞争性的配准精度，同时提供丰富的中间可视化和置信度度量。

Conclusion: 通过嵌入隐式视觉推理范式，提出了一种可解释、可靠且临床可行的无监督医学图像配准方法，在保持精度的同时增强了透明度和临床信任度。

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [25] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 提出MFM-Geom几何多模态基础模型，结合双参数MRI和临床报告，利用对称正定矩阵和黎曼深度学习进行前列腺癌识别，在少量数据下表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 前列腺癌是男性常见癌症，双参数MRI和临床变量对识别和治疗决策至关重要，但现有过程依赖专家主观判断，且多数计算机辅助诊断方法仅关注影像而忽略临床背景，受数据稀缺限制难以学习稳健表示

Method: 提出MFM-Geom几何多模态基础模型，从双参数MRI和临床报告中学习表示，编码视觉发现和临床变量信息；在表示分类头中，利用对称正定矩阵和黎曼深度学习整合来自生物医学多模态基础模型的影像-文本表示

Result: 仅使用10%训练数据，MFM-Geom优于基于类别标记嵌入的基线分类方法（提升8.3%，AUC-PR达90.67）；在外部数据集上的泛化验证了微调生物医学基础模型的稳健性，达到AUC-PR 90.6

Conclusion: MFM-Geom通过整合多模态数据和几何深度学习方法，有效解决了前列腺癌诊断中的主观性和数据稀缺问题，展示了在少量数据下学习稳健表示的能力

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [26] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: 开发了一款离线可用的移动应用，通过深度学习模型识别可可病害，帮助菲律宾小农户改善作物健康管理


<details>
  <summary>Details</summary>
Motivation: 菲律宾可可小农户面临技术落后、病虫害严重、信息获取困难等问题，缺乏大型种植园的资源和技术支持，需要适合偏远地区的解决方案

Method: 开发离线移动应用程序，集成深度学习模型进行可可病害识别和管理，模型训练后部署到移动端，支持田间诊断

Result: 病害识别模型验证准确率达96.93%，可可黑荚感染程度检测模型准确率79.49%，现场测试与专家评估一致性达84.2%

Conclusion: 该技术方案为小农户提供了可访问的技术工具，能够改善可可作物健康和提高生产力，特别适合偏远地区使用

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [27] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: 本文提出CAPA框架，通过注意力贡献度进行视觉令牌剪枝和FFN线性近似，在保持性能的同时显著提升大型视觉语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型推理效率受限于处理数千个视觉令牌的高成本，但哪些令牌和计算可以安全移除尚不明确。现有基于注意力分数的令牌重要性评估方法不够准确。

Method: 提出注意力贡献度作为更准确的视觉令牌选择标准，识别出可剪枝的概率转储和必需的结构锚点。基于此开发CAPA框架：在关键功能转换处使用注意力贡献度剪枝视觉令牌，并通过高效的线性近似减少FFN计算。

Result: 在多个基准测试和基线模型上的实验表明，CAPA实现了优秀的效率-性能权衡，并提高了模型的鲁棒性。

Conclusion: 注意力贡献度是比传统注意力分数更准确的视觉令牌重要性评估指标，CAPA框架通过双策略方法有效提升大型视觉语言模型的推理效率，同时保持模型性能。

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [28] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: SANEval是一个新的文本到图像模型评估基准，通过结合大语言模型和开放词汇目标检测器，解决了现有基准在词汇封闭、诊断能力不足、缺乏可解释反馈等问题，实现了对多对象、属性和空间关系的细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂提示（涉及多个对象、属性和空间关系）时存在瓶颈，但缺乏足够的评估方法。现有基准通常受限于封闭词汇表，缺乏细粒度诊断能力，无法提供可解释的反馈来诊断和修复特定的组合失败问题。

Method: 提出SANEval（空间、属性和数字评估）基准，建立了一个可扩展的开放词汇组合评估流程。该方法结合大语言模型进行深度提示理解，并使用LLM增强的开放词汇目标检测器来稳健评估组合一致性，不受固定词汇表的限制。

Result: 在六个最先进的文本到图像模型上进行广泛实验，证明SANEval的自动评估能更准确地反映人类评估结果。该指标在属性绑定、空间关系和数字能力等任务上，与现有基准相比具有统计上不同的斯皮尔曼等级相关性结果。

Conclusion: SANEval为组合文本到图像生成和评估的未来研究提供了全面的基准，将发布SANEval数据集和开源评估流程，以促进该领域的发展。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [29] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: World-Shaper是一个统一的几何感知框架，直接在等距柱状投影域中实现全景图像编辑，通过生成-编辑范式解决配对数据稀缺问题，并引入几何感知学习策略处理几何失真。


<details>
  <summary>Details</summary>
Motivation: 现有基于透视的图像编辑方法无法建模全景图像的空间结构，传统的立方体贴图分解方法由于与球面几何不匹配而破坏全局一致性，因此需要直接在等距柱状投影域中重新构建全景编辑方法。

Method: 采用生成-编辑范式，首先通过可控全景生成合成多样化的配对示例用于监督编辑学习；引入几何感知学习策略，显式地强制执行位置感知形状监督，并通过渐进训练隐式地内化全景先验知识。

Result: 在PEBench新基准上的广泛实验表明，该方法在几何一致性、编辑保真度和文本可控性方面优于最先进方法，能够实现具有统一编辑控制的连贯灵活360°视觉世界创建。

Conclusion: World-Shaper框架成功地在等距柱状投影域中实现了全景图像编辑，通过几何感知学习策略解决了传统方法的几何失真问题，为创建连贯的360°视觉体验提供了有效解决方案。

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [30] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: PLACID是一个利用预训练图像到视频扩散模型进行多物体合成的框架，通过视频时序先验保持物体一致性，并使用合成数据训练实现随机初始化的物体收敛到连贯布局。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在工作室级别的多物体合成方面存在不足，无法同时满足：完美保持物体身份、精确背景和颜色保真度、布局和设计元素控制、完整且吸引人的展示。现有模型常改变物体细节、遗漏或重复物体、产生尺寸错误或不一致的布局。

Method: 1. 利用预训练的文本控制图像到视频扩散模型，通过视频时序先验保持物体一致性、身份和背景细节。2. 提出新颖的数据策展策略，生成合成序列：随机放置的物体平滑移动到目标位置，这些合成数据在训练时与视频模型的时序先验对齐。

Result: 广泛的定量评估和用户研究表明，PLACID在多物体合成方面超越了最先进的方法，实现了更优的身份、背景和颜色保持，减少了物体遗漏，产生了视觉上更吸引人的结果。

Conclusion: PLACID框架成功解决了工作室级别多物体合成的挑战，通过结合视频时序先验和合成数据训练，能够生成保持物体一致性、背景保真度和视觉吸引力的高质量合成图像。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [31] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 提出一种推理时方法，通过识别和移除不稳定的潜在标记来缓解自回归视频生成中的时间漂移问题


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成在生成长视频时存在严重的时间漂移问题，错误会随时间累积放大。作者认为这主要源于推理时的错误传播，而非模型容量不足

Method: 在推理时识别不稳定的潜在标记（其表示与先前生成批次显著偏离），并在自回归上下文中移除这些被污染的标记，防止不可靠信息影响后续生成

Result: 该方法显著改善了长时域时间一致性，无需修改模型架构、训练过程或离开潜在空间

Conclusion: 通过推理时移除不稳定的潜在标记，可以有效缓解自回归视频生成中的时间漂移问题，提高长视频生成的时间一致性

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [32] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind是一个诊断性基准测试，用于评估多模态大语言模型在细粒度时空理解上的能力，通过视频对共享相同静态内容但不同时间结构的对比设计，揭示模型主要依赖静态视觉捷径而非真正的时间逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在静态语义理解上表现出色，但对时间动态的理解仍然脆弱。视频推理和具身AI需要细粒度的时空理解能力，而现有基准测试往往将识别与时间推理混为一谈，无法准确评估模型的时间理解能力。

Method: TimeBlind采用认知科学启发的三层次分类：识别原子事件、描述事件属性、推理事件间依赖关系。使用最小对范式：视频对共享完全相同的静态视觉内容，仅时间结构不同，配合互补问题设计来消除语言先验。包含600个精心策划的实例（2400个视频-问题对）。

Result: 评估了20多个最先进的MLLMs（包括GPT-5、Gemini 3 Pro等），最佳模型的实例准确率（正确区分视频对中的两个视频）仅为48.2%，远低于人类表现（98.2%）。这表明前沿模型严重依赖静态视觉捷径而非真正的时间逻辑推理。

Conclusion: TimeBlind揭示了当前多模态大语言模型在时间理解上的根本缺陷，为下一代视频理解系统提供了重要的诊断工具。数据集和代码已开源，可用于推动模型在时空推理能力上的进步。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [33] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: LogicGaze是一个评估视觉语言模型在序列推理中视觉基础能力的新基准框架，专注于检测模型是否能够验证因果链的真实性并拒绝视觉矛盾的扰动


<details>
  <summary>Details</summary>
Motivation: 虽然序列推理增强了视觉语言模型执行复杂多模态任务的能力，但它们在将推理链基于实际视觉证据方面的可靠性尚未得到充分探索。当前模型存在幻觉问题，需要评估其验证序列因果链的能力

Method: 从ShareGPT4Video的40,000个视频片段和Flickr30k图像子集中构建数据集，整合因果序列与视觉矛盾但语言合理的扰动。采用三重评估协议：因果验证、基础叙事合成和扰动拒绝

Result: 评估揭示了最先进的视觉语言模型（如Qwen2.5-VL-72B）存在显著脆弱性，表明它们在验证序列推理步骤真实性方面存在不足

Conclusion: LogicGaze框架为评估多模态推理的鲁棒性和可信度提供了重要工具，所有资源已在匿名存储库中公开，推动更可靠、可信的多模态推理发展

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [34] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: SAM2CT：首个可提示分割模型，可将放射科医生的稀疏标注（箭头/线条）转换为CT体积的3D分割，利用历史GSPS标注大规模生成分割数据集


<details>
  <summary>Details</summary>
Motivation: CT影像机器学习模型需要大量高质量标注数据，但3D分割标注成本高昂。临床PACS中已有大量CT图像和报告，放射科医生常规标注的稀疏标注（如线条测量和箭头）可作为潜在数据源，但需要有效方法将其转换为3D分割。

Method: 提出SAM2CT模型，基于SAM2扩展提示编码器以支持箭头和线条输入，并引入Memory-Conditioned Memories（MCM）内存编码策略，专门针对3D医学体积设计，实现从稀疏标注到3D分割的转换。

Result: 在公共病变分割基准测试中，SAM2CT优于现有可提示分割模型和类似训练的基线，箭头提示Dice相似系数0.649，线条提示0.757。在临床PACS的60个GSPS标注上，87%的分割结果临床可接受或仅需微调。在急诊科特定发现上表现出强零样本性能。

Conclusion: 大规模挖掘历史GSPS标注是生成3D CT分割数据集的有前景且可扩展的方法，SAM2CT为此提供了有效的技术解决方案。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [35] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: 论文通过集成多个计算机视觉模型评估自动驾驶感知系统在恶劣条件下的鲁棒性，发现低光照条件对感知性能影响最大，且距离越远感知鲁棒性越差。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的可行性严重依赖感知系统的性能，这些系统不仅要在理想条件下可靠工作，还要在自然和对抗性驾驶因素干扰下保持准确。感知错误和检测延迟可能导致严重后果，因此需要评估自动驾驶车辆感知系统的鲁棒性并探索提高可靠性的策略。

Method: 使用基于模型集成的预测敏感性量化方法评估感知性能，捕捉多个模型之间的分歧和推理变异性。在模拟环境和真实世界条件下测试恶劣驾驶场景，提出感知性能评估的概念架构。基于自动驾驶车辆在不同路面（干湿沥青）和速度下在停车标志前的停车距离开发感知评估标准。实验使用了五种最先进的计算机视觉模型：YOLO (v8-v9)、DETR50、DETR101和RT-DETR。

Result: 低光照条件（如雾和低太阳高度）对感知模型性能影响最大。对抗性道路条件（如道路物体遮挡）会增加感知敏感性，当对抗性道路条件与恶劣天气条件结合时，模型性能下降更明显。此外，距离道路物体越远，对感知性能的影响越大，感知鲁棒性越差。

Conclusion: 自动驾驶感知系统在恶劣条件下的鲁棒性评估至关重要，特别是低光照条件和远距离目标检测对感知性能影响显著。通过模型集成和敏感性量化方法可以有效评估感知系统的可靠性，为改进自动驾驶感知技术提供重要参考。

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [36] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: MAD-RAG：一种无需训练的双问题干预方法，解决检索增强生成中的注意力分散问题，通过解耦视觉定位和上下文整合来提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将RAG失败归因于对检索上下文关注不足，但本文发现了一个被忽视的失败模式：注意力分散。当检索上下文足够相关时，检索文本会全局抑制视觉注意力，导致注意力从问题相关区域转移，使原本能正确回答的问题失败。

Method: 提出MAD-RAG，一种无需训练的干预方法，通过双问题表述解耦视觉定位和上下文整合，结合注意力混合来保留图像条件证据。

Result: 在OK-VQA、E-VQA和InfoSeek数据集上的实验表明，MAD-RAG在不同模型家族中始终优于现有基线，相对于原始RAG基线分别获得最高4.76%、9.20%和6.18%的绝对增益。成功纠正了高达74.68%的失败案例，且计算开销可忽略。

Conclusion: 注意力分散是RAG在视觉问答中的一个重要失败模式，MAD-RAG通过解耦视觉定位和上下文整合有效缓解了这一问题，显著提升了模型性能且计算成本低。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [37] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: ReLAPSe是一个基于强化学习的对抗框架，用于从未学习过的扩散模型中恢复概念，通过策略学习而非逐实例优化实现高效概念恢复。


<details>
  <summary>Details</summary>
Motivation: 现有对抗方法存在局限性：基于优化的方法计算成本高（需要逐实例迭代搜索），而基于推理和启发式的方法缺乏目标模型潜在视觉表示的反馈。需要一种更高效、可扩展的方法来测试未学习模型的鲁棒性。

Method: 将概念恢复重新表述为强化学习问题，使用带有可验证奖励的强化学习（RLVR）训练智能体，利用扩散模型的噪声预测损失作为模型内在且可验证的反馈信号。这种闭环设计直接对齐文本提示操作与潜在视觉残差。

Result: ReLAPSe能够高效、近乎实时地恢复细粒度身份和风格，跨越多种最先进的未学习方法，为未学习扩散模型提供了可扩展的严格红队测试工具。

Conclusion: 通过从逐实例优化转向全局策略学习，ReLAPSe实现了对未学习扩散模型概念恢复的高效方法，为评估未学习方法的鲁棒性提供了新工具。

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [38] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 该研究提出使用比较学习而非直接评分来评估图像描述质量，通过对比两个描述哪个更好匹配图像来训练模型，显著降低了人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 人类对图像描述进行直接评分既耗时又主观，而比较两个描述哪个更好匹配图像则相对容易。因此需要一种能够利用这种比较判断而非直接评分的机器学习框架。

Method: 使用VICR数据集，通过ResNet-50提取视觉特征，MiniLM提取文本特征，同时训练回归模型和比较学习模型。比较学习模型通过人类对两个描述的偏好判断进行训练。

Result: 回归模型表现更好（皮尔逊相关系数0.7609，斯皮尔曼相关系数0.7089），但比较学习模型随着数据增加稳步提升并接近回归基线。人类评估显示比较标注速度更快且标注者间一致性更高。

Conclusion: 比较学习能够有效建模人类偏好，同时显著降低人工标注成本，为图像描述质量评估提供了一种更高效的方法。

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [39] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: 本研究对自动驾驶系统中的两种主流目标检测模型YOLOv5和Faster R-CNN进行了全面实验分析，发现YOLOv5在mAP、召回率和训练效率方面表现更优，而Faster R-CNN在检测小目标、远距离物体和复杂光照条件下具有优势。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的目标检测技术对车辆感知环境至关重要，但现有深度学习模型（如YOLO、SSD、Faster R-CNN）在特定自动驾驶应用场景中的适用性指导有限。不同方法的选择会显著影响系统性能、鲁棒性和效率，因此需要对这些模型进行系统比较分析。

Method: 本研究采用实验分析方法，比较两种主流目标检测模型：单阶段检测器YOLOv5和两阶段检测器Faster R-CNN。使用包含真实和合成图像的多样化数据集进行评估，考虑平均精度均值（mAP）、召回率和推理速度等多种指标。同时分析了不同置信度阈值和真实场景下的模型表现。

Result: YOLOv5在mAP、召回率和训练效率方面表现更优，特别是在数据集规模和图像分辨率增加时优势更明显。Faster R-CNN在检测小目标、远距离物体方面具有优势，在挑战性光照条件下表现良好。两种模型在不同置信度阈值和真实场景下的行为特征也得到了分析。

Conclusion: 研究为自动驾驶系统选择合适的目标检测模型提供了实用指导：YOLOv5适合需要高效率和良好整体性能的场景，而Faster R-CNN更适合需要检测小目标和应对复杂环境条件的应用。两种模型各有优势，应根据具体应用需求进行选择。

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [40] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: 提出基于动态4D-CTA脑部扫描的血管标注新方法，通过多时相数据增强血管可视化，训练深度学习模型实现高质量脑动脉和静脉分割。


<details>
  <summary>Details</summary>
Motivation: 传统脑血管标注需要大量人工努力，且CTA扫描中的骨骼和软组织干扰血管可视化。本研究旨在开发一种减少人工标注负担、提高血管分割准确性的方法。

Method: 利用动态4D-CTA的多时相数据，通过减影技术去除骨骼和软组织干扰，增强血管可视化。使用同一分割标注应用于多个时相，将数据集扩大4-5倍，并训练nnUNet模型。数据集包含25名患者的110张训练图像和14名患者的165张测试图像。

Result: 在TopBrain数据集上，动脉平均mDC达到0.846，静脉达到0.957。平均定向Hausdorff距离（动脉0.304mm，静脉0.078mm）和拓扑敏感性（动脉0.877，静脉0.974）均显示优异性能，显著优于其他类似规模数据集。

Conclusion: 提出的动态4D-CTA血管标注方法有效减少了人工标注负担，通过数据增强提高了模型鲁棒性，在脑动脉和静脉分割任务中实现了最先进的性能，代码和模型权重已开源。

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [41] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 该研究评估了基于Transformer的视觉语言模型在巴西葡萄牙语图像描述任务中的表现，比较了原生标注和自动翻译数据集，发现Swin-DistilBERTimbau模型表现最佳，ViTucano在文本指标上优于大型多语言模型，GPT-4在图像-文本对齐方面表现最好。


<details>
  <summary>Details</summary>
Motivation: 当前图像描述研究主要集中在英语模型上，巴西葡萄牙语等低资源语言面临专业数据集和模型缺乏的挑战。现有研究通过自动翻译现有数据集来缓解资源稀缺问题，但需要评估这种方法的有效性。

Method: 使用包含巴西葡萄牙语母语者手动创建描述的Flickr30K版本，与自动从英语翻译到葡萄牙语的版本进行比较。采用跨上下文方法，在一个数据集上训练的模型在另一个数据集上测试以评估翻译影响。结合注意力图进行模型推理解释，并使用CLIP-Score指标评估图像-描述对齐。

Result: Swin-DistilBERTimbau在所有模型中表现最稳定，展现出强大的跨数据集泛化能力。巴西葡萄牙语预训练模型ViTucano在传统文本评估指标上超越了更大的多语言模型（GPT-4o、LLaMa 3.2 Vision）。GPT-4模型获得最高的CLIP-Score，表明其图像-文本对齐能力更强。注意力分析揭示了系统性偏见，包括性别误分类、对象枚举错误和空间不一致性。

Conclusion: 该研究为巴西葡萄牙语图像描述任务提供了重要基准，展示了跨原生-翻译评估的价值。结果表明专门针对目标语言预训练的模型在文本指标上可能优于更大的通用模型，而大型多语言模型在图像-文本对齐方面表现更佳。注意力分析揭示了模型偏见，为未来改进提供了方向。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [42] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 3DGS²-TR是一种用于加速3D高斯泼溅场景训练的二阶优化器，通过Hessian矩阵对角近似和参数级信任区域技术，在减少50%训练迭代的同时获得更好重建质量，内存开销仅比ADAM多17%


<details>
  <summary>Details</summary>
Motivation: 现有二阶优化方法（如3DGS-LM和3DGS2）依赖显式或密集曲率表示，计算和内存成本高，难以扩展到大规模场景。需要一种高效、内存友好的二阶优化器来加速3DGS训练。

Method: 1) 使用Hutchinson方法仅近似Hessian矩阵的对角元素，实现完全矩阵自由；2) 引入基于平方Hellinger距离的参数级信任区域技术，正则化高斯参数更新；3) 计算和内存复杂度与ADAM相同（O(n)）。

Result: 在相同参数初始化和无致密化条件下，相比ADAM：1) 使用50%更少训练迭代获得更好重建质量；2) 峰值GPU内存开销仅增加17%（小于1GB）；3) 比3DGS-LM内存减少85%；4) 支持扩展到超大场景和分布式训练。

Conclusion: 3DGS²-TR是一种高效、内存友好的二阶优化器，通过Hessian对角近似和参数级信任区域技术，显著加速3DGS训练，在保持高质量重建的同时大幅降低计算和内存需求，为大规模场景训练提供了可行方案。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [43] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: 该研究开发了一个实验室安全监控系统，通过生成合成数据集评估视觉语言模型在实验室安全监控中的表现，发现模型在文本场景图输入时表现良好，但在纯视觉输入时性能下降，为此提出了场景图引导对齐方法来提升视觉输入下的危险检测性能。


<details>
  <summary>Details</summary>
Motivation: 实验室中轻微的不安全行为可能导致严重伤害，但持续的安全监控受到人力资源限制。虽然视觉语言模型有望实现自主实验室安全监控，但由于缺乏视觉评估数据（大多数安全事故主要记录为非结构化文本），其在真实环境中的有效性尚不清楚。

Method: 1. 引入结构化数据生成流程，将文本实验室场景转换为对齐的三元组（图像、场景图、真实标签），使用大语言模型作为场景图架构师，图像生成模型作为渲染器；2. 在包含1,207个样本和362个独特场景的合成数据集上评估7个开源和闭源模型；3. 提出后训练上下文工程方法——场景图引导对齐，通过将视觉输入转换为结构化场景图来弥合视觉语言模型的感知差距。

Result: 实验表明，视觉语言模型在给定文本场景图时表现有效，但在纯视觉设置中性能显著下降，表明模型难以直接从像素中提取结构化对象关系。提出的场景图引导对齐方法能够改善视觉语言模型在纯视觉设置下的危险检测性能。

Conclusion: 视觉语言模型在实验室安全监控中具有潜力，但需要解决从视觉输入中提取结构化关系的挑战。通过场景图引导对齐方法可以弥合感知差距，提升模型在真实视觉环境中的危险检测能力。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [44] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Text-DJ是一种针对大型视觉语言模型的新型越狱攻击，通过将有害查询分解为多个语义相关但更良性的子查询，并添加大量无关的干扰查询，以图像网格形式呈现，成功绕过模型的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型的安全防护主要关注显式文本输入或相关视觉场景分析，但忽略了模型光学字符识别能力可能被利用的漏洞。研究者旨在探索通过OCR功能绕过安全防护的新攻击方式。

Method: 方法分为三个阶段：1) 将单个有害查询分解为多个语义相关但更良性的子查询；2) 选择与有害查询最大程度无关的干扰查询集合；3) 将所有子查询和干扰查询以图像网格形式同时呈现给模型，其中子查询位于网格中间位置。

Result: 该方法成功绕过了最先进大型视觉语言模型的安全对齐机制。攻击成功的原因包括：1) 将文本提示转换为图像，绕过标准文本过滤器；2) 引入干扰，使模型的安全协议无法在大量无关查询中链接分散的子查询。

Conclusion: 研究揭示了大型视觉语言模型OCR能力对分散、多图像对抗性输入的脆弱性，突显了需要为碎片化多模态输入开发防御机制的必要性。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [45] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: DISK是一种无需训练的自适应推理方法，用于自回归世界模型，通过双分支控制器协调视频和自我轨迹的扩散变换器，在保持性能的同时实现2倍速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决自回归世界模型中视频和轨迹预测的计算成本高问题，需要在保持运动-外观一致性的同时实现高效推理。

Method: 使用双分支控制器协调两个耦合的扩散变换器（视频和轨迹），通过跨模态跳过决策和高阶潜在差异跳过测试，在自回归前向链中传播控制器统计信息。

Result: 在NuPlan和NuScenes数据集上，轨迹扩散速度提升2倍，视频扩散速度提升1.6倍，同时保持L2规划误差、视觉质量（FID/FVD）和NAVSIM PDMS分数。

Conclusion: DISK能够以显著降低的成本实现实用的长时域视频和轨迹预测，为自回归世界模型提供了高效的推理解决方案。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [46] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: LatentLens是一种新的可解释性方法，通过将视觉标记表示与大型文本语料库中的上下文化文本表示进行比较，为视觉语言模型中的视觉标记提供自然语言描述，显著提高了视觉标记的可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解为什么大型语言模型能够轻松处理视觉标记，需要开发能够揭示LLM处理过程中每一层视觉标记表示内容的可解释性方法。现有方法（如LogitLens）严重低估了视觉标记的可解释性。

Method: LatentLens方法：1）编码大型文本语料库并存储每个标记的上下文化表示；2）将视觉标记表示与这些文本表示进行比较；3）通过top-k最近邻表示提供视觉标记的自然语言描述。

Result: 在10个不同的VLM上评估，LatentLens显示大多数视觉标记在所有研究模型和所有层中都是可解释的。定性分析表明，LatentLens产生的描述具有语义意义，比单个标记提供更细粒度的人类可理解解释。

Conclusion: LatentLens为视觉标记提供了有效的可解释性方法，证明了视觉和语言表示之间的对齐，为分析潜在表示开辟了新方向。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [47] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: PSGS是一个两阶段框架，通过双层优化架构生成语义连贯的全景图，然后通过全景滑动机制初始化全局一致的3D高斯点云，实现从文本生成高质量3D场景。


<details>
  <summary>Details</summary>
Motivation: 从文本生成逼真的3D场景对于VR、AR和游戏等沉浸式应用至关重要。现有方法受限于有限的3D-文本数据和不一致的多视角拼接，导致场景过于简化。

Method: 1. 双层优化架构：布局推理层将文本解析为结构化空间关系，自优化层通过迭代MLLM反馈细化视觉细节；2. 全景滑动机制：通过策略性采样重叠视角初始化全局一致的3D高斯点云；3. 训练时加入深度和语义一致性损失。

Result: PSGS在全景图生成方面优于现有方法，并能产生更具吸引力的3D场景，为可扩展的沉浸式内容创作提供了稳健解决方案。

Conclusion: PSGS框架通过结合语义连贯的全景生成和全局一致的3D重建，有效解决了文本驱动3D场景生成中的关键挑战，实现了高质量的沉浸式内容创建。

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [48] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: ZS-TreeSeg是一个零样本树冠分割框架，通过将树冠建模为星凸对象，利用Cellpose-SAM在拓扑流场中实现数学分离，无需训练即可处理密集重叠树冠


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法标注成本高且泛化能力有限，而基础模型（如SAM）缺乏领域知识，在密集树冠中容易欠分割。需要一种无需训练、能处理密集重叠树冠的解决方案

Method: 提出ZS-TreeSeg零样本框架，从两个成熟任务（冠层语义分割和细胞实例分割）迁移知识，将树冠建模为星凸对象，在拓扑流场中使用Cellpose-SAM基于向量收敛实现数学分离

Result: 在NEON和BAMFOREST数据集上的实验和视觉检查表明，该框架在不同传感器类型和冠层密度下具有鲁棒泛化能力，为树冠实例分割和标签生成提供了无需训练的解决方案

Conclusion: ZS-TreeSeg通过零样本方法成功解决了密集重叠树冠的分割难题，为森林生物量估算和生态监测提供了高效、无需训练的工具

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [49] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: GTATrack是一个用于鱼眼相机足球比赛多目标跟踪的层次化框架，通过Deep-EIoU进行运动无关的在线关联和GTA进行轨迹级优化，在SoccerTrack Challenge 2025中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 体育场景中的多目标跟踪面临球员运动不规则、外观相似、频繁遮挡等挑战，而静态鱼眼相机引入的几何畸变和极端尺度变化进一步加剧了这些困难。

Method: 提出GTATrack层次化跟踪框架，包含两个核心组件：Deep Expansion IoU用于运动无关的在线关联，Global Tracklet Association用于轨迹级优化。采用两阶段设计实现短期匹配和长期身份一致性，并使用伪标签策略提升小目标和畸变目标的检测召回率。

Result: 在SoccerTrack Challenge 2025中获得第一名，HOTA得分0.60，显著减少误报至982个，在鱼眼相机足球跟踪中达到最先进的准确率。

Conclusion: GTATrack通过局部关联和全局推理的协同作用，有效解决了身份切换、遮挡和跟踪碎片化问题，为鱼眼相机体育跟踪提供了有效的解决方案。

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [50] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: SketchMod通过变换源笔划来对齐目标草图的模式，实现灵活的笔画级草图编辑，通过学习缩放、方向和位置三个关键偏移属性来调整源笔划，使其与目标草图在空间比例、局部几何和语义布局上对齐。


<details>
  <summary>Details</summary>
Motivation: 现有草图编辑方法仅通过重新定位源笔划来实现编辑，但当源笔划在大小和方向上存在显著差异时，仅重新定位而不进行进一步调整无法产生合理的编辑结果。例如，将过大的源笔划直接锚定到目标草图上而不进行适当缩放会导致语义不一致。

Method: 提出SketchMod方法，通过学习三个关键偏移属性（缩放、方向和位置）来细化源笔划，使其与目标草图对齐：1）通过缩放匹配空间比例；2）通过旋转对齐局部几何；3）通过位移满足语义布局。同时通过暴露捕获的笔划属性来精确控制笔划轮廓。

Result: 实验结果表明，SketchMod在笔画级草图编辑上实现了精确和灵活的性能表现。

Conclusion: SketchMod通过变换源笔划来对齐目标草图的模式，实现了更灵活和精确的笔画级草图编辑，解决了仅重新定位源笔划而不进行适当调整的局限性。

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [51] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: RGBX-R1框架通过UAV提示策略构建视觉模态思维链，采用两阶段训练范式（CS-SFT和ST-RFT），将MLLM从RGB模态扩展到红外、深度、事件等X模态，在RGBX-Grounding基准上显著优于基线方法22.71%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要基于RGB模态预训练，限制了在红外、深度、事件等其他视觉模态上的性能，而这些模态在复杂场景中至关重要。需要扩展MLLM的感知和推理能力以覆盖多种X视觉模态。

Method: 提出RGBX-R1框架：1) 使用UAV（理解-关联-验证）提示策略构建视觉模态思维链（VM-CoT），将RGB理解能力扩展到X模态；2) 采用两阶段训练范式：冷启动监督微调（CS-SFT）在VM-CoT指导下监督推理过程，赋予基础模态认知；时空强化微调（ST-RFT）基于GRPO，使用模态理解时空奖励（MuST）强化模态推理。

Result: 构建了首个RGBX-Grounding基准，大量实验验证了该方法在多模态理解和空间感知方面的优越性，在三个RGBX grounding任务上比基线方法提升了22.71%。

Conclusion: RGBX-R1框架通过创新的提示策略和两阶段训练范式，成功扩展了MLLM对多种视觉模态的感知和推理能力，为解决复杂场景中的多模态理解问题提供了有效方案。

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [52] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: SparseCut是一种用于多模态大语言模型（MLLMs）的通用跨模态融合架构，通过稀疏快捷连接实现多层次视觉特征的高效分层融合，提升跨模态理解能力而不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要关注扩大语言模型规模或构建更高质量的训练数据，但很少关注如何有效将跨模态知识整合到语言空间中。特别是在视觉语言模型中，仅使用高层视觉特征进行模态对齐往往会丢弃中低层特征中的丰富语义信息，限制了模型的跨模态理解能力。

Method: 提出SparseCut架构，在跨模态编码器和LLM之间引入稀疏快捷连接，实现多层次视觉特征的高效分层融合。进一步引入高效多粒度特征融合模块，在通过快捷连接路由之前进行视觉特征融合，保持原始语言上下文且不增加整体输入长度。

Result: 实验表明，SparseCut显著提升了MLLMs在各种多模态基准测试中的性能，对不同基础LLM具有通用性和可扩展性。

Conclusion: SparseCut通过稀疏快捷连接和多粒度特征融合，有效解决了MLLMs中跨模态知识整合不足的问题，在保持计算效率的同时显著提升了跨模态理解能力。

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [53] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 该论文提出了一种新的无训练分割方法，将分割重新定义为扩散诱导亲和图上的随机流平衡问题，通过马尔可夫传播方案实现零样本分割，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有无训练分割方法存在多个根本性缺陷：1）基于谱图分割假设，需要预先选择聚类数量；2）谱松弛导致边界过度平滑；3）对噪声或多模态亲和分布高度敏感；4）忽视局部邻域结构的重要性，而这对稳定亲和传播和保留细粒度轮廓至关重要。

Method: 将无训练分割重新定义为扩散诱导亲和图上的随机流平衡问题，引入马尔可夫传播方案，执行基于随机游走的标签扩散，采用自适应剪枝策略抑制不可靠的转移同时增强置信度高的亲和路径，整合全局扩散注意力与从稳定扩散中提取的局部邻域。

Result: 在七个广泛使用的语义分割基准测试中，该方法实现了最先进的零样本性能，相比基于谱聚类的方法，产生了更锐利的边界、更连贯的区域和显著更稳定的掩码。

Conclusion: 通过将分割重新定义为随机流平衡问题并引入马尔可夫传播方案，成功克服了传统谱图分割方法的局限性，在无训练分割领域取得了显著改进，特别是在边界质量和区域一致性方面。

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [54] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 提出MRAD框架，用直接内存检索替代参数拟合，实现零样本异常检测，包含无需训练的基础版本和两个轻量级变体，在16个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法通常使用提示学习或复杂建模来拟合数据分布，导致训练/推理成本高且跨域稳定性有限。需要一种更高效、稳定的方法。

Method: 提出MRAD框架：1) MRAD-TF冻结CLIP图像编码器，构建图像级和像素级两级记忆库；2) MRAD-FT用两个线性层微调检索度量；3) MRAD-CLIP将正常/异常区域先验注入CLIP文本提示作为动态偏置。

Result: 在16个工业和医学数据集上，MRAD框架在异常分类和分割任务中均表现出优越性能，无论是无需训练还是基于训练的设置下都表现一致。

Conclusion: 充分利用原始数据的经验分布而非仅依赖模型拟合，可以实现更强的异常检测性能。直接内存检索是有效的零样本异常检测方法。

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [55] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: SAGE是一种动态调整推测解码树结构的方法，通过实时预测不确定性优化视觉语言模型的推理加速，相比静态树方法获得更长的接受长度和更快的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法使用静态树结构，无法适应不同生成步骤中预测难度的变化，导致接受长度不理想和加速效果有限。需要一种能根据实时预测不确定性动态调整树结构的方法。

Method: 提出SAGE框架，利用输出熵作为置信度指标，根据实时预测不确定性动态调整推测树结构：对高置信度预测构建更深更窄的树以最大化推测深度，对不确定预测构建更浅更宽的树以多样化探索。

Result: SAGE在多个基准测试中表现出色：在不损失输出质量的情况下，为LLaVA-OneVision-72B提供高达3.36倍的解码加速，为Qwen2.5-VL-72B提供高达3.18倍的解码加速。

Conclusion: SAGE通过动态调整推测树结构，有效解决了静态树方法在适应不同预测难度方面的局限性，显著提高了视觉语言模型的推理效率。

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [56] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet是一种完全非参数化的3D点云分类和部件分割方法，不包含学习权重，使用确定性算子构建点特征，通过自适应高斯-傅里叶位置编码实现跨尺度和采样密度的稳定性。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需学习权重的非参数化3D点云处理方法，能够适应不同尺度和采样密度，在少样本设置下表现优异，同时具有较好的内存使用和推理时间效率。

Method: 使用确定性算子（最远点采样、k近邻、池化）构建点特征，核心是自适应高斯-傅里叶位置编码，其带宽和高斯-余弦混合参数根据输入几何自适应选择。对于分割任务，额外加入固定频率的傅里叶特征提供全局上下文。

Result: 在ModelNet40/ModelNet-R、ScanObjectNN和ShapeNetPart数据集上，NPNet在非参数化基线方法中表现强劲，特别是在ModelNet40的少样本设置下效果显著，相比之前的非参数化方法具有更好的内存使用和推理时间。

Conclusion: NPNet证明了完全非参数化方法在3D点云处理任务中的有效性，通过自适应位置编码机制解决了跨尺度和密度变化的稳定性问题，为少样本学习和资源受限场景提供了有前景的解决方案。

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [57] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: GLAD是一种创新的生成式语言辅助跟踪模型，利用扩散模型进行文本描述和模板图像的生成式多模态融合，以增强语言和图像之间的兼容性，提升模板图像语义信息，从而在低语义图像场景下实现更好的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言跟踪方法在处理低语义图像（如模糊、低分辨率等）时面临挑战，这些图像会损害跨模态理解性能。虽然语言辅助通常用于处理这些问题，但由于文本和视觉特征之间存在差距，直接的特征拼接和融合效果有限。

Method: 提出GLAD模型，采用扩散模型进行生成式多模态融合，将文本描述和模板图像进行融合，增强语言和图像之间的兼容性，提升模板图像的语义信息。这种方法能够恢复模糊和语义模糊的模板图像，改善多模态特征。

Result: 实验表明，该方法在多个基准测试中建立了新的最先进水平，并实现了令人印象深刻的推理速度。相比现有的融合范式有显著改进。

Conclusion: GLAD通过生成式多模态融合有效解决了低语义图像对视觉语言跟踪性能的影响，提高了跨模态理解的鲁棒性，为视觉语言跟踪领域提供了新的解决方案。

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [58] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: BDG方法通过多角度多尺度灰度共生矩阵进行退化类型和程度的精细判别，并将扩散训练分为生成、桥接和恢复三个阶段，在保持纹理恢复能力的同时整合判别信息，提升多任务多退化场景下的图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 通用图像恢复需要同时解决高质量图像分布采样和基于退化调整输出的挑战，现有方法难以在保持丰富纹理恢复能力的同时有效处理多种退化类型和程度。

Method: 提出BDG框架：1) 设计MAS-GLCM进行退化类型和程度的精细判别；2) 将扩散训练分为生成、桥接和恢复三个阶段，在保持扩散模型纹理恢复能力的同时整合MAS-GLCM的判别信息。

Result: 在不改变架构的情况下，BDG在全能恢复和真实世界超分辨率任务中取得显著性能提升，主要表现为保真度大幅提高且不损害感知质量。

Conclusion: BDG通过桥接退化判别和生成过程，有效解决了通用图像恢复中的关键挑战，在多种退化场景下实现了优异的性能平衡。

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [59] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: MAUGen是一个基于扩散模型的多模态框架，能够通过文本提示生成逼真面部表情和对应的AU标签，并创建了大规模合成数据集MIFA。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、人口多样性且带有精确AU发生和强度标注的面部图像数据，是开发泛化性AU识别系统的主要瓶颈。

Method: 提出MAUGen框架，包含两个关键模块：1）多模态表示学习模块，在统一潜在空间中捕捉文本描述、面部身份、表情图像和AU激活之间的关系；2）基于扩散的图像标签生成器，将联合表示解码为对齐的面部图像-标签对。

Result: 创建了MIFA数据集，包含全面的AU标注和身份变化。实验表明MAUGen在合成逼真、人口多样性面部图像和语义对齐AU标签方面优于现有方法。

Conclusion: MAUGen通过生成大规模合成数据集，解决了AU识别领域的数据稀缺问题，为开发更鲁棒的AU识别系统提供了重要资源。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [60] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种可调节风格强度的3D风格迁移方法Tune-Your-Style，允许用户灵活调整场景中的风格注入强度，以满足不同的内容-风格平衡需求。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法采用固定输出范式，难以适应不同用户对内容-风格平衡的多样化需求。需要一种能够灵活调整风格强度的可定制化解决方案。

Method: 引入高斯神经元显式建模风格强度，参数化可学习风格调节器实现强度可调的风格注入。提出可调风格化引导，通过跨视图风格对齐从扩散模型获得多视图一致的风格化视图，采用两阶段优化策略通过调节全风格引导和零风格引导之间的平衡提供稳定高效的指导。

Result: 实验表明该方法不仅能够产生视觉上吸引人的结果，而且在3D风格迁移方面展现出灵活的定制能力。

Conclusion: 提出的Tune-Your-Style范式通过可调节风格强度增强了3D风格迁移的定制性，为满足用户多样化的内容-风格平衡需求提供了有效解决方案。

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [61] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 该研究通过稀疏自编码器分析LVLM的内部表示机制，发现幻觉主要源于图像特定神经元的异常激活，并提出对比神经元引导方法在预填充阶段增强视觉基础、减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有缓解LVLM幻觉的方法主要关注输出层调整，而对产生幻觉的内部机制缺乏深入理解。作者希望从表示层面探究幻觉产生的根源，为更有效的干预提供理论基础。

Method: 引入稀疏自编码器分解稠密视觉嵌入为稀疏可解释神经元；通过神经元级分析识别不同类型神经元；提出对比神经元引导方法，通过对比干净和噪声输入识别图像特定神经元，选择性增强信息性神经元并抑制扰动诱导的激活。

Result: 研究发现幻觉主要源于图像特定神经元的破坏或虚假激活，而始终激活神经元保持稳定；CNS方法在幻觉检测和通用多模态基准测试中均能有效减少幻觉，同时保持整体多模态理解能力。

Conclusion: 从表示层面分析LVLM幻觉机制为理解其内部工作原理提供了新视角；CNS方法通过在预填充阶段操作，与现有解码阶段方法完全兼容，为减少幻觉提供了有效且可解释的解决方案。

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [62] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: FaceSnap：基于Stable Diffusion的单参考图像个性化肖像生成方法，无需微调即可在单次推理中实现高保真面部细节


<details>
  <summary>Details</summary>
Motivation: 现有个性化肖像生成方法要么需要耗时微调且缺乏泛化性，要么无法实现面部细节的高保真度，需要一种更高效、更精确的解决方案

Method: 提出FaceSnap方法，包含三个核心模块：1）面部属性混合器从低层具体特征和高层抽象特征提取融合信息；2）关键点预测器保持不同姿态下的参考身份一致性；3）身份保持模块将信息注入UNet

Result: 实验结果表明，FaceSnap在个性化和定制化肖像生成方面表现优异，超越了该领域的其他最先进方法

Conclusion: FaceSnap是一种即插即用的高效方法，仅需单张参考图像即可在单次推理中生成高度一致的面部细节，具有良好的扩展性和实用性

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [63] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: VIZOR是一个无需训练、端到端的框架，直接从原始3D场景构建密集、视角不变的3D场景图，使用相对于物体正面方向的空间关系定义，实现跨视角一致性，并在零样本物体定位任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解方法通常需要多模态输入（2D图像、深度图、物体标签等），且从特定参考视角构建场景图，导致空间关系（如"左/右"）在不同视角下不一致，泛化能力有限。

Method: 提出VIZOR框架：1）直接从原始3D场景构建密集场景图；2）空间关系基于每个物体的正面方向定义，实现视角不变性；3）无需训练数据即可推断开放词汇的空间和邻近关系；4）完全训练自由、端到端。

Result: 在场景图生成和下游任务（如查询式物体定位）上进行了广泛评估。VIZOR在场景图生成方面优于现有方法，在Replica和Nr3D数据集上的零样本定位准确率分别提升22%和4.81%。

Conclusion: VIZOR通过视角不变的空间关系定义和无需训练的方法，有效解决了现有3D场景图生成方法的泛化问题和视角依赖性问题，在场景理解和推理任务中表现出色。

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [64] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: Diff-PC是一个基于扩散模型的零样本肖像定制框架，通过3D面部先验、ID编码器、ID控制器和ID注入器等技术，实现高身份保真度、精确面部控制和多样化背景的肖像生成。


<details>
  <summary>Details</summary>
Motivation: 现有肖像定制方法在身份保真度和面部控制方面存在不足，需要开发能够精确保持身份特征、控制面部属性并生成多样化背景的肖像生成方法。

Method: 1. 使用3D面部预测器重建包含参考身份、目标表情和姿态的3D感知面部先验；2. 设计ID编码器融合局部和全局面部特征；3. 开发ID控制器利用3D面部引导身份特征对齐；4. 引入ID注入器增强身份保真度和面部可控性；5. 在收集的身份中心数据集上进行训练。

Result: Diff-PC在身份保真度、面部控制和文本-图像一致性方面超越了现有最先进方法，并且兼容多种风格的基础模型。

Conclusion: Diff-PC是一个有效的零样本肖像定制框架，能够生成具有高身份保真度、精确面部控制和多样化背景的逼真肖像，解决了现有方法的局限性。

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [65] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: NOVA是一种非对比视觉语言对齐框架，通过联合嵌入预测和分布正则化，无需负采样、动量编码器或梯度停止，简化了训练过程


<details>
  <summary>Details</summary>
Motivation: 当前主流的对比学习方法（如CLIP）需要大批量、精心设计的负采样和大量超参数调优，训练过程复杂且不稳定。研究者希望开发一种更简单、更稳定的非对比视觉语言对齐方法

Method: NOVA通过预测增强图像视图的文本嵌入来对齐视觉表示到冻结的领域特定文本编码器，同时通过Sketched Isotropic Gaussian Regularization (SIGReg)强制各向同性高斯结构，消除负采样、动量编码器或梯度停止的需求

Result: 在胸部X光零样本分类任务中，使用ClinicalBERT作为文本编码器和从头训练的Vision Transformers，NOVA在三个基准数据集上优于多个标准基线方法，同时表现出更一致的训练运行

Conclusion: 非对比视觉语言预训练为对比方法提供了更简单、更稳定、更有效的替代方案，NOVA框架通过简化训练目标（仅需一个超参数）实现了这一目标

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [66] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出一种计算高效的超分辨率方法，从各向异性的3D解剖照片重建中生成解剖一致的各向同性体积，提高神经病理学分析的精度


<details>
  <summary>Details</summary>
Motivation: 现有方法从2D解剖照片重建3D脑体积时，在高各向异性（厚切片）情况下会产生粗糙、过度平滑的结构重建，影响解剖学准确性和形态测量精度

Method: 引入计算高效的超分辨率步骤，通过插值切片从各向异性的3D重建生成解剖一致的各向同性体积；使用领域随机化的合成数据进行训练，确保方法在不同解剖协议和大切片厚度下的泛化能力

Result: 插值后的体积改善了自动分割效果，获得了更高的Dice分数，特别是在皮质和白质区域；在表面重建和图谱配准任务验证中，显示了更准确的皮质表面和MRI配准

Conclusion: 通过提高基于照片的重建的分辨率和解剖保真度，该方法加强了神经病理学与神经影像学之间的联系，方法已公开可用

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [67] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: 该论文通过五个创新贡献探索如何利用视频元素间的时间关系提升视频理解能力，包括自动标注框架、参数高效微调策略、长视频建模、细粒度运动关系建模以及大视觉语言模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法在处理时间关系方面存在局限，需要更有效地利用视频元素间的时间关系来提升模型对视频内容动态特性的理解和推理能力。

Method: 1) 基于大视觉语言模型的自动标注框架，采用抗噪声对比学习目标和减性角度边界；2) 使用"循环适配器"的参数高效微调策略，在低数据环境下捕捉时间动态；3) 集成状态空间层进行高效长视频建模，并引入两个新的长时基准；4) 新颖的对比学习框架，显式建模运动与视频片段间的细粒度关系；5) 对大视觉语言模型的全面实证研究。

Result: 研究表明显式时间建模能显著提升模型对视频内容的表示和推理能力，识别出视觉-语言接口是时间推理的瓶颈，并提出"时间导向的配方"来提升视频理解能力。

Conclusion: 通过五个方面的创新贡献，论文证明了显式时间建模对于提升视频理解能力的重要性，为处理视频内容的动态特性提供了系统性的解决方案。

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [68] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar是一个能够生成长时间虚拟人视频的框架，通过双教师增强训练算法和多模态条件动态调制技术，显著提升了文本指令对齐能力，支持复杂全身动作、动态相机轨迹和背景转换。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人模型在说话、公开演讲和唱歌等场景表现出色，但在文本指令对齐方面存在局限，特别是当提示涉及复杂元素如大幅全身动作、动态相机轨迹、背景转换或人机交互时。

Method: 1. 引入双教师增强训练算法，使模型能够从基础模型转移固有的文本可控性，同时学习音频-视觉同步。2. 在训练过程中，基于不同的去噪时间步动态调制多模态条件（如音频和文本）的强度，以减轻异构条件信号之间的冲突。

Result: GSB评估结果显示，JoyAvatar模型在生成自然、时间连贯的全身动作和动态相机运动方面优于Omnihuman-1.5和KlingAvatar 2.0等最先进模型，同时保持了准确的唇形同步和身份一致性等基本虚拟人能力。

Conclusion: JoyAvatar通过创新的训练算法和条件调制技术，显著扩展了虚拟人模型的能力，支持复杂应用如多人对话和非人类角色扮演，为高质量虚拟人视频生成提供了有效解决方案。

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [69] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: 该研究提出了一种针对高粱气孔成分的半监督实例分割框架，通过补丁预处理和伪标签策略显著提升了微小气孔结构的检测性能。


<details>
  <summary>Details</summary>
Motivation: 高粱作为耐旱作物对气候适应性农业至关重要，但气孔自动分析困难，因为气孔尺寸小（小于40μm）且在不同基因型和叶面间形状多变。现有方法在处理嵌套微小结构和标注瓶颈方面面临挑战。

Method: 提出半监督实例分割框架：1）收集并标注包含11,060个人工标注补丁的高粱叶片图像数据集，涵盖三个气孔成分（气孔孔、保卫细胞和复合区域）；2）将高分辨率显微图像分割为重叠小补丁以改善微小结构检测；3）对未标注图像应用伪标签策略，生成额外56,428个伪标注补丁。

Result: 基准测试显示显著性能提升：语义分割模型的最高mIoU从65.93%提升至70.35%，实例分割模型的最高AP从28.30%提升至46.10%。

Conclusion: 结合补丁预处理和半监督学习能显著改善精细气孔结构的分割，该框架支持可扩展的气孔性状提取，促进AI驱动表型分析在作物科学中的广泛应用。

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [70] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的化妆迁移方法，通过构建高质量数据集、设计特征解耦框架和文本引导机制，解决了现有方法在数据集限制、特征解耦和可控性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有化妆迁移方法存在三个主要问题：1) 数据集有限且质量不高；2) 身份特征和化妆特征解耦不充分；3) 可控性较弱，难以实现细粒度和区域特定的化妆控制。

Method: 1) 采用"训练-生成-过滤-再训练"策略构建高质量数据集；2) 设计基于扩散模型的框架，解耦身份和化妆特征；3) 提出文本引导机制，支持自然语言提示的细粒度区域控制。

Result: 在基准测试和实际场景中，该方法在保真度、身份保持和灵活性方面均有显著提升，能够准确应用多样化化妆风格，同时保持面部结构和肤色。

Conclusion: 该方法通过高质量数据集、特征解耦框架和文本引导控制，为化妆迁移任务提供了更稳定、可控和高质量的解决方案，相比GAN方法具有明显优势。

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [71] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: 提出一种基于扩散的算法，用于从双层点云中分离内层和外层表面，特别针对TSDF融合中截断引起的"双层伪影"问题，适用于具有开放边界的点云。


<details>
  <summary>Details</summary>
Motivation: 在室内或医学3D重建中，TSDF融合的截断会导致"双层伪影"问题，产生错误的内层和外层表面。现有方法难以处理具有开放边界的点云，需要一种轻量级的后处理模块来准确分离内层和外层表面。

Method: 采用基于扩散的算法，专门处理具有开放边界的点云（即存在拓扑开口/孔洞的表面），而不是表面区域缺失的点云。该方法能够稳健处理水密和开放边界模型，从20,000个内层点和20,000个外层点中提取内层表面。

Result: 算法能够在约10秒内从双层点云中提取内层表面，有效解决了重叠表面和法线混乱等问题。特别适用于室内场景建模和医学成像等需要精确表面表示的应用。

Conclusion: 该方法提供了一个轻量级的后处理模块，用于TSDF融合后的内层/外层表面分离，不旨在替代完整的变分或基于学习的重建流程，但能有效处理双层伪影问题，支持水密和开放边界几何。

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [72] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: HSI-VAR将高光谱图像修复重新定义为自回归生成问题，通过渐进建模光谱和空间依赖关系，在保持结构细节的同时大幅降低计算成本，相比扩散模型实现95.5倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 真实世界的高光谱图像常受噪声、模糊和波段缺失等多种退化影响。现有扩散模型方法需要数百次迭代步骤，计算成本过高；而回归模型则产生过度平滑的结果，无法保留关键结构细节。

Method: HSI-VAR将HSI修复重新定义为自回归生成问题，包含三个关键创新：1) 潜在条件对齐，耦合潜在先验和条件嵌入的语义一致性；2) 退化感知引导，将混合退化编码为嵌入空间中的线性组合；3) 空间-光谱适应模块，在解码阶段细化两个域的细节。

Result: 在九个一体化HSI修复基准测试中，HSI-VAR达到最先进性能，在ICVL数据集上实现3.77 dB PSNR提升，推理速度比基于扩散的方法快95.5倍，计算成本降低近50%。

Conclusion: HSI-VAR通过自回归生成方法有效解决了高光谱图像修复的计算效率和结构保持问题，为真实世界HSI修复提供了高度实用的解决方案。

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [73] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 本研究评估了基于U-Net的深度学习模型在臂丛神经超声图像分割中的表现，重点关注数据集组成和标注策略对分割性能的影响。研究发现多设备数据训练对低性能采集源有正则化效果，多类别监督会降低神经分割性能，神经尺寸与分割精度呈正相关。


<details>
  <summary>Details</summary>
Motivation: 超声引导下区域麻醉的成功依赖于准确的神经定位，但由于图像对比度低、斑点噪声大以及患者间解剖结构变异，手动识别仍然具有挑战性。本研究旨在评估深度学习在超声神经分割中的应用，为临床数据约束下开发鲁棒的神经分割系统提供方法学指导。

Method: 使用U-Net架构进行深度学习神经分割，研究数据集组成（来自SIEMENS ACUSON NX3 Elite和Philips EPIQ5两种超声设备）和标注策略（从二分类神经分割扩展到多类别监督：动脉、静脉、神经、肌肉）对分割性能的影响。

Result: 1. 多设备数据训练对低性能采集源有正则化效果，但无法超越目标域匹配的单源训练；2. 多类别监督导致神经特异性Dice分数下降9%-61%，可能由于类别不平衡和边界模糊；3. 神经尺寸与分割精度呈中度正相关（Pearson r=0.587, p<0.001），小神经仍是主要挑战。

Conclusion: 研究结果为在现实临床数据约束下开发鲁棒的超声神经分割系统提供了方法学指导，强调了数据集组成、标注策略和神经尺寸对分割性能的重要影响，为优化超声引导区域麻醉的深度学习应用提供了实证依据。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [74] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: DVLA-RL是一种新的少样本学习方法，通过双级视觉语言对齐和强化学习门控机制，在仅使用少量样本的情况下实现类特定判别和泛化表示，在九个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法虽然引入大语言模型通过类别名称生成语义嵌入来丰富视觉表示，但忽视了从低层到高层语义的渐进自适应对齐，导致语义增益有限。需要解决视觉和语言之间更精细的对齐问题。

Method: 提出DVLA-RL框架，包含双级语义构建(DSC)和RL门控注意力(RLA)。DSC基于类别名称和支持样本生成判别性属性，渐进选择最相关属性并合成连贯类别描述。RLA将跨模态融合建模为序列决策过程，通过轻量级策略自适应调整自注意力和交叉注意力的贡献。

Result: DVLA-RL在三种不同少样本学习场景的九个基准测试中实现了新的最先进性能，表明该方法能够仅用少量支持样本实现类特定判别和泛化表示。

Conclusion: DVLA-RL通过双级视觉语言对齐和强化学习门控机制，实现了从低层属性到高层语义的渐进自适应对齐，有效提升了少样本学习的性能，为视觉语言融合提供了新思路。

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [75] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 该研究提出Any3D-VLA模型，通过将3D点云信息与2D视觉表示融合，增强视觉-语言-动作模型在复杂场景中的空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要使用2D图像作为视觉输入，限制了其在复杂场景中的空间理解能力。研究旨在探索如何通过融入3D信息来增强VLA模型的能力。

Method: 提出Any3D-VLA框架，统一模拟器、传感器和模型估计的点云数据，构建多样化输入，学习领域无关的3D表示并与对应的2D表示融合，解决3D数据稀缺和跨环境领域差异问题。

Result: 实验表明，将视觉输入显式提升为点云能产生比2D表示更好的补充表示。Any3D-VLA在模拟和真实世界实验中展现出性能提升和领域差距缓解的优势。

Conclusion: 通过融合3D点云信息，Any3D-VLA有效增强了VLA模型的空间理解能力，解决了3D数据稀缺和跨环境领域差异的挑战，为复杂场景下的视觉-语言-动作任务提供了更强大的解决方案。

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [76] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: Paracosm是一种零样本组合图像检索方法，通过大型多模态模型直接生成"心理图像"来匹配目标图像，无需训练即可在多个基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 组合图像检索的核心挑战在于"心理图像"仅由多模态查询隐式定义而无法物理获取。现有方法使用LMM生成文本描述再进行文本-视觉匹配，但这种方法不够直接准确。

Method: 提出Paracosm方法：1) 使用LMM为多模态查询直接生成"心理图像"；2) 为数据库中的每个真实图像生成对应的合成版本以减少合成-真实域差距；3) 在LMM构建的"paracosm"中进行匹配。

Result: Paracosm在四个具有挑战性的基准测试中显著优于现有的零样本方法，实现了零样本组合图像检索的最先进性能。

Conclusion: 通过直接生成"心理图像"而非文本描述，Paracosm提供了一种更准确、训练免费的零样本组合图像检索方法，克服了现有方法的局限性。

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [77] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: 提出一种用于临床皮肤科联邦学习的身份无关病理保护框架，通过无反转的Rectified Flow Transformers实现高保真身份转换，在边缘设备上生成隐私合规的合成替代图像，保护患者隐私同时保持诊断特征。


<details>
  <summary>Details</summary>
Motivation: 临床皮肤科联邦学习面临患者隐私保护与诊断特征保留的矛盾需求。传统去识别方法会降低病理保真度，而标准生成编辑技术依赖计算密集的反转过程，不适合资源受限的边缘设备。

Method: 提出身份无关病理保护框架，利用无反转的Rectified Flow Transformers（FlowEdit）实现高保真身份转换；引入"分段合成"机制，在本地生成反事实的健康和病理双胞胎对；提取与生物标记和语义伪影解耦的差异红斑掩码。

Result: 系统在近实时（少于20秒）内完成身份转换，适合临床节点本地部署；在高分辨率临床样本上验证显示，合成身份间的IoU稳定性大于0.67；生成隐私合规的合成替代图像，从源头减轻梯度泄漏风险。

Conclusion: 该框架通过边缘生成隐私合规的合成替代图像，为联邦环境中的高精度皮肤图像分析提供了安全途径，解决了隐私保护与病理保真度之间的平衡问题。

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [78] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: TransNormal：利用预训练扩散先验和DINOv3语义的单步法向估计框架，专为透明物体设计，在ClearGrasp和ClearPose基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 透明物体的单目法向估计对实验室自动化至关重要，但由于复杂的光线折射和反射，传统深度和法向传感器经常失败，阻碍了具身AI在科学环境中的部署。

Method: 提出TransNormal框架，通过交叉注意力机制整合DINOv3的密集视觉语义提供几何线索，采用多任务学习目标和基于小波的正则化来保留细粒度结构细节，并创建了TransNormal-Synthetic物理数据集。

Result: 在ClearGrasp基准上，平均误差降低24.4%，11.25°准确率提升22.8%；在ClearPose基准上，平均误差降低15.2%，显著优于现有最先进方法。

Conclusion: TransNormal通过整合预训练扩散先验和DINOv3语义，有效解决了透明物体法向估计的挑战，为实验室自动化和具身AI应用提供了可靠解决方案。

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [79] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于二阶几何统计的视觉地点识别框架，无需训练即可捕捉几何稳定性，在零样本场景下表现出色


<details>
  <summary>Details</summary>
Motivation: 当前视觉地点识别方法要么依赖数据密集型监督，要么使用简单的一阶统计，往往忽略了内在的结构相关性，难以应对剧烈的环境和视角变化

Method: 将场景建模为对称正定流形上的协方差描述符，扰动表现为可处理的同余变换，通过几何感知的黎曼映射将这些描述符投影到线性化的欧几里得嵌入中，从而解耦信号结构和噪声

Result: 该方法在零样本场景下实现了高度竞争力的性能，特别是在具有挑战性的零样本场景中表现出色，超越了现有的最先进基线方法

Conclusion: 提出的二阶几何统计框架为视觉地点识别提供了一种无需训练的有效解决方案，能够捕捉几何稳定性并实现强大的零样本泛化能力

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [80] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: Distill3R框架将大型3D基础模型的几何推理能力蒸馏到可在单工作站训练的小型学生模型中，参数减少9倍，推理速度提升5倍，训练时间从GPU集群一周缩短到单工作站3天。


<details>
  <summary>Details</summary>
Motivation: 当前多视图3D重建依赖需要大规模计算集群训练的基础模型，这为大多数学术实验室设置了高门槛。为了解决计算资源不平等问题，需要开发能在单工作站上训练的高效模型。

Method: 1. 离线缓存管道：将繁重的教师模型推理与训练循环解耦，通过压缩的监督信号进行训练；2. 置信感知蒸馏损失：利用教师模型的不确定性信息，使在普通硬件上训练成为可能。

Result: 提出的7200万参数学生模型相比6.5亿参数教师模型，参数减少9倍，推理速度提升5倍。学生模型可在单工作站3天内完成训练，而教师模型需要GPU集群训练一周。学生模型保持了结构一致性和定性几何理解能力。

Conclusion: Distill3R为没有大规模计算资源的实验室提供了可复现的单工作站训练方案，促进了3D视觉研究的民主化和边缘部署的高效化，旨在为特定领域数据训练提供可访问的研究基线。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [81] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: DIAMOND是一种无需训练的方法，通过轨迹校正来减少文本到图像生成中的伪影，无需修改模型权重或进行区域细化


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如FLUX）仍存在视觉和解剖伪影问题，现有方法多为后处理方式，无法在核心图像形成过程中有效干预，且需要修改模型权重或计算成本高昂的区域细化

Method: DIAMOND采用无训练方法，在推理过程中应用轨迹校正，通过在每个生成步骤重建干净样本的估计，主动引导生成过程远离导致伪影的潜在状态

Result: 该方法可扩展到标准扩散模型，为高保真、无伪影图像合成提供鲁棒的零样本路径，无需额外训练或权重修改

Conclusion: DIAMOND提供了一种有效的训练免费方法，可在现代生成架构中实现高质量、无伪影的图像合成，解决了现有方法的局限性

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [82] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: ConsensusDrop是一种无需训练的视觉语言模型token压缩框架，通过融合视觉编码器显著性和LLM跨注意力信号，实现高效token剪枝和合并，在保持准确性的同时大幅减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大量冗余视觉token导致计算成本高昂。现有token缩减方法要么使用视觉编码器显著性（广泛但查询无关），要么使用LLM跨注意力（查询感知但稀疏且成本高），两者单独使用都不够充分。

Method: 提出ConsensusDrop框架：1) 通过协调视觉编码器显著性和查询感知的跨注意力信号，推导出共识排名；2) 保留信息最丰富的token，同时通过编码器引导的token合并压缩其余token；3) 无需训练，适用于多种开源VLM。

Result: 在LLaVA-1.5/NeXT、Video-LLaVA等开源VLM上，ConsensusDrop在相同token预算下优于先前的剪枝方法，提供更强的准确性-效率帕累托前沿，即使在激进的token缩减下也能保持接近基线的准确性，同时减少TTFT和KV缓存占用。

Conclusion: 融合视觉编码器显著性和LLM跨注意力信号比单一模态token选择更有效。ConsensusDrop通过共识排名和token合并实现了高效且准确的视觉token压缩，为视觉语言模型的实际部署提供了实用的解决方案。

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [83] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 该论文提出了HitEmotion基准测试和ToM引导的推理方法，用于评估和增强多模态大语言模型的深层情感理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型发展迅速，但其深层情感理解能力仍然有限。作者认为真正的情感智能需要显式建模心智理论，因为情感源于这一认知基础。

Method: 1. 引入HitEmotion基准测试：基于心智理论的分层基准，诊断不同认知深度的能力断点；2. 提出ToM引导的推理链：追踪心理状态并校准跨模态证据以实现忠实的情感推理；3. 提出TMPO方法：使用强化学习，以中间心理状态作为过程级监督来指导和加强模型推理。

Result: 实验表明：1. HitEmotion暴露了最先进模型在深层情感推理上的缺陷，特别是在认知要求高的任务上；2. ToM引导的推理链和TMPO提高了最终任务准确性，并产生了更忠实、更连贯的推理过程。

Conclusion: 该工作为研究社区提供了实用的工具包，用于评估和增强多模态大语言模型基于认知的情感理解能力。数据集和代码已开源。

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [84] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: VAMOS-OCTA：一种用于修复OCTA运动伪影的深度学习框架，通过血管感知多轴监督恢复被运动损坏的B扫描图像


<details>
  <summary>Details</summary>
Motivation: 手持式OCTA在非合作或儿科患者中易受运动伪影影响，导致3D采集时出现未采样区域，在en face投影中产生空白带，严重影响图像质量

Method: 使用2.5D U-Net架构，以相邻B扫描堆栈为输入重建损坏的中心B扫描，采用新颖的血管感知多轴正交监督（VAMOS）损失函数，结合血管加权强度重建与轴向和横向投影一致性

Result: VAMOS-OCTA在合成和真实世界损坏数据上均优于现有方法，能产生具有清晰毛细血管、恢复血管连续性和干净en face投影的重建结果

Conclusion: 多轴监督为恢复运动退化的3D OCTA数据提供了强大的约束，VAMOS-OCTA能同时增强B扫描清晰度和体积投影准确性

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [85] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: SRVAU-R1提出了一种反思增强的视频异常理解框架，通过引入反思导向的思维链数据集和反思感知学习范式，显著提升了多模态大语言模型在视频异常理解任务中的深度推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的方法在视频异常理解任务中主要关注表面描述，缺乏对异常行为的深度推理，如显式的自我反思和自我修正能力。

Method: 提出了SRVAU-R1框架：1）创建首个面向视频异常理解的反思导向思维链数据集，包含初始推理、自我反思和修正推理的结构化监督；2）采用反思感知学习范式，结合监督微调和强化微调来增强多模态推理能力。

Result: 在多个视频异常基准测试上的实验表明，SRVAU-R1在时间异常定位准确性和推理质量方面均显著优于现有方法。

Conclusion: 通过引入反思机制，SRVAU-R1成功提升了多模态大语言模型在视频异常理解中的深度推理能力，为相关任务提供了有效的解决方案。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [86] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: LocalScore是一种用于开放集生物识别的评分算法，通过利用k近邻显式结合图库特征分布的局部密度，显著提升开放集检索和验证性能，且具有架构无关、损失独立、计算开销小的特点。


<details>
  <summary>Details</summary>
Motivation: 开放集生物识别面临未注册探针的挑战，现有方法通常将同一主体的多个样本压缩为单一全局表示，导致次优决策边界和较差的开放集鲁棒性，而现实部署中多样本图库越来越普遍。

Method: 提出LocalScore评分算法，利用k近邻显式结合图库特征分布的局部密度，该方法是架构无关、损失独立的，计算开销小，可作为现有生物识别系统的即插即用解决方案。

Result: 在多模态实验中，LocalScore在开放集检索方面将FNIR@FPIR从53%降低到40%，在验证方面将TAR@FAR从51%提升到74%，均取得显著增益。理论分析和实证验证解释了该方法在不同数据集特性下的最佳增益条件。

Conclusion: LocalScore通过显式建模图库特征的局部密度分布，有效解决了开放集生物识别中未注册探针的检测问题，为现有系统提供了简单高效的改进方案。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [87] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 自动标注的甲状腺结节数据集能显著提升深度学习模型性能，使用全部数据比仅用高精度子集效果更好


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节超声图像诊断中，深度学习模型训练数据获取困难，先前研究提出的自动标注方法效果未知，需要验证自动标注数据对模型性能的实际提升效果

Method: 在手动标注和自动标注数据集上分别训练深度学习模型，同时使用自动标注数据集中精度较高的子集进行训练，比较不同数据集训练模型的性能差异

Result: 手动标注数据集训练的模型AUC为0.643，自动标注数据集训练的模型AUC为0.694（P<0.001），高精度子集训练的模型AUC为0.689（P>0.43），自动标注数据显著优于手动标注数据，且全部数据优于高精度子集

Conclusion: 自动标注数据集能显著提升深度学习算法性能，建议使用全部自动标注数据而非仅使用高精度子集

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [88] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: FUSE-Flow：一种帧级、无状态、线性可扩展的点云流式重建框架，通过自适应空间哈希加权聚合实现实时多视角点云重建，在保持几何细节的同时抑制噪声。


<details>
  <summary>Details</summary>
Motivation: 实时多视角点云重建在VR、AR、机器人导航、数字孪生等领域有广泛应用，但现有方法（基于体素融合、时间累积或全局优化）存在计算复杂度高、内存占用大、可扩展性有限的问题，难以同时实现实时性能、重建质量和多相机可扩展性。

Method: 提出FUSE-Flow框架：1）每帧独立生成点云片段，通过测量置信度和3D距离一致性两个权重进行融合以抑制噪声并保留几何细节；2）引入基于自适应空间哈希的加权聚合方法，根据局部点云密度自适应划分3D空间，每个单元选择代表性点进行加权融合，处理稀疏和密集区域；3）利用GPU并行化实现高吞吐、低延迟的点云生成与融合，具有线性复杂度。

Result: 实验表明，该框架在重叠、深度不连续和动态场景中提高了重建稳定性和几何保真度，同时在现代GPU上保持实时帧率，验证了其有效性、鲁棒性和可扩展性。

Conclusion: FUSE-Flow成功解决了实时多视角点云重建中的关键挑战，通过帧级无状态设计和自适应空间哈希加权聚合，实现了线性可扩展的高质量实时重建，为VR、AR、机器人导航等应用提供了有效的解决方案。

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [89] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出VEQ框架，针对MoE视觉语言模型的量化问题，通过双感知量化方法同时处理跨模态差异和专家异质性，显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: MoE视觉语言模型虽然性能优异，但内存和计算成本过高，需要压缩。现有量化方法忽视了两种关键异质性：视觉和语言token之间的固有差异，以及不同专家的非均匀贡献。

Method: 提出视觉专家量化(VEQ)框架，包含两个核心组件：1)模态专家感知量化，利用专家激活频率优先最小化关键专家的误差；2)模态亲和感知量化，通过整合token-专家亲和度与模态信息构建增强的Hessian矩阵来指导校准过程。

Result: 在W3A16配置下，VEQ在Kimi-VL上平均准确率提升2.04%，在Qwen3-VL上提升3.09%，优于现有最先进的量化方法，在各种多模态任务中表现出优越的鲁棒性。

Conclusion: VEQ通过同时考虑跨模态差异和专家异质性，为MoE视觉语言模型提供了一种有效的训练后量化解决方案，显著提升了量化性能。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [90] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: 本文提出了一个自动将单人教学视频转换为双人多模态任务指导对话的框架，并创建了HowToDIV数据集，包含507个对话、6,636个问答对和24小时视频，为多模态程序性任务辅助提供了基准。


<details>
  <summary>Details</summary>
Motivation: 日常任务需要专业知识，但AI助手在AR辅助方面进展受限，主要原因是缺乏大规模多模态对话数据集，而人工收集数据成本高、复杂度大。

Method: 基于大语言模型构建全自动流水线，将单人教学视频自动转换为专家-新手双人多模态任务指导对话，提供可扩展且成本效益高的数据收集替代方案。

Result: 创建了HowToDIV多模态数据集，包含507个对话、6,636个问答对和24小时视频，涵盖多个领域，每个会话包含多轮专家-新手交互，并使用Gemma 3和Qwen 2.5提供了基准结果。

Conclusion: 提出的框架为多模态程序性任务辅助提供了可扩展的数据生成方法，HowToDIV数据集为相关研究提供了有价值的基准资源。

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [91] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: ReLayout是一个无需三元组数据、支持多种编辑操作的设计布局编辑框架，通过关系图保持未编辑元素的布局结构，使用多模态大语言模型实现自监督学习。


<details>
  <summary>Details</summary>
Motivation: 设计布局编辑是设计工作流中的关键任务，但面临用户需求表达模糊、缺乏编辑操作标准化格式、需要保持未编辑元素布局结构、以及缺乏（原始设计、编辑操作、编辑后设计）三元组数据等挑战。

Method: 1. 引入四种基本编辑操作并标准化编辑操作格式；2. 提出关系图作为未编辑元素位置和大小关系的约束；3. 开发关系感知设计重建（RADR）方法，通过从元素、关系图和合成编辑操作重建设计来模拟编辑过程；4. 使用多模态大语言模型作为RADR骨干，统一多种编辑操作。

Result: 定性和定量评估以及用户研究表明，ReLayout在编辑质量、准确性和布局结构保持方面显著优于基线模型。

Conclusion: ReLayout框架成功解决了设计布局编辑中的数据稀缺和结构保持问题，实现了无需三元组数据的多功能设计布局编辑，为自动化设计工作流提供了重要进展。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [92] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: ResDec是一种无需训练的新方法，利用历史信息和LVLM的内部推理机制来减少语言先验导致的幻觉，提升视觉基础能力


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然在多模态任务中表现良好，但容易受到语言先验影响产生幻觉，即生成语法正确但与实际视觉输入不匹配的内容

Method: 提出Residual Decoding (ResDec)方法，这是一种无需训练的解码方法，利用历史信息辅助解码，依赖LVLM的内部隐式推理机制和token logits演化机制来纠正偏差

Result: ResDec能有效抑制语言先验导致的幻觉，显著提升视觉基础能力，减少物体幻觉，同时在综合LVLM基准测试中表现优异

Conclusion: ResDec是一种有效的训练免费方法，能够减少LVLM的幻觉问题，提升模型性能，具有广泛适用性

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [93] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯射线追踪的断层扫描重建框架，相比现有的基于splatting的模型，通过解析计算线积分避免了局部仿射近似误差，并支持非线性几何校正。


<details>
  <summary>Details</summary>
Motivation: 现有的R2-Gaussian方法在断层扫描重建中采用局部仿射近似，将3D高斯映射到2D探测器上，这种近似会降低重建的定量精度，并且难以整合非线性几何校正。

Method: 提出基于3D高斯射线追踪的断层扫描重建框架，通过解析计算射线穿过3D高斯基元的线积分，避免局部仿射坍塌，同时射线追踪公式提供了对射线起点和方向的显式控制。

Result: 该方法提供了比splatting模型更物理一致的前向投影模型，能够精确应用非线性几何校正（如PET中的弧校正），扩展了高斯基重建在真实断层扫描系统中的适用性。

Conclusion: 基于3D高斯射线追踪的框架克服了现有splatting方法的局限性，提高了投影精度，为更广泛的断层扫描系统提供了更准确的重建解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [94] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: 提出DRFormer框架，通过双正则化双向Transformer融合DINO的局部纹理特征和CLIP的全局语义特征，解决行人重识别中的遮挡和姿态变化问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单一范式（要么基于DINO的局部纹理特征，要么基于CLIP的全局语义特征），忽略了两种架构的互补优势。细粒度判别细节和全局语义特征都能帮助解决行人重识别中的遮挡和姿态变化挑战。

Method: 提出双正则化双向Transformer（DRFormer）框架，通过双正则化机制确保多样化的特征提取，并平衡两种模型的贡献。该框架协同融合DINO的局部纹理挖掘能力和CLIP的全局语义差异捕捉能力。

Result: 在五个基准测试上的大量实验表明，该方法有效协调了局部和全局表示，在性能上达到了与最先进方法竞争的水平。

Conclusion: 通过分析两种架构的互补作用，提出的DRFormer框架成功融合了视觉基础模型（DINO）和视觉语言模型（CLIP）的优势，为行人重识别任务提供了更全面的特征表示。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [95] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 该论文提出了一种基于PDE约束优化的图像分割方法，将物理先验通过变分正则化整合到深度学习模型中，在显微镜图像分割任务上取得了更好的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分割面临测量噪声、弱边界和有限标注数据等挑战，传统的无约束经验风险最小化方法容易产生不稳定解和泛化能力差的问题。需要将物理先验整合到深度学习模型中以提高分割性能。

Method: 将图像分割建模为PDE约束优化问题，通过变分正则化将物理先验整合到深度学习模型中。使用包含数据保真项和基于反应-扩散方程、相场界面能量的惩罚项的复合目标函数，所有组件都实现为可微残差损失。在LIVECell数据集上训练和评估，使用UNet作为无约束基线模型。

Result: 实验结果表明，与无约束深度学习基线相比，PDE正则化模型在分割准确性和边界保真度方面有显著提升。在低样本情况下表现出更好的稳定性和泛化能力，特别是在未见过的细胞类型上评估时。

Conclusion: PDE约束优化方法能够增强数据驱动学习框架，为变分方法、统计学习和科学机器学习之间提供了原则性的桥梁。结构化先验的整合显著提高了分割性能，特别是在数据有限的情况下。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [96] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: PISA提出了一种训练自由的片段稀疏注意力机制，通过精确计算关键块和近似计算非关键块来平衡速度与质量，在扩散Transformer中实现亚二次复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在视频和图像生成中至关重要，但其注意力机制的二次复杂度限制了效率。现有的块稀疏注意力通过只关注关键键值块来加速计算，但在高稀疏度下会因丢弃上下文信息而导致质量下降。

Method: PISA（Piecewise Sparse Attention）采用"精确或近似"策略而非传统的"保留或丢弃"范式：对关键块保持精确计算，对非关键块通过块级泰勒展开进行高效近似，从而在亚二次复杂度下覆盖完整的注意力范围。

Result: 实验结果显示，PISA在Wan2.1-14B上实现1.91倍加速，在Hunyuan-Video上实现2.57倍加速，同时在稀疏注意力方法中保持最高质量。在FLUX图像生成中，PISA实现1.2倍加速且不损害视觉质量。

Conclusion: PISA通过发现非关键块注意力分数的分布稳定性，提出了一种既能加速计算又能保持质量的新型稀疏注意力设计，有效弥合了速度与质量之间的差距。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [97] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: MedAD-38K：首个大规模多模态多中心医学异常检测基准，包含诊断思维链标注和结构化VQA对。提出两阶段训练框架（认知注入+一致性组相对策略优化），模型MedAD-R1在基准上取得SOTA性能，超越基线10%以上。


<details>
  <summary>Details</summary>
Motivation: 当前医学异常检测（MedAD）依赖监督微调（SFT）在简单碎片化数据集上，阻碍了模型进行合理推理和鲁棒多模态泛化的能力发展。需要更全面的基准和训练方法来提升AI临床决策支持的可信度和可解释性。

Method: 1）提出MedAD-38K基准：首个大规模多模态多中心医学异常检测基准，包含诊断思维链（CoT）标注和结构化视觉问答（VQA）对；2）两阶段训练框架：第一阶段认知注入（Cognitive Injection）使用SFT注入基础医学知识并对齐"先思考后回答"范式；第二阶段一致性组相对策略优化（Con-GRPO），引入一致性奖励确保推理过程与最终诊断相关且逻辑一致。

Result: 提出的MedAD-R1模型在MedAD-38K基准上取得最先进（SOTA）性能，超越强基线10%以上。模型能够生成透明且逻辑一致的推理路径，为增强AI临床决策支持的可信度和可解释性提供了有前景的方法。

Conclusion: 通过MedAD-38K基准和两阶段训练框架（认知注入+Con-GRPO），成功开发出能够进行透明、逻辑一致推理的医学异常检测模型，显著提升了AI在临床决策支持中的可信度和可解释性。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [98] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: 提出DVE方法，一种无需训练的概念擦除技术，专门针对流匹配模型，通过分析速度场的向量结构来选择性移除特定概念


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然能生成高质量图像，但会复制不良概念（如NSFW内容、版权风格、特定对象），现有概念擦除方法主要针对DDPM模型且需要微调，而流匹配模型作为新兴生成范式需要专门解决方案

Method: 提出差分向量擦除（DVE）方法，基于语义概念隐含在速度场方向结构中的洞察，构建描述目标概念与锚概念之间方向差异的差分向量场，在推理时通过将速度场投影到差分方向来选择性移除概念特定成分

Result: 在FLUX模型上的大量实验表明，DVE在NSFW抑制、艺术风格移除和对象擦除等广泛任务上持续优于现有基线，同时保持图像质量和多样性

Conclusion: DVE为流匹配模型提供了一种无需训练的概念擦除方法，通过利用速度场的向量结构实现精确概念抑制，为安全可控的生成模型部署提供了有效解决方案

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [99] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: PandaPose提出了一种新的3D人体姿态提升方法，通过将2D姿态先验传播到3D锚点空间作为统一中间表示，解决了传统方法中2D姿态误差传播和自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从2D到3D建立关节到关节的映射，存在两个根本限制：1）从预测的2D姿态到3D预测的不可避免的误差传播；2）处理自遮挡情况的固有困难。

Method: 提出PandaPose方法，包含三个核心组件：1）规范坐标系中的关节级3D锚点，提供准确鲁棒的先验；2）深度感知的关节级特征提升，分层整合深度信息解决自遮挡歧义；3）锚点-特征交互解码器，将3D锚点与提升的特征结合生成统一锚点查询，进一步用于锚点到关节的集成预测。

Result: 在Human3.6M、MPI-INF-3DHP和3DPW三个基准测试上验证了方法的优越性。在Human3.6M的挑战性条件下，相比SOTA方法误差减少了14.7%，定性比较进一步展示了方法的有效性和鲁棒性。

Conclusion: PandaPose通过将2D姿态先验传播到3D锚点空间作为统一中间表示，有效解决了传统3D人体姿态提升方法中的误差传播和自遮挡问题，在多个基准测试上取得了显著性能提升。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [100] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: LightCity是一个用于城市场景逆渲染的高质量合成数据集，包含多种光照条件、间接光和阴影效果，用于基准测试相关任务。


<details>
  <summary>Details</summary>
Motivation: 城市场景逆渲染在自动驾驶和数字孪生中很重要，但面临复杂光照条件的挑战，缺乏合适的数据集来研究这些挑战对内在分解和3D重建的影响。

Method: 创建了LightCity数据集，包含300多个天空图、高度可控的光照、街景和航拍视角的5万多张图像，以及深度、法线、材质组件、直接光和间接光等丰富属性。

Result: 利用LightCity对城市环境中的三个基本任务进行基准测试，并进行了全面的分析，为相关研究提供了坚实基础。

Conclusion: LightCity填补了城市场景逆渲染数据集的空白，为研究复杂光照条件下的内在分解和3D重建提供了重要资源。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [101] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: Koo-Fu CLIP是一种基于Fukunaga-Koontz线性判别分析的监督式CLIP适应方法，通过在白化嵌入空间中抑制类内变异并增强类间区分，显著提升CLIP的分类性能。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型提供了强大的通用表示，但其原始嵌入在监督分类任务中表现有限，存在类分离不足和维度冗余的问题，需要优化以适应具体分类任务。

Method: 基于Fukunaga-Koontz线性判别分析，在白化的CLIP嵌入空间中操作，通过封闭形式的线性投影抑制类内变异并增强类间区分，同时实现有效的维度约简。

Result: 在ImageNet-1K上，Koo-Fu CLIP将top-1准确率从75.1%提升至79.1%，在扩展到14K和21K类别时仍保持一致的性能提升。支持10-12倍的压缩而几乎不损失准确率。

Conclusion: Koo-Fu CLIP提供了一种轻量高效的CLIP表示适应方法，通过几何重塑显著改善类可分性，支持高效的大规模分类和检索应用。

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [102] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CRT（Corruption Restoration Transformer）的即插即用视觉变换器，用于增强视觉-语言-动作模型对图像损坏的鲁棒性，使其在传感器级视觉干扰下仍能保持接近基线的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在受控环境中表现良好，但在真实世界部署中面临严重挑战，特别是对图像损坏（如电子噪声、死像素、镜头污染等传感器级伪影）的脆弱性。现有研究主要关注场景几何造成的物理遮挡，而传感器级图像损坏这一关键问题尚未得到充分探索。

Method: 提出了CRT（Corruption Restoration Transformer），这是一种即插即用、模型无关的视觉变换器。通过对抗性训练目标，CRT能够从损坏的输入中恢复干净的观测，而无需对底层模型进行计算昂贵的微调。

Result: 实验表明，最先进的VLA模型（如π0.5和SmolVLA）在常见信号伪影下性能会从90%的成功率骤降至2%。而CRT能够有效恢复丢失的性能，使VLA模型即使在严重视觉损坏下也能保持接近基线的成功率。

Conclusion: CRT为VLA模型提供了一种有效的解决方案，使其能够抵抗传感器级视觉干扰，从而提高了视觉-语言-动作模型在真实世界部署中的可靠性和鲁棒性。

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [103] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了EEmoDB，这是目前最大的图像诱发情感理解数据集，包含120万QA对和3.6万细粒度评估数据，并开发了EEmo-Logic多模态大语言模型，通过指令微调和GRPO优化，在情感理解和评估任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图像诱发情感理解方面存在局限性，要么只能进行粗粒度情感感知，要么缺乏推理能力。为了弥补这一差距，需要构建更全面的数据集和更强大的模型来理解图像的多维情感属性和强度细微差别。

Method: 1. 构建EEmoDB数据集：包含5个分析维度和5个任务类别，通过自动生成从12.5万张图像创建120万QA对（EEmoDB-QA），从2.5万张图像收集3.6万细粒度评估数据（EEmoDB-Assess）。2. 开发EEmo-Logic模型：基于多模态大语言模型，通过指令微调和任务定制化的组相对偏好优化（GRPO）进行训练，采用新颖的奖励设计。

Result: EEmo-Logic在领域内和跨领域数据集上都表现出稳健的性能，在情感问答和细粒度评估任务中表现优异。模型代码已开源。

Conclusion: EEmoDB数据集和EEmo-Logic模型为图像诱发情感理解提供了全面的解决方案，推动了机器共情的发展，并支持多样化的人机交互应用。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [104] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 提出EMFormer架构和累积上下文微调方法，通过多尺度特征提取和动态损失平衡，解决长期天气预报中的灾难性遗忘、误差累积和高计算开销问题，在天气预测和视觉任务上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 长期天气预报对社会经济规划和灾害准备至关重要，但现有方法存在灾难性遗忘、误差累积和高训练开销等限制，需要新的解决方案来提升长期上下文建模能力并降低计算成本。

Method: 1) 提出高效多尺度Transformer (EMFormer)，通过单次卷积在训练和推理中提取多尺度特征；2) 采用累积上下文微调提升时间一致性而不损害短期精度；3) 提出复合损失函数，通过正弦权重动态平衡不同损失项，自适应指导预训练和微调优化轨迹。

Result: 实验表明该方法在天气预报和极端事件预测中表现优异，显著提升长期预测精度。EMFormer在视觉基准（ImageNet-1K和ADE20K）上展现出强泛化能力，同时相比传统多尺度模块实现5.69倍加速。

Conclusion: 提出的跨预训练、微调和预测的完整管道有效解决了长期天气预报的关键挑战，通过创新的架构设计和训练策略实现了精度和效率的双重提升，为长期天气建模提供了实用解决方案。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [105] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: Med3D-R1是一个用于3D医学视觉语言模型的强化学习框架，通过两阶段训练（监督微调和强化学习）提升临床推理能力，在CT-RATE和RAD-ChestCT基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发具有稳健临床推理能力的3D视觉语言模型面临挑战：1）体数据医学影像的固有复杂性；2）模型容易过拟合表面报告模式；3）缺乏可解释性感知的奖励设计。

Method: 提出Med3D-R1强化学习框架，包含两阶段训练：1）监督微调阶段：引入残差对齐机制连接3D特征与文本嵌入，采用异常重加权策略强调临床信息标记；2）强化学习阶段：重新设计一致性奖励以促进连贯的逐步诊断推理。

Result: 在两个3D诊断基准测试中达到最先进性能：CT-RATE准确率41.92%，RAD-ChestCT准确率44.99%，表明在异常诊断和临床推理方面有改进，优于先前方法。

Conclusion: 该方法有望通过实现更可靠和透明的3D医学视觉语言系统来增强真实世界的诊断工作流程。

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [106] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 提出TRA框架，通过文本细化和对齐模块，利用视频描述中的文本特征补充视觉特征，提升点监督时序动作定位性能


<details>
  <summary>Details</summary>
Motivation: 当前点监督时序动作定位方法仅考虑视觉特征，忽略了文本侧的有用语义信息，需要利用文本特征补充视觉特征

Method: 提出文本细化和对齐框架，包含基于点的文本细化模块和基于点的多模态对齐模块，通过预训练多模态模型生成视频帧描述，利用点标注细化文本，在多模态特征对比学习中减少视觉与语言模态差距

Result: 在五个广泛使用的基准测试中表现出优越性能，计算开销分析显示可在单张24GB RTX 3090 GPU上运行，具有实用性和可扩展性

Conclusion: 通过有效利用文本特征补充视觉特征，提出的TRA框架在点监督时序动作定位中取得了良好性能，证明了多模态方法的价值

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [107] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于交通流感知的车道感知模块（TFM），通过提取实时交通流特征来增强现有车道感知算法在遮挡或车道缺失场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的车道检测方法在视觉传感器提供线索不足（如遮挡、车道缺失）时性能显著下降。虽然有些方法使用高精地图作为补充信息，但这些方案面临订阅成本高和实时性有限的问题。因此需要探索新的信息源来提升车道感知能力。

Method: 提出交通流感知车道感知模块（TFM），该模块能够有效提取实时交通流特征，并将其与现有车道感知算法无缝集成。该方法源于真实自动驾驶场景，并在开源算法和数据集上进行了验证。

Result: 在四个主流模型和两个公共数据集（Nuscenes和OpenLaneV2）上的广泛实验表明，TFM能够持续提升性能，在Nuscenes数据集上实现了最高+4.1%的mAP增益。

Conclusion: 交通流作为一种创新的信息源，能够在不增加额外成本的情况下提供实时能力，有效解决了现有车道感知方法在视觉线索不足时的性能瓶颈问题。

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [108] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 研究发现大语言模型中存在跨语言共享安全神经元(SS-Neurons)，这是一个微小但关键的神经元子集，共同调节跨语言安全行为。通过针对这些神经元的训练策略，可以显著提升非高资源语言的安全性。


<details>
  <summary>Details</summary>
Motivation: 多语言安全性存在显著不平衡，非高资源语言的安全性远不如高资源语言，且安全对齐的神经机制尚不明确。

Method: 首先识别单语安全神经元(MS-Neurons)并验证其因果作用，然后识别跨语言共享安全神经元(SS-Neurons)，提出基于语言资源分布和模型架构的神经元导向训练策略。

Result: 抑制SS-Neurons会导致非高资源语言安全性同时下降，而增强它们能提高跨语言防御一致性。针对这一微小神经元子集的微调优于现有方法，显著提升非高资源语言安全性且保持模型通用能力。

Conclusion: SS-Neurons作为高资源语言向非高资源语言传递安全能力的桥梁，针对这一关键神经元子集的训练策略能有效解决多语言安全不平衡问题。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [109] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map是一个联合优化线和平面的3D线图构建框架，通过显式建模可学习的线和平面基元，在保持高效率的同时实现准确详细的3D线图构建。


<details>
  <summary>Details</summary>
Motivation: 从物理和拓扑角度研究3D线图构建问题：3D线最自然地作为有限3D平面块的边缘出现。现有方法缺乏对线和平面之间关系的显式建模。

Method: 提出LiP-Map框架，显式建模可学习的线和平面基元，通过构建平面和线基元之间的交互来集成平面拓扑，而不是施加成对共面约束。

Result: 在ScanNetV2、ScanNet++、Hypersim、7Scenes和Tanks&Temple等100多个场景上，LiP-Map在准确性和完整性方面优于最先进方法，显著提升了线辅助视觉定位性能，重建效率高（通常每场景3-5分钟）。

Conclusion: LiP-Map首次将平面拓扑集成到3D线图构建中，为人造环境中的结构化重建提供了原则性方法，在多个数据集上表现出优越性能。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [110] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: 本文提出了一种交互一致的对象移除方法，不仅移除目标对象，还移除相关的交互元素，解决了传统方法只移除命名目标而留下交互证据的问题。


<details>
  <summary>Details</summary>
Motivation: 基于图像的对象移除通常只擦除命名的目标，留下交互证据导致语义不一致。需要解决交互一致的对象移除问题，不仅要移除目标对象，还要移除相关的交互元素。

Method: 提出REORM框架，利用多模态大语言模型推断需要联合移除的元素，采用模块化设计，包括MLLM驱动的分析、掩码引导的移除和自校正机制，并提供本地部署变体。

Result: 在ICOREval基准测试中，REORM优于最先进的图像编辑系统，证明了其在产生交互一致结果方面的有效性。

Conclusion: 该研究解决了交互一致的对象移除问题，提出的REORM框架能够有效识别和移除与目标对象相关的交互元素，产生语义一致的编辑结果。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [111] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: ReDiStory是一个无需训练的框架，通过推理时重组提示嵌入来改进多帧故事生成，在保持提示保真度的同时提升身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法将身份和帧提示拼接成统一表示，但在复杂故事中常引入帧间语义干扰，削弱了身份保持能力。

Method: ReDiStory将文本嵌入显式分解为身份相关和帧特定组件，然后通过抑制跨帧共享方向来去相关帧嵌入，减少跨帧干扰。

Result: 在ConsiStory+基准测试中，ReDiStory在多个身份一致性指标上相比1Prompt1Story取得了一致的提升。

Conclusion: ReDiStory通过推理时提示嵌入重组，在不修改扩散参数或需要额外监督的情况下，有效提升了多帧故事生成的身份一致性。

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [112] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: DeCorStory是一个无需训练的无推理框架，通过Gram-Schmidt提示嵌入去相关来减少帧间语义干扰，解决文本到图像故事生成中的视觉和语义一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法（如One-Prompt-One-Story）将所有提示串联成单个序列，导致强嵌入相关性，引起颜色泄漏、背景混合和身份漂移等问题。

Method: 采用Gram-Schmidt提示嵌入去相关来正交化帧级语义，然后通过奇异值重加权增强提示特定信息，并使用身份保持交叉注意力在扩散过程中稳定角色身份。

Result: 实验显示在提示-图像对齐、身份一致性和视觉多样性方面都有持续改进，在无需训练基线中达到最先进的性能。

Conclusion: DeCorStory是一个无需模型修改或微调的框架，可无缝集成到现有扩散管道中，有效解决了文本到图像故事生成中的跨帧一致性挑战。

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [113] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: MED框架分析视觉工具使用强化学习，发现性能提升主要来自内在能力改善而非工具使用，当前RL只是学会与工具安全共存而非掌握工具


<details>
  <summary>Details</summary>
Motivation: 视觉工具使用强化学习虽然能提升性能，但不清楚这种提升是来自工具使用的改进还是模型内在能力的演化，需要区分这两种因素

Method: 提出MED框架：1) 分离内在能力变化与工具诱导效应；2) 将工具诱导性能差异分解为增益和损害项；3) 探究驱动演化的机制

Result: 在两个不同工具先验的VLM和六个基准测试中发现：改进主要由内在学习主导，工具使用RL主要减少工具诱导损害（如调用错误和工具模式干扰），在基于工具纠正内在失败方面进展有限

Conclusion: 当前视觉工具使用强化学习只是学会与工具安全共存，而非真正掌握工具的使用

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [114] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出一种将固定压缩率VAE转换为支持多级时间压缩模型的技术，通过最小微调解决高压缩率下的性能下降问题，并验证其在视频扩散模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在潜在视频扩散模型中，连续VAE实现更高压缩率是可取的，但当添加额外采样层而不扩展隐藏通道维度时，效率会显著下降。需要解决高压缩率下的性能下降问题。

Method: 提出一种技术将固定压缩率VAE转换为支持多级时间压缩的模型，提供简单的最小微调方法。研究不同压缩级别对具有不同特征视频片段性能的影响，并将多级时间压缩VAE与基于扩散的生成模型DiT集成。

Result: 提供了多级时间压缩方法有效性的实证证据，展示了在扩散框架中成功并发训练和兼容性，说明了多级时间压缩的潜在应用。

Conclusion: 该技术能够有效解决高压缩率下的性能下降问题，多级时间压缩VAE与扩散模型兼容，为视频压缩和生成提供了新的可能性。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [115] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: NOVA是一个基于熵分析的无训练令牌缩减加速框架，用于视觉自回归模型，通过自适应确定加速激活规模并动态计算不同规模和层的令牌缩减比例来加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有VAR令牌缩减方法存在三个关键限制：启发式阶段划分、非自适应调度和有限加速范围，未能充分利用加速潜力。熵变化本质上反映了预测不确定性的转变，为捕捉建模动态演化提供了原则性度量。

Method: NOVA通过在线识别规模熵增长的拐点来自适应确定推理过程中的加速激活规模。通过规模联动和层联动比例调整，动态计算每个规模和层的不同令牌缩减比例，剪枝低熵令牌，同时重用先前规模残差派生的缓存来加速推理并保持生成质量。

Result: 广泛的实验和分析验证了NOVA作为一个简单而有效的无训练加速框架的有效性。

Conclusion: NOVA通过熵分析实现了对视觉自回归模型的有效训练免费加速，解决了现有令牌缩减方法的关键限制，显著提升了推理效率。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [116] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: T2M Mamba：通过周期性-显著性感知Mamba和周期性差分跨模态对齐模块，解决文本到动作生成中的长期序列漂移和语义等价改写脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成模型存在两个核心局限：1) 将动作周期性和关键帧显著性视为独立因素，忽略了它们的耦合关系，导致长序列生成漂移；2) 对语义等价改写脆弱，微小同义词替换会扭曲文本嵌入，通过解码器传播产生不稳定或错误的动作

Method: 提出T2M Mamba框架：1) 周期性-显著性感知Mamba，通过增强密度峰值聚类进行关键帧权重估计，通过FFT加速自相关进行动作周期性估计，以最小计算开销捕获耦合动态；2) 周期性差分跨模态对齐模块(PDCAM)，增强文本和动作嵌入的鲁棒对齐

Result: 在HumanML3D和KIT-ML数据集上的大量实验证实了方法的有效性，取得了FID 0.068的优异结果，在所有其他指标上均获得一致提升

Conclusion: T2M Mamba通过同时建模动作周期性和关键帧显著性之间的耦合关系，并增强跨模态对齐的鲁棒性，有效解决了文本到动作生成中的长期序列漂移和语义等价改写脆弱性问题

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [117] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出TLGA攻击框架，专门针对视频MoE模型的组件级漏洞进行研究，包括路由器独立弱点和路由器-专家协同弱点，并提出了相应的防御方法J-TLAT。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型在视频理解任务中表现良好，但其对抗鲁棒性研究不足。现有攻击方法通常将MoE视为统一架构，忽视了路由器和专家模块等关键组件的独立和协同弱点。

Method: 提出Temporal Lipschitz-Guided Attacks (TLGA)框架：1) 针对路由器的独立攻击；2) Joint Temporal Lipschitz-Guided Attacks (J-TLGA)协同攻击路由器和专家；3) Joint Temporal Lipschitz Adversarial Training (J-TLAT)联合训练防御协同弱点。

Result: TLGA揭示了MoE组件的独立弱点，J-TLGA显著放大了对抗效果并暴露了MoE架构的协同弱点。J-TLAT增强了组件级鲁棒性，框架是即插即用的，相比密集模型减少60%以上推理成本，在多种数据集和架构上一致提升对抗鲁棒性。

Conclusion: 该研究填补了MoE对抗鲁棒性研究的空白，通过组件级攻击和防御框架，有效缓解了MoE的独立和协同弱点，为构建更鲁棒的MoE视频理解模型提供了新思路。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [118] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen框架通过多生成器聚合和程序化硬负样本课程，提升合成数据的多样性和鲁棒性，相比单源方法在多项基准上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于合成数据的视觉语言预训练方法主要依赖单一生成器扩展，这带来了特定频谱偏差并限制了特征多样性，需要更高效的数据构造方法

Method: 采用多生成器聚合方法，在不同架构生成器的交集上训练以消除模型特定伪影；引入程序化硬负样本课程来增强细粒度语法理解；重新分配数据预算从单一描述到多源变体

Result: 相比领先的单源基线SynthCLIP，在聚合多任务基准上提升19.0%，在SugarCrepe++组合性基准上提升9.1%

Conclusion: 结构多样性比单纯增加单源样本数量是更高效的数据扩展规律，多生成器聚合方法能构建更鲁棒的特征空间

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [119] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: ALI方法通过融合像素对齐的视觉编码器特征到潜在内在表示框架中，平衡语义抽象和光度保真度，在复杂材质（如金属、玻璃）上显著提升图像重照明质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在内在表示的图像重照明方法在处理金属、玻璃等挑战性材质时效果不佳。研究发现，顶级语义编码器的特征反而会降低重照明质量，揭示了语义抽象与光度保真度之间的基本权衡。

Method: 提出增强潜在内在表示（ALI），将像素对齐视觉编码器的特征融合到潜在内在框架中，平衡语义上下文和密集光度结构。同时采用自监督细化策略缓解配对真实世界数据的稀缺问题。

Result: ALI在重照明任务上取得显著改进，特别是在复杂、高光材质上获得最大提升。该方法仅使用未标记的真实世界图像对进行训练。

Conclusion: 通过平衡语义抽象和光度保真度，ALI框架有效解决了图像重照明中的材质挑战，为处理复杂反射特性提供了有效解决方案。

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [120] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出抛物线位置编码(PaPE)，一种基于抛物线的视觉模态位置编码方法，在注意力架构中为图像、点云、视频和事件相机流等视觉模态编码位置信息


<details>
  <summary>Details</summary>
Motivation: 现有工作主要将语言中的1D序列位置编码扩展到视觉中的nD结构，但对视觉特性的考虑不充分。需要设计能充分考虑视觉特性的位置编码方法

Method: 基于抛物线设计位置编码，从先前工作中提炼出五个原则：平移不变性、旋转不变性(PaPE-RI)、距离衰减、方向性和上下文感知

Result: 在8个数据集（涵盖4种模态）上评估，PaPE或PaPE-RI在7个数据集上达到最佳性能。在ImageNet-1K外推实验中，PaPE表现优异，比次优位置编码绝对提升高达10.5%

Conclusion: PaPE是一种有效的视觉模态位置编码方法，能充分考虑视觉特性，在多种视觉任务上表现优异，且具有良好的外推能力

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [121] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 该研究比较了三种视觉方法（直接目标检测、分割辅助分类、视觉语言模型）在识别驾驶员注视点对应道路场景语义对象上的性能，发现YOLOv13和Qwen2.5-VL-32b表现最佳，其中大型VLM在夜间识别小物体方面更具优势。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员在驾驶过程中的视觉注意力分布（通过注视行为表征）对于开发下一代高级驾驶辅助系统和提高道路安全至关重要。该研究旨在从车辆前视摄像头捕捉的道路场景中识别驾驶员注视点对应的语义对象。

Method: 研究采用三种不同的视觉方法：1）直接目标检测（YOLOv13）；2）分割辅助分类（SAM2结合EfficientNetV2 vs YOLOv13）；3）基于查询的视觉语言模型（Qwen2.5-VL-7b vs Qwen2.5-VL-32b）。通过注视点与对象语义的关联来识别驾驶员关注的物体。

Result: 直接目标检测（YOLOv13）和Qwen2.5-VL-32b显著优于其他方法，Macro F1-Score均超过0.84。大型VLM（Qwen2.5-VL-32b）在识别交通灯等小型安全关键物体方面表现出更强的鲁棒性，尤其在夜间恶劣条件下。分割辅助方法因"部分与整体"语义差距导致召回率大幅下降。

Conclusion: 研究揭示了传统检测器的实时效率与大型VLM提供的更丰富上下文理解和鲁棒性之间的基本权衡。这些发现为未来人类感知智能驾驶员监控系统的设计提供了关键见解和实践指导。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [122] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 研究量化视觉Transformer在分布外检测中的表现，发现大规模预训练会降低低比特量化的鲁棒性，数据增强是更好的选择


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在视觉任务中表现出色，但实现可访问和实时使用仍具挑战。量化能减少内存和推理成本，但可能导致性能损失。现有研究主要关注分布内任务行为，而注意力机制可能通过探索分布外情况提供量化属性的新见解。

Method: 研究量化的小型流行视觉Transformer（DeiT、DeiT3和ViT）在常见分布外数据集上的行为。分析包括分布内校准和分布外检测，比较不同预训练规模（ImageNet-22k vs ImageNet-1k）对量化鲁棒性的影响。

Result: 分布内分析显示4位模型存在初始不稳定性，特别是那些在较大ImageNet-22k上训练的模型。最强的FP32模型DeiT3在量化后性能下降17%，成为最弱的4位模型之一。分布外检测揭示：在ImageNet-22k上预训练的ViT和DeiT3分别经历了15.0%和19.2%的平均AUPR-out量化下降，而仅在ImageNet-1k上预训练的对应模型只经历了9.5%和12.0%的下降。

Conclusion: 大规模数据集预训练可能阻碍低比特量化在分布外检测中的鲁棒性，数据增强可能是更有益的选择。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [123] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 该论文提出了Logit Lens Loss (LLL)，一种补充损失函数，用于解决自回归视觉语言模型中视觉信息扩散问题，使Logit Lens可视化在可解释性方面变得实用有效。


<details>
  <summary>Details</summary>
Motivation: 在自回归视觉语言模型中，Logit Lens原本可用于可视化图像token的概念内容，但由于视觉信息会扩散到语言token中，导致局部视觉信息被破坏，使得Logit Lens可视化在可解释性方面变得不可用。

Method: 提出了Logit Lens Loss (LLL)，作为下一个token预测(NTP)的补充损失。LLL旨在使视觉token嵌入与描述其图像区域的文本概念（如包含"猫"的补丁与"猫"这个词）在语义上更加对齐，无需架构修改或大规模训练。LLL约束了自注意力层中图像和文本token的混合，防止图像token丢失其局部视觉信息。

Result: LLL不仅使Logit Lens变得实用，能够在图像中生成有意义的对象置信度图，而且提高了视觉中心任务（如分割）的性能，无需附加任何特殊头部。

Conclusion: Logit Lens Loss通过防止视觉信息扩散，有效解决了自回归视觉语言模型中Logit Lens可视化的可用性问题，同时提升了模型在视觉任务上的性能，为VLM的可解释性提供了实用解决方案。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [124] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 提出Cognitive Supersensing训练范式，通过视觉潜在意象预测让MLLMs具备类人视觉意象能力，显著提升复杂认知问题解决能力


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在开放词汇感知任务上表现优异，但在解决需要视觉记忆和抽象视觉细节的复杂认知问题时能力有限。现有方法主要在文本空间扩展思维链推理，忽视了类似人类视觉空间画板和视觉意象的视觉推理机制。

Method: 1. 引入Cognitive Supersensing训练范式，通过Latent Visual Imagery Prediction (LVIP)头联合学习视觉认知潜在嵌入序列并与答案对齐，形成基于视觉的内部推理链；2. 引入强化学习阶段，基于接地的视觉潜在优化文本推理路径；3. 提出CogSense-Bench基准，评估五个认知维度。

Result: 使用Cognitive Supersensing训练的MLLMs在CogSense-Bench上显著优于最先进基线，并在域外数学和科学VQA基准上表现出更好的泛化能力，表明内部视觉意象可能是连接感知识别与认知理解的关键。

Conclusion: 内部视觉意象能力对于提升MLLMs的认知理解能力至关重要，Cognitive Supersensing范式通过赋予MLLMs类人视觉意象能力，有效缩小了感知识别与认知理解之间的差距。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [125] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出CLEAR框架，首次系统研究并联合去除屏幕拍摄图像中的摩尔纹和闪烁带两种复合伪影，通过构建大规模数据集和频率域分解重组模块实现有效恢复。


<details>
  <summary>Details</summary>
Motivation: 移动设备拍摄显示屏图像时，摩尔纹和闪烁带两种伪影同时存在且强耦合，导致严重视觉质量下降。现有针对单一伪影的方法无法处理这种复合场景，需要新的解决方案。

Method: 提出CLEAR统一恢复框架：1) 构建包含摩尔纹和闪烁带的大规模数据集；2) 引入基于ISP的闪烁模拟流程以稳定训练并扩展退化分布；3) 设计频率域分解与重组模块；4) 提出轨迹对齐损失以增强复合伪影建模。

Result: 大量实验表明，该方法在多个评估指标上持续优于现有图像恢复方法，验证了其在复杂真实场景中的有效性。

Conclusion: CLEAR框架首次系统解决了屏幕拍摄图像中摩尔纹和闪烁带的联合去除问题，通过创新的数据集构建、频率域处理和损失函数设计，为复合伪影恢复提供了有效解决方案。

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [126] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA将基因组建模重新定义为OCR式文档理解，通过视觉DNA编码器和文档解码器，在减少有效token数量的同时实现更好的基因组分析性能


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型大多采用大语言模型架构，将DNA视为一维token序列。这种连续读取方式与稀疏、不连续的基因组语义结构不匹配，导致在低信息背景上浪费计算资源，并且无法实现理解驱动的长上下文压缩。

Method: 提出基于视觉的框架OpticalDNA，将DNA渲染为结构化视觉布局，训练OCR能力的视觉-语言模型。包含视觉DNA编码器（生成紧凑、可重构的视觉token用于高保真压缩）和文档解码器。定义了基于核心基因组原语的提示条件目标：读取、区域定位、子序列检索和掩码跨度补全。

Result: 在多样化基因组基准测试中，OpticalDNA始终优于近期基线；在长达450k碱基的序列上，以近20倍更少的有效token实现最佳整体性能，并且超越了激活参数多达985倍的模型，同时仅调整256k可训练参数。

Conclusion: OpticalDNA通过将基因组建模重新定义为OCR式文档理解，实现了布局感知的DNA表示，在减少有效token预算的同时保留了细粒度的基因组信息，为基因组分析提供了更高效的计算框架。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [127] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: MUN是一个多模态非常识推理基准，评估模型处理偏离典型视觉或上下文期望场景的能力，并提出基于检索的上下文学习框架提升模型在低频、非典型场景的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态环境中的常识推理仍然是人工智能的基础挑战，需要评估模型处理偏离典型视觉或上下文期望场景的能力，特别是在现实世界、文化多样和非典型场景中的鲁棒性和适应性。

Method: 提出基于检索的上下文学习（R-ICL）框架，利用新颖的多模态集成检索器（MER）识别语义相关的示例，即使图像和文本对故意不协调，将大型模型的推理能力转移到小型模型而无需额外训练。

Result: 实验显示，与基线ICL方法相比，R-ICL在低频、非典型设置中平均提升8.3%，证明了该方法在非常规场景中的有效性。

Conclusion: MUN基准为评估和改进视觉语言模型在现实世界、文化多样和非典型场景中的鲁棒性和适应性开辟了新方向，R-ICL框架为处理非常规多模态推理提供了有效方法。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [128] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 本文提出了Vision-DeepResearch基准（VDR-Bench），包含2000个VQA实例，用于评估多模态大语言模型在视觉-文本深度研究系统中的真实搜索能力，并提出了多轮裁剪搜索工作流来提升视觉检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前评估多模态大语言模型在视觉-文本深度研究系统中的搜索能力存在两个主要问题：1）现有基准测试不够视觉搜索中心化，答案往往通过文本问题中的交叉线索泄露，或可以从现有MLLMs的先验知识中推断；2）评估场景过于理想化，图像搜索可以通过近似完全匹配获取信息，而文本搜索则过于直接且挑战性不足。

Method: 构建了Vision-DeepResearch基准（VDR-Bench），包含2000个VQA实例，通过精心设计的多阶段筛选流程和严格的专家评审创建所有问题。同时提出了一个简单的多轮裁剪搜索工作流，以解决当前MLLMs视觉检索能力不足的问题。

Result: VDR-Bench能够更真实地评估视觉-深度研究系统在实际场景中的表现。提出的多轮裁剪搜索策略被证明能有效提升模型在真实视觉检索场景中的性能。

Conclusion: 该研究为未来多模态深度研究系统的设计提供了实用指导，提出的基准测试和工作流有助于更准确地评估和提升MLLMs在视觉-文本搜索任务中的实际能力。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [129] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: 提出一种单步扩散的图像压缩方法，显著提升推理速度，同时通过基于特征的判别器保持感知质量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像压缩方法虽然能在低码率下获得高感知质量，但需要大量去噪步骤，导致推理延迟高、计算开销大，限制了实际部署应用

Method: 提出仅需单步扩散过程的图像压缩方法，显著提升推理速度；引入基于紧凑特征表示的判别器（而非原始像素），利用特征能更好捕捉高级纹理和结构细节的特点来增强重建图像的感知质量

Result: 实验结果表明，该方法在保持可比压缩性能的同时，相比近期基于扩散的方法实现了46倍的推理速度提升

Conclusion: 提出的单步扩散图像压缩方法有效解决了扩散模型压缩方法推理延迟高的问题，在保持感知质量的同时大幅提升推理速度，具有实际部署价值

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [130] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: HandMCM是一种基于状态空间模型（Mamba）的3D手部姿态估计新方法，通过局部信息注入/过滤和对应关系建模模块，有效学习关键点在不同遮挡场景下的动态运动学拓扑结构，在严重遮挡场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D手部姿态估计对于增强现实等人机交互应用至关重要，但由于手部自遮挡以及与物体交互造成的遮挡，这项任务面临重大挑战。现有方法在处理这些遮挡场景时存在局限性。

Method: 提出HandMCM方法，基于状态空间模型（Mamba），包含局部信息注入/过滤模块和对应关系建模模块，能够有效学习关键点在不同遮挡场景下的动态运动学拓扑结构。同时整合多模态图像特征，增强输入的鲁棒性和表示能力。

Result: 在三个基准数据集上的实证评估表明，该方法显著优于当前最先进的方法，特别是在涉及严重遮挡的挑战性场景中表现突出。

Conclusion: HandMCM展示了在3D手部姿态估计中应用状态空间模型的潜力，能够提高实际应用中的准确性和可靠性，特别是在处理遮挡问题上具有显著优势。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [131] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: 提出TAFS-GRPO框架，通过温度退火采样和组相对策略优化，解决流匹配模型中奖励信号稀疏和不精确的问题，实现高效少步文本到图像生成与人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型方法通常依赖大量去噪步骤，且面临奖励信号稀疏和不精确的问题，导致人类偏好对齐效果不佳。

Method: 提出TAFS-GRPO框架：1) 温度退火少步采样：在单步采样结果上迭代注入自适应时间噪声，在保持图像语义完整性的同时引入随机性；2) 步感知优势集成机制结合GRPO，避免奖励函数可微性要求，提供密集且步特定的奖励信号。

Result: 实验表明TAFS-GRPO在少步文本到图像生成中表现优异，显著提高了生成图像与人类偏好的对齐程度。

Conclusion: TAFS-GRPO框架有效解决了流匹配模型中奖励信号稀疏问题，实现了高效少步生成与人类偏好的良好对齐，代码和模型将开源以促进进一步研究。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [132] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Samba和Samba+框架，基于Mamba模型解决多种显著目标检测任务，通过空间邻域扫描和上下文感知上采样提升性能，并通过多任务联合训练实现统一模型。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测模型受限于CNN的有限感受野和Transformer的二次计算复杂度，需要一种既能获得全局感受野又计算高效的解决方案。

Method: 提出Samba框架：1）基于Mamba架构；2）引入显著性引导的Mamba块，采用空间邻域扫描算法保持显著区域空间连续性；3）提出上下文感知上采样方法促进层次特征对齐和聚合。进一步提出Samba+：通过多任务联合训练，包含中心辐射图注意力模块进行跨模态交互融合，以及模态锚定持续学习策略缓解模态冲突和灾难性遗忘。

Result: Samba在6个SOD任务的22个数据集上超越现有方法且计算成本更低；Samba+使用单一训练模型在这些任务和数据集上取得更优结果。

Conclusion: 提出的Samba框架在多种显著目标检测任务中表现出色，Samba+通过多任务联合训练实现了更统一和通用的模型，展示了该框架的潜力。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [133] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: 提出UV-M3TL框架，通过双分支空间通道多模态嵌入和自适应特征解耦多任务损失，同时识别驾驶员行为、情绪、车辆行为和交通环境，缓解任务间负迁移问题，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 高级驾驶辅助系统需要同时理解驾驶员行为和感知导航环境，但联合学习这些异构任务会导致任务间负迁移，损害系统性能。现有方法难以有效处理多任务学习中的任务冲突和知识转移问题。

Method: 提出统一且通用的多模态多任务学习框架UV-M3TL，包含两个核心组件：1) 双分支空间通道多模态嵌入(DB-SCME)，通过双分支结构显式建模任务共享和任务特定特征，增强跨任务知识转移同时缓解任务冲突；2) 自适应特征解耦多任务损失(AFD-Loss)，基于学习动态的自适应加权机制和特征解耦约束，提高联合优化稳定性并引导模型学习多样化的多任务表示。

Result: 在AIDE数据集上，UV-M3TL在驾驶员行为识别、驾驶员情绪识别、车辆行为识别和交通环境识别四个任务上均达到最先进性能。在BDD100K、CityScapes、NYUD-v2和PASCAL-Context等公开多任务感知基准测试中，该框架在不同任务组合下均表现优异，在大多数任务上取得SOTA结果。

Conclusion: UV-M3TL框架有效解决了多模态多任务学习中的任务间负迁移问题，通过创新的双分支结构和自适应损失设计，实现了跨任务的协同学习和性能提升，为高级驾驶辅助系统提供了强大的多任务感知能力。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [134] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: ToPi是一个针对DiTs中上下文生成任务的训练免费令牌剪枝框架，通过离线校准驱动的敏感性分析识别关键注意力层，并设计影响度量来选择性剪枝上下文令牌，实现超过30%的推理加速同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 在上下文生成中，输入拼接导致序列长度急剧增加，造成显著计算瓶颈。现有的令牌缩减技术主要针对文本到图像合成设计，采用统一的缩减策略，忽略了参考上下文和目标潜在表示在空间、时间和功能维度上的角色不对称性。

Method: ToPi框架包含：1）离线校准驱动的敏感性分析识别关键注意力层；2）基于这些层设计新颖的影响度量来量化每个上下文令牌的贡献；3）选择性剪枝策略；4）适应扩散轨迹演化的时间更新策略。

Result: 实证评估表明，ToPi能够在复杂图像生成任务中实现超过30%的推理加速，同时保持结构保真度和视觉一致性。

Conclusion: ToPi是针对DiTs中上下文生成的有效训练免费令牌剪枝框架，通过考虑上下文和目标潜在表示的角色不对称性，实现了显著的推理加速而不损害生成质量。

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [135] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: Omni-Judge研究评估全模态大语言模型能否作为文本条件音视频生成的人类对齐评判者，发现其在语义对齐任务上表现优异，但在高帧率感知指标上存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前Sora 2和Veo 3等文本到视频生成模型能够从文本提示直接生成高保真视频和同步音频，但评估这种三模态输出仍是一个未解决的挑战。人类评估可靠但成本高且难以扩展，传统自动指标（如FVD、CLAP、ViCLIP）存在模态分离、复杂提示处理困难、可解释性有限等问题。全模态大语言模型（omni-LLMs）因其能自然处理音频、视频和文本、支持丰富推理并提供可解释的思维链反馈，成为有前景的替代方案。

Method: 研究者引入Omni-Judge研究，评估全模态大语言模型能否作为文本条件音视频生成的人类对齐评判者。研究涵盖九个感知和对齐指标，与传统指标进行相关性比较，并分析模型在不同类型任务上的表现。

Result: Omni-Judge在相关性方面与传统指标相当，在语义要求高的任务（如音频-文本对齐、视频-文本对齐、音频-视频-文本一致性）上表现优异。但在高帧率感知指标（包括视频质量和音频-视频同步）上表现不佳，这归因于有限的时间分辨率。Omni-Judge提供可解释的解释，能够暴露语义或物理不一致性，支持基于反馈的细化等实际下游应用。

Conclusion: 研究结果突显了全模态大语言模型作为多模态生成统一评估器的潜力和当前局限性。虽然它们在语义对齐任务上表现出色，但受限于时间分辨率，在高帧率感知评估方面仍需改进。Omni-Judge的可解释性反馈为实际应用提供了实用价值。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [136] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: PISCES提出了一种基于双重最优传输对齐奖励的无标注后训练方法，用于提升文本到视频生成的视觉质量和语义对齐，无需人工偏好标注。


<details>
  <summary>Details</summary>
Motivation: 当前基于奖励的后训练方法要么依赖大规模人工偏好标注，要么使用预训练视觉语言模型中的未对齐嵌入，导致可扩展性有限或监督效果不佳。

Method: PISCES通过双重最优传输对齐奖励模块，在分布层面和离散标记层面桥接文本和视频嵌入：1) 分布级OT对齐质量奖励捕捉整体视觉质量和时间一致性；2) 离散标记级OT对齐语义奖励强制文本和视频标记之间的语义时空对应。

Result: 在短视频和长视频生成实验中，PISCES在VBench的质量和语义评分上优于基于标注和无标注方法，人类偏好研究进一步验证了其有效性。双重OT对齐奖励模块兼容多种优化范式。

Conclusion: PISCES是首个通过最优传输视角改进生成后训练中无标注奖励监督的方法，为文本到视频生成提供了可扩展且有效的质量提升方案。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [137] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: 该论文分析了当前世界模型研究的碎片化现状，提出了统一的设计规范，强调世界模型应该是整合交互、感知、符号推理和空间表示的规范性框架。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型研究存在碎片化问题，主要集中在将世界知识注入到孤立任务中（如视觉预测、3D估计、符号基础），缺乏统一的定义和框架。这些任务特定的集成虽然能带来性能提升，但缺乏系统性的连贯性，无法实现整体的世界理解。

Method: 分析现有碎片化方法的局限性，提出统一的世界模型设计规范。建议一个稳健的世界模型不应是能力的松散集合，而应该是一个整合交互、感知、符号推理和空间表示的规范性框架。

Result: 提出了结构化视角来指导未来研究，旨在建立更通用、稳健和原则性的世界模型。

Conclusion: 世界模型需要从碎片化的任务特定方法转向统一的规范性框架，整合多种认知能力，以实现更全面和系统的世界理解。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [138] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 提出一个联邦学习框架，结合动态自适应焦点损失和客户端感知聚合策略，解决医疗图像分类中的数据异质性和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型需要大量数据，但医疗图像受隐私法规限制难以获取。联邦学习虽能保护隐私，但面临数据异质性和类别不平衡的挑战，影响模型泛化能力。

Method: 提出动态自适应焦点损失（DAFL），根据客户端样本分布和类别数据分布动态调整类别不平衡系数；采用加权聚合策略，适应数据规模和特征以捕获客户端间差异。

Result: 在ISIC、Ocular Disease和RSNA-ICH三个公开数据集上，该框架在大多数情况下优于DenseNet121、ResNet50、ViT-S/16、ViT-L/32、FedCLIP、Swin Transformer、CoAtNet和MixNet，准确率提升0.98%到41.69%。

Conclusion: 提出的联邦学习框架有效解决了医疗图像分类中的数据异质性和类别不平衡问题，通过动态自适应焦点损失和客户端感知聚合策略显著提升了模型性能。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [139] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReCALL框架解决生成式多模态大语言模型适配为检索器时的能力退化问题，通过诊断-生成-精炼流程，在CIRR和FashionIQ数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将生成式MLLM适配为单嵌入判别式检索器时存在范式冲突，导致原生细粒度推理能力退化，需要解决这种能力退化问题。

Method: 提出ReCALL框架：1) 通过自引导信息实例挖掘诊断检索器的认知盲点；2) 通过CoT提示基础MLLM生成纠正指令和三元组，并用VQA一致性过滤进行质量控制；3) 通过分组对比方案在这些三元组上进行持续训练，内化细粒度视觉语义区分。

Result: 在CIRR和FashionIQ数据集上的广泛实验表明，ReCALL能持续重新校准退化能力，并实现最先进的性能。

Conclusion: ReCALL框架有效解决了生成式MLLM适配为检索器时的能力退化问题，通过诊断-生成-精炼流程重新校准了判别式嵌入空间与MLLM内在组合推理的对齐。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [140] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: 提出了一种用于视频序列中时间一致的人体中心密集预测方法，通过合成数据管道生成逼真的人体帧和运动对齐序列，训练统一的ViT密集预测器，结合几何先验和特征可靠性改进，实现空间学习和时间学习。


<details>
  <summary>Details</summary>
Motivation: 现有模型在单帧精度上表现良好，但在运动、遮挡和光照变化下容易出现闪烁问题，且缺乏针对多个密集任务的配对人体视频监督数据。

Method: 1) 构建可扩展的合成数据管道，生成逼真的人体帧和运动对齐序列，提供像素级准确的深度、法线和掩码标签；2) 训练统一的ViT密集预测器，通过CSE嵌入注入显式人体几何先验，使用轻量级通道重加权模块改进几何特征可靠性；3) 采用两阶段训练策略：静态预训练获取空间表示，动态序列监督细化时间一致性。

Result: 在THuman2.1和Hi4D数据集上达到最先进性能，并能有效泛化到真实世界视频中。

Conclusion: 该方法通过合成数据管道和统一预测器设计，成功解决了视频序列中人体中心密集预测的时间一致性问题，在多个基准测试中表现优异。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [141] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 本文评估了NetVLAD作为SLAM中回环检测模块的性能，与DBoW在KITTI数据集上进行比较，通过Faiss加速实现实时查询，证明NetVLAD在精度和鲁棒性上优于DBoW。


<details>
  <summary>Details</summary>
Motivation: 传统词袋方法（如DBoW）在回环检测中效率高，但在外观变化和感知混淆情况下性能下降；而基于深度学习的视觉位置识别描述符（如NetVLAD）虽然鲁棒性更强，但计算成本高，被认为难以实现实时SLAM。本文旨在评估NetVLAD作为回环检测模块的实用性。

Method: 在KITTI数据集上对NetVLAD和DBoW进行实证评估，引入细粒度Top-K精确率-召回率曲线来更好地反映回环检测场景（查询可能没有或存在多个有效匹配），并使用Faiss加速的最近邻搜索实现实时查询速度。

Result: 通过Faiss加速，NetVLAD实现了实时查询速度，同时在精度和鲁棒性方面优于DBoW，成为SLAM中回环检测的实用替代方案。

Conclusion: NetVLAD作为回环检测模块不仅实现了实时性能，还在准确性和鲁棒性上超越了传统词袋方法DBoW，为SLAM系统提供了一个实用的即插即用替代方案。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [142] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: VRGaussianAvatar是一个实时全身3D高斯溅射虚拟现实系统，仅使用头戴显示器追踪信号就能实现实时全身3D高斯溅射虚拟化身，通过并行管道和双目批处理技术提升渲染效率。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟现实中的虚拟化身系统在实时性、渲染质量和计算效率方面存在挑战，需要一种能够仅使用头戴显示器追踪信号就能实现高质量实时全身虚拟化身的解决方案。

Method: 系统采用并行管道架构，包含VR前端和GA后端。VR前端使用逆运动学估计全身姿态，GA后端从单张图像重建3D高斯溅射虚拟化身。引入双目批处理技术，将左右眼视图在单个批次中联合处理以减少冗余计算。

Result: 系统能够维持交互式VR性能，在感知外观相似性、具身感和合理性方面优于基于图像和视频的网格虚拟化身基线，支持高分辨率VR显示。

Conclusion: VRGaussianAvatar成功实现了仅使用HMD追踪信号的实时全身3D高斯溅射虚拟化身，通过并行管道和双目批处理技术达到了高质量的VR体验，在用户研究中表现出优越的感知质量。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [143] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 提出一种无需人工标注的显微图像语义分割框架，通过相场模拟生成微观结构形态，使用CycleGAN将模拟图像转换为真实SEM图像，训练U-Net模型在实验图像上实现优异泛化性能。


<details>
  <summary>Details</summary>
Motivation: 显微图像语义分割对高通量材料表征至关重要，但专家标注数据成本高昂、主观性强且稀缺。基于物理的模拟虽可扩展，但存在显著的领域差距，缺乏实验数据中复杂的纹理、噪声模式和成像伪影。

Method: 1. 利用相场模拟生成大量微观结构形态，获得完美的内在真实掩码；2. 使用CycleGAN进行非配对图像到图像转换，将干净的模拟图像转换为大规模高保真真实SEM图像数据集；3. 仅在此合成数据上训练U-Net模型。

Result: 在未见过的实验图像上，模型实现了平均边界F1分数0.90和交并比0.88的优异性能。通过t-SNE特征空间投影和香农熵分析验证，合成图像在统计和特征上与真实数据流形无法区分。

Conclusion: 该生成框架完全解耦了模型训练与人工标注，将数据稀缺问题转化为数据丰富问题，为加速材料发现和分析提供了稳健且完全自动化的解决方案。

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [144] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: FastPhysGS：基于4D物理模拟的快速3D高斯泼溅框架，通过实例感知粒子填充和双向图解耦优化，在1分钟内实现高保真物理模拟


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D高斯泼溅扩展到4D物理模拟存在挑战：要么依赖手动参数调整，要么从视频扩散模型提取动态，限制了泛化能力和优化效率。基于LLMs/VLMs的方法存在文本/图像到3D的感知差距，导致物理行为不稳定，且常忽略3DGS的表面结构，产生不合理的运动。

Method: 提出FastPhysGS框架：1) 实例感知粒子填充(IPF)结合蒙特卡洛重要性采样(MCIS)，高效填充内部粒子同时保持几何保真度；2) 双向图解耦优化(BGDO)，自适应策略快速优化从VLM预测的材料参数。

Result: 实验表明FastPhysGS仅使用7GB运行时内存，在1分钟内实现高保真物理模拟，性能优于先前工作，具有广泛的应用潜力。

Conclusion: FastPhysGS提供了一个快速、稳健的基于物理的动态3D高斯泼溅模拟框架，解决了现有方法在泛化能力、优化效率和物理合理性方面的限制。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [145] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: 提出DenVisCoM Mamba块和混合架构，用于实时准确估计光流和视差，通过统一架构处理多视图几何和运动任务


<details>
  <summary>Details</summary>
Motivation: 多视图几何和运动任务本质相关，需要统一架构同时处理光流和视差估计，并解决实时推理、内存占用和准确性之间的平衡问题

Method: 基于DenVisCoM Mamba块和Transformer注意力块的混合架构，专门为联合估计运动和3D密集感知任务设计

Result: 在大量数据集上验证了准确性和实时处理的平衡，实验结果表明模型能够实时准确估计光流和视差

Conclusion: 提出的混合架构有效解决了实时推理、内存占用和准确性之间的权衡，为联合运动估计和3D感知提供了可行方案

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [146] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 简单的线性分类器在冻结的视觉基础模型特征上训练，在真实场景中超越复杂专用检测器，准确率提升超30%，但存在重捕获、传输等局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在精心设计的基准测试中表现优异，但在真实场景中性能急剧下降，需要更可靠的检测方法。

Method: 使用现代视觉基础模型（如Perception Encoder、MetaCLIP 2、DINOv3）的冻结特征，仅训练简单的线性分类器进行检测。

Result: 该方法在标准基准测试中与专用检测器相当，在真实场景数据集中表现显著优于专用检测器，准确率提升超过30%。

Conclusion: 基础模型的大规模预训练数据包含合成内容，使其具备检测能力；建议AI取证应从静态基准测试转向利用基础模型不断演化的世界知识，以实现真实世界的可靠性。

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [147] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: TAPTQ是一种专门为3D几何学习设计的尾感知后训练量化方法，通过渐进式校准构造、三元搜索优化和TRE引导的模块补偿，解决了3D模型量化中的特征分布复杂和校准开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 3D几何模型的复杂性和规模对资源受限平台的部署提出了重大挑战。现有的后训练量化方法主要针对2D视觉Transformer优化，无法有效迁移到3D模型，因为3D模型具有复杂的特征分布和过高的校准开销。

Method: 1. 渐进式粗到细校准构造策略：构建高度紧凑的子集，实现统计纯度和几何代表性；2. 三元搜索求解器：将量化区间搜索重新表述为优化问题，将计算复杂度从O(N)降低到O(log N)；3. TRE引导的模块补偿：使用尾相对误差指标自适应识别和修正对长尾激活异常值敏感的模块中的失真。

Result: 在VGGT和Pi3基准测试上的广泛实验表明，TAPTQ在精度上始终优于最先进的后训练量化方法，同时显著减少了校准时间。

Conclusion: TAPTQ为3D几何学习提供了一种高效的后训练量化解决方案，通过创新的校准策略、优化算法和误差补偿机制，成功解决了3D模型量化中的关键挑战，实现了精度和效率的平衡。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [148] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ObjEmbed是一种新颖的多模态大语言模型嵌入模型，通过将图像分解为多个区域嵌入（每个对应一个对象）和全局嵌入，实现细粒度图像-文本对齐，支持视觉定位、局部图像检索和全局图像检索等多种任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型在全局图像-文本对齐方面表现出色，但在图像区域与特定短语之间的细粒度对齐方面存在困难。需要一种能够同时处理对象级和图像级任务的统一模型。

Method: ObjEmbed将输入图像分解为多个区域嵌入（每个对应一个对象）和全局嵌入。为每个区域生成两个互补嵌入：用于语义匹配的对象嵌入和预测定位质量的IoU嵌入。最终对象匹配分数结合语义相似度和预测的IoU，实现更准确的检索。所有对象和完整图像都在单次前向传播中编码。

Result: 在18个多样化基准测试中表现出优越性能，展示了强大的语义辨别能力。模型能够同时处理区域级和图像级任务，支持视觉定位、局部图像检索和全局图像检索等多种视觉理解任务。

Conclusion: ObjEmbed通过对象导向的表示方法，结合语义和空间信息，实现了细粒度的图像-文本对齐。其多功能性和高效编码使其成为解决视觉语言理解中对象级对齐挑战的有效解决方案。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [149] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: 该研究扩展了智能停车系统，从区域级车位计数升级为精确的停车位级监控，通过距离感知匹配和自适应边界框划分方法，在边缘设备上实现98.80%的准确率和8秒推理时间，并引入数字孪生雏形和基于电视盒的应用支持服务器。


<details>
  <summary>Details</summary>
Motivation: 现有智能停车系统虽然能准确估计区域内空闲车位数量，但无法提供具体停车位级别的信息，限制了系统支持更高级应用的能力。需要从区域级监控升级到停车位级监控，以提供更精细的洞察。

Method: 1. 采用基于空间容差的距离感知匹配方法进行停车位级监控；2. 针对挑战性空间引入自适应边界框划分方法；3. 使用YOLOv11m模型（40.5MB大小）进行车辆检测；4. 引入数字孪生雏形（Digital Shadow）可视化停车位实体；5. 基于改造的电视盒构建应用支持服务器。

Result: 在资源受限的边缘设备上实现了98.80%的平衡准确率，推理时间为8秒。系统能够提供停车位级别的精确监控，支持云服务、停车指示牌和统计机器人之间的可扩展通信，同时通过硬件重用提高了可持续性。

Conclusion: 该研究成功将智能停车系统从区域级监控升级为停车位级监控，通过创新的距离感知匹配和自适应边界框划分方法，在保持高准确率的同时实现了边缘设备上的高效运行。数字孪生雏形和应用支持服务器的引入为系统向完整数字孪生演进和可持续发展奠定了基础。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [150] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: Mind-Brush是一个统一的智能框架，将图像生成转变为动态的知识驱动工作流，通过"思考-研究-创造"范式解决现有模型在复杂知识推理和实时适应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型主要作为静态的文本到像素解码器，难以理解用户的隐含意图，缺乏复杂知识推理能力，且受限于静态内部先验，无法适应现实世界的动态变化。

Method: 提出Mind-Brush统一智能框架，模拟人类"思考-研究-创造"范式：主动检索多模态证据以锚定分布外概念，使用推理工具解决隐含视觉约束，将生成转变为动态知识驱动工作流。

Result: 在提出的Mind-Bench基准测试（500个样本涵盖实时新闻、新兴概念、数学和地理推理等领域）上，Mind-Brush显著提升了统一模型能力，使Qwen-Image基线实现了从零到一的能力飞跃，同时在WISE和RISE等现有基准上也取得了优异结果。

Conclusion: Mind-Brush通过将生成过程转变为动态的知识驱动工作流，有效解决了现有模型在意图理解、复杂知识推理和实时适应方面的局限性，为智能图像生成提供了新的框架方向。

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [151] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: MagicFuse：基于扩散模型的单图像融合框架，仅使用单张低质量可见光图像生成跨光谱场景表示，性能媲美多模态融合方法


<details>
  <summary>Details</summary>
Motivation: 解决在恶劣条件下只有可见光传感器可用时，如何继续获得多模态图像融合优势的问题。传统数据级融合需要多模态输入，而实际应用中可能只有可见光图像可用。

Method: 提出单图像融合概念，将数据级融合扩展到知识级。开发MagicFuse框架，包含三个分支：1）光谱内知识增强分支挖掘可见光谱中被遮挡的场景信息；2）跨光谱知识生成分支学习转移到红外光谱的热辐射分布模式；3）多域知识融合分支整合两个分支的扩散流概率噪声，通过连续采样获得跨光谱场景表示。同时施加视觉和语义约束确保表示满足人类观察并支持下游语义决策。

Result: 大量实验表明，MagicFuse仅依赖单张退化可见光图像，就能实现与最先进多模态输入融合方法相当甚至更好的视觉和语义表示性能。

Conclusion: MagicFuse成功将传统数据级融合扩展到知识级，仅使用单张可见光图像就能生成高质量的跨光谱场景表示，为恶劣条件下的图像分析提供了实用解决方案。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [152] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: 本文提出了一种基于MEMS-LiDAR的隐私合规人员检测方法，通过结合真实数据和CARLA模拟生成的合成数据，在工业环境中实现高性能且符合GDPR要求的人员检测。


<details>
  <summary>Details</summary>
Motivation: 工业室内空间需要可靠检测未经授权人员以防止安全事故，但传统基于深度学习的视觉方法存在光照敏感、隐私违规问题，且需要大量耗时的人工标注数据。

Method: 使用MEMS-LiDAR采集匿名化3D点云数据，结合CARLA模拟框架生成的合成场景数据来增强训练数据集，减少真实数据采集和标注的工作量。

Result: 混合数据方法相比仅使用真实数据的模型，平均精度提高了44个百分点，同时将人工标注工作量减少了50%。

Conclusion: 该方法提供了一种可扩展、成本效益高的替代方案，系统展示了合成LiDAR数据如何在工业环境中结合高性能人员检测与GDPR合规性。

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [153] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: DDP-WM是一种新型世界模型，通过解耦动态预测原理，将潜在状态演化分解为稀疏的主要动态和次要背景更新，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有密集Transformer世界模型计算开销大，阻碍实时部署，需要解决效率与性能的瓶颈问题。

Method: 提出DDP-WM模型，采用解耦动态预测原理，通过高效历史处理和动态定位分离主要动态，使用交叉注意力机制进行背景更新。

Result: 在导航、桌面操作、可变形物体和多体交互等任务中表现优异，在Push-T任务上实现约9倍推理加速，MPC成功率从90%提升到98%。

Conclusion: DDP-WM为开发高效、高保真世界模型提供了有前景的路径，显著提升了计算效率和规划性能。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [154] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 提出了一种用于地下矿山岩体结构面自动识别的新方法，结合单次滤波、循环方位变换和层次聚类技术，在真实矿山环境中实现了高精度的结构面特征提取。


<details>
  <summary>Details</summary>
Motivation: 地下矿山岩体结构面的准确识别对岩体稳定性评估、开挖安全和运营效率至关重要。虽然无人机和移动激光扫描技术能高效采集点云数据，但在完全封闭的岩体表面等真实场景中，开发鲁棒且高效的自动结构面识别方法仍是一个未解决的研究问题。

Method: 提出了一种新的自动结构面识别方法，包含三个关键技术：1) 单次滤波策略，使用信号处理技术一次性隔离平面区域并抑制噪声和高曲率伪影；2) 创新的循环方位变换方案，解决笛卡尔聚类在极坐标方位数据上的局限性，实现倾角和倾向在笛卡尔空间中的准确表示；3) 层次聚类技术，处理不同密度分布并识别聚类，无需用户定义聚类数量。

Result: 该方法在真实矿山采场数据上验证，与使用Virtual Compass工具手动选取的结构面平面以及广泛使用的自动结构映射技术进行对比。所提方法表现出最低的平均绝对误差：倾角误差1.95°，倾向误差2.20°，离散误差低于3°，优于其他技术。

Conclusion: 该方法为地下矿山岩体结构面的自动识别提供了一种高效、准确的解决方案，能够处理真实场景中的复杂数据，为岩体稳定性评估和安全分析提供了可靠的技术支持。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [155] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: STT-LTF是一个时空Transformer框架，用于长期卫星图像时间序列预测，能够同时处理空间上下文和时间序列，在异质性地中海景观中实现准确的多尺度预测。


<details>
  <summary>Details</summary>
Motivation: 解决异质性地中海景观中长期卫星图像时间序列分析的挑战，包括复杂空间模式、季节变化和多年代环境变化在不同尺度上的相互作用。

Method: 提出STT-LTF框架，通过统一Transformer架构处理多尺度空间补丁和长达20年的时间序列，采用空间掩码、时间掩码和水平采样等自监督学习策略，直接预测任意未来时间点。

Result: 在1984-2024年Landsat数据上，STT-LTF实现了MAE 0.0328和R^2 0.8412的预测性能，优于传统统计方法、CNN、LSTM和标准Transformer。

Conclusion: STT-LTF框架能够有效处理不规则时间采样和可变预测范围，特别适合分析经历快速生态转变的异质性地中海景观，为长期环境监测提供了强大工具。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [156] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文提出了一种针对自回归视频扩散模型的训练免费注意力优化框架，通过压缩KV缓存、加速交叉注意力和稀疏化自注意力，实现了5-10倍的端到端加速，同时保持视觉质量稳定。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在推理时面临核心注意力层的瓶颈：随着生成进行，KV缓存不断增长，导致延迟增加和GPU内存占用上升，这限制了可用时间上下文并损害了长距离一致性。

Method: 提出了统一的训练免费注意力框架：1) TempCache通过时间对应性压缩KV缓存以限制缓存增长；2) AnnCA使用快速近似最近邻匹配选择帧相关提示词来加速交叉注意力；3) AnnSA通过限制每个查询仅与语义匹配的键进行交互来稀疏化自注意力。

Result: 实验表明，该方法实现了5-10倍的端到端加速，同时保持了近乎相同的视觉质量，并在长序列生成中维持稳定的吞吐量和近乎恒定的峰值GPU内存使用，而先前方法会逐渐变慢且内存使用不断增加。

Conclusion: 通过识别并解决自回归视频扩散中的冗余问题，提出的注意力优化框架显著提升了推理效率，同时保持生成质量，为长格式合成、视频世界模型和交互式神经游戏引擎等应用提供了实用解决方案。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [157] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: 提出LDRNet用于胸部CT图像的大变形配准，通过粗到细的配准场优化和创新的细化块与刚性块，在速度和精度上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有深度学习医学图像配准算法主要针对脑部图像，而胸部CT配准面临更大的变形、更复杂的背景和区域重叠问题，需要专门的大变形配准方法

Method: 提出LDRNet无监督深度学习方法：1) 先预测粗分辨率配准场，然后从粗到细进行细化；2) 引入细化块在不同分辨率下优化配准场；3) 使用刚性块从高层特征学习变换矩阵

Result: 在私有数据集和公开数据集SegTHOR上评估，相比传统方法和深度学习模型VoxelMorph、RCN、LapIRN，LDRNet在大变形图像配准上达到最先进性能且速度更快

Conclusion: LDRNet能够有效处理胸部CT图像的大变形配准问题，在精度和速度方面均优于现有方法，为复杂医学图像配准提供了有效解决方案

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [158] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出GPD框架，通过渐进式蒸馏加速视频扩散模型的推理过程，将采样步数从48减少到6，同时保持视觉质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成方面取得了显著成功，但去噪过程的高计算成本仍然是主要瓶颈。现有方法在减少扩散步数方面显示出潜力，但在应用于视频生成时往往会出现显著的质量下降。

Method: 提出引导渐进式蒸馏(GPD)框架，包含两个关键组件：1)在线生成训练目标，降低优化难度同时提高计算效率；2)潜在空间中的频域约束，促进细粒度细节和时序动态的保留。教师模型逐步指导学生模型以更大的步长操作。

Result: 应用于Wan2.1模型时，GPD将采样步数从48减少到6，同时在VBench基准上保持有竞争力的视觉质量。与现有蒸馏方法相比，GPD在流程简单性和质量保持方面都显示出明显优势。

Conclusion: GPD框架成功解决了视频扩散模型推理加速中的质量退化问题，通过渐进式蒸馏策略实现了高效且高质量的快速视频生成。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [159] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: VIA-Bench是一个评估多模态大语言模型在视觉错觉和异常场景下鲁棒性的基准测试，包含6类视觉错觉，超过1000个高质量问答对，测试了20多个先进模型，发现模型在违背常识先验的场景中存在显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在标准分布数据上表现出色甚至超越人类水平，但其鲁棒性在违背常识先验的视觉错觉和异常场景中尚未得到充分检验，存在评估盲区。

Method: 构建VIA-Bench基准，包含六类核心视觉错觉：颜色错觉、运动错觉、格式塔错觉、几何与空间错觉、一般视觉错觉和视觉异常；通过人工参与循环审查构建超过1000个高质量问答对；评估20多个最先进的多模态大语言模型，包括专有、开源和推理增强模型。

Result: 评估发现多模态大语言模型存在显著脆弱性，特别是思维链推理对鲁棒性提升有限，在错觉刺激下模型逻辑容易崩溃，形成"脆弱幻象"；揭示了机器感知与人类感知的根本差异。

Conclusion: 解决这种感知瓶颈对于人工通用智能的发展至关重要，VIA-Bench基准将公开发布数据和代码，为未来研究提供重要评估工具。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [160] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出基于街景图像引导的数据采集策略，利用公开街景图像识别兴趣点，用于跨国家ADAS/ADS系统感知模型的高效适应，相比随机采样仅需一半目标域数据即可达到相当性能。


<details>
  <summary>Details</summary>
Motivation: ADAS和ADS系统在不同国家部署面临挑战，由于立法、交通基础设施和视觉惯例的差异导致域偏移，降低感知性能。传统跨国家数据收集依赖大量道路驾驶，成本高且效率低。

Method: 提出街景引导的数据采集策略，利用公开街景图像识别兴趣点。引入两种POI评分方法：基于视觉基础模型的KNN特征距离方法和基于视觉-语言模型的视觉归因方法。采用收集-检测协议，构建Zenseact Open Dataset与Mapillary街景图像的共定位数据集。

Result: 在交通标志检测任务上，该方法仅使用一半目标域数据即可达到与随机采样相当的性能。提供了全国家分析的成本估算，证明大规模街景处理在经济上仍然可行。

Conclusion: 街景引导的数据采集策略为跨国家模型适应提供了高效且经济有效的解决方案，能够显著减少数据收集成本并提高模型部署效率。

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [161] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: SPIRIT是一个统一且兼容视觉基础模型的框架，通过轻量级物理信息插件将VFM适配到红外小目标检测，解决了模态差异和跨帧关联不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在实际应用中需要同时支持单帧分析和视频模式跟踪。虽然可以利用视觉基础模型缓解红外数据稀缺问题，但红外小目标的弱辐射信号和有限语义线索与可见光谱图像存在显著模态差异，导致语义导向的VFM和外观驱动的跨帧关联在IRSTD中不可靠。

Method: 提出SPIRIT框架，包含两个轻量级物理信息插件：空间上，PIFR通过近似秩稀疏分解来细化特征，抑制结构化背景成分并增强稀疏目标信号；时间上，PGMA将历史衍生的软空间先验注入内存交叉注意力中，约束跨帧关联，实现鲁棒的视频检测，并在缺乏时间上下文时自然回归到单帧推理。

Result: 在多个IRSTD基准测试上的实验表明，该方法相对于基于VFM的基线方法取得了持续增益，并实现了最先进的性能。

Conclusion: SPIRIT通过物理信息插件成功地将视觉基础模型适配到红外小目标检测任务中，解决了模态差异问题，实现了统一且鲁棒的单帧和多帧推理框架。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [162] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: CloDS是一个无监督学习框架，通过多视角视觉观测学习布料动力学，无需已知物理属性作为监督，采用三阶段流程实现视频到几何的映射和动力学模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要已知物理属性作为监督或输入，限制了在未知条件下的应用。为了解决这一挑战，作者提出了布料动力学接地（CDG）场景，旨在从多视角视觉观测中无监督学习布料动力学。

Method: CloDS采用三阶段流程：1）视频到几何接地；2）在接地网格上训练动力学模型。为处理大非线性变形和严重自遮挡，提出了双位置不透明度调制，通过基于网格的高斯泼溅实现2D观测到3D几何的双向映射，同时考虑高斯分量的绝对和相对位置。

Result: 综合实验评估表明，CloDS能够有效地从视觉数据中学习布料动力学，同时对未见配置保持强大的泛化能力。

Conclusion: CloDS是一个有效的无监督动态学习框架，能够从多视角视觉观测中学习布料动力学，解决了现有方法需要已知物理属性的限制，具有良好的泛化性能。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [163] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 该论文提出了WS-IMUBench基准，系统评估了弱监督IMU时序动作定位方法在仅有序列级标签下的表现，通过大量实验揭示了不同方法的迁移性、效果和局限性。


<details>
  <summary>Details</summary>
Motivation: 传统IMU动作识别只能进行片段分类，无法捕捉真实世界行为的丰富时序结构。虽然IMU时序动作定位能预测动作类别和起止时间，但需要密集的帧级边界标注，成本高昂且难以扩展。因此需要研究仅使用序列级标签的弱监督方法。

Method: 引入WS-IMUBench基准，系统评估7种代表性弱监督方法在7个公开IMU数据集上的表现，进行了超过3,540次模型训练和7,080次推理评估。研究三个核心问题：方法跨模态迁移性、弱监督效果、以及失败模式分析。

Result: 研究发现：(1) 迁移性具有模态依赖性，时域方法通常比基于图像提案的方法更稳定；(2) 在有利数据集上（如动作较长、传感维度较高），弱监督方法可以与传统监督方法竞争；(3) 主要失败模式源于短动作、时序模糊性和提案质量差。

Conclusion: 论文为推进WS-IMU-TAL提出了具体方向（如IMU特定提案生成、边界感知目标、更强时序推理），并建立了可复现的基准模板、数据集、协议和分析框架，加速社区在可扩展弱监督IMU时序动作定位方面的进展。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [164] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: VIBE是一个视觉指令图像编辑基准，包含三个层次的交互：指示性定位、形态操作和因果推理，用于评估模型跟随视觉指令（如草图）进行图像编辑的能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑系统主要基于文本指导，而人类交流本质上是多模态的，视觉指令（如草图）能更有效地传达空间和结构意图。需要填补视觉指令图像编辑评估的空白。

Method: 引入VIBE基准，包含三个层次的高质量多样化测试案例：指示性定位、形态操作和因果推理，复杂度递增。提出基于LMM-as-a-judge的评估框架，使用任务特定指标进行细粒度评估。

Result: 评估了17个开源和专有图像编辑模型，发现专有模型展现出早期视觉指令跟随能力，表现优于开源模型。但随着任务难度增加，即使是性能最强的系统表现也显著下降。

Conclusion: 视觉指令图像编辑仍面临挑战，特别是在复杂任务上。专有模型目前领先但仍有改进空间，这为未来研究指明了方向。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [165] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: 研究发现，在检测多模态虚假信息时，基于像素级别的深度伪造检测器效果有限，而基于语义理解和外部证据的事实核查系统表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息中的欺骗通常不仅来自图像像素级篡改，更来自图像-文本对共同表达的语义和上下文主张。然而大多数深度伪造检测器只检测像素级伪造，不考虑主张层面的含义，尽管它们越来越多地集成到自动事实核查管道中。这引发了一个核心科学和实践问题：像素级检测器是否为验证图像-文本主张提供了有用信号，还是引入了误导的真实性先验，破坏了基于证据的推理？

Method: 使用两个互补的基准测试MMFakeBench和DGM4，评估：(1)最先进的仅图像深度伪造检测器，(2)基于证据的事实核查系统（通过蒙特卡洛树搜索进行工具引导检索，通过多智能体辩论进行深思熟虑推理），(3)将检测器输出作为辅助证据注入的混合事实核查系统。

Result: 深度伪造检测器在MMFakeBench上的F1分数为0.26-0.53，在DGM4上为0.33-0.49，价值有限。将检测器预测纳入事实核查管道会因非因果真实性假设使性能降低0.04-0.08 F1。相比之下，基于证据的事实核查系统表现最佳，在MMFakeBench上达到约0.81 F1，在DGM4上达到0.55 F1。

Conclusion: 多模态主张验证主要由语义理解和外部证据驱动，像素级伪影信号并不能可靠地增强对现实世界图像-文本虚假信息的推理。基于证据的方法优于依赖像素级检测器的方法。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [166] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: Lazy Attention：一种通过跨层共享相似注意力模式来减少MLLMs冗余计算的高效注意力机制，能降低35%以上KV缓存使用并提升1.5倍吞吐量，仅牺牲约1%性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）因视觉编码器中大量视觉token导致推理成本高昂，现有token级优化方法会破坏KV缓存完整性，影响长文本生成任务。

Method: 提出Lazy Attention机制，通过研究发现超过一半解码层的注意力语义相似，允许某些层继承前层的注意力模式。开发了专为MLLMs设计的轻量级层共享缓存Q Cache，支持跨相邻层查询重用。

Result: 在多个基准测试中，该方法能减少超过35%的KV缓存使用，实现1.5倍吞吐量提升，仅牺牲约1%性能。相比SOTA token级方法，实现了更好的准确性保持。

Conclusion: Lazy Attention是一种高效且灵活的注意力机制，能与现有token修剪方法正交部署，兼容Flash Attention和KV缓存等推理框架，为MLLMs推理优化提供了新方向。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [167] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: DSXFormer：一种用于高光谱图像分类的新型双池化光谱挤压-扩展变换器，通过动态上下文注意力机制平衡光谱区分性和计算效率，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临光谱维度高、光谱-空间相关性复杂、标记训练样本有限等挑战。现有基于Transformer的方法在保持计算效率的同时难以获得足够的光谱区分性。

Method: 提出DSXFormer模型，包含：1）双池化光谱挤压-扩展（DSX）块，利用全局平均池化和最大池化自适应重新校准光谱特征通道；2）动态上下文注意力（DCA）机制，在基于窗口的Transformer架构中动态捕获局部光谱-空间关系；3）补丁提取、嵌入和合并策略，实现高效多尺度特征学习。

Result: 在四个高光谱基准数据集（Salinas、Indian Pines、Pavia University、Kennedy Space Center）上分别达到99.95%、98.91%、99.85%、98.52%的分类准确率，优于现有最先进方法。

Conclusion: DSXFormer通过光谱双池化挤压-扩展和动态上下文注意力的联合集成，在光谱强调和空间上下文表示之间实现了有效平衡，为高光谱图像分类提供了高效且高性能的解决方案。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [168] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: RS-MPOD：一种用于遥感图像的多模态开放词汇检测框架，通过视觉提示和文本提示的结合来解决传统文本提示在遥感场景中语义不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 遥感中的开放词汇目标检测通常依赖纯文本提示来指定目标类别，但这种方法假设推理时的类别查询可以通过预训练诱导的文本-视觉对齐可靠地接地。实际上，在遥感场景中，由于任务和应用特定的类别语义，这种假设经常失效，导致开放词汇设置下的类别指定不稳定。

Method: 提出RS-MPOD多模态开放词汇检测框架，将类别指定从纯文本提示扩展到包含实例接地的视觉提示、文本提示及其多模态集成。框架包含视觉提示编码器（从示例实例中提取基于外观的类别线索，实现无需文本的类别指定）和多模态融合模块（在两种模态都可用时整合视觉和文本信息）。

Result: 在标准、跨数据集和细粒度遥感基准测试上的广泛实验表明：视觉提示在语义模糊和分布偏移下能提供更可靠的类别指定；当文本语义良好对齐时，多模态提示提供了一个保持竞争力的灵活替代方案。

Conclusion: RS-MPOD通过引入视觉提示和多模态集成，有效解决了遥感场景中开放词汇检测的类别指定不稳定问题，为遥感目标检测提供了更可靠和灵活的解决方案。

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [169] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: 论文提出一种通过缩放分隔符token隐藏状态的方法，解决多图像输入时跨图像信息泄漏问题，无需额外训练或推理成本即可提升多图像任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在单图像任务上表现良好，但在多图像输入时性能下降，主要原因是跨图像信息泄漏问题。现有模型虽然使用分隔符token标记图像边界，但这些token未能有效阻止跨图像信息泄漏。

Method: 提出通过缩放分隔符token的隐藏状态来增强其有效性。这种方法通过强化图像内部交互并限制不必要的跨图像交互，增强模型保留图像特定信息的能力。

Result: 实验显示在Mantis、MuirBench、MIRB、QBench2等多图像基准测试中性能提升。在需要清晰区分的纯文本任务上，如TQABench、MultiNews、WCEP-10等多文档和多表格理解基准测试中也表现改善。

Conclusion: 提出的分隔符token隐藏状态缩放方法能有效解决跨图像信息泄漏问题，提升模型在多图像和多文档任务中的性能，且无需额外训练或推理成本。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [170] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 提出一种新的扩散模型方法，通过引入掩码特征和额外损失项，实现对用户定义图像区域的精确局部控制，同时让模型根据原始提示自主生成其余区域。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成方法虽然能通过文本提示生成高质量图像，但实现精确控制需要反复试错。现有图像级控制方法（如边缘、分割、深度图）对整个图像施加统一条件，缺乏局部控制能力。

Method: 提出新的训练框架，包含掩码特征和额外损失项。该损失项利用任何扩散步骤中初始潜在向量的预测，增强当前步骤与潜在空间中最终样本的对应关系，从而实现精确的局部控制。

Result: 大量实验表明，该方法能有效合成具有受控局部条件的高质量图像，在用户定义区域实现精确控制的同时保持整体图像质量。

Conclusion: 该方法解决了扩散模型中局部控制不足的问题，通过创新的训练框架实现了对用户指定区域的精确控制，同时保留模型根据原始提示自主生成其余区域的能力。

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [171] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: SurfSplat：基于2D高斯泼溅的稀疏图像3D重建框架，通过表面连续性先验和强制alpha混合策略，解决了现有方法几何不连续和颜色偏差问题，并提出了高分辨率渲染一致性评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的方法在稀疏图像3D重建中存在问题：它们生成的几何表面不连续，产生离散、颜色偏差的点云，在正常分辨率下看似合理但在近距离观察时出现严重伪影。

Method: 1) 使用2D高斯泼溅基元，提供更强的各向异性和更高的几何精度；2) 引入表面连续性先验；3) 采用强制alpha混合策略；4) 提出高分辨率渲染一致性评估指标。

Result: 在RealEstate10K、DL3DV和ScanNet数据集上的实验表明，SurfSplat在标准指标和新提出的HRRC指标上都优于现有方法，实现了从稀疏输入的高保真3D重建。

Conclusion: SurfSplat通过2D高斯泼溅基元、表面连续性先验和强制alpha混合策略，有效解决了稀疏图像3D重建中的几何不连续和纹理失真问题，为高保真重建提供了鲁棒解决方案。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [172] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: UniDriveDreamer：用于自动驾驶的单阶段统一多模态世界模型，直接生成多模态未来观测，无需中间表示或级联模块


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型主要专注于单模态生成（多摄像头视频或LiDAR序列），缺乏统一的多模态生成方法，这限制了自动驾驶系统的全面感知能力

Method: 1. 使用LiDAR专用VAE编码LiDAR序列，视频VAE编码多摄像头图像；2. 提出统一潜在锚定(ULA)显式对齐两种模态的潜在分布；3. 对齐特征融合后由扩散transformer联合建模几何对应和时间演化；4. 结构化场景布局信息作为条件信号指导合成

Result: UniDriveDreamer在视频和LiDAR生成方面均优于先前最先进方法，同时在下游任务中带来可测量的改进

Conclusion: 该研究提出了首个单阶段统一多模态世界模型，能够直接生成自动驾驶的多模态未来观测，为自动驾驶系统提供了更全面的感知能力

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [173] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 该论文提出了ClueTracer方法，通过追踪推理路径中的关键线索传播来抑制多模态推理模型中的幻觉问题，无需额外训练即可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理模型在解决复杂视觉问题时容易出现幻觉，生成与输入图像或问题无关的内容。作者发现这是由于"推理漂移"现象导致的：模型在收集视觉线索时过度关注与问题无关的实体，逐渐使推理轨迹与视觉基础脱钩。

Method: 提出了ClueRecall评估指标来衡量视觉线索检索能力，并开发了ClueTracer方法。这是一个无需训练、无需参数、架构无关的插件，通过从问题出发追踪关键线索在推理路径中的传播（问题→输出→视觉标记），定位任务相关区域并抑制对无关区域的注意力。

Result: ClueTracer无需额外训练即可显著提升所有推理架构的性能：在推理基准测试中提升1.21倍，在非推理设置中也能获得1.14倍的增益。该方法适用于包括R1-OneVision、Ocean-R1、MM-Eureka等多种模型。

Conclusion: 通过识别和解决推理漂移问题，ClueTracer提供了一种有效抑制多模态推理模型幻觉的方法，无需额外训练即可显著提升模型性能，具有广泛的适用性和实用性。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [174] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: Auto-Comp是一个自动化合成基准生成管道，用于评估视觉语言模型在组合推理中的缺陷，特别是颜色绑定和空间关系理解方面的失败。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在组合推理中存在关键缺陷，经常混淆"红色立方体和蓝色球体"与"蓝色立方体和红色球体"。需要细粒度、可控的分析方法来解构这些失败的根本原因。

Method: 引入Auto-Comp自动化合成管道，生成可扩展的基准测试。通过生成最小化描述和LLM生成的上下文描述配对图像，进行受控的A/B测试，以分离核心绑定能力与视觉语言复杂性。

Result: 评估20个VLM模型发现，CLIP和SigLIP模型家族在颜色绑定和空间关系方面普遍存在组合推理失败。新型"混淆基准"揭示模型对低熵干扰物高度敏感，其组合失败超出了已知的词袋限制。

Conclusion: 发现了一个令人惊讶的权衡：视觉语言上下文虽然提供全局场景线索有助于空间推理，但同时通过引入视觉杂乱而阻碍局部属性绑定。发布了Auto-Comp管道和所有生成的基准测试。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [175] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: SegmentMIL：基于Transformer的多视图多示例学习框架，用于患者级别的冠状动脉狭窄分类，无需视图级标注，仅需患者级监督即可同时预测狭窄存在并定位受影响区域


<details>
  <summary>Details</summary>
Motivation: 冠状动脉狭窄是心血管疾病的主要原因，目前基于单视图血管造影的深度学习模型需要昂贵的视图级标注，且无法捕捉多视图间的时空动态和依赖关系，而这些对临床诊断至关重要

Method: 提出SegmentMIL框架，基于Transformer的多视图多示例学习方法，仅使用患者级监督（无需视图级标注），联合预测狭窄存在并定位受影响解剖区域（区分左右冠状动脉及其分段）

Result: 在内部和外部评估中均获得高性能，优于视图级模型和经典MIL基线，显示出作为临床可行且可扩展的冠状动脉狭窄诊断方案的潜力

Conclusion: SegmentMIL是一个有效的患者级冠状动脉狭窄分类框架，无需昂贵的视图级标注，能够捕捉多视图间的依赖关系，在真实临床数据上表现优异，具有临床实用性和可扩展性

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [176] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: UrbanGS是一个用于大规模城市场景重建的3D高斯泼溅框架，通过深度一致D-Normal正则化和空间自适应高斯剪枝策略，解决了几何一致性、内存效率和计算可扩展性等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在有限场景中能实现高质量实时渲染，但扩展到大规模城市场景时面临几何一致性、内存效率和计算可扩展性等关键挑战。现有方法依赖单目法线估计器，能有效更新旋转参数但难以更新位置参数，且在大规模场景中缺乏有效的几何精度保证和内存优化策略。

Method: 1. 深度一致D-Normal正则化模块：将D-Normal约束与外部深度监督结合，全面更新所有几何参数；采用基于梯度一致性和逆深度偏差的自适应置信度加权机制，增强多视角深度对齐和几何一致性。2. 空间自适应高斯剪枝（SAGP）策略：根据局部几何复杂度和可见性动态调整高斯密度，减少冗余。3. 统一分区和视图分配方案：消除边界伪影并优化计算负载。

Result: 在多个城市数据集上的广泛实验表明，UrbanGS在渲染质量、几何精度和内存效率方面均取得优越性能，为高保真大规模场景重建提供了系统解决方案。

Conclusion: UrbanGS通过创新的深度一致正则化和自适应剪枝策略，成功解决了3DGS在大规模城市场景中的几何一致性、内存效率和计算可扩展性挑战，为城市规模应用提供了有效的重建框架。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [177] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo是一个基于Transformer的快速图像到视频扩散框架，通过高度压缩的潜在空间、改进的扩散Transformer架构和多分辨率生成策略，在保持竞争力的同时实现了一个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 开发一个快速高效的图像到视频生成框架，在保持生成质量的同时显著提升生成速度，解决现有方法速度较慢的问题。

Method: 1) 设计新的视频自动编码器，实现64×64×4的高压缩潜在空间；2) 采用扩散Transformer架构，引入新的层内存设计增强层间信息流和上下文重用；3) 通过多分辨率生成策略，使用少量步数的DIT上采样器提升视频保真度。

Result: 最终模型包含140亿参数的DIT基础模型和140亿参数的DIT上采样器，在与其他流行开源模型保持竞争力的同时，实现了一个数量级的加速。

Conclusion: FSVideo框架通过创新的架构设计和训练策略，成功实现了高质量且快速的图像到视频生成，为高效视频生成提供了有效解决方案。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [178] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出DSKD方法，通过教师分类器引导轻量级扩散模型对学生特征进行去噪，然后使用LSH引导的特征蒸馏在原始和去噪学生特征之间进行知识蒸馏，消除师生特征分布差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法通常通过探索有意义的特征处理和损失函数来对齐师生特征信息。但由于师生特征分布存在差异，学生模型可能从教师那里学习到不兼容的信息。

Method: 提出教师引导的学生扩散自知识蒸馏(DSKD)：1) 利用教师分类器通过轻量级扩散模型引导去噪学生特征的采样过程；2) 提出基于局部敏感哈希(LSH)引导的特征蒸馏方法，在原始和去噪学生特征之间进行蒸馏；3) 去噪学生特征封装了教师知识，可视为教师角色。

Result: 在视觉识别任务上的实验表明，DSKD在各种模型和数据集上显著优于现有的知识蒸馏方法。

Conclusion: DSKD方法能够消除师生映射方式和特征分布之间的差异，同时从教师那里学习有意义的知识。

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [179] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: iCCDM改进连续条件扩散模型，采用EDM框架和自适应邻域训练策略，在多个数据集上超越现有方法，包括Stable Diffusion 3等大型文本到图像模型，同时显著降低采样成本。


<details>
  <summary>Details</summary>
Motivation: 现有CCDM模型虽然优于先前方法，但仍存在局限性，包括依赖过时的扩散框架和采样效率低下的问题，最近甚至被GAN方法CcGAN-AVAR超越。

Method: 提出iCCDM框架，采用更先进的Elucidated Diffusion Model (EDM)框架并进行重大修改，引入新颖的矩阵形式EDM公式和自适应邻域训练策略。

Result: 在四个基准数据集（分辨率从64×64到256×256）上的广泛实验表明，iCCDM始终优于现有方法，包括最先进的大规模文本到图像扩散模型（如Stable Diffusion 3、FLUX.1和Qwen-Image）。

Conclusion: iCCDM在提高生成质量的同时显著降低了采样成本，解决了原始CCDM的局限性，为连续条件图像生成提供了更高效的解决方案。

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [180] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit是一个无需训练、基于光流的分钟级视频编辑框架，通过分治策略解决长视频编辑中的计算开销和时序一致性问题


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑技术擅长短视频处理，但在扩展到长时长视频时面临巨大挑战：计算开销过高，难以在数千帧中保持全局时序一致性

Method: 采用分治策略进行分段编辑，包含两个核心模块：Velocity Blend通过对齐相邻片段的光流场来修正运动不一致性，消除闪烁和边界伪影；Attention Sink将局部片段特征锚定到全局参考帧，有效抑制累积结构漂移

Result: 大量定量和定性实验表明，MLV-Edit在时序稳定性和语义保真度方面持续优于最先进的方法

Conclusion: MLV-Edit提供了一个无需训练、基于光流的框架，成功解决了分钟级视频编辑中的关键挑战，实现了高效且一致的长视频编辑

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [181] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 提出基于AI的异常检测框架，用于毒理学研究中啮齿动物肝脏组织病理学全切片图像的病理检测，包括已知病理和未见过的罕见病理识别。


<details>
  <summary>Details</summary>
Motivation: 药物诱导毒性是临床前开发和早期临床试验失败的主要原因。组织病理学评估是毒性评估的金标准，但依赖专家病理学家，在大规模筛查中形成瓶颈。需要AI解决方案来早期检测不良反应，减少损耗并加速安全药物开发。

Method: 1. 构建包含健康组织和已知病理像素级标注的新数据集；2. 使用预训练Vision Transformer (DINOv2)通过低秩适应(LoRA)进行微调，实现组织分割；3. 使用马氏距离提取分布外检测特征；4. 提出类别特定阈值以更好处理组织学数据的类别依赖性变异；5. 使用假阴性和假阳性率的平均值优化阈值。

Result: 优化后阈值实现：仅0.16%的病理组织被分类为健康，仅0.35%的健康组织被分类为病理。应用于已知毒理学发现的鼠标肝脏全切片图像，框架能准确检测异常，包括罕见的分布外形态学变化。

Conclusion: 这项工作展示了AI驱动的组织病理学在支持临床前工作流程、减少后期失败和提高药物开发效率方面的潜力。该框架能够检测已知病理和未见过的罕见病理，为大规模毒性筛查提供了有效解决方案。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [182] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: 该研究提出使用物理模拟的CBCT生成几何对齐的训练数据对，避免传统配准偏差对合成CT模型训练和评估的影响，通过几何对齐指标而非传统强度指标来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于配准的CBCT到CT合成方法存在固有配准偏差，这种偏差会传播到训练模型中并污染标准评估指标，导致模型性能评估失真，可能只是更好地复制了配准伪影而非真实的解剖结构。

Method: 提出基于物理的CBCT模拟方法，通过构造方式提供几何对齐的训练数据对，结合使用几何对齐指标（如归一化互信息）对输入CBCT而非有偏差的金标准进行评估。

Result: 在两个独立的盆腔数据集上，基于合成数据训练的模型实现了更好的几何对齐（归一化互信息：0.31 vs 0.22），尽管传统强度分数较低。几何对齐指标与临床评估一致，而强度指标在可变形配准数据中显示出与临床评估相反的关联。

Conclusion: 临床观察者在87%的案例中更偏好基于合成数据训练的模型输出，表明几何保真度而非与有偏差金标准的强度一致性，更符合临床需求。归一化互信息等几何对齐指标能可靠预测观察者偏好。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [183] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 提出基于深度学习的自动化框架，从历史地图中提取细粒度城市变化信息，解决空间错位、制图差异和质量退化等问题，实现系统性定量分析。


<details>
  <summary>Details</summary>
Motivation: 历史地图提供了城市长期转型的独特记录，但由于空间错位、制图变化和文档质量退化等问题，从历史地图系列中提取一致且细粒度的变化信息仍然具有挑战性，限制了大多数分析只能采用小规模或定性方法。

Method: 提出一个完全自动化的深度学习框架，采用模块化设计，整合密集地图对齐、多时相目标检测和变化分析三个核心模块，将历史地图分析从临时视觉比较转向系统性定量表征。

Result: 实验证明所提出的对齐和目标检测方法具有鲁棒性能。应用于1868-1937年巴黎的案例研究，揭示了城市转型的空间和时间异质性，突显了其在社会科学和人文学科研究中的相关性。

Conclusion: 该框架的模块化设计支持适应不同的制图背景和下游应用，为历史地图的定量分析提供了系统化解决方案，推动了城市变化研究的科学化进程。

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [184] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 提出CIEC框架，仅使用粗粒度图像/句子级标注实现多模态弱监督篡改定位，包含图像和文本两个分支，性能接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态篡改定位方法依赖成本高昂的细粒度标注（如补丁/标记级标注），需要开发仅使用粗粒度标注的弱监督方法。

Method: 提出CIEC框架，包含两个分支：1）基于图像的弱监督定位分支，使用TRPS模块结合视觉和文本线索锁定可疑区域；2）基于文本的弱监督定位分支，使用VCTG模块利用视觉偏差辅助标记定位，并采用多种约束减少干扰。

Result: 大量实验证明CIEC的有效性，在多个评估指标上取得了与全监督方法相当的结果。

Conclusion: CIEC框架仅需粗粒度标注即可实现有效的多模态篡改定位，为减少标注成本提供了可行方案。

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [185] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: TopoField：一种拓扑感知的隐式建模框架，用于修复CT图像中提取的肺树拓扑不完整问题，并统一进行解剖标记和肺段重建


<details>
  <summary>Details</summary>
Motivation: 从CT图像提取的肺树经常存在拓扑不完整问题（如缺失或断开的分支），这会严重影响下游解剖分析，现有方法效率有限且对结构损坏的鲁棒性不足

Method: 使用稀疏表面和骨架点云表示肺解剖结构，学习连续的隐式场来修复拓扑，无需完整或显式的断开标注，通过在已有不完整树上引入合成结构破坏进行训练

Result: 在Lung3D+数据集上的实验表明，TopoField能显著改善拓扑完整性，在挑战性不完整场景下实现准确的解剖标记和肺段重建，计算效率高（每例仅需1秒多）

Conclusion: TopoField将拓扑修复作为首要建模问题，通过隐式表示实现高效的多任务推理，适用于大规模和时间敏感的临床应用

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [186] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: SSI-DM通过跳过数学奇点区域，在标准反转前添加少量噪声，解决了扩散模型反转中的非高斯噪声问题，提高了图像编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型反转方法在早期去噪步骤中存在不准确性，导致生成非高斯噪声，降低了图像编辑能力。研究发现根本原因是数学奇点使反转问题本质上不适定。

Method: 提出SSI-DM方法，通过在标准反转前添加少量噪声来绕过奇点区域，这种方法能产生具有自然高斯特性的反转噪声，同时保持重建保真度。

Result: 在公共图像数据集上，该方法在重建和插值任务中表现出优越性能，为扩散模型反转提供了原则性和高效的解决方案。

Conclusion: SSI-DM作为一种即插即用技术，与通用扩散模型兼容，通过跳过奇点区域有效解决了反转中的非高斯噪声问题，显著提升了图像编辑效果。

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [187] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: MAIN-VLA框架通过建模意图和环境抽象，在复杂动态环境中实现更高效的视觉-语言-动作决策，显著提升决策质量、泛化能力和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作方法在高度复杂动态环境（如3D开放世界、大型PvP游戏）中，难以从冗余传感器流中提取动作关键信号，决策效率低下。

Method: 提出MAIN-VLA框架，包含意图抽象（将冗长语言指令压缩为显式语义原语）和环境语义抽象（将视觉流投影为结构化拓扑可供性表示），并通过对齐这两种抽象模态实现无参数令牌剪枝。

Result: 在开放世界Minecraft和大型PvP环境（Game for Peace和Valorant）的广泛实验中，MAIN-VLA达到新的最先进水平，在决策质量、泛化能力和推理效率方面表现优异。

Conclusion: MAIN-VLA通过显式建模意图和环境抽象，将决策建立在深度语义对齐而非表面模式匹配上，有效解决了复杂动态环境中的感知冗余问题，实现了高效决策。

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [188] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 提出Causal Forcing方法，通过使用自回归教师模型进行ODE初始化，解决双向视频扩散模型蒸馏到自回归模型时的架构差距问题，显著提升实时交互视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练的双向视频扩散模型蒸馏为少步自回归模型时，存在架构差距问题。传统方法使用ODE蒸馏初始化自回归学生模型，但要求帧级单射性条件，而双向教师模型蒸馏违反这一条件，导致性能下降。

Method: 提出Causal Forcing方法，使用自回归教师模型进行ODE初始化，从而弥合架构差距。该方法避免了传统方法中双向教师模型蒸馏违反帧级单射性条件的问题。

Result: 实验结果表明，该方法在所有指标上均优于所有基线方法，相比SOTA Self Forcing方法，在Dynamic Degree指标上提升19.3%，在VisionReward指标上提升8.7%，在Instruction Following指标上提升16.7%。

Conclusion: Causal Forcing通过使用自回归教师模型进行ODE初始化，有效解决了双向视频扩散模型蒸馏到自回归模型时的架构差距问题，显著提升了实时交互视频生成的性能。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [189] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: HieraNav是一个多粒度、开放词汇的目标导航任务，要求智能体根据自然语言指令在四个语义层级（场景、房间、区域、实例）导航。作者提出了LangMap基准，基于真实3D室内扫描构建，包含人类验证的标注和任务，用于评估语言驱动的具身导航。


<details>
  <summary>Details</summary>
Motivation: 物体与语言之间的关系对于人机有意义的通信和实际有用的具身智能至关重要。现有导航任务通常局限于特定粒度或词汇，缺乏多粒度、开放词汇的评估框架。

Method: 构建LangMap基准：基于真实3D室内扫描，提供区域标签、区分性区域描述、覆盖414个对象类别的区分性实例描述，以及超过18K个导航任务。每个目标都有简洁和详细两种描述，支持不同指令风格的评估。

Result: LangMap在标注质量上优于GOAT-Bench，区分性准确率提高23.8%，同时使用四倍少的词汇。评估显示：更丰富的上下文和记忆能提高成功率，但长尾、小型、上下文依赖和远距离目标以及多目标完成仍然具有挑战性。

Conclusion: HieraNav和LangMap为推进语言驱动的具身导航建立了一个严格的测试平台，支持多粒度、开放词汇的导航任务评估。

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [190] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: LatentMorph是一个将隐式潜在推理集成到文本到图像生成中的新框架，通过四个轻量级组件实现连续潜在空间推理，避免了显式推理的瓶颈，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法缺乏动态推理和细化的能力，而当前基于推理的方法依赖显式思维过程，需要在固定步骤将中间推理解码为离散文本，导致效率低下、信息丢失和认知不匹配。

Method: 提出LatentMorph框架，包含四个轻量级组件：1) condenser将中间生成状态总结为紧凑视觉记忆；2) translator将潜在思维转换为可操作指导；3) shaper动态引导下一个图像标记预测；4) RL训练的invoker自适应决定何时调用推理。所有推理都在连续潜在空间中进行。

Result: 实验表明：1) 在Janus-Pro基础上，GenEval提升16%，T2I-CompBench提升25%；2) 在WISE和IPV-Txt等抽象推理任务上，比显式推理方法(TwiG)分别提升15%和11%；3) 推理时间减少44%，标记消耗减少51%；4) 在推理调用方面与人类直觉的认知对齐度达到71%。

Conclusion: LatentMorph通过在连续潜在空间中执行隐式推理，有效解决了显式推理方法的效率瓶颈和信息损失问题，实现了更自适应、高效的文本到图像生成，同时与人类认知过程更加一致。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [191] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: LiFlow：首个基于流匹配的3D LiDAR场景补全框架，通过解决扩散方法中训练与推理初始分布不匹配的问题，在遮挡和远距离稀疏场景下提升自动驾驶感知性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中采集的LiDAR点云常受遮挡和远距离稀疏问题影响，现有基于扩散概率模型的方法存在训练与推理初始分布不匹配的问题，限制了场景补全效果

Method: 提出首个流匹配框架用于3D LiDAR场景补全，采用最近邻流匹配损失和Chamfer距离损失，同时优化局部结构和全局覆盖的点云对齐

Result: LiFlow在多个指标上达到最先进性能，优于现有的扩散基方法

Conclusion: 流匹配框架有效解决了扩散方法中的初始分布不匹配问题，为3D LiDAR场景补全提供了更优的解决方案，代码已开源

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [192] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: DiScene是一个基于稀疏查询的占用预测框架，通过多级知识蒸馏实现高效鲁棒的占用预测，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前密集方法在空体素上存在计算浪费，而稀疏查询方法在复杂室内场景中缺乏鲁棒性，需要解决效率与精度之间的权衡问题。

Method: 提出DiScene框架，包含两个关键创新：1）多级一致知识蒸馏策略，通过四个层级的协调对齐将大型教师模型的层次表示转移到轻量学生模型；2）教师引导初始化策略，使用优化参数预热加速模型收敛。

Result: 在Occ-Scannet基准上，DiScene达到23.2 FPS，比基线方法OPUS提升36.1%，甚至优于深度增强版本OPUS†。集成深度后，DiScene†超越EmbodiedOcc 3.7%，推理速度快1.62倍。在Occ3D-nuScenes基准和实际场景中也表现出良好泛化能力。

Conclusion: DiScene通过多级知识蒸馏实现了高效且鲁棒的占用预测，在各种环境中都表现出优越性能，为机器人感知提供了有效的几何和语义理解解决方案。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [193] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 提出基于残差向量量化变分自编码器(RVQ-VAEs)的方法，通过粗到细的表示学习实现人体运动数据中风格与内容的解耦，支持无需微调的实时风格迁移


<details>
  <summary>Details</summary>
Motivation: 人体运动数据既包含语义内容又包含细微风格特征，现有方法难以有效解耦这两者以实现风格迁移。需要一种能够区分粗粒度运动属性（内容）和细粒度表达细节（风格）的方法。

Method: 使用残差向量量化变分自编码器(RVQ-VAEs)学习从粗到细的运动表示；结合对比学习和新颖的信息泄漏损失与码本学习，在不同码本中组织内容和风格；提出量化码交换的推理时技术，无需对未见风格进行微调

Result: 框架在多种推理应用中表现出强大多功能性，包括风格迁移、风格移除和运动混合，能够实现无需微调的实时风格迁移

Conclusion: 该方法有效解耦了人体运动数据中的风格与内容，通过层次化表示学习和创新的码本组织策略，实现了高效、无需微调的风格迁移，为运动数据处理提供了灵活的多功能框架

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [194] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO是一个两阶段的直接偏好优化框架，让短上下文视觉语言模型能够理解超长视频，无需长视频标注。通过合成偏好三元组和递归字幕生成多段推理查询，仅用16K合成样本就在多个长视频基准上超越最先进开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对短上下文设计，难以处理超长视频理解任务。获取长视频标注成本高昂，需要开发无需长视频标注的高效方法，使短上下文模型能够扩展到长视频理解。

Method: 采用两阶段框架：第一阶段合成偏好三元组，将问题锚定到单个短片段，通过视觉相似性和问题特异性过滤减轻位置偏差，并仅评估锚定片段来近似参考模型的长上下文评分；第二阶段使用递归字幕管道生成长视频场景级元数据，然后用大语言模型生成多段推理查询和不受欢迎的响应，通过多段推理任务对齐模型偏好。

Result: 仅使用16K合成样本且无需昂贵人工标注，LongVPO在多个长视频基准上超越了最先进的开源模型，同时在短视频任务（如MVBench）上保持强大性能，为高效长视频理解提供了可扩展的范式。

Conclusion: LongVPO通过创新的两阶段直接偏好优化框架，成功实现了短上下文视觉语言模型向超长视频理解的扩展，提供了一种无需长视频标注的高效、可扩展的长视频理解解决方案。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [195] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: SNGP模型通过谱归一化和高斯过程层改进数字病理学中的不确定性估计和OOD检测，相比确定性模型和MC Dropout表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学深度学习模型在分布外（OOD）设置中往往过度自信且校准不佳，限制了临床信任和采用。医疗影像工作流需要能够准确拒绝OOD输入的内在不确定性感知特性。

Method: 实现谱归一化神经高斯过程（SNGP），通过谱归一化和用高斯过程层替换最终密集层来改进单模型不确定性估计和OOD检测。在三个生物医学分类任务（白细胞、淀粉样斑块、结直肠组织病理学）的六个数据集上评估SNGP与确定性模型和蒙特卡洛Dropout的对比。

Result: SNGP在分布内性能相当的同时，显著改善了不确定性估计和OOD检测能力。

Conclusion: SNGP及相关模型为数字病理学中的不确定性感知分类提供了有用框架，支持安全部署并建立与病理学家的信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [196] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出UnifiedReward-Flex，一种统一的个性化视觉生成奖励模型，通过上下文自适应推理解决现有奖励模型对内容特定视觉线索不敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型通常采用Bradley-Terry风格偏好建模或使用生成式VLM作为评判者，但存在"一刀切"范式，假设单一偏好分布或依赖固定评估标准，对内容特定视觉线索不敏感，导致与主观和上下文相关的人类偏好系统性错位。

Method: 提出UnifiedReward-Flex模型，首先解释语义意图并基于视觉证据，然后在预定义和自生成的高层维度下实例化细粒度标准，动态构建分层评估。训练采用两阶段：1) 从先进闭源VLM蒸馏结构化高质量推理轨迹进行SFT；2) 在精心策划的偏好对上执行直接偏好优化(DPO)。

Result: 将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成，广泛结果显示其优越性。

Conclusion: UnifiedReward-Flex通过耦合奖励建模与灵活上下文自适应推理，解决了现有奖励模型对内容特定视觉线索不敏感的问题，为视觉生成提供了更个性化的奖励模型。

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [197] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出MultiBO方法，通过多轮人类偏好反馈优化个性化图像生成，即使语言提示无法完全描述目标图像时也能逐步逼近用户心中的特定图像。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型虽然能通过语言提示生成接近用户心中特定图像的图片，但语言描述存在局限性，用户难以仅通过语言提示完全消除生成图像与心中目标图像之间的差距。然而，人类仍然能够判断新图像是否比当前图像更接近目标图像。

Method: 提出MultiBO（多选择偏好贝叶斯优化）方法：1）基于当前最佳图像生成K个新图像；2）获取用户对这些图像的偏好反馈；3）利用反馈指导扩散模型；4）生成新的K个图像集。通过B轮用户反馈迭代优化，逐步逼近目标图像。

Result: 30名用户的定性评分和与5个基线的定量指标比较显示，MultiBO在有限反馈轮次内能显著缩小生成图像与用户心中目标图像之间的差距，证明多选择人类偏好反馈能有效提升个性化图像生成质量。

Conclusion: 人类的多选择偏好反馈可以有效地指导生成模型进行个性化图像生成，即使模型对目标图像没有任何先验信息。这种方法为语言描述有限情况下的精确图像生成提供了新途径。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [198] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Infinite-World是一个鲁棒的交互式世界模型，能够在复杂真实世界环境中维持超过1000帧的连贯视觉记忆，通过分层无姿态记忆压缩器和不确定性感知动作标注模块解决真实世界视频训练难题。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在合成数据上训练效果良好，但在真实世界视频中面临姿态估计噪声和视角重访稀缺的问题，缺乏有效的训练范式。

Method: 1. 分层无姿态记忆压缩器(HPMC)：递归将历史潜在表示蒸馏为固定预算表示，无需显式几何先验；2. 不确定性感知动作标注模块：将连续运动离散化为三态逻辑，最大化利用原始视频数据；3. 重访密集微调策略：使用30分钟数据集激活长距离闭环能力。

Result: 通过客观指标和用户研究验证，Infinite-World在视觉质量、动作可控性和空间一致性方面表现出优越性能。

Conclusion: Infinite-World通过创新的记忆压缩和动作标注方法，成功解决了真实世界视频训练中的关键挑战，实现了长序列视觉记忆的鲁棒维护。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [199] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: Catalyst是一个后处理OOD检测框架，利用池化前特征图的原始通道统计信息（如均值、标准差、最大激活值）动态计算缩放因子γ，通过弹性缩放增强现有基线方法的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的后处理方法主要依赖全局平均池化后的logits或特征向量，忽略了池化前特征图中丰富的原始通道统计信息。这些被丢弃的信号包含互补信息，可以显著提升OOD检测性能。

Method: Catalyst框架从池化前特征图的原始统计信息（均值、标准差、最大激活值）中动态计算输入相关的缩放因子γ，然后将γ与现有基线分数进行乘法融合，实现弹性缩放，从而扩大ID和OOD分布之间的距离。

Result: Catalyst在多个数据集上显著提升OOD检测性能：在CIFAR-10（ResNet-18）上平均误报率降低32.87%，在CIFAR-100（ResNet-18）上降低27.94%，在ImageNet（ResNet-50）上降低22.25%。该框架与logit-based方法（Energy、ReAct、SCALE）和距离检测器（KNN）都能无缝集成。

Conclusion: 池化前统计信息在OOD检测中具有未开发的潜力，Catalyst框架作为现有方法的补充，通过利用这些被忽略的信号实现了显著且一致的性能提升，证明了其通用性和有效性。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [200] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: 提出SelvaMask热带森林数据集和基于视觉基础模型的检测-分割管道，在热带森林树冠分割任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 热带森林对全球生态平衡至关重要，但现有树冠分割方法在热带森林中性能较低，缺乏高质量数据集

Method: 引入SelvaMask数据集（包含8800+人工标注树冠），提出模块化检测-分割管道，使用领域特定的检测提示器适配视觉基础模型

Result: 在密集热带森林中性能达到SOTA，优于零样本通用模型和全监督端到端方法，在外部热带和温带数据集上验证了泛化能力

Conclusion: SelvaMask既是具有挑战性的基准，也是实现广义森林监测的关键推动因素，代码和数据集将公开

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [201] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 提出基于Swin U-Net的门控多头Transformer架构，通过切片级结构检测和像素级分割的联合训练，有效抑制放疗自动分割中的解剖学不合理假阳性（幻觉）。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习自动分割模型在缺乏目标结构的切片中经常产生解剖学不合理的假阳性（幻觉），影响放疗临床工作流程的可靠性。

Method: 基于Swin U-Net的门控多头Transformer架构，增强切片间上下文集成和并行检测头，联合执行切片级结构检测（通过多层感知器）和像素级分割（通过上下文增强流），检测输出门控分割预测以抑制解剖无效切片中的假阳性，训练使用切片级Tversky损失处理类别不平衡。

Result: 在The Cancer Imaging Archive的前列腺解剖边缘案例数据集上，门控模型显著优于非门控分割基线，平均Dice损失为0.013±0.036 vs 0.732±0.314，检测概率与解剖存在强相关，有效消除虚假分割。非门控模型在所有切片中表现出更高的变异性和持续的假阳性。

Conclusion: 基于检测的门控增强了自动分割应用的鲁棒性和解剖合理性，在不影响有效切片分割质量的情况下减少幻觉预测，为提高临床放疗自动勾画工作流程的可靠性提供了有前景的方法。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [202] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen是一种简单的像素扩散框架，通过感知监督直接在像素空间生成图像，避免了VAE在潜在扩散中引入的伪影和瓶颈，在ImageNet-256上达到FID 5.11的优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的像素扩散方法难以优化包含大量感知无关信号的高维像素流形，导致性能落后于潜在扩散模型。作者希望开发一种无需VAE、潜在表示或辅助阶段的更简单但更强大的生成范式。

Method: PixelGen引入两种互补的感知损失来引导扩散模型学习更有意义的感知流形：LPIPS损失促进学习更好的局部模式，基于DINO的感知损失增强全局语义。该方法直接在像素空间进行端到端训练，无需VAE或潜在表示。

Result: 在ImageNet-256上，PixelGen在不使用分类器自由引导的情况下仅用80个训练周期就达到了FID 5.11。在大规模文本到图像生成中，获得了0.79的GenEval分数，超越了强大的潜在扩散基线模型。

Conclusion: PixelGen通过感知监督成功优化了像素扩散，提供了一种无需VAE、潜在表示或辅助阶段的更简单但更强大的生成范式，在图像生成质量上超越了现有的潜在扩散方法。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [203] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: PPoGA是一个基于知识图谱的问答框架，通过预测处理机制和自我修正能力解决LLMs在复杂推理中的认知固定性问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识图谱的大语言模型在复杂问答中容易因初始推理计划错误而失败，类似于人类的认知功能固定性，无法重新调整方法，导致追求不可行的解决方案。

Method: 提出PPoGA框架，采用规划器-执行器架构分离高层策略与低层执行，利用预测处理机制预测结果，核心创新是自我修正机制，包括路径修正和计划修正。

Result: 在三个具有挑战性的多跳知识图谱问答基准测试（GrailQA、CWQ、WebQSP）上进行广泛实验，结果表明PPoGA实现了最先进的性能，显著优于现有方法。

Conclusion: 这项工作强调了元认知能力（如问题重构）对于构建更强大和灵活的人工智能推理系统的关键重要性。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [204] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: MediGRAF是一个混合图检索增强框架，通过结合结构化数据查询（Neo4j Text2Cypher）和非结构化语义搜索（向量嵌入），实现临床电子健康记录的全面自然语言查询，显著提高信息检索的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录系统包含大量临床信息，给临床医生带来认知负担，关键细节容易被忽视。大型语言模型在临床环境中存在上下文基础和幻觉问题，现有检索方法通常孤立处理结构化数据或非结构化数据，无法同时整合两者。

Method: 提出MediGRAF混合图RAG系统，独特结合Neo4j Text2Cypher能力进行结构化关系遍历，以及向量嵌入进行非结构化叙述检索，实现对患者完整病程的自然语言查询。使用MIMIC-IV数据集的10名患者数据（生成5,973个节点和5,963个关系）。

Result: 事实查询实现100%召回率（所有相关信息都被检索并输出），复杂推理任务平均专家质量得分4.25/5，零安全违规。系统在不同查询复杂度下均表现良好。

Conclusion: 混合图基础方法显著推进了临床信息检索，为标准的LLM部署提供了更安全、更全面的替代方案，能够更好地支持临床决策。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [205] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: G-MemLLM：一种记忆增强的LLM架构，通过GRU风格的门控更新机制解决长上下文推理中的信息稀释问题，显著提升多跳推理和关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM受限于有限的上下文窗口，在多跳推理中难以保持长期事实一致性。现有方法如上下文压缩或循环标记存在"上下文腐化"和信息稀释问题。

Method: 提出G-MemLLM架构，将冻结的LLM主干与可训练的潜在记忆库集成，采用GRU风格的门控更新逻辑，选择性更新、保留或覆盖潜在记忆槽位。

Result: 在HotpotQA和ZsRE基准测试中，从GPT-2到Llama 3.1-8B各规模模型均显著提升：Llama 3.1-8B在ZsRE上准确率提升13.3%，GPT-2在HotpotQA上答案F1提升8.56分，Llama 3.1-8B支持事实F1提升6.89分。

Conclusion: G-MemLLM通过门控记忆机制有效缓解了长序列推理中的知识梯度消失问题，显著增强了LLM的多跳推理能力和关系抽取精度，且具有良好的跨模型规模扩展性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [206] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: SafeTalkCoach是一个多智能体对话生成框架，专门用于模拟父母与子女关于性健康的对话，并附带数据集，旨在解决此类对话数据稀缺和LLM生成对话缺乏真实性和多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 父母与子女关于性健康的有效沟通虽然重要，但由于其私密性和敏感性，现实世界中的对话数据稀缺且难以收集。现有的LLM对话生成方法可能偏离最佳实践，且经常缺乏真实性和多样性。

Method: SafeTalkCoach是一个多样性驱动的多智能体对话生成框架，整合了众包和合成的场景、既定性健康指南、基于证据的人物角色、自适应控制模块和层次化多样化策略。

Result: 评估表明，SafeTalkCoach能够生成多样化的对话，同时在实践中保持真实性、沟通质量和可控性。

Conclusion: SafeTalkCoach框架和数据集旨在支持AI研究和健康沟通实践，为解决性健康教育中的对话数据稀缺问题提供了有效工具。

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [207] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: 提出大型本体模型（LOM），通过构建-对齐-推理框架解决企业知识管理中多源异构数据整合和语义推理问题，在复杂图推理任务上优于DeepSeek-V3.2


<details>
  <summary>Details</summary>
Motivation: 企业级知识管理面临多源异构数据整合困难、传统知识图谱在隐式关系发现和复杂问答中语义理解不足的问题

Method: 提出统一构建-对齐-推理框架：1）从结构化数据库和非结构化文本构建双层企业本体并融合；2）三阶段训练流程：本体指令微调、文本-本体对齐、多任务指令调优；3）构建全面的训练和评估数据集

Result: 4B参数的LOM在基准测试中达到89.47%准确率，在复杂图推理任务上优于DeepSeek-V3.2，表明本体结构与语言的有效融合

Conclusion: LOM框架成功解决了企业知识管理中多源数据整合和语义推理的挑战，通过本体与语言的有效融合实现了优越的推理性能

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [208] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: RDD是一种可逆扩散解码框架，通过检测停滞状态并回溯到早期块来改进扩散语言模型的块状解码，避免因不可逆承诺导致的生成停滞问题。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型通过块状解码实现并行令牌生成，但其不可逆的承诺可能导致停滞问题——当上下文不理想时，反向扩散过程无法进一步进展。需要解决这种早期承诺错误导致的生成失败。

Method: 提出可逆扩散解码（RDD）框架：1）检测停滞作为反向过程的状态依赖失败；2）通过缓存的模型状态实现高效回溯到早期块而无需重新计算；3）应用置信度引导的重掩码，选择性重新初始化不确定令牌同时保留可靠上下文。

Result: 实验表明RDD在最小计算开销下提高了生成鲁棒性和质量，优于基线方法。可逆公式允许解码从早期承诺错误中恢复，同时保持基于扩散生成的并行效率。

Conclusion: RDD通过引入可逆性到块状扩散生成中，有效解决了扩散语言模型解码中的停滞问题，在保持并行效率的同时提高了生成鲁棒性。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [209] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: DIVERGE是一个插件式智能RAG框架，通过反思引导生成和记忆增强迭代优化来解决现有RAG系统在开放性问题中多样性不足的问题，在保持质量的同时显著提升回答多样性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统假设每个查询只有一个正确答案，忽略了常见的信息寻求场景中可能存在多个合理答案。这种设计限制了创造性，并损害了公平包容的信息获取。研究发现标准RAG系统未能充分利用检索上下文的多样性，即使增加检索多样性也无法产生多样化的生成结果。

Method: 提出DIVERGE框架，采用反思引导生成和记忆增强迭代优化的方法。该框架是插件式的智能RAG系统，通过新颖的反思机制引导生成过程，利用记忆增强技术进行迭代优化，在保持答案质量的同时促进多样化的观点表达。

Result: 在真实世界的Infinity-Chat数据集上，DIVERGE相比竞争基线和先前最先进方法实现了最佳的多样性-质量权衡，在保持质量的同时显著提高了多样性。新提出的多样性-质量权衡评估指标与人类判断有良好相关性。

Conclusion: 研究揭示了当前基于LLM的系统在开放式信息寻求中的系统性局限，表明明确建模多样性可以缓解这一问题。DIVERGE框架为处理多答案场景提供了有效解决方案，促进了更公平包容的信息访问。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [210] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: 该研究首次建立了大规模基准来评估大语言模型在科学问答中的不确定性量化方法，分析了20个不同变体模型在7个科学问答数据集上的表现，发现指令微调导致概率极化，而答案频率是最可靠的校准方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学问答中应用日益广泛，但现有不确定性量化方法在需要事实检索和推理能力的科学领域缺乏有效验证。需要建立可靠的不确定性量化基准来评估这些方法的校准性能。

Method: 建立了首个大规模基准来评估推理密集型问答中的不确定性量化指标，涵盖20个大语言模型（基础模型、指令微调模型和推理变体），使用7个科学问答数据集（包括多项选择和算术问答任务），通过提示模拟开放问答设置，评估了代表主流方法的UQ方法，共分析了685,000个长格式回答。

Result: 在token级别，指令微调导致概率质量极化，降低了token级别置信度作为不确定性估计的可靠性；在序列级别，语言化方法存在系统性偏差且与正确性相关性差，而答案频率（跨样本一致性）产生最可靠的校准。研究还发现仅依赖ECE作为UQ方法性能评估指标会产生误导性结果。

Conclusion: 当前大语言模型的不确定性量化方法存在严重局限性，标准基准测试实践也有不足。答案频率是最可靠的校准方法，而指令微调会损害不确定性估计的可靠性。需要更全面的评估框架来推动可信的LLM应用。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [211] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

TL;DR: 本文发现Patchscopes框架中LLMs在解码隐藏表征时过度依赖固有语言模式而非上下文信息，导致解释不忠实，并提出BALOR方法通过logit重校准来抑制模型偏见、增强上下文信息。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通过Patchscopes框架能够从隐藏表征生成人类可读解释，但研究发现LLMs在解码时倾向于依赖固有语言模式，这会覆盖隐藏表征中编码的上下文信息，导致解释不忠实。例如，即使隐藏表征编码了"紫色"属性，LLMs仍会生成"绿色"解释，反映了强烈的先验关联。

Method: 首先设计数据集评估Patchscopes在偏见情况下的忠实性，发现平均忠实性下降18.84%。然后提出BALOR方法：将未修补提示的输出logits视为捕获模型偏见，与修补上下文信息下获得的logits进行对比，通过这种对比重新校准logit分布，从而在生成过程中抑制模型偏见并放大上下文信息。

Result: 实验表明BALOR在多个LLMs上一致优于现有基线方法，实现了高达33%的相对性能提升，有效提高了Patchscopes解释的忠实性。

Conclusion: Patchscopes框架存在系统性不忠实问题，LLMs在解码隐藏表征时过度依赖固有语言模式。BALOR方法通过logit重校准有效抑制模型偏见、增强上下文信息，显著提升了隐藏表征解释的忠实性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [212] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

TL;DR: 本文提出一个两阶段流水线，用于从市政会议纪要中提取元数据，包括会议编号、日期、地点等非标准化信息，并评估了不同模型的性能、推理成本和碳足迹。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要作为地方治理的官方文件，格式和写作风格各异，其中的元数据（如会议编号、日期、地点、参与者等）缺乏标准化，难以自动提取。现有的命名实体识别模型不适用于这种特定领域类别，需要专门解决方案。

Method: 提出两阶段流水线：1) 使用问答模型识别包含元数据的开头和结尾文本段；2) 应用基于Transformer的模型（BERTimbau和XLM-RoBERTa，有/无CRF层）进行细粒度实体提取，并通过去词汇化增强。评估了开源模型（Phi）和闭源模型（Gemini）的性能。

Result: 在领域内表现出色，优于更大的通用大语言模型。但跨市政评估显示泛化能力有限，反映了市政记录的多样性和语言复杂性。建立了市政会议纪要元数据提取的首个基准。

Conclusion: 该工作为市政会议纪要元数据提取提供了首个基准，为该领域未来研究奠定了坚实基础。虽然领域内表现良好，但跨市政泛化能力仍需改进，反映了市政文档的复杂性和多样性。

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [213] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: 该研究通过检测模型分析ICLR和Nature Communications的同行评审，发现AI生成内容在2022年前极少，到2025年分别达到约20%和12%，表明AI在学术评审中的使用迅速增加。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，研究者关注AI在学术同行评审中的角色和影响，需要了解AI生成内容在评审过程中的出现趋势和程度。

Method: 使用基于历史评审数据训练的检测模型，分别应用于国际学习表征会议（ICLR）和自然通讯（Nature Communications）后续评审周期的评审内容，分析AI生成内容的时间演变趋势。

Result: 2022年前AI生成内容检测极少，之后显著增加，到2025年ICLR约20%、Nature Communications约12%的评审被分类为AI生成；Nature Communications在2024年第三到第四季度增长最为显著。

Conclusion: AI辅助内容在同行评审中的存在迅速增加，需要进一步研究其对学术评价的影响和意义。

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [214] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: DETOUR是一个双智能体评估基准，用于模拟"舌尖现象"的多轮信息检索过程，包含1011个提示，测试模型在模糊、未充分指定场景下的检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估智能体在"舌尖现象"搜索过程中的基准仅限于单轮设置，无法真实模拟人们在对话中通过多轮交互回忆信息的过程，需要更现实的评估框架。

Method: 设计了DETOUR双智能体评估基准：主智能体（被评估对象）通过查询记忆智能体来识别回忆实体，记忆智能体在评估中保持一致。基准包含1011个提示，涵盖文本、图像、音频和视频多种模态。

Result: 当前最先进的模型在该基准上表现仍然不佳，在所有模态（文本、图像、音频、视频）上仅达到36%的准确率，表明模型在模糊、未充分指定场景下的能力仍有待提升。

Conclusion: DETOUR基准揭示了当前模型在模拟真实"舌尖现象"搜索过程中的局限性，强调了增强模型在模糊、未充分指定场景下能力的重要性。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [215] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

TL;DR: DecompressionLM是一个无状态框架，用于零样本概念图提取，无需预定义查询或跨序列共享状态，通过低差异序列和算术解码实现确定性并行生成，评估压缩模型的知识广度和事实基础。


<details>
  <summary>Details</summary>
Motivation: 现有知识探测方法依赖预定义查询，只能提取已知概念，限制了发现语言模型实际编码内容的能力。需要解决基于解码的探测方法的三个限制：跨序列耦合导致概率集中在高频前缀、竞争解码效应抑制长尾概念、顺序探索带来的可扩展性约束。

Method: DecompressionLM使用Van der Corput低差异序列结合算术解码，实现确定性、高度并行的生成，无需跨序列共享状态。该方法通过零样本概念图提取，发现语言模型编码的内容，不依赖预定义查询。

Result: 在两个模型系列和五种量化变体中，激活感知量化（AWQ-4bit）将概念覆盖率提高了30-170%，而均匀量化（GPTQ-Int4）导致71-86%的覆盖率崩溃。基于语料库的验证显示，MMLU-Pro Law模型中排名最高和最低的模型之间存在17个百分点的幻觉差距。

Conclusion: DecompressionLM建立了概念覆盖率作为评估压缩模型知识广度和事实基础的重要补充维度，对模型部署具有实用价值，揭示了不同量化方法对知识保留的显著差异。

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [216] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

TL;DR: 评估大语言模型对土耳其语反身代词绑定关系的捕捉能力，发现两个模型在本地绑定偏好上存在显著差异


<details>
  <summary>Details</summary>
Motivation: 评估最先进的大语言模型是否能够捕捉土耳其语反身代词（kendi和kendisi）的绑定关系，比较不同模型在语言结构理解上的差异

Method: 构建包含100个句子的平衡数据集，对比本地与非本地先行词；测试两个系统：OpenAI链式思维推理模型和基于LLaMA-2并针对土耳其语数据微调的Trendyol-LLM-7B-base-v0.1；使用句子级困惑度和强制选择范式评估先行词选择

Result: Trendyol-LLM在约70%的试验中偏好本地绑定，表现出强烈的局部性偏差；而o1 Mini在本地和长距离解读之间的选择几乎均匀分布，显示出两个系统在绑定行为上的显著对比

Conclusion: 不同大语言模型在土耳其语反身代词绑定关系的处理上存在显著差异，模型架构和训练数据对语言结构理解有重要影响

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [217] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

TL;DR: 提出基于集成梯度归因的段级选择性学习框架，通过识别高归因强度但中等一致性的重要推理段，在监督微调中只学习这些关键部分，提高模型准确性和输出效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成的长链思维中只有小部分对答案预测有实质贡献，大部分包含重复或截断内容。这种输出冗余在监督微调后进一步传播，模型学习模仿冗长但无信息的模式，可能降低性能。

Method: 使用集成梯度归因量化每个token对最终答案的影响，聚合为两个段级指标：归因强度（整体归因大小）和方向一致性（段内token归因方向是否一致）。基于这两个指标，提出段级选择性学习框架，识别具有高归因强度但中等一致性的重要段（反映深度推理而非浅层推理），对这些重要段应用选择性SFT，同时对不重要段屏蔽损失。

Result: 在多个模型和数据集上的实验表明，该方法提高了准确性和输出效率，实现了从长推理轨迹中更有效的学习。

Conclusion: 通过集成梯度归因识别关键推理段，并应用选择性监督微调，可以有效减少模型学习冗余内容，提升推理性能和效率。

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [218] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 该研究探讨了基于大语言模型的多智能体系统中的曼德拉效应，开发了MANBENCH基准来评估该现象，并提出缓解策略，平均减少74.40%的效应。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中智能体容易受到集体认知偏见的影响，特别是曼德拉效应（群体集体错误记忆现象），这限制了我们对多智能体系统中记忆偏见的理解，并引发了关于错误信息传播的伦理担忧。

Method: 提出了MANBENCH基准，评估四种易受曼德拉效应影响的任务类型，使用五种不同智能体角色和记忆时间尺度的交互协议。评估了多个LLM驱动的智能体，并提出缓解策略：提示级防御（认知锚定和来源审查）和模型级基于对齐的防御。

Result: 量化了多智能体系统中的曼德拉效应，分析了不同因素对其影响。提出的缓解策略相比基线平均减少了74.40%的曼德拉效应。

Conclusion: 该研究为开发更具韧性和伦理对齐的协作多智能体系统提供了有价值的见解，揭示了LLM多智能体系统中的集体记忆偏见问题，并提出了有效的缓解方法。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [219] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

TL;DR: LLMs在摘要生成中表现出一致的内部重要性判断模式，与早期基线模型显著不同，且模型家族比模型规模更能影响重要性判断模式。通过行为分析和计算分析发现，某些注意力头与重要性分布高度对齐，中后期层能有效预测重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在摘要生成任务上已达到最先进水平，但其内部驱动信息选择的重要性判断机制仍然不透明。研究者希望揭示LLMs在摘要生成中如何判断信息重要性，以及这种重要性判断在模型内部如何表征。

Method: 采用行为分析和计算分析相结合的方法。行为分析方面：为每个文档生成一系列长度控制的摘要，基于每个信息单元被选择的频率推导出经验重要性分布。计算分析方面：识别与经验重要性分布对齐的注意力头，分析不同层对重要性预测的能力。

Result: 1. LLMs在重要性判断上表现出高度一致性，与pre-LLM基线模型有显著差异；2. LLMs的重要性判断模式更多按模型家族聚类，而非按模型规模；3. 某些注意力头与经验重要性分布高度对齐；4. 中后期层对重要性预测能力最强。

Conclusion: 该研究初步揭示了LLMs在摘要生成中的重要性判断机制及其内部表征方式，为解释和最终控制这些模型中的信息选择开辟了路径，有助于理解LLMs的决策过程并提高其透明度。

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [220] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: SENSE模型通过将词汇嵌入映射到Lancaster感觉运动规范，预测单词的感觉运动关联，并在行为研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入主要基于共现模式，而人类语言理解根植于感觉和运动经验，需要建立词汇与感觉运动体验之间的计算模型。

Method: 开发SENSE学习投影模型，将词汇嵌入映射到Lancaster感觉运动规范；进行行为研究，让281名参与者从候选非词中选择具有特定感觉运动关联的词汇。

Result: SENSE评分与人类选择率在11种模态中的6种上存在显著相关性；对非词选择率的亚词汇分析揭示了内感受规范的系统性音素模式。

Conclusion: SENSE模型能够有效预测词汇的感觉运动关联，为从文本数据中计算提出候选音素模式提供了路径。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [221] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 本文提出Intention-Tuning框架，通过动态选择LLM层来学习意图表示并迁移到修订生成任务，在小规模修订语料上实现高效性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于上下文的文本生成任务中表现出色，但在基于意图的生成任务（如修订生成）中应用不足。现有方法难以处理复杂的多意图场景，而微调方法需要大量标注数据，这在修订领域既昂贵又稀缺。

Method: 提出Intention-Tuning框架，这是一种意图自适应的分层LLM微调方法。框架动态选择LLM层的子集来学习意图表示，然后将这些表示迁移到修订生成任务中。

Result: 实验结果表明，Intention-Tuning在小规模修订语料上是有效且高效的，性能优于多个参数高效微调基线方法。

Conclusion: Intention-Tuning为解决基于意图的文本生成任务提供了一种有效方法，特别是在数据稀缺的修订领域，通过动态层选择和表示迁移实现了更好的性能。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [222] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

TL;DR: GlobalHealthAtlas是一个大规模多语言公共卫生推理数据集，包含28万实例，涵盖15个公共卫生领域和17种语言，分为三个难度级别，并提出了LLM辅助的质量控制流程和领域对齐评估器。


<details>
  <summary>Details</summary>
Motivation: 公共卫生推理需要基于科学证据、专家共识和安全约束进行群体层面的推断，但目前作为结构化机器学习问题仍未被充分探索，缺乏监督信号和基准测试。

Method: 1) 构建GlobalHealthAtlas数据集：从公开公共卫生资源中提取280,210个实例，涵盖15个领域和17种语言，按健康素养、流行病学推理和政策推理三个难度级别分层；2) 提出LLM辅助的构建和质量控制流程，包括检索、去重、证据基础检查和标签验证；3) 开发领域对齐评估器，从多样LLM的高置信度判断中蒸馏，从准确性、推理、完整性、共识对齐、术语规范和洞察力六个维度评估输出。

Result: 创建了一个大规模、多语言、分层的公共卫生推理数据集，并建立了相应的质量控制机制和评估框架，为训练和评估LLM在安全关键公共卫生推理任务上提供了可重复的基础设施。

Conclusion: 这些贡献使得能够在超越传统问答基准的安全关键公共卫生推理任务上，对大型语言模型进行可重复的训练和评估，填补了该领域的空白。

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [223] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: 多语言大语言模型治理需从文化角度重构，解决数据不平等、规范错位和问责缺失问题，避免在规模化与中立性名义下复制全球不平等


<details>
  <summary>Details</summary>
Motivation: 当前多语言大语言模型的治理框架主要基于英语中心数据、同质用户群体和抽象公平概念，忽视了低资源语言和文化边缘化社区的需求，导致系统性风险。需要建立文化根基的治理框架来应对这些挑战。

Method: 基于人本计算和AI治理的跨文化视角，综合现有关于多语言模型行为、数据不对称和社会技术危害的证据，提出文化根基的治理框架。识别三个相互关联的治理挑战，并提出概念性议程而非技术基准。

Result: 识别出三个核心治理挑战：1)训练数据和评估实践中的文化与语言不平等；2)全球部署与本地规范、价值观和权力结构之间的错位；3)针对边缘化语言社区受害的有限问责机制。提出将多语言AI治理重构为社会文化和权利基础问题。

Conclusion: 文化根基的治理对于确保多语言语言模型不会在规模化和中立性名义下复制现有全球不平等至关重要。需要从数据管理、透明度和参与式问责等方面进行设计和政策调整，建立真正包容的多语言AI治理框架。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [224] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: 该论文提出了一种带注释的逐步代码生成框架，通过将表格问答推理分解为多行可执行程序并添加自然语言注释，提高了表格问答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 表格问答对大型语言模型具有挑战性，因为传统的表格线性化会破坏结构化数据的二维关系。现有方法依赖端到端答案生成或单行程序查询，通常数值准确性有限且可解释性较差。

Method: 提出带注释的逐步代码生成框架，将表格问答推理分解为多行可执行Python程序，并添加简洁的自然语言注释，在代码生成过程中融入显式推理。

Result: 在WikiTableQuestions基准测试中，使用Qwen2.5-Coder-7B-Instruct模型达到70.9%准确率，超越Repanda基线（67.6%）。通过与强大的端到端表格问答模型集成，通过轻量级答案选择机制进一步提升至84.3%准确率。

Conclusion: 带注释的逐步代码生成框架通过将推理过程分解为可执行程序并添加注释，显著提高了表格问答的准确性和可解释性，为结构化数据推理提供了有效解决方案。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [225] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: BERT模型在早期层形成论元结构构式的特定信息，中层达到最佳聚类分离，后期维持这种表示结构


<details>
  <summary>Details</summary>
Motivation: 研究BERT模型如何处理四种基本论元结构构式，探索其内部表示结构

Method: 采用多维分析框架，包括MDS、t-SNE降维，GDV作为聚类分离指标，FDR作为线性诊断探针，以及注意力机制分析

Result: 发现分层表示结构：构式特定信息在早期层出现，中层形成最大可分离聚类，后期处理阶段维持这种表示

Conclusion: BERT模型对论元结构构式的处理呈现层次化表示结构，构式信息在模型的不同层中逐步形成和维持

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [226] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

TL;DR: Kanade是一个单层解耦语音分词器，能够从连续语音信号中提取语音学和韵律信息，同时抑制说话人身份等无关信息，实现高质量的语音合成。


<details>
  <summary>Details</summary>
Motivation: 好的语言模型始于好的分词器。对于语音建模尤为重要，因为语音是包含语言和非语言信息的连续信号。理想的语音分词器应该能够提取语音学和韵律信息，抑制说话人身份等语言无关信息，并支持高质量合成。

Method: Kanade是一个单层解耦语音分词器，通过分离声学常量来创建单一token流，捕获丰富的语音学和韵律信息。该方法不需要现有解耦编解码器常依赖的辅助方法。

Result: 实验表明，Kanade在说话人解耦和词汇可用性方面达到最先进水平，同时保持优秀的重建质量。

Conclusion: Kanade实现了理想的语音分词器设计，能够有效分离语音内容与说话人特征，为语音建模提供了高质量的token化表示。

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [227] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: Hermes是一个基于大语言模型的自动字幕翻译框架，通过说话人分离、术语识别和表达增强三个模块解决字幕翻译中的语义连贯性、代词术语翻译和表达力问题。


<details>
  <summary>Details</summary>
Motivation: 跨语言字幕翻译在娱乐本地化中至关重要，但尚未在机器翻译领域得到充分探索。虽然大语言模型显著提升了机器翻译的通用能力，但字幕文本的独特特性（如语义连贯性、代词术语翻译和翻译表达力）仍带来持续挑战。

Method: 提出Hermes框架，整合三个核心模块：1）说话人分离模块处理说话人识别；2）术语识别模块处理专业术语翻译；3）表达增强模块提升翻译的表达力和自然度。

Result: 实验表明，Hermes在说话人分离方面达到最先进性能，并能生成表达力强、上下文连贯的翻译，从而推进了跨语言字幕翻译的研究。

Conclusion: Hermes框架有效解决了字幕翻译中的关键挑战，为跨语言字幕翻译领域提供了新的解决方案，展示了基于大语言模型的字幕翻译系统的潜力。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [228] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: LAVE是一种专门为扩散大语言模型设计的约束解码方法，通过前瞻验证确保生成语法正确的输出，显著提升句法正确性且计算开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在生成由上下文无关文法定义的形式语言（如源代码、化学表达式）时，作为概率模型仍难以可靠生成语法正确的输出。现有约束解码技术面临两大障碍：1）dLLMs的非自回归特性使大多数现有方法不适用；2）当前专门为dLLMs设计的方法可能允许无法完成有效句子的中间输出，限制了实际可靠性。

Method: LAVE利用dLLMs的关键特性——在每个前向传播中并行预测所有位置标记分布的能力。每当模型提出新标记时，LAVE使用这些分布进行前瞻，高效可靠地验证提议标记的有效性。这种设计通过可靠保留中间输出扩展为有效句子的潜力来确保约束可靠性。

Result: 在四个广泛使用的dLLMs和三个代表性基准测试上的大量实验表明，LAVE始终优于现有基线，在句法正确性方面实现显著改进，同时产生可忽略的运行时开销。

Conclusion: LAVE为扩散大语言模型提供了一种有效的约束解码方法，解决了现有技术面临的挑战，能够可靠生成语法正确的形式语言输出，具有实际应用价值。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [229] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

TL;DR: 该论文介绍了提交给RANLP2025会议"PolyHope-M"任务的系统，使用RoBERTa和XLM-RoBERTa模型进行英语和德语希望言论检测，取得了较好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 研究希望言论检测任务，特别是针对英语和德语两种语言，探索预训练大语言模型在该任务上的应用效果，以提升自然语言处理任务性能。

Method: 使用RoBERTa模型处理英语希望言论检测，使用多语言模型XLM-RoBERTa同时处理英语和德语两种语言的希望言论检测任务。

Result: RoBERTa在英语任务上获得加权F1分数0.818和准确率81.8%；XLM-RoBERTa获得加权F1分数0.786和准确率78.5%。

Conclusion: 预训练大语言模型对自然语言处理任务性能有显著提升作用，证明了这些模型在不同语言希望言论检测任务中的有效性。

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [230] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: 论文提出将大语言模型安全对齐视为对预对齐数据分布的系统性扭曲，将弱到强越狱建模为预测聚合问题，推导出最优聚合策略，并展示现有logit算术越狱方法只是该框架在交叉熵损失下的特例。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全对齐会在模型对齐输出与底层预对齐数据分布之间产生系统性差异，现有越狱方法缺乏统一的理论框架来解释和优化这种差异。

Method: 将安全对齐对下一个token预测的影响建模为预对齐分布的系统性扭曲，将弱到强越狱转化为预测聚合问题，推导出损失诱导对偶空间中的梯度偏移最优聚合策略，并提出新的混合聚合规则。

Result: 在红队基准测试和数学效用任务中，该方法相比现有方法实现了更高的攻击成功率，在安全加固的gpt-oss-120b模型上表现出色，且具有更低的"越狱税"。

Conclusion: 该框架为理解安全对齐对模型预测的影响提供了统一的理论视角，推导出的最优聚合策略能够有效提升越狱攻击效果，特别是在高度安全加固的模型上。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [231] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

TL;DR: 该论文通过VAE框架推进语义表示学习，使语言表示/模型在语义和几何上更可解释，并通过塑造潜在空间几何实现局部、准符号、组合控制


<details>
  <summary>Details</summary>
Motivation: 使语言模型的内部语义表示能够被系统解释、精确结构和可靠控制，提高潜在空间的解释性和可控性

Method: 在VAE框架下探索两个互补方向：(1) 句子级学习与控制：在潜在空间中解耦和操作特定语义特征以指导句子生成；(2) 推理级学习与控制：在潜在空间中隔离和引导推理行为以控制自然语言推理

Result: 引入了一套新颖的理论框架和实践方法，通过相应实验证明，这些方法增强了自然语言潜在空间的解释性和可控性

Conclusion: 论文朝着构建内部语义表示可系统解释、精确结构和可靠控制的语言模型迈进，为语义表示学习提供了新的理论框架和方法论

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [232] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: 该研究探讨了指令微调的小型语言模型在客户服务多轮问答中的应用，通过历史摘要策略保持对话连续性，并与商业大语言模型进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在客户服务问答系统中表现优异但计算成本高、部署受限，而小型语言模型虽然更高效，但在需要对话连续性和上下文理解的多轮客户服务问答中的有效性尚未充分探索。

Method: 采用历史摘要策略来保持对话状态，引入基于对话阶段的定性分析方法，评估了9个指令微调的低参数小型语言模型，并与3个商业大语言模型进行比较，使用词汇和语义相似度指标以及人类评估和LLM-as-a-judge方法进行综合评估。

Result: 不同小型语言模型表现差异显著，部分模型展现出接近大语言模型的性能，而其他模型在保持对话连续性和上下文对齐方面存在困难。

Conclusion: 研究结果凸显了低参数语言模型在现实世界客户服务问答系统中的潜力和当前局限性，为资源受限环境下的部署提供了重要参考。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [233] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: EchoReview是一个基于引用上下文的数据合成框架，通过挖掘学术引用中的集体评估信号，将科学界长期判断转化为结构化评审数据，用于训练自动化评审系统。


<details>
  <summary>Details</summary>
Motivation: 随着科学投稿量快速增长，传统同行评审系统面临可扩展性压力，需要既可扩展又可靠的自动化评审方法。现有基于真实评审数据的监督微调方法受限于数据单一来源以及人类评审的主观性和不一致性。

Method: 提出EchoReview框架，系统性地从学术引用中挖掘隐含的集体评估信号，将科学界长期判断转化为结构化评审风格数据。基于此构建了EchoReview-16K数据集（首个大规模、跨会议、跨年的引用驱动评审数据集），并训练了EchoReviewer-7B自动化评审器。

Result: 实验结果表明，EchoReviewer-7B在证据支持和评审全面性等核心评审维度上取得了显著且稳定的改进，验证了引用上下文作为可靠自动化同行评审的稳健有效数据范式。

Conclusion: 引用上下文可以作为可靠自动化同行评审的稳健有效数据范式，EchoReview框架为解决传统评审系统的可扩展性问题提供了新途径。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [234] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: ExperienceWeaver：一个分层框架，通过将嘈杂的多维反馈提炼为结构化知识（具体技巧和高级策略），在少样本设置下提升临床文本改进性能


<details>
  <summary>Details</summary>
Motivation: 临床文本改进对医疗效率至关重要，但面临高质量数据有限和医疗文档复杂约束的挑战。现有LLM方法在少样本设置中存在局限：监督微调需要大量数据且成本高，检索增强生成通常只能提供表面修正而无法捕捉修订背后的推理过程。

Method: 提出ExperienceWeaver分层框架，将重点从数据检索转向经验学习。该框架将嘈杂的多维反馈提炼为结构化、可操作的知识，包括错误特定的技巧和高层次的策略。通过将这些提炼的经验注入到智能体管道中，模型学习"如何修订"而不仅仅是"修订什么"。

Result: 在四个临床数据集上的广泛评估表明，ExperienceWeaver在少样本设置中持续提升性能，超越了Gemini-3 Pro等最先进模型。

Conclusion: ExperienceWeaver通过经验学习而非简单检索的方法，有效解决了临床文本改进在少样本设置中的挑战，为医疗文档质量提升提供了更高效、更智能的解决方案。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [235] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: CURP框架通过双向用户编码器和离散原型码本提取多维用户特征，实现参数高效（约2000万参数，占模型总大小0.2%）的即插即用个性化生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示或训练的用户建模方法在个性化质量与计算/数据效率之间难以平衡，需要更高效的个性化解决方案。

Method: 提出CURP框架，采用双向用户编码器和离散原型码本提取多维用户特征，实现参数高效的即插即用个性化。

Result: 在多种生成任务实验中，CURP相比强基线方法表现出更优的性能和泛化能力，同时提供更好的可解释性和可扩展性。

Conclusion: CURP框架在保持高质量个性化的同时，显著降低了计算和参数需求，为LLM个性化应用提供了高效实用的解决方案。

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [236] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 该论文提出A²D方法，通过自适应能力分解增强RLVR效果，将复杂问题分解为简单子问题来指导推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法由于信息有限，模型只能进行盲探索，在复杂问题上容易失败。需要在不依赖教师模型的情况下为RLVR过程提供额外信息。

Method: 提出A²D方法：1) 先训练一个分解器，将复杂问题分解为简单子问题；2) 用分解器标注训练数据中的子问题；3) 在子问题指导下训练推理器。

Result: A²D在性能上优于竞争基线，可作为即插即用模块应用于不同RLVR算法，分析显示RLVR过程影响分解器性能和行为，特定类型的指导能更好增强推理器的探索和利用能力。

Conclusion: A²D方法通过自适应能力分解有效增强了RLVR的效果，为复杂问题解决提供了有效的子问题指导框架。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [237] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文提出了一种基于推理锚点的过程奖励方法（APR），通过识别大推理模型中答案首次稳定出现的位置，专门惩罚后续无意义的重复验证，从而在减少计算资源的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）虽然增强了大推理模型（LRMs）的能力，但引入了"过度思考"的副作用。研究发现LRMs在推理过程中经常进行无意义的重复自我验证，即使已经得到最终答案后仍在继续，这造成了结构冗余。

Method: 提出基于锚点的过程奖励（APR）方法：首先识别推理锚点（答案首次稳定的位置），然后专门惩罚锚点后的答案稳定尾部（AST）——即无意义的重复验证部分。利用适合长度惩罚的策略优化算法进行训练。

Result: 在1.5B和7B规模的五个数学推理数据集上，APR方法实现了性能-效率的帕累托前沿，同时显著减少了强化学习训练所需的计算资源。

Conclusion: 通过识别和惩罚大推理模型中无意义的重复验证行为，APR方法能够有效解决过度思考问题，在保持性能的同时提高推理效率，为大语言模型的优化提供了新思路。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [238] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

TL;DR: WordCraft是一个基于多模态大语言模型的交互式工具，帮助中文母语英语学习者通过关键词法有效记忆词汇，解决关键词生成、关联构建和心像形成等挑战。


<details>
  <summary>Details</summary>
Motivation: 中文母语英语学习者在应用关键词法记忆词汇时面临三大挑战：难以生成音韵合适的关键词、构建连贯关联、创造生动心像以促进长期记忆。现有方法要么完全自动化关键词生成（损害学习者参与度），要么只关注结果而缺乏过程指导。

Method: 首先进行形成性研究（N=18），了解学习者和教育者的困难与需求。基于这些发现，开发了WordCraft——一个基于多模态大语言模型的以学习者为中心的交互式工具，通过引导学习者完成关键词选择、关联构建和图像形成三个步骤来搭建关键词法框架。

Result: 两项用户研究表明，WordCraft不仅保留了生成效应（学习者主动参与的优势），而且在效果和可用性方面都达到了高水平。

Conclusion: WordCraft通过过程导向的交互式指导，有效解决了中文母语英语学习者在应用关键词法时的核心困难，提升了词汇记忆效果，同时保持了学习者的主动参与。

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [239] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 论文提出了一种基于迭代上下文学习的新方法，用于从大型语言模型中提取信任先验，发现GPT-4.1的信任先验与人类高度一致，并建立了基于刻板印象的预测模型。


<details>
  <summary>Details</summary>
Motivation: 构建以人为本、可信赖的AI系统需要维持校准的信任水平，但如何表征AI系统自身的信任水平是一个根本性挑战。研究旨在开发一种方法来量化AI系统的信任先验，并理解其信任决策机制。

Method: 采用基于迭代上下文学习的新颖启发方法，应用行为博弈论中的信任游戏来操作化信任概念。从多个领先的大型语言模型中提取信任先验，特别关注GPT-4.1，并分析其对不同玩家角色的响应模式。

Result: GPT-4.1的信任先验与人类观察到的信任模式高度一致。研究进一步发现，GPT-4.1能够根据玩家角色特征区分信任水平，且提取的信任变化可以通过基于感知温暖度和能力的刻板印象模型很好地预测。

Conclusion: 该方法成功地从大型语言模型中提取了有意义的信任先验，GPT-4.1展现出与人类相似的信任模式，基于刻板印象的模型能够有效预测信任变化，为理解AI系统的信任机制提供了重要工具。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [240] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

TL;DR: 该研究从表征视角分析大语言模型推理能力的训练演化，发现后训练对初始表征质量提升有限，但能驱动推理过程中的表征分布转移，从而提升任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过显式生成结果分析语言模型推理能力的训练演化，将推理过程视为黑箱，无法揭示内部变化机制。为了解构这种不透明性，需要从表征视角研究模型内部状态的动态变化。

Method: 采用表征视角分析模型内部状态动态，通过跨不同训练阶段模型的综合实验，结合统计分析和反事实实验，探究内部状态与外部输出的关系。

Result: 1) 后训练对静态初始表征质量提升有限；2) 推理过程涉及显著的连续表征分布转移；3) 后训练使模型能够驱动这种转移朝向更优的任务解决分布；4) 生成正确性与最终表征高度相关；5) 生成标记的语义（而非推理计算或参数差异）是表征转移的主要驱动力。

Conclusion: 该研究提供了对推理过程及训练对推理增强影响的新理解，为未来模型分析和优化提供了有价值的洞见，强调表征动态在推理能力演化中的核心作用。

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [241] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: HyLRA是一种基于层间稀疏性分析的混合注意力框架，通过动态规划确定最优层间策略，在敏感层保留完整注意力，在容忍层重用前层关键token索引，从而在保持性能的同时提升推理吞吐量6%-46%。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理在大型语言模型中面临注意力二次计算复杂度和KV缓存内存占用的瓶颈。现有稀疏注意力方法依赖固定模式或激进剪枝，无法在效率和准确性之间达到最优平衡。

Method: 基于层间稀疏性分析发现注意力机制的双重特性：层内敏感性和层间相似性。采用离线动态规划方法确定最优层间策略，敏感层保留完整注意力，容忍层重用前层top-k索引，避免二次计算。

Result: HyLRA将推理吞吐量提升6%-46%，同时保持可比性能（准确度下降<1%），在多个基准测试中持续优于现有稀疏注意力方法。

Conclusion: HyLRA通过层间混合注意力策略有效克服了密集注意力的二次计算瓶颈，在保持模型性能的同时显著提升推理效率，为长上下文推理提供了实用解决方案。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [242] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

TL;DR: Omni-RRM是首个开源的多模态奖励模型，通过结构化多维度偏好判断和维度化理由，在文本、图像、视频和音频上实现对齐，无需人工标注训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的性能受限于粗糙的对齐技术，缺乏有效的奖励模型。现有奖励模型主要是视觉中心、返回不透明的标量分数，且依赖昂贵的人工标注。

Method: 1) 构建Omni-Preference数据集：通过自动管道合成候选响应对，使用强教师模型协调和过滤偏好，为每对提供模态感知的基于量规的理由；2) 两阶段训练：监督微调学习基于量规的输出，然后通过强化学习(GRPO)提高对困难低对比度对的判别能力。

Result: Omni-RRM在视频基准(ShareGPT-V上80.2%)和音频基准(Audio-HH-RLHF上66.8%)上达到最先进准确率，在图像任务上显著优于现有开源奖励模型，比基础模型整体准确率提升17.7%。还能通过Best-of-N选择提升下游性能，并可迁移到纯文本偏好基准。

Conclusion: Omni-RRM是首个开源的多模态奖励模型，通过完全自动化的数据生成和两阶段训练，实现了跨文本、图像、视频和音频的结构化多维度偏好判断，显著提升了多模态对齐性能。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [243] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: 论文提出Factuality-Controlled Generation框架，让用户指定事实性约束，在信息量和事实准确性之间取得平衡，通过合成数据训练显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在编码知识时具有不同程度的置信度，面临信息量与事实准确性之间的权衡。不同应用需要不同的平衡，需要让用户能够控制输出的真实性水平。

Method: 提出Factuality-Controlled Generation框架，允许用户在查询时指定事实性约束。使用合成数据训练模型，评估框架在遵守事实性约束和保持信息量两个维度上的表现。

Result: 合成训练显著提高了模型遵守事实性要求的能力，同时保持了输出信息量。模型能够更好地平衡信息量和事实准确性。

Conclusion: Factuality-Controlled Generation框架为控制语言模型输出的事实性提供了有效方法，通过合成数据训练可以显著提升模型在遵守约束和保持信息量方面的表现。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [244] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: 该研究提出统一文本评分模型（包括密集检索器、重排序器和奖励模型）的对抗鲁棒性研究框架，开发了新的对抗训练方法，并在RLHF中展示了实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的对抗鲁棒性研究在不同应用和攻击方法之间是碎片化的，这掩盖了共享的漏洞。需要建立一个统一的框架来研究文本评分模型的对抗鲁棒性。

Method: 提出基于文本评分原则的统一研究框架，将对抗攻击和对抗训练方法适应到不同模型角色。开发了多种针对文本评分模型的对抗训练方法，并展示了组合不同训练方法的效果。

Result: 研究表明当前的对抗训练方法通常短视且难以跨攻击泛化。新提出的对抗训练方法能够提供更强的鲁棒性，同时提高任务效果。在RLHF中，对抗训练的奖励模型能够减轻奖励攻击并支持训练更好对齐的LLM。

Conclusion: 通过统一的文本评分框架研究对抗鲁棒性，开发了有效的对抗训练方法，这些方法不仅提高了模型的鲁棒性，还在实际应用中（如RLHF）显示出重要价值，有助于训练更好对齐的语言模型。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [245] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该研究创建了ILSIC语料库，包含印度法律中500+法条的普通人查询和法庭判决，用于比较法庭和普通人数据在法律条文识别任务中的差异，发现仅用法庭数据训练的模型对普通人查询效果不佳，但某些场景下从法庭到普通人数据的迁移学习有益。


<details>
  <summary>Details</summary>
Motivation: 法律条文识别是法律NLP的基础任务，传统上使用法庭判决中的事实作为输入查询。然而在实际应用中，输入查询通常是普通人提出的非正式问题。虽然存在一些普通人LSI数据集，但很少有研究探索法庭和普通人数据在LSI任务中的差异。

Method: 创建ILSIC语料库，包含500+印度法律条文的普通人查询和法庭判决；进行广泛实验，包括在普通人数据集上的零样本和少样本推理、检索增强生成和监督微调基准测试；分析模型在法庭和普通人数据上的表现差异。

Result: 研究发现：1）仅用法庭判决训练的模型在测试普通人查询时效果不佳；2）在某些场景下，从法庭到普通人数据的迁移学习可以带来益处；3）对查询类别和法条频率进行了细粒度分析。

Conclusion: 该研究强调了考虑数据来源差异对法律条文识别任务的重要性，ILSIC语料库为研究法庭和普通人数据的差异提供了资源，并展示了迁移学习在某些情况下的有效性，为实际应用中的法律NLP系统开发提供了指导。

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [246] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 研究在计算受限条件下探索Schwartz高阶类别是否能为句子级人类价值检测提供可用结构，发现硬性层次约束反而降低性能，而标签阈值调优和轻量集成效果更好。


<details>
  <summary>Details</summary>
Motivation: 探索Schwartz高阶（HO）类别是否能为句子级人类价值检测提供可用的结构，特别是在严格的计算预算限制下（单个8GB GPU）。

Method: 在ValueEval'24/ValuesML数据集（74K英语句子）上比较三种方法：(i) 直接监督的transformer模型，(ii) 使用硬掩码强制层次结构的HO→values流水线，(iii) Presence→HO→values级联方法；同时评估低成本附加组件（词典、短上下文、主题）、标签阈值调优、小型指令调优LLM基线（≤10B）、QLoRA和简单集成。

Result: HO类别可以从单个句子中学习（例如最容易的双极对达到Macro-F1≈0.58），但硬性层次门控不是可靠的改进：它通常通过错误累积和召回抑制降低最终任务的Macro-F1。相比之下，标签阈值调优是高杠杆调节手段（最高提升+0.05 Macro-F1），小型transformer集成提供最一致的额外增益（最高+0.02 Macro-F1）。小型LLM作为独立系统落后于监督编码器，但可以在跨家族集成中提供互补错误。

Conclusion: HO结构在描述上有用，但通过硬门控强制实施会损害句子级价值检测；稳健的改进来自校准和轻量集成。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [247] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: 提出一个轻量级多模态基线模型，用于《老友记》对话中的情感识别，结合文本分类器和语音表示模型，采用简单的后期融合策略


<details>
  <summary>Details</summary>
Motivation: 为SemEval-2024 Task 3数据集提供一个可访问的参考实现，而不是追求最先进的方法，旨在支持未来更严格的比较

Method: 结合基于transformer的文本分类器和自监督语音表示模型，采用简单的后期融合集成策略

Result: 在有限训练协议下获得经验结果，展示了多模态融合在何时优于单模态模型

Conclusion: 该预印本提供了透明的基础实现，旨在支持未来更严格的比较研究

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [248] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: Neural FOXP2：通过定位和操控语言神经元，使特定语言（如印地语或西班牙语）成为LLM的主要语言，解决英语在预训练中的主导地位问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs是多语言的，但英语在预训练中占据主导地位，其他语言虽然存在于参数记忆中但被系统性地抑制。需要一种机制来安全地操控语言默认设置。

Method: Neural FOXP2三阶段方法：1) 定位：训练每层SAE分解激活，量化特征对英语vs目标语言的选择性，追踪特征到最强贡献单元得到语言神经元集；2) 操控方向：通过谱低秩分析定位可控语言转移几何，构建激活差异矩阵并执行SVD提取主导方向；3) 操控：在低到中层应用带符号的稀疏激活偏移，沿目标语言主导方向添加正偏移，对英语神经元添加补偿性负偏移。

Result: 该方法能够识别出稀疏、低秩的控制电路（语言神经元），并通过安全操控使目标语言成为模型的主要语言，实现可控的目标语言默认设置。

Conclusion: 语言默认性由稀疏、低秩的控制电路（语言神经元）控制，可以通过Neural FOXP2方法进行机制性隔离和安全操控，使特定语言成为LLM的主要语言。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [249] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: 提出Structural Confidence框架，通过分析LLM隐藏状态轨迹的多尺度结构信号来增强输出正确性预测，相比传统置信度估计方法更稳健高效


<details>
  <summary>Details</summary>
Motivation: LLM在错误成本高的领域部署日益增多，但传统置信度估计方法（如token似然、语义相似性、多样本一致性）在分布偏移、领域专业文本和计算限制下表现脆弱

Method: 提出Structural Confidence框架，单次前向传播、模型无关，基于模型最后一层隐藏状态轨迹提取多尺度结构信号，结合频谱、局部变化和全局形状描述符来捕捉内部稳定性模式

Result: 在四个异构基准测试（FEVER、SciFact、WikiBio-hallucination、TruthfulQA）上进行跨领域评估，在AUROC和AUPR指标上相比基线方法表现强劲，且只需单次确定性前向传播

Conclusion: Structural Confidence为资源受限的LLM应用提供了高效、稳健的事后置信度估计实用基础，相比需要多次随机生成和辅助模型的采样一致性方法更具优势

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [250] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: MedSpeak是一个基于知识图谱的ASR错误校正框架，通过结合语义关系和语音信息来改进医疗术语识别，提升医疗问答系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于自动语音识别（ASR）的医疗问答系统在识别医疗术语时存在准确性问题，这影响了整个问答系统的性能。

Method: 提出MedSpeak框架，利用医疗知识图谱中的语义关系和语音信息，结合大语言模型的推理能力，对噪声转录进行校正，改进下游答案预测。

Result: 在基准测试上的综合实验结果表明，MedSpeak显著提高了医疗术语识别的准确性和整体医疗问答性能，成为该领域的先进解决方案。

Conclusion: MedSpeak通过知识图谱辅助的ASR错误校正，有效解决了医疗问答系统中医疗术语识别不准确的问题，为医疗语音问答提供了先进的解决方案。

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [251] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: DISPO是一种新的强化学习算法，通过分离正确和错误响应的重要性采样权重上下裁剪，实现四个可控的策略更新机制，在数学推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在可验证奖励场景中存在权衡：PPO风格方法（如GRPO/DAPO）训练稳定但学习速度慢，而REINFORCE风格方法（如CISPO）学习效率高但性能不稳定。需要一种既能保持训练稳定性又能提高学习效率的方法。

Method: DISPO算法将正确和错误响应的重要性采样权重上下裁剪分离，形成四个可控的策略更新机制。通过针对性消融实验揭示每个机制对训练的影响：正确响应中，权重>1增加平均标记熵（探索），权重<1减少熵（蒸馏）；错误响应中，过度限制的裁剪会导致性能崩溃。

Result: DISPO在AIME'24上达到61.04%的准确率，优于CISPO的55.42%和DAPO的50.21%。在各种基准测试和模型上都显示出类似的性能提升。

Conclusion: DISPO通过分离控制正确和错误响应的策略更新机制，在保持探索-蒸馏平衡的同时防止灾难性失败，为大型语言模型的数学推理能力提升提供了一种简单而有效的强化学习解决方案。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [252] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

TL;DR: 本文在大型语言模型的隐藏状态中识别出一个稀疏奖励子系统，类似于人脑中的生物奖励系统，包含代表模型内部状态价值预期的价值神经元，以及编码奖励预测误差的多巴胺神经元。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中奖励子系统的启发，研究大型语言模型内部是否存在类似的奖励处理机制，以理解模型推理过程中的内部价值评估过程。

Method: 通过干预实验识别LLM隐藏状态中的价值神经元，分析其在不同数据集、模型规模和架构中的稳健性，并识别编码奖励预测误差的多巴胺神经元。

Result: 发现价值神经元在多样化数据集、模型规模和架构中具有稳健性，在不同数据集和基于同一基础模型的微调模型间具有显著可迁移性；识别出编码奖励预测误差的多巴胺神经元，在高奖励时激活增强，低奖励时激活减弱。

Conclusion: 大型语言模型内部存在类似于生物大脑的稀疏奖励子系统，包含价值神经元和多巴胺神经元，这些神经元对模型的推理过程具有重要作用，揭示了LLM内部的价值评估机制。

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [253] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: DeALOG是一个去中心化的多智能体框架，用于跨文本、表格和图像的多模态问答，通过专门的智能体协作和共享自然语言日志实现


<details>
  <summary>Details</summary>
Motivation: 跨文本、表格和图像的复杂问答需要整合多样化信息源，需要一个支持专门处理、协调和可解释性的框架

Method: 引入DeALOG框架，使用专门的智能体（表格、上下文、视觉、总结和验证），通过共享的自然语言日志作为持久内存进行通信，实现去中心化协作

Result: 在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA等数据集上表现出有竞争力的性能

Conclusion: 共享日志、智能体专业化和验证对准确性至关重要，DeALOG通过模块化组件和自然语言通信提供了可扩展的方法

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [254] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: RULES：一个通过强化学习训练LLM进行引理判断的框架，将任务分解为前置条件检查和结论效用检查两部分，通过分段感知损失掩码提高鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在数学基准测试中表现良好，但经常误用引理，在不验证假设的情况下直接应用结论。需要一种结构化方法来提高模型正确判断引理适用性的能力。

Method: 将引理判断形式化为结构化预测任务：给定陈述和候选引理，模型必须输出前置条件检查和结论效用检查。RULES框架通过两段式输出编码这一规范，使用强化学习加分段感知损失掩码进行训练，将惩罚分配给导致错误的相应部分。

Result: 在领域内任务上，RULES相比普通模型和单标签RL基线获得了一致的性能提升；在适用性破坏性扰动上表现出更大的改进；在端到端任务上达到持平或适度提升。消融研究表明两段式输出和分段感知强化学习对鲁棒性都是必要的。

Conclusion: RULES框架通过结构化引理判断任务和分段感知训练机制，有效提高了大型语言模型在数学推理中正确应用引理的能力，特别是在处理适用性破坏性扰动时表现出更强的鲁棒性。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [255] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

TL;DR: 提出了一种高效蒸馏方法，将现有基于token训练的LLMs转换为字节语言模型，仅需约125B字节数据，保留原模型大部分性能


<details>
  <summary>Details</summary>
Motivation: 字节语言模型虽然前景广阔，但现有方法需要从头训练数万亿字节，成本极高。因此需要一种更经济的方法将现有token训练的LLMs转换为字节模型

Method: 采用两阶段课程学习：1）渐进知识蒸馏，将字节级表示与token训练教师模型的嵌入对齐；2）字节级监督微调，实现完全在字节空间的端到端生成

Result: 在Llama、Qwen、OLMo等多个模型系列上验证，蒸馏后的字节语言模型仅使用约125B字节数据，就能保留教师模型的大部分性能

Conclusion: 提出的蒸馏方法能够高效地将token训练的LLMs转换为字节语言模型，显著降低了训练成本，为字节语言模型的广泛应用提供了可行路径

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [256] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

TL;DR: 评估大型语言模型能否准确模拟新手推理和元认知判断，发现GPT-4生成的内容过于连贯、冗长且缺乏人类思考的变异性，同时高估学习者表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注问题解决准确性，忽略了人类学习中碎片化、不完美的推理特征。需要评估LLMs能否真实模拟新手推理和元认知判断，以指导自适应学习系统的设计。

Method: 使用630条化学辅导问题中的出声思考记录，包含学生提示使用、尝试和问题上下文。在最小化和扩展上下文提示下比较LLM生成推理与人类学习者话语，评估模型预测学习者逐步成功的能力。

Result: GPT-4生成流畅且上下文适当的延续，但其推理系统性过于连贯、冗长且变异性低于人类出声思考。随着提示中问题解决上下文更丰富，这些效应加剧。学习者表现被持续高估。

Conclusion: LLMs在模拟学习方面存在认识论限制，这归因于其训练数据包含类似专家的解决方案，缺乏情感表达和工作记忆约束。评估框架可指导未来设计更忠实支持新手学习和自我调节的自适应系统。

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [257] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 首次系统研究多语言MLLMs中的语音偏见，构建BiasInEar数据集，评估9个模型在语言、口音、性别和选项顺序扰动下的表现，发现MLLMs对语言和选项顺序高度敏感。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对多语言多模态大语言模型中语音偏见的系统性研究，需要建立统一的评估框架来评估语音集成LLMs的公平性和鲁棒性。

Method: 构建BiasInEar数据集，基于Global MMLU Lite，涵盖英语、中文和韩语，平衡性别和口音，共70.8小时语音。使用四种互补指标（准确率、熵、APES和Fleiss' κ）评估9个代表性模型在语言、口音、性别和选项顺序扰动下的表现。

Result: MLLMs对人口统计因素相对鲁棒，但对语言和选项顺序高度敏感，表明语音会放大现有结构偏见。架构设计和推理策略显著影响跨语言的鲁棒性。

Conclusion: 本研究建立了评估语音集成LLMs公平性和鲁棒性的统一框架，弥合了基于文本和语音评估之间的差距，为未来研究提供了基准数据集和方法论。

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [258] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: LLMs在相同人格提示下，会根据不同对话情境（破冰、谈判、群体决策、共情任务）表现出不同的语言、行为和情感结果，显示出人格表达具有情境敏感性而非固定不变。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在相同人格提示下，其行为实现如何随对话情境变化，以及这种变化反映的是不一致性还是类似人类的情境敏感适应。

Method: 研究在四种对话情境（破冰、谈判、群体决策、共情任务）中使用相同的人格提示，分析LLMs的语言、行为和情感结果，通过Whole Trait Theory理论框架进行解释。

Result: 结果显示情境线索系统地影响人格表达和情感基调，相同特质在不同社交和情感需求下表达方式不同，表明LLMs表现出情境敏感而非固定的人格表达。

Conclusion: LLMs的人格表达具有情境敏感性，能够灵活适应社交互动目标和情感条件，这类似于人类行为的情境适应，而非简单的不一致性。

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [259] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: 论文提出"知识净化"概念，通过整合多个教师大语言模型的推理过程为单一推理，解决多教师蒸馏中的知识冲突和资源需求问题，并提出了五种净化方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在多教师模型场景下面临知识冲突和高资源需求的问题，需要更高效的方法来整合多个教师模型的知识，以优化蒸馏效果并促进轻量级模型的实用部署。

Method: 提出知识净化概念，将多个教师LLM的推理过程整合为单一推理；设计了五种不同角度的净化方法，包括基于路由的方法等创新技术。

Result: 实验表明这些方法不仅提升了蒸馏模型的性能，还能有效缓解知识冲突；基于路由的方法展现出强大的泛化能力。

Conclusion: 知识净化技术能够优化多教师蒸馏过程，为强大而轻量级模型的实际部署提供了创新解决方案，展示了净化技术在知识蒸馏领域的潜力。

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [260] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 本文针对大语言模型在垂直领域翻译的局限性，以视觉媒体字幕翻译为主题，提出了自适应局部偏好优化方法，构建了多语言字幕平行语料库，显著提升了翻译质量的多维评估表现。


<details>
  <summary>Details</summary>
Motivation: 随着应用场景日益复杂，大语言模型在垂直领域翻译的局限性逐渐显现。本研究聚焦于如何构建满足领域定制需求的翻译大语言模型，特别以视觉媒体字幕翻译为主题，探索如何训练表达生动、富有表现力的翻译模型。

Method: 研究调查了字幕翻译及其他领域的直译与意译情况，验证了LLM作为翻译奖励模型和评估器的可靠性。构建并发布了多语言字幕平行语料库数据集，提出了自适应局部偏好优化方法来处理细粒度偏好对齐问题。

Result: 实验结果表明，自适应局部偏好优化方法在翻译质量的多维评估中取得了出色的性能表现。

Conclusion: 本研究成功构建了针对视觉媒体字幕翻译的定制化大语言模型，通过自适应局部偏好优化方法有效解决了垂直领域翻译的细粒度偏好对齐问题，为领域定制化翻译模型的发展提供了有效解决方案。

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [261] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: 提出了一种验证器引导的自适应推理框架，通过过程奖励模型动态指导推理轨迹的生成和选择，相比均匀分配计算资源的测试时计算扩展方法，在数学推理任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统测试时计算扩展方法存在三个主要问题：1）均匀分配推理计算资源；2）使用固定的采样策略；3）仅在重排序阶段使用验证。这导致计算资源分配效率低下，无法根据推理过程的质量动态调整计算策略。

Method: 提出验证器引导的自适应框架，将推理视为迭代的轨迹生成和选择过程。每个问题运行多次推理迭代，每次迭代中：1）可选生成高级计划；2）选择推理工具集和计算策略；3）生成候选推理轨迹。使用过程奖励模型作为统一控制信号：在迭代内，步骤级PRM分数用于指导生成过程中的剪枝和扩展；在迭代间，聚合的轨迹奖励用于选择最终响应。

Result: 在多个数据集上，动态的PRM引导方法持续优于直接的测试时计算扩展方法，在MATH-500上取得大幅提升，在AIME24和AMO-Bench等更难基准上实现数倍改进。通过理论FLOPs和计算强度指标分析效率，验证了验证引导的分配方法能将计算集中在高效用推理路径上。

Conclusion: 验证器引导的自适应推理框架通过动态调整计算资源和策略，显著提升了数学推理任务的性能。该方法的核心优势在于能够根据推理过程的质量实时调整计算分配，避免计算浪费，将资源集中在最有希望的推理路径上。

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [262] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

TL;DR: LORE通过细粒度对比学习激活LLM的逻辑推理能力，提升知识密集型任务中的检索效果，无需外部监督或资源


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中表现不佳，因为传统检索器过度依赖表面相似性，无法处理涉及复杂逻辑关系的查询。模型本身具备逻辑分析能力，但在标准训练中未被充分利用。

Method: LORE引入细粒度对比学习，激活模型潜在的逻辑推理能力，引导嵌入向量向符合逻辑结构的证据对齐，而不是浅层相似性。该方法无需外部监督、额外资源或预检索分析，保持索引兼容性。

Result: LORE持续提升检索效用和下游生成质量，同时保持效率。数据集和代码已公开。

Conclusion: LORE通过激活LLM内在的逻辑推理能力，有效解决了传统检索器在复杂逻辑查询中的局限性，为知识密集型任务提供了更有效的检索增强方案。

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [263] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的框架，将基于LLM的时序点过程扩展到视觉模态，通过自适应序列压缩机制解决长上下文问题，在预测准确性和生成文本分析质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时序点过程方法在处理多模态数据时面临序列长度急剧增加的问题，导致基于注意力的模型难以生成需要长距离理解的连贯长文本描述。需要解决多模态内容生成和事件动态推理的挑战。

Method: 提出基于时间相似性的自适应序列压缩机制，减少序列长度同时保留关键模式；采用两阶段范式：先在压缩序列上进行预训练，然后针对下游任务进行监督微调；将文本生成定位为与时序和类型预测并列的核心能力。

Result: 在包括挑战性的DanmakuTPP-QA基准测试在内的广泛实验中，该方法在预测准确性和生成文本分析质量方面均优于最先进的基线方法。

Conclusion: 该框架成功将LLM-based TPPs扩展到视觉模态，通过自适应压缩机制有效解决了长上下文问题，实现了高质量的多模态事件序列建模和文本分析生成。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [264] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: LogiQAte是一个诊断性基准测试，通过逻辑等价但表面形式混淆的问题来评估大语言模型的真实推理能力，发现现有模型在混淆问题上的表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在标准形式下能很好处理逻辑推理任务，但在逻辑等价但表面形式混淆的问题上表现不佳，这表明模型缺乏对问题的深层理解，只是依赖表面形式匹配。

Method: 提出了Logifus结构保持逻辑混淆框架，并基于此创建了LogiQAte基准测试，包含1,108个问题，涵盖四个推理任务：混淆一阶逻辑、混淆血缘关系、混淆数字序列和混淆方向感。

Result: 在六个最先进的模型上评估发现，混淆严重降低了零样本性能：GPT-4o平均下降47%，GPT-5下降27%，推理模型o4-mini下降22%。

Conclusion: 当前大语言模型解析问题时缺乏深层理解，只是依赖表面形式，迫切需要构建真正理解并保持意义超越表面形式的模型。

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [265] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

TL;DR: 研究探讨微调数据集的语言特性如何影响大语言模型的文化对齐表现，发现词汇导向特性在不同模型和基准测试中表现最稳健


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的全球部署引发了文化错位担忧，但用于文化适应的微调数据集的语言特性尚未被充分理解，需要研究哪些语言特性与文化表现相关

Method: 对阿拉伯语、中文和日语数据集计算轻量级语言、语义和结构指标，进行主成分分析，然后微调三个主要LLM家族（LLaMA、Mistral、DeepSeek），并在文化知识、价值观和规范基准上评估

Result: PCA成分与下游表现相关，但这些关联强烈依赖于模型；词汇导向成分（PC3）最稳健，在不同模型和基准测试中表现更一致，而强调语义或多样性极端（PC1-PC2）通常中性或有害

Conclusion: 微调数据集的语言特性对文化对齐有重要影响，但效果因模型而异；词汇导向特性是最可靠的预测指标，为文化对齐的数据集设计提供了实用指导

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [266] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 该研究提出了一个利用语言类型学改进低资源语言翻译质量的框架，无需平行训练数据或模型重训练，通过语言类型学分析和生成过程中的语言消歧来提高翻译质量。


<details>
  <summary>Details</summary>
Motivation: 主要语言模型主要基于高资源语言训练，对主导类型学模式存在系统性偏见，导致在翻译到类型学差异较大的低资源语言时出现结构不一致问题。需要一种无需平行训练数据就能改善翻译质量的方法。

Method: 框架包含两个组件：1) 通用元语言框架(UMF)，将语言表示为16个类型学维度的结构化配置文件，使用差异加权评分；2) 计算引擎，在生成过程中进行语言消歧，在候选选择过程中进行类型学合规性评分。

Result: 在9个语言对上的评估显示干预率与英语的类型学距离强相关。在341个英语句子的实验中，框架对保守处理语言的干预精度为48.16%，对形态密集语言的干预精度为28.15%，对结构配置语言的干预精度为86.26%。

Conclusion: 该框架无需平行训练数据，可与任何能产生多个候选输出的LLM配合使用，为资源不足的语言提供了实用的部署方案，通过语言类型学指导显著改善了低资源语言的翻译质量。

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [267] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

TL;DR: PedagoSense系统通过两阶段策略分类和LLM生成，在对话式学习中检测和推荐教学策略，提升互动质量


<details>
  <summary>Details</summary>
Motivation: 解决对话式学习中提高互动质量的挑战，通过检测和推荐有效的教学策略来改进师生对话

Method: 结合两阶段策略分类器与大型语言模型生成：先使用二元分类器检测是否存在教学策略，再进行细粒度分类识别具体策略；同时基于对话上下文推荐策略，并用LLM生成符合该策略的响应

Result: 在人工标注的师生对话数据集上评估，教学策略检测表现优异，数据增强带来持续增益；但细粒度分类仍具挑战性

Conclusion: PedagoSense将教学理论与基于LLM的响应生成实践相结合，为更自适应的教育技术搭建桥梁

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [268] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 该论文揭示了注意力机制中的注意力汇现象与混合专家机制的内在联系，并提出了一种基于负载平衡的注意力汇感知训练方法来解决注意力头崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的注意力机制存在注意力汇现象（过度关注第一个token），现有方法如Sink Attention和Gated Attention试图解决此问题，但缺乏对这些注意力机制之间关系的全面分析。作者旨在深入理解注意力汇现象的本质及其与混合专家机制的关系。

Method: 通过理论和实证证据证明Vanilla Attention和Sink Attention中的注意力汇自然构建了注意力层内的混合专家机制。为缓解注意力头崩溃问题，提出了一种带有辅助负载平衡损失的注意力汇感知训练算法。

Result: 实验表明，该方法在Vanilla Attention、Sink Attention和Gated Attention中均实现了有效的注意力头负载平衡，并提升了模型性能。

Conclusion: 该研究为注意力机制提供了新的视角，揭示了注意力层内固有的混合专家结构，鼓励进一步探索注意力机制的内在特性。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [269] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: ASTER框架通过针对性的冷启动策略解决强化学习中工具集成推理的交互崩溃问题，使用仅4K交互密集轨迹实现最优性能，在数学基准测试中达到SOTA水平


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言模型的工具集成推理中存在交互崩溃问题，即模型无法维持多轮工具使用，退化到大量内部推理和简单的后验代码验证。需要解决这一挑战以实现有效的工具集成推理扩展。

Method: 提出ASTER框架，采用针对性的冷启动策略，优先选择交互密集的轨迹。研究了三个关键问题：冷启动SFT如何诱导工具使用行为先验、冷启动轨迹的交互密度如何影响探索和下游RL结果、RL交互预算如何影响学习动态和泛化能力。

Result: 仅使用4K交互密集轨迹的小型专家冷启动集就能获得最强的下游性能，建立了强大的先验，在扩展RL训练期间实现了优越的探索。ASTER-4B在竞争性数学基准测试中达到SOTA，在AIME 2025上达到90.0%，超越了包括DeepSeek-V3.2-Exp在内的领先开源模型。

Conclusion: ASTER框架通过精心设计的冷启动策略有效解决了工具集成推理中的交互崩溃问题，证明了交互密集轨迹在建立强大行为先验中的关键作用，为强化学习在工具集成推理中的可扩展应用提供了有效解决方案。

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [270] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

TL;DR: Chronos是一个轻量级即插即用的时序推理评分器，将推理轨迹建模为时间序列，通过学习token概率特征分配质量分数，采用加权投票机制，显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法（如多数投票和启发式token级评分）将推理轨迹或token同等对待，容易受到轨迹质量大幅变化和局部逻辑失败的影响，需要更精细的轨迹质量评估方法。

Method: Chronos将每个推理轨迹建模为时间序列，学习捕获token概率的轨迹特征，据此分配质量分数，并采用加权投票机制进行决策。

Result: 在领域内和领域外基准测试中，Chronos在各种模型上均带来显著提升，计算开销可忽略。Chronos@128在HMMT25上相对Pass@1提升34.21%，相对Maj@128提升22.70%（使用Qwen3-4B-Thinking-2507模型）。

Conclusion: Chronos作为轻量级即插即用的时序推理评分器，能够有效评估推理轨迹质量，通过加权投票机制显著提升大语言模型的推理性能，具有广泛适用性和实用性。

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [271] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: 该论文提出"Token Priority"作为解决细粒度自回归生成与粗粒度监督信号之间粒度不匹配问题的关键桥梁，将SFT重新定义为精确的分布重塑过程，而非简单优化。


<details>
  <summary>Details</summary>
Motivation: 从拟合经验数据到实现真正人类效用的转变受到粒度不匹配的根本约束，细粒度的自回归生成往往由粗粒度或均匀的信号监督，需要建立更精确的监督机制。

Method: 提出Token Priority框架，将SFT形式化为精确的分布重塑过程，将原始数据与理想对齐流形对齐。将现有方法分为两个机制：用于噪声过滤的Positive Priority和用于毒性模式遗忘的Signed Priority。

Result: 通过统一视角分析近期突破性进展，重新审视现有进展和局限性，识别关键挑战，并为未来研究提供方向建议。

Conclusion: Token Priority是连接经验数据拟合与人类效用实现的关键桥梁，为SFT提供了更精确的理论框架，有助于推动语言模型对齐领域的发展。

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [272] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: 该论文提出了"推理问答"新任务，要求模型从仅提供线索的文本中推断答案，构建了QUIT数据集，发现现有QA方法在推理任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统大多关注答案提取，但有些问题需要从文本中推断答案，而不仅仅是直接提取。目前缺乏专门研究推理问答的任务和数据集。

Method: 提出推理问答新任务，构建QUIT数据集（7,401个问题，240万段落），基于人类和机器生成线索，使用LLM进行可回答性标注和人工验证。全面评估检索器、重排序器和LLM阅读器。

Result: 传统QA方法在推理任务上表现不佳：检索器效果差，重排序器提升有限，微调改进不一致。即使是面向推理的LLM也无法超越较小的通用模型。

Conclusion: 当前QA流程尚未准备好处理基于推理的任务。推理问答建立了一类新的QA任务，推动从间接文本证据进行理解和推理的研究方向。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [273] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: TerminalTraj是一个可扩展的终端轨迹数据生成管道，通过Docker化环境解决终端任务数据的可执行性和可验证性问题，生成了5万多个经过验证的终端轨迹，显著提升了模型在终端基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练面向终端任务的智能体模型需要高质量的终端轨迹数据，但大规模构建此类数据面临两大挑战：可执行性（需要合适的Docker环境）和可验证性（异构任务输出难以统一验证）。

Method: 提出TerminalTraj管道，包含三个步骤：1) 筛选高质量仓库构建Docker化执行环境；2) 生成与Docker对齐的任务实例；3) 合成带有可执行验证代码的智能体轨迹。

Result: 构建了32K个Docker镜像，生成了50,733个经过验证的终端轨迹，涵盖8个领域。基于Qwen2.5-Coder训练的模型在TerminalBench上性能显著提升，TerminalTraj-32B模型在小于100B参数的模型中表现优异。

Conclusion: TerminalTraj有效解决了终端轨迹数据生成的可扩展性问题，为训练终端任务智能体提供了高质量数据，显著提升了模型性能，并展示了改进的测试时扩展行为。

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [274] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: PARSE是首个波斯语开放领域推理问答基准，包含10,800个问题，涵盖布尔、多项选择和事实型格式，支持波斯语LLM的公平评估和实用模型适配。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为拥有约1.3亿使用者的语言，缺乏全面的开放领域资源来评估具备推理能力的问答系统。现有高质量基准在低资源语言中稀缺，需要填补这一空白。

Method: 通过受控的LLM生成流水线构建基准，包含多阶段过滤、标注和一致性检查以确保语言和事实质量。采用人类评估进行验证，并测试了多语言和波斯语LLM在不同提示策略下的表现。

Result: 波斯语提示和结构化提示（布尔/多项选择使用思维链，事实型使用少样本）能提升性能。微调进一步改善结果，特别是对波斯语专用模型。基准支持公平比较和实用模型适配。

Conclusion: PARSE填补了波斯语QA研究的关键空白，为低资源环境下开发和评估具备推理能力的LLM提供了坚实基础，支持公平比较和实用模型适配。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [275] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: Pacer是一种动态控制推测解码中草稿长度的新方法，通过轻量级可训练预验证层实现块级预验证，相比固定草稿长度能显著提升解码速度


<details>
  <summary>Details</summary>
Motivation: 研究发现推测解码中不同解码步骤的最优草稿长度差异很大，固定草稿长度限制了解码速度的进一步提升潜力

Method: 提出Pacer方法，使用轻量级可训练预验证层对草稿令牌进行块级预验证，在预验证失败时停止草稿生成，实现动态草稿长度控制

Result: Pacer在多个SD模型对和基准测试中表现优异，相比自回归解码最高达到2.66倍加速，始终优于标准推测解码，与Ouroboros结合时最高达到3.09倍加速

Conclusion: Pacer通过动态控制草稿长度有效解决了固定草稿长度的限制，显著提升了推测解码的效率，为大型语言模型推理加速提供了新思路

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [276] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: EverMemBench是一个新的对话记忆基准测试，包含多参与者、多群组的长对话（超100万token），评估记忆系统在细粒度回忆、记忆意识和用户画像理解三个维度的能力，揭示了现有方法在多跳推理、时序推理和记忆检索方面的严重局限。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆基准测试主要关注双人、单一主题的对话，无法捕捉现实世界对话的复杂性。需要一个新的基准测试来评估长期对话记忆系统在多参与者、多群组、跨主题交织、时序演化信息等真实场景下的表现。

Method: 提出了EverMemBench基准测试，包含多参与者、多群组的对话，跨越超过100万token，具有时序演化信息、跨主题交织和角色特定人物设定。通过1000多个问答对评估记忆系统在三个维度：细粒度回忆、记忆意识和用户画像理解。

Result: 评估揭示了三个关键局限：1) 多参与者场景下多跳推理崩溃，即使oracle模型也只能达到26%准确率；2) 时序推理仍未解决，需要超越时间戳匹配的版本语义；3) 记忆意识受限于检索，当前基于相似度的方法无法弥合查询与隐式相关记忆之间的语义鸿沟。

Conclusion: EverMemBench为开发下一代记忆架构提供了一个具有挑战性的测试平台，揭示了现有对话记忆系统在复杂真实场景下的严重不足，特别是在多参与者推理、时序处理和记忆检索方面需要根本性改进。

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [277] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: DreamOn是一个新颖的扩散框架，解决了扩散语言模型在代码填充任务中需要固定长度掩码序列的限制，实现了动态可变长度的生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然提供了灵活的非自回归填充能力，但实际应用受到固定长度掩码序列要求的限制。当预定义的掩码大小与理想完成长度不匹配时，代码填充性能会严重下降。

Method: DreamOn在扩散过程中增加了两个长度控制状态，使模型能够基于自身预测自主扩展或收缩输出长度。该方法只需对现有扩散语言模型的训练目标进行最小修改，无需架构改变。

Result: 基于Dream-Coder-7B和DiffuCoder-7B构建的DreamOn在HumanEval-Infilling和SantaCoder-FIM上的填充性能与最先进的自回归模型相当，并匹配使用真实长度获得的oracle性能。

Conclusion: DreamOn消除了扩散语言模型实际部署的基本障碍，显著提高了其在可变长度生成方面的灵活性和适用性。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [278] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: CRAFT是一个基于强化学习的框架，通过双重奖励机制优化多跳问答中的推理过程，解决推理崩溃、推理-答案不一致和格式控制丢失三大挑战，提高答案准确性和推理忠实度。


<details>
  <summary>Details</summary>
Motivation: 多跳问答中的检索增强生成面临三大挑战：1) 推理崩溃 - 多跳组合和噪声检索使推理不稳定；2) 推理-答案不一致 - LLM生成的不确定性和证据-干扰物混合导致答案正确但推理不忠实；3) 格式控制丢失 - 传统思维链生成偏离结构化输出格式要求。

Method: 提出CRAFT框架，采用基于组相对策略优化的强化学习方法，使用双重奖励机制：确定性奖励确保结构正确性，基于评判的奖励验证语义忠实度。支持可控的推理轨迹变体，系统分析结构和规模对推理性能和忠实度的影响。

Result: 在三个多跳问答基准测试中，CRAFT在答案准确性和推理忠实度方面均有提升，CRAFT 7B模型在多个推理轨迹设置下与闭源LLMs达到竞争性性能。

Conclusion: CRAFT通过强化学习框架有效解决了多跳问答中的推理忠实度问题，双重奖励机制确保了结构正确性和语义忠实度，为可控推理轨迹分析提供了系统方法。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [279] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

TL;DR: XDLM提出了一种新的离散生成模型，通过平稳噪声核桥接了掩码扩散语言模型（MDLM）和均匀噪声扩散语言模型（UDLM），在语义理解和生成质量之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有离散生成模型中，MDLM擅长语义理解和零样本泛化，UDLM在少步生成质量上表现优异，但两者都无法在理解和生成能力上取得平衡。需要一种能同时兼顾两种能力的统一框架。

Method: 提出XDLM模型，通过平稳噪声核统一MDLM和UDLM两种范式，将两者作为特例恢复。采用代数简化后验概率的方法缓解内存瓶颈，实现了理论上的统一和实际效率的提升。

Result: XDLM在理解和生成能力之间推进了帕累托前沿：在零样本文本基准上超越UDLM 5.4分；在少步图像生成上FID为54.1，优于MDLM的80.8；在调优8B参数大语言模型时，仅用32步就达到15.0 MBPP，性能翻倍。

Conclusion: XDLM成功统一了离散生成模型的两种主要范式，在语义理解和生成质量之间取得了更好的平衡，训练动态分析显示其具有优越的长程扩展潜力。

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [280] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: RISE方法解决LLM解释中的冗余问题，通过量化每个输入的独特影响来提供更稳定、清晰的归因分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型使用包含冗余信息的广泛上下文，标准解释方法难以处理冗余和重叠上下文，导致归因分数不稳定，影响可解释性并带来安全风险。

Method: 提出RISE（冗余不敏感解释评分）方法，量化每个输入相对于其他输入的独特影响，最小化冗余影响，提供更清晰的稳定归因。

Result: 实验表明RISE比传统方法提供更稳健的解释，强调条件信息对于可信LLM解释和监控的重要性。

Conclusion: RISE方法能够有效区分上下文中的关键元素和相关元素，为LLM提供更可靠、稳定的解释框架，增强模型的可解释性和安全性监控。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [281] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

TL;DR: 该论文提出了一种基于学生熵引导的位置选择知识蒸馏方法（SE-KD），通过系统分析位置、类别和样本三个维度的选择性蒸馏，优化大型语言模型的知识蒸馏效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的知识蒸馏方法中，选择性蒸馏（选择部分token位置、词汇类别或训练样本进行监督）虽然被广泛采用，但缺乏对重要性信号、选择策略及其相互作用的系统理解，需要明确在自回归LLMs中"在哪里蒸馏"和"如何蒸馏"的问题。

Method: 首先解构选择性知识蒸馏的三个维度（位置、类别、样本），系统比较不同重要性信号和选择策略；然后基于分析结果，提出学生熵引导的位置选择方法（SE-KD）；最后将该方法扩展到类别和样本维度（SE-KD 3X），实现全面的效率优化。

Result: SE-KD在多个基准测试中，相比密集蒸馏在准确性、下游任务一致性和内存效率方面都有提升。SE-KD 3X进一步带来互补的效率增益，使离线教师缓存变得可行，实际应用中减少了70%的墙上时间、18%的峰值内存和80%的存储使用，同时保持性能不变。

Conclusion: 通过系统分析选择性知识蒸馏的三个维度，提出的学生熵引导方法（SE-KD）有效优化了大型语言模型的知识蒸馏过程，在保持性能的同时显著提升了训练效率和资源利用率，为实际部署提供了可行的解决方案。

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [282] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: SentiFuse是一个灵活、模型无关的框架，通过标准化层和多种融合策略整合异构情感模型，在多个社交媒体数据集上优于单个模型和简单集成。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型各有优势但缺乏有效的统一整合框架，无法充分利用模型间的互补性。

Method: 提出SentiFuse框架，包含标准化层和三种融合策略：决策级融合、特征级融合和自适应融合，支持异构模型的系统整合。

Result: 在Crowdflower、GoEmotions和Sentiment140三个大规模社交媒体数据集上，SentiFuse始终优于单个模型和简单集成。特征级融合效果最佳，F1分数比最佳单个模型和简单平均提升高达4%，自适应融合在处理否定、混合情感和复杂情感表达等挑战性案例时增强鲁棒性。

Conclusion: 系统利用模型互补性能够实现更准确可靠的情感分析，SentiFuse为整合异构情感模型提供了有效的统一框架。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [283] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 该论文针对孟加拉语等低资源语言的问答模型，创建了首个孟加拉语反事实问答数据集BanglaCQA，并开发了多种模型管道来分析模型在答案生成时对预编码知识和上下文输入的依赖程度。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）的问答模型面临标注数据有限和语言复杂性的挑战。现有孟加拉语QA数据集缺乏分析模型依赖预编码知识还是上下文输入的结构，需要专门的数据集和方法来研究这一问题。

Method: 1. 创建BanglaCQA数据集：扩展现有孟加拉语数据集，集成反事实段落和可回答性标注；2. 开发微调管道：针对编码器-解码器语言特定和多语言基线模型；3. 设计基于提示的管道：针对仅解码器LLM；4. 应用LLM和人工评估技术：基于语义相似度衡量答案质量。

Result: 研究发现思维链（CoT）提示在反事实场景中特别有效，尤其是在仅解码器LLM中，能够独特地提取预编码知识。研究提供了模型在不同QA设置下性能的详细分析，揭示了低资源语言环境中知识来源的依赖模式。

Conclusion: 该工作不仅为分析孟加拉语QA中的知识来源引入了新颖框架，还发现了关键发现，为低资源语言环境中的反事实推理开辟了更广泛的研究方向。

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [284] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 论文提出ConPress方法，通过多问题上下文压力诱导模型自我压缩推理轨迹，减少推理token使用量59%（MATH500）和33%（AIME25）同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通常通过生成长思维链来解决推理密集型任务，导致大量推理开销。作者发现自我压缩现象：当多个独立可回答的问题出现在同一提示中时，模型会自发地为每个问题生成更短的推理轨迹。

Method: 提出ConPress（从上下文压力中学习），一种轻量级自监督微调方法：构建多问题提示诱导自我压缩，采样模型输出，解析过滤每个问题的轨迹以获得简洁正确的推理轨迹，然后直接用于监督微调。

Result: 仅使用8k微调样本，ConPress在MATH500上减少推理token使用量59%，在AIME25上减少33%，同时保持有竞争力的准确率。

Conclusion: ConPress方法能够内部化压缩推理行为，无需外部教师、手动修剪或强化学习，在单问题设置中实现高效的推理压缩。

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [285] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: Ebisu是一个针对日语金融语言理解的基准测试，包含两个专家标注的任务：JF-ICR（评估投资者问答中的隐含承诺与拒绝识别）和JF-TE（评估专业披露中嵌套金融术语的层次提取与排序）。


<details>
  <summary>Details</summary>
Motivation: 日语金融语言结合了粘着语结构、混合书写系统和高语境交流规范，依赖间接表达和隐含承诺，这对大语言模型构成了重大挑战，需要专门的基准来评估和改进模型在这方面的能力。

Method: 研究者开发了Ebisu基准，包含两个基于语言和文化背景的任务：1) JF-ICR：评估投资者问答中的隐含承诺与拒绝识别；2) JF-TE：评估专业披露中嵌套金融术语的层次提取与排序。使用开源和专有的大语言模型进行评估，涵盖通用模型、日语适应模型和金融专用模型。

Result: 即使是当前最先进的系统在这两个任务上都表现不佳。模型规模的增加带来的改进有限，语言和领域特定的适应并不能可靠地提升性能，仍然存在显著的性能差距。

Conclusion: Ebisu为推进基于语言和文化背景的金融自然语言处理提供了一个专注的基准测试，所有数据集和评估脚本都已公开发布，有助于改进大语言模型在日语金融语言理解方面的能力。

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [286] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: Rubric-ARM：通过强化学习联合优化评分标准生成器和评判器，解决传统奖励模型在不可验证领域（如创意写作）只能预测标量分数的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型只能预测标量分数，无法捕捉不可验证领域（如创意写作、开放式指令遵循）中响应质量的多维度特性。现有方法要么依赖静态评分标准，要么采用分离的训练流程，存在局限性。

Method: 提出Rubric-ARM框架，将评分标准生成视为潜在动作，通过强化学习从偏好反馈中联合优化评分标准生成器和评判器。采用交替优化策略缓解同时更新的非平稳性问题，并提供理论分析证明该策略能减少训练中的梯度方差。

Result: 在多个基准测试中，Rubric-ARM实现了最先进的性能，显著提升了离线强化学习和在线强化学习设置中的下游策略对齐效果。

Conclusion: Rubric-ARM通过联合学习评分标准和评判器，有效解决了传统奖励模型在不可验证领域的局限性，为多维度响应质量评估提供了新框架。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [287] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

TL;DR: 本文提出AROA框架，通过论证稀有性自动评估学生议论文的原创性，将原创性定义为参考语料库中的稀有程度，包含结构、论点、证据和认知深度四个维度，并发现质量与原创性存在权衡关系，AI生成文本在结构复杂性上可与人类媲美但论点原创性较低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能够轻松生成高质量文本，传统的以质量为中心的写作评估正在失去意义。如果教育的核心目标是培养批判性思维和原创观点，评估范式必须从质量转向原创性。

Method: 提出基于论证稀有性的原创性评估框架（AROA），将原创性定义为参考语料库中的稀有程度，通过四个互补组件进行评估：结构稀有性、论点稀有性、证据稀有性和认知深度。使用密度估计量化每个组件的稀有性，并通过质量调整机制整合，将质量和原创性视为独立的评估维度。

Result: 实验发现质量与论点稀有性存在强负相关，表明存在质量-原创性权衡：更高质量的文本倾向于依赖典型的论点模式。AI生成论文在结构复杂性上可与人类论文相媲美，但其论点稀有性显著低于人类论文，表明大语言模型可以复制论证形式但在内容原创性上存在局限。

Conclusion: AROA框架为评估学生议论文的原创性提供了有效方法，揭示了质量与原创性之间的权衡关系，并证明大语言模型在结构模仿方面表现出色但在内容原创性方面存在不足，为教育评估范式从质量转向原创性提供了实证支持。

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [288] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: FS-Researcher是一个基于文件系统的双智能体框架，通过持久化工作空间解决LLM在深度研究任务中上下文长度限制问题，实现超越上下文窗口的深度研究扩展。


<details>
  <summary>Details</summary>
Motivation: 深度研究作为LLM智能体的代表性长视野任务，其长轨迹常常超出模型上下文限制，压缩了证据收集和报告编写的token预算，阻碍了有效的测试时扩展。

Method: 采用基于文件系统的双智能体框架：Context Builder智能体作为图书管理员浏览互联网、编写结构化笔记、将原始资料归档到可超越上下文长度的分层知识库；Report Writer智能体将知识库作为事实来源，分章节撰写最终报告。

Result: 在两个开放式基准测试（DeepResearch Bench和DeepConsult）上，FS-Researcher在不同骨干模型上均实现了最先进的报告质量。分析显示最终报告质量与分配给Context Builder的计算量呈正相关，验证了文件系统范式下有效的测试时扩展。

Conclusion: FS-Researcher通过文件系统作为持久化外部内存和跨智能体/会话的共享协调媒介，实现了超越上下文窗口的深度研究扩展，支持迭代优化，为解决LLM在长视野任务中的上下文限制提供了有效方案。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [289] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了一种新的句子表示方法Value Aggregation(VA)，通过聚合注意力值向量来更好地捕获句子级语义，超越了传统基于隐藏状态的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型(LLM)的句子表示方法大多依赖最后一层隐藏状态，但这些状态主要优化于下一个词预测任务，往往无法有效捕获全局的句子级语义。

Method: 提出了Value Aggregation(VA)方法，通过聚合多个层和token索引上的注意力值向量来获得句子表示。进一步提出了Aligned Weighted VA(AlignedWVA)，利用最后一个token的注意力分数作为权重，通过输出投影矩阵(W_O)将加权值向量对齐到LLM残差流的公共空间。

Result: 在无需训练的设置下，VA超越了其他基于LLM的嵌入方法，甚至匹配或超越了基于集成的MetaEOL。AlignedWVA在无需训练的LLM嵌入方法中达到了最先进的性能，显著超越了高成本的MetaEOL。

Conclusion: 注意力值向量比隐藏状态能更有效地捕获句子语义，Value Aggregation方法为获得强大的LLM嵌入模型提供了新途径，通过微调VA有潜力获得更优的嵌入模型。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [290] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: 提出Certified Semantic Smoothing (CSS)框架，通过分层随机消融和噪声增强对齐调优，为LLMs提供可证明的鲁棒性保证，显著降低梯度攻击成功率同时保持高良性效用。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型对自适应越狱攻击仍然脆弱，经验性防御方法如GCG容易被绕过，需要提供可证明的安全保证。

Method: 1) 提出Certified Semantic Smoothing (CSS)框架，通过分层随机消融将输入分为不可变结构提示和可变载荷，利用超几何分布推导严格的l0范数保证；2) 使用Noise-Augmented Alignment Tuning (NAAT)将基础模型转化为语义去噪器，解决稀疏上下文下的性能退化问题。

Result: 在Llama-3上的实验显示，该方法将梯度攻击的成功率从84.2%降至1.2%，同时保持94.1%的良性效用，显著优于将效用降至74.3%的字符级基线方法。

Conclusion: 该框架提供了确定性的安全证书，确保模型在可证明半径内对所有对抗性变体保持鲁棒，为LLM安全提供了理论保证和实践解决方案。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [291] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: A2Eval是一个自动化的智能体评估框架，通过两个协作智能体自动生成平衡、紧凑的评估套件和执行评估流程，显著降低评估成本和偏差。


<details>
  <summary>Details</summary>
Motivation: 当前具身VLM评估依赖静态、专家定义、人工标注的基准测试，存在严重冗余和覆盖不平衡问题。这种劳动密集型范式消耗大量计算和标注资源，增加成本，扭曲模型排名，阻碍迭代开发。

Method: 提出Agentic Automatic Evaluation (A2Eval)框架，包含两个协作智能体：Data Agent自主归纳能力维度并组装平衡、紧凑的评估套件；Eval Agent合成和验证可执行的评估流程，实现完全自主的高保真评估。

Result: 在10个基准测试和13个模型上评估，A2Eval将评估套件压缩85%，总体计算成本降低77%，速度提升4.6倍，同时保持评估质量。纠正系统性排名偏差，将人类对齐度提升至Spearman's rho=0.85，保持高排名保真度(Kendall's tau=0.81)。

Conclusion: A2Eval为高保真、低成本的具身评估建立了新标准，解决了传统评估方法的冗余、成本高和偏差问题，实现了完全自动化的评估流程。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [292] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 提出Steering Vector Fields (SVF)方法，通过上下文相关的梯度场实现更可靠的语言模型推理时控制


<details>
  <summary>Details</summary>
Motivation: 现有steering vectors方法在实践中不可靠，存在概念不可控、效果不稳定、长文本和多属性控制效果差等问题，需要更可靠的推理时控制方法

Method: 提出Steering Vector Fields (SVF)，学习可微分的概念评分函数，使用其局部梯度作为每个激活点的控制方向，实现上下文相关的干预，支持协调的多层干预和统一的长文本、多属性控制框架

Result: 在多个LLM和steering任务中，SVF提供了更强、更可靠的控制，提升了推理时steering的实用性

Conclusion: 通过几何视角分析steering vectors的失败原因，提出的SVF方法通过上下文相关的梯度场实现了更可靠的语言模型推理时控制，为轻量级模型控制提供了更实用的解决方案

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [293] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: CoDiQ框架通过测试时缩放实现细粒度难度控制，生成竞赛级难题并保持可解性，构建了44K竞赛级问题语料库，显著提升大型推理模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成方法缺乏精确难度控制、计算成本高，且难以大规模生成竞赛级问题，需要一种能够精细控制难度同时保证问题可解性的新框架。

Method: 提出CoDiQ框架：1) 发现测试时缩放趋势（扩展推理token预算增加难度但降低可解性）和定义模型生成有效高难度问题能力上限的内在属性；2) 基于Qwen3-8B开发CoDiQ-Generator，提高生成高难度问题的上限；3) 构建包含44K竞赛级问题序列的CoDiQ-Corpus。

Result: 人工评估显示CoDiQ生成的问题比LiveCodeBench/AIME显著更具挑战性，同时保持超过82%的可解性。在CoDiQ-Corpus上训练的大型推理模型推理性能大幅提升，验证了扩展受控难度训练问题能增强推理能力。

Conclusion: CoDiQ框架通过精细难度控制和保证可解性，成功生成了高质量的竞赛级问题语料库，有效提升了大型推理模型的推理能力，为相关研究提供了开源资源支持。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [294] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: 论文提出"计数假说"，认为大语言模型的编码策略可能是上下文学习的基础机制


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然为LLMs提供了新的应用方式，但其底层机制仍不清楚，使得错误修正和诊断变得困难，因此需要更好地理解ICL的局限性以及LLMs如何支持ICL

Method: 受ICL特性和LLMs功能模块的启发，提出了"计数假说"，认为LLMs的编码策略可能是ICL的基础，并提供了支持证据

Result: 提出了"计数假说"作为解释ICL机制的理论框架，并提供了支持该假说的证据

Conclusion: 为了更好地理解和改进上下文学习，需要深入研究LLMs的编码策略如何支持ICL，计数假说为这一研究方向提供了理论基础

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [295] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 现代推理后训练导致探索崩溃：温度采样不再提升pass@n准确率，提出潜在探索解码(LED)方法，通过聚合中间层后验选择高熵配置，无需额外训练即可提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习后训练在数学和代码推理方面表现出色，但研究发现这种后训练会导致意外的探索崩溃现象——基于温度的采样不再提高pass@n准确率。后训练模型的最终层后验熵急剧减少，而中间层熵相对较高，这种熵不对称性为改进解码策略提供了机会。

Method: 提出潜在探索解码(LED)方法：这是一种深度条件解码策略。通过累积和聚合中间层后验，选择具有最大熵的深度配置作为探索候选。该方法不需要额外训练或参数，直接应用于现有模型。

Result: 在多个推理基准测试和模型上，LED一致性地提高了pass@1和pass@16准确率，分别提升了0.61和1.03个百分点。该方法有效解决了后训练导致的探索崩溃问题。

Conclusion: 现代推理后训练确实会导致探索崩溃，但通过利用中间层的高熵特性，LED方法能够在不增加训练成本的情况下有效提升推理性能，为大型推理模型的解码策略提供了新思路。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [296] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 论文提出Game of Thought (GoT)框架，使用博弈论方法提升LLMs在信息缺失情况下的主动信息搜索能力，通过二十问游戏的对抗变体进行评估。


<details>
  <summary>Details</summary>
Motivation: LLMs在现实场景中常面临信息不足的问题，现有方法在简化假设下会降低最坏情况性能，这在高风险应用中存在严重隐患。

Method: 引入战略语言搜索(SLS)问题作为二人零和扩展形式博弈，提出Game of Thought (GoT)框架，应用博弈论技术近似纳什均衡策略。

Result: 实验结果表明，相比直接提示方法和启发式搜索方法，GoT在所有测试设置中都能持续提升最坏情况性能。

Conclusion: 通过博弈论方法可以显著提升LLMs在信息搜索任务中的最坏情况性能，为高风险应用提供了更可靠的解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [297] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: MedAraBench是一个大规模阿拉伯语医学多选题数据集，涵盖19个医学专业和5个难度级别，旨在解决阿拉伯语在NLP特别是医学应用中资源不足的问题。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语在自然语言处理研究中代表性不足，特别是在医学应用领域，缺乏开源数据和基准测试资源，这限制了大型语言模型多语言能力的评估和发展。

Method: 通过手动数字化阿拉伯语地区医学专业人员创建的大量学术材料构建数据集，进行广泛预处理并分为训练集和测试集。采用专家人工评估和LLM-as-a-judge两种框架评估数据质量。

Result: 数据集质量高且多样化，涵盖19个医学专业和5个难度级别。评估了8个最先进的开源和专有模型（如GPT-5、Gemini 2.0 Flash、Claude 4-Sonnet），结果显示需要进一步的领域特定增强。

Conclusion: 发布数据集和评估脚本，以扩大医学数据基准的多样性，扩展LLM评估套件的范围，并增强模型在临床环境中的多语言部署能力。

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [298] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

TL;DR: 研究探索了使用内部模型信号诊断LLM激活导向的可靠性，提出了信息论指标NBF和KL散度来预测导向成功与否，并建立了更强的评估基线。


<details>
  <summary>Details</summary>
Motivation: 尽管激活导向技术被广泛使用，但对其何时成功或失败的内在机制因素了解不足，先前研究主要依赖黑盒输出或基于LLM的评判，需要更深入的理解。

Method: 使用两个信息论指标：基于熵的归一化分支因子(NBF)和词汇空间中导向激活与目标概念之间的KL散度；利用LLM生成的标注作为真实标签，分析这些机制信号对导向成功的预测能力；为CAA和稀疏自编码器导向方法建立更强的评估基线。

Result: 机制信号为识别成功导向和估计失败概率提供了有意义的预测能力；建立了高可靠性的研究，显示两个架构不同的LLM之间具有高度一致的评判者间一致性。

Conclusion: 有效的激活导向对应于解码步骤中结构化的熵保持和一致的KL对齐，内部模型信号可以可靠地诊断导向的可靠性，为激活导向方法提供了更强的评估框架。

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [299] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: BBPE16是一种基于UTF-16的字节级BPE分词器，相比广泛使用的UTF-8 BBPE，它在处理非拉丁文字（特别是中日韩文字）时能显著减少token数量，提升计算效率，同时保持语言无关性和跨语言token共享能力。


<details>
  <summary>Details</summary>
Motivation: 现有的UTF-8字节级BPE（BBPE）分词器虽然具有语言无关性和完整的Unicode覆盖，但在处理中日韩等非拉丁文字时，由于UTF-8的变长编码特性，会导致token序列过长，增加计算负载和内存使用。

Method: 提出BBPE16分词器，基于UTF-16编码，使用统一的2字节代码单元表示大多数现代文字系统。该方法保留了BBPE的语言无关特性，同时显著提升了跨语言token共享能力。

Result: 在单语、双语、三语ASR以及多语言持续学习设置中，BBPE16获得了相当或更好的准确率。对于中文，token数量最多减少10.4%，解码迭代最多降低10.3%，加快了微调和推理速度，减少了内存使用。

Conclusion: BBPE16是一种实用的多语言ASR分词选择方案，在保持语言无关性的同时，通过减少token序列长度显著提升了计算效率，特别适合处理中日韩等非拉丁文字。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [300] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: COMI是一个从粗到细的自适应上下文压缩框架，通过联合优化语义相关性和多样性，在高压缩率下有效减少LLM长上下文场景的计算负担和信息冗余。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景中的部署受到计算效率低下和信息冗余的阻碍，需要有效的上下文压缩方法来减少输入长度和消除冗余。

Method: 提出COMI框架，包含两个阶段：1) 粗粒度组重分配：基于组间边际信息增益动态分配压缩率；2) 细粒度令牌合并：通过组内MIG加权机制融合令牌。引入边际信息增益(MIG)作为指导压缩的度量标准。

Result: 在问答和摘要任务上的广泛实验表明，COMI大幅优于现有基线，例如在32倍压缩约束下，Qwen2-7B在NaturalQuestions上实现了约25分的精确匹配提升。

Conclusion: COMI通过从粗到细的自适应压缩框架，有效解决了LLM长上下文场景中的计算效率和信息冗余问题，显著提升了压缩性能。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [301] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: 本文提出了一种增强自动作文评分系统性能的新方法，通过两阶段微调、分数对齐和不确定性感知自训练三种关键技术，在有限数据和完整数据场景下都取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 在现实教育场景中，自动作文评分系统面临标注数据极度稀缺的挑战，这严重限制了鲁棒AES系统的开发和实际应用。现有方法在有限数据条件下性能受限，需要更有效的技术来提升模型性能。

Method: 提出了三种关键技术：1）两阶段微调策略，利用低秩适应技术使AES模型更好地适应目标提示作文；2）分数对齐技术，提高预测分数与真实分数分布之间的一致性；3）不确定性感知自训练，利用未标注数据扩展训练集，同时减轻标签噪声传播。这些技术在DualBERT模型上实现。

Result: 在ASAP++数据集上的实验表明：在32数据设置下，所有三种技术都提升了性能，它们的集成达到了完整数据性能的91.2%（仅使用约1000个标注样本）。分数对齐技术在有限数据和完整数据设置下都持续提升性能，当集成到DualBERT中时，在完整数据设置下达到了最先进的结果。

Conclusion: 提出的三种关键技术有效解决了自动作文评分系统在数据稀缺场景下的性能问题，特别是在有限数据条件下显著提升了模型性能，为实际教育应用提供了更可行的解决方案。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [302] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

TL;DR: WorldCup是一个用于大语言模型的多比特水印框架，通过分层竞争机制将信息比特直接嵌入到token选择中，实现了容量、可检测性、鲁棒性、文本质量和解码效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成越来越类似人类的文本，水印技术为可靠的归属认证提供了有前景的解决方案。现有的多比特水印方法主要通过种子驱动的方式扩展零比特方案，导致间接信息流、有限的有效容量和次优的解码性能。

Method: WorldCup将采样视为自然通信信道，通过分层竞争机制在token选择中直接嵌入消息比特，使用互补信号指导。采用熵感知调制来保持生成质量，并通过置信度感知解码支持鲁棒的消息恢复。

Result: 综合实验表明，WorldCup在容量、可检测性、鲁棒性、文本质量和解码效率方面实现了良好的平衡，一致优于现有基线方法。

Conclusion: WorldCup为未来大语言模型水印研究奠定了坚实基础，通过直接嵌入信息比特的分层竞争机制，解决了现有方法的局限性。

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [303] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: Zero2Text是一种无需训练、基于递归在线对齐的框架，用于对抗向量数据库中的嵌入反转攻击，在严格黑盒和跨域设置下有效恢复文本，无需泄露数据对。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成的普及使向量数据库成为关键基础设施，但嵌入反转攻击带来严重隐私风险。现有方法面临基本权衡：基于优化的方法需要计算量巨大的查询，而基于对齐的方法依赖于可访问域内训练数据的不现实假设，在严格黑盒和跨域设置中无效。

Method: 提出Zero2Text训练免费框架，基于递归在线对齐。不同于依赖静态数据集的方法，Zero2Text将LLM先验与动态岭回归机制协同，迭代地将生成与目标嵌入实时对齐，无需任何训练数据。

Result: 在MS MARCO基准测试中针对OpenAI受害者模型，Zero2Text实现了比基线方法高1.8倍的ROUGE-L分数和6.4倍的BLEU-2分数，从未知领域恢复句子而无需任何泄露数据对。实验表明标准防御（如差分隐私）无法有效缓解这种自适应威胁。

Conclusion: Zero2Text通过递归在线对齐方法成功突破了现有嵌入反转攻击方法的限制，在严格黑盒和跨域设置下有效工作，揭示了向量数据库隐私保护的新挑战和现有防御的不足。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [304] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: 提出SOG方法，使用特殊token <SOG_k>在统一token空间中表示图结构，解决LLM处理图数据时的结构幻觉问题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理非结构化数据方面表现出色，但在处理图数据时面临结构幻觉的挑战。现有方法要么将图转化为自然语言导致token消耗过大和注意力分散，要么转化为连续嵌入但与原始文本token严重不对齐

Method: 提出拓扑感知的结构tokenizer，将每个图拓扑映射为高度选择性的单个token <SOG_k>；构建混合结构的问答语料库，将新的结构token与现有文本token对齐；在统一token空间中显式输入拓扑结构并共享结构信息

Result: 在五个图级基准测试中，相比基线方法实现了9.9%到41.4%的性能提升，同时表现出可解释性和一致性；方法可灵活扩展到节点级任务，支持全局和局部结构理解

Conclusion: SOG方法通过特殊token <SOG_k>在统一token空间中表示图结构，使LLM能够以简洁准确的方式理解、生成和推理图数据，有效解决了结构幻觉问题

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [305] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 该研究首次从数据中心的视角系统探讨了数据分布对上下文压缩质量的影响，发现输入熵与压缩质量负相关，编码器与解码器内在数据差距显著降低压缩增益。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景中面临计算效率低下和信息冗余问题，现有研究主要关注模型侧改进，而数据分布本身对上下文压缩的影响尚未得到充分探索。

Method: 采用基于自编码器的框架评估压缩表示的语义完整性，从输入数据和内在数据（模型内部预训练知识）两个维度系统研究数据分布对压缩质量的影响。

Result: 实验结果显示：1）编码器测量的输入熵与压缩质量负相关，而在冻结解码器设置下，解码器测量的熵无显著关系；2）编码器与解码器内在数据差距显著降低压缩增益且难以缓解。

Conclusion: 基于研究发现提出了优化压缩增益的实用指南，强调了从数据中心视角理解上下文压缩的重要性，为改进长上下文LLM部署提供了新方向。

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [306] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: 多模态大语言模型通过将源代码渲染为图像进行压缩，可实现高达8倍的token缩减，同时保持代码理解能力，为大规模软件系统的高效推理提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模扩大，传统基于文本的LLM在源代码理解上面临计算效率瓶颈，因为文本token数量线性增长导致计算成本增加。多模态LLM的出现为通过图像模态压缩代码表示提供了机会。

Method: 将源代码渲染为图像，利用图像模态固有的可压缩性，通过调整分辨率将图像缩放到原始token成本的一小部分，同时保持对视觉模型的识别性。这是首次系统研究MLLM在代码理解中的有效性。

Result: 1) MLLM能有效理解压缩后的代码，实现高达8倍的压缩；2) MLLM能有效利用语法高亮等视觉线索，在4倍压缩下仍能提高代码补全性能；3) 克隆检测等代码理解任务对视觉压缩表现出异常强的韧性，某些压缩比甚至略微优于原始文本输入。

Conclusion: 研究揭示了MLLM在代码理解中的潜力和当前局限性，指出向图像模态代码表示转变是通向更高效推理的途径。图像压缩为大规模代码处理提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [307] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: 本文提出句子曲线语言模型（SCLM），通过将静态词嵌入替换为连续句子曲线表示，解决了传统语言模型中目标词嵌入对上下文不敏感的问题，从而更好地建模全局句子结构。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型（包括扩散语言模型）使用静态词嵌入来表示目标句子，这种表示对邻近词不敏感，导致模型更关注局部词预测而忽视句子全局结构。需要一种能更好捕捉句子整体结构的表示方法。

Method: 提出句子曲线表示法，将句子表示为样条曲线，其控制点影响句子中的多个词。基于此提出句子曲线语言模型（SCLM），扩展扩散语言模型来预测句子曲线而非静态词嵌入。

Result: SCLM在IWSLT14和WMT14数据集上取得了扩散语言模型中的最先进性能，训练稳定且无需繁琐的知识蒸馏，在LM1B数据集上也显示出相比离散扩散语言模型的潜力。

Conclusion: 句子曲线表示通过正则化效应促进全局结构建模，为语言模型提供了一种更有效的句子表示方法，在多个基准测试中表现出优越性能。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [308] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

TL;DR: AXE是一个创新的网页结构化数据提取系统，通过将HTML DOM视为需要修剪的树而非纯文本，使用专门的修剪机制去除无关节点，让小型0.6B LLM能够生成精确的结构化输出，同时通过GXR确保每个提取都可追溯到源节点。


<details>
  <summary>Details</summary>
Motivation: 解决网页结构化数据提取中的两难困境：手工启发式方法脆弱易失效，而大型语言模型成本过高。需要一种既准确又经济实用的解决方案。

Method: AXE采用自适应XPath提取器管道，将HTML DOM视为需要修剪的树，使用专门的"修剪"机制去除样板文本和无关节点，保留高密度上下文。通过Grounded XPath Resolution确保每个提取都可物理追溯到源节点，使用仅0.6B参数的小型LLM生成结构化输出。

Result: 在SWDE数据集上实现了88.1%的F1分数，达到了零样本状态的最先进性能，超越了多个更大规模的完全训练替代方案。系统具有低资源占用特点。

Conclusion: AXE提供了一种实用且经济高效的大规模网页信息提取路径，通过专门的适配器实现了小型模型的高精度提取，平衡了准确性和成本效益。

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [309] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: RAM框架通过自适应混合阅读策略压缩长文本上下文，结合精读重要内容和略读次要内容，在保持性能的同时实现12倍加速


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文场景部署面临计算效率低下和信息冗余两大挑战，需要一种既能保持性能又能提高效率的上下文压缩方法

Method: 提出RAM框架，采用自适应混合阅读策略：将上下文分段并行编码，高相关段完整保留（精读），低相关段通过查询引导压缩为摘要向量（略读），并使用对比学习优化精读与略读的决策边界

Result: 在多个问答和摘要基准测试中优于现有基线，在平均长度16K、最大长度32K的长输入上实现最高12倍的端到端加速

Conclusion: RAM框架通过模拟人类阅读行为，有效解决了大语言模型在长上下文场景中的计算效率和信息冗余问题，实现了性能与效率的平衡

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [310] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 提出PretrainRL框架，通过在预训练阶段集成强化学习来巩固事实知识，解决LLMs的事实幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在事实幻觉问题，生成可验证的虚假信息。根本原因是预训练语料库中数据分布不平衡，导致"低概率真相"和"高概率虚假"状态。现有方法要么回避问题，要么面临灾难性遗忘

Method: 提出PretrainRL框架，采用"去偏然后学习"的核心原则。通过高效的负采样策略发现高概率虚假信息，在预训练阶段使用强化学习主动重塑模型的概率分布，降低高概率虚假信息的权重，为低概率真相创造学习空间

Result: 在三个公共基准测试上的广泛实验表明，PretrainRL显著减轻了事实幻觉，并优于最先进的方法

Conclusion: PretrainRL通过从根源上解决数据分布不平衡问题，有效缓解了LLMs的事实幻觉，提供了一种新颖的预训练阶段知识巩固方法

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [311] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

TL;DR: 论文介绍了"Vibe Coding"教学法，在NLP课程中使用LLM作为编程助手，通过强制提示记录和反思评估，将学习重点从语法熟练度转向概念掌握。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展给NLP教育带来了挑战和机遇，需要探索如何有效利用LLM作为编程助手，同时保持对概念理解和批判性思维的关注。

Method: 提出"Vibe Coding"教学法，在本科高年级NLP课程中实施，学生使用LLM完成7个实验，通过强制提示记录和基于反思问题的评估，主要考核概念理解而非代码实现。

Result: 19名学生课程反馈显示高满意度（平均分4.4-4.6/5.0），学生特别赞赏减少了调试的认知负担，能更专注于NLP概念。但也面临时间限制、LLM输出验证和任务规范清晰度等挑战。

Conclusion: 当采用强制提示记录和反思评估的结构化方法时，LLM辅助学习可以将重点从语法熟练度转向概念掌握，为学生适应AI增强的专业环境做好准备。

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [312] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: CatRAG提出了一种上下文感知的RAG框架，通过动态调整知识图谱的随机游走来解决静态图谱方法中的"语义漂移"问题，显著提升了多跳推理的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的RAG方法（如HippoRAG）存在"静态图谱谬误"：它们依赖索引时确定的固定转移概率，忽略了边相关性的查询依赖性，导致随机游走被高连接度的"枢纽"节点分散，无法完整检索多跳查询所需的证据链。

Method: CatRAG在HippoRAG 2架构基础上，将静态知识图谱转换为查询自适应的导航结构，包含三个核心组件：1）符号锚定：注入弱实体约束来正则化随机游走；2）查询感知的动态边权重：动态调整图结构，剪枝不相关路径并增强与查询意图对齐的路径；3）关键事实段落权重增强：通过成本高效的偏置结构性地锚定随机游走到可能的证据。

Result: 在四个多跳基准测试中，CatRAG持续优于现有最先进基线。虽然标准召回率指标显示适度提升，但CatRAG在推理完整性（即无间隙地恢复整个证据路径的能力）方面实现了显著改进。

Conclusion: CatRAG通过上下文感知的图遍历有效弥合了检索部分上下文与实现完全基于证据的推理之间的差距，为解决多跳RAG中的语义漂移问题提供了有效方案。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [313] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Moe-Ctc：一种混合专家架构，通过中间CTC监督联合促进专家专业化和泛化，显著提升带口音语音识别性能


<details>
  <summary>Details</summary>
Motivation: 当前语音识别模型主要基于少数高资源英语变体训练，导致对其他口音识别性能显著下降。口音无关方法对重口音或未见口音效果有限，而口音特定方法依赖有限且通常嘈杂的标签数据。

Method: 提出Moe-Ctc架构，采用混合专家模型结合中间CTC监督。训练时使用口音感知路由鼓励专家捕获口音特定模式，推理时过渡到无标签路由。每个专家配备自己的CTC头以对齐路由和转录质量，路由增强损失进一步稳定优化。

Result: 在Mcv-Accent基准测试中，在低资源和高资源条件下，对已见和未见口音均取得一致性能提升，相比FastConformer基线实现高达29.3%的相对WER降低。

Conclusion: Moe-Ctc通过联合促进专家专业化和泛化，有效解决了带口音语音识别挑战，在多种口音条件下均表现出显著性能改进。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [314] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: 提出正交层次分解（OHD）框架，通过正交树归纳方法将复杂表格分解为列树和行树，分别捕捉垂直和水平层次依赖关系，提升LLMs对复杂表格的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（表格线性化或规范化网格建模）难以明确捕捉复杂表格（含多级表头、合并单元格、异构布局）的层次结构和跨维度依赖关系，导致结构语义与文本表示不匹配。

Method: 提出正交层次分解（OHD）框架：1）基于空间-语义共约束的正交树归纳（OTI）方法，将不规则表格分解为列树和行树；2）设计双路径关联协议对称重构每个单元格的语义谱系；3）引入LLM作为语义仲裁器对齐多级语义信息。

Result: 在AITQA和HiTab两个复杂表格问答基准测试中，OHD框架在多个评估指标上持续优于现有表示范式。

Conclusion: OHD框架通过正交层次分解有效捕捉复杂表格的结构特征，解决了现有表示方法在层次结构和跨维度依赖关系建模上的不足，显著提升了LLMs对复杂表格的理解和推理能力。

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [315] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 该论文提出了EVK（嵌入虚拟化知识）方法，通过嵌入空间的受控扰动来表征模型知识，构建了EVK-Bench评估基准来量化编辑引起的知识漂移，并开发了EVK-Align模块来约束嵌入级知识漂移。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识编辑方法的评估通常局限于预定义基准，只能评估编辑事实及其有限的相关知识，无法充分理解编辑对模型知识系统的广泛影响。需要更全面的评估方法来探索编辑对模型知识系统的整体影响。

Method: 1. 提出EVK方法：通过嵌入空间的受控扰动来表征模型知识，探索超出显式数据标注的更广泛虚拟化知识区域
2. 构建EVK-Bench评估基准：在嵌入层面量化编辑引起的潜在知识漂移
3. 开发EVK-Align模块：约束编辑过程中的嵌入级知识漂移，可无缝集成到现有编辑方法中

Result: 实验表明，该方法能够实现更全面的评估，同时在保持编辑准确性的前提下显著改善知识保留效果，揭示了传统基于样本的指标无法捕捉的影响。

Conclusion: EVK方法通过嵌入空间分析提供了更全面的知识编辑评估框架，EVK-Bench基准能够量化传统方法无法检测的知识漂移，而EVK-Align模块能够有效约束知识漂移，为知识编辑方法提供了更可靠的评估和改进工具。

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [316] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 论文提出了一种基于激活引导的自采样框架，用于高效学习链式思维推理，通过诱导目标LLM生成风格对齐、可变长度的推理轨迹，无需教师指导，解决了监督微调方法中高质量监督数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的链式思维推理能力提升往往伴随着冗余的推理过程，需要探索LLM是否能像人类系统1推理那样获得快速思维模式，同时解决监督微调方法中高质量监督数据稀缺的核心瓶颈。

Method: 提出基于激活引导的自采样框架，诱导目标LLM生成风格对齐、可变长度的推理轨迹；使用黄金答案过滤数据，通过(i)类人双认知系统和(ii)渐进压缩课程进行监督微调；探索仅使用预测一致数据的自进化机制，无需黄金答案。

Result: 在数学基准测试和医学领域的跨领域泛化测试中，该方法对通用LLM和R1风格LLM都带来了稳定的性能提升。

Conclusion: 该方法能够有效诱导LLM生成高效的链式思维推理，解决了监督数据稀缺问题，为LLM获得类似人类系统1的快速推理模式提供了可行路径。

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [317] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 该研究揭示了R1风格大语言模型自我反思行为的内在机制，通过追踪反思行为的激活轨迹，发现了从潜在监控到话语调节再到显性反思的三阶段结构化进程。


<details>
  <summary>Details</summary>
Motivation: R1风格的大语言模型因其自我反思能力受到关注，但其内部机制尚不清楚。研究者希望通过追踪反思行为的激活轨迹来理解这种自我反思行为的内在工作原理。

Method: 使用logit lens技术读取token级别的语义，追踪反思行为的层间激活轨迹。通过有针对性的干预实验，揭示不同阶段之间的因果链关系。

Result: 发现了反思行为的结构化进程：1）潜在控制层：近似线性方向编码思考预算语义；2）语义枢纽层：话语级线索（转折点和总结线索）出现并主导概率质量；3）行为显性层：反思行为token的似然度开始上升直至被采样。干预实验揭示了这些阶段之间的因果链关系。

Conclusion: 研究结果表明R1风格LLM的自我反思过程类似于人类的元认知过程，从潜在监控到话语级调节，再到显性的自我反思。这为理解大语言模型的内部工作机制提供了重要洞见。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [318] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: xMemory提出了一种新的智能体记忆检索方法，通过解耦聚合的方式构建层次化记忆结构，相比传统RAG在对话流中能提供更紧凑、多样化的上下文，提升回答质量和token效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法针对大规模异构语料库设计，但智能体记忆是有限、连贯的对话流，其中记忆片段高度相关且常有重复。固定top-k相似性检索会返回冗余上下文，而后处理剪枝可能删除时间关联的前提条件，影响正确推理。

Method: xMemory采用解耦到聚合的方法：将记忆分解为语义组件，组织成层次结构，并使用此结构驱动检索。通过稀疏性-语义目标指导记忆的分割与合并，构建完整单元的层次结构，并维护可搜索且忠实的高层节点组织。推理时采用自上而下的检索策略，为多事实查询选择紧凑、多样化的主题和语义，仅在减少读者不确定性时扩展到情节和原始消息。

Result: 在LoCoMo和PerLTQA数据集上，使用三种最新LLM进行的实验显示，xMemory在回答质量和token效率方面均取得一致性的提升。

Conclusion: 检索应超越相似性匹配，在潜在组件上操作。xMemory通过层次化记忆结构和智能检索策略，有效解决了智能体记忆检索中的冗余和关联性问题，为对话流记忆管理提供了更优方案。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [319] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

TL;DR: NEAT提出了一种基于神经元激活动态的早期推理退出框架，无需额外训练或测试时计算，通过监测神经元激活模式来减少推理过程中的冗余步骤，平均减少22-28%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在"过度思考"问题，即在已经得到正确答案后仍生成冗余推理步骤。现有早期退出方法依赖输出级启发式或训练探测模型，需要额外计算或标注数据集。

Method: NEAT通过识别与退出相关的神经元，跟踪推理过程中的神经元激活模式，动态触发早期退出或抑制反思，无需训练或额外测试时计算。

Result: 在四个推理基准测试和六个不同规模和架构的模型上，NEAT平均减少22%到28%的token使用量，同时保持推理准确性。

Conclusion: NEAT提供了一种高效、无需训练的早期推理退出方法，有效缓解大型推理模型的过度思考问题，显著减少计算开销而不影响推理质量。

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [320] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

TL;DR: WildGraphBench是一个新的GraphRAG基准测试，使用维基百科的长文档和异构参考文献来评估真实场景下的图检索增强生成性能


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG基准测试大多使用短而精选的文本段落作为外部知识，无法充分评估在长上下文和大规模异构文档的真实场景下的系统性能

Method: 利用维基百科的结构特点，以外部参考文献作为检索语料库，引用链接的陈述作为真实答案，构建包含12个顶级主题、1100个问题的基准测试，涵盖三种复杂度级别

Result: 实验表明当前GraphRAG管道在证据来自中等数量来源时有助于多事实聚合，但这种聚合范式可能过度强调高层陈述而牺牲细粒度细节，导致在摘要任务上表现较弱

Conclusion: WildGraphBench填补了GraphRAG在真实场景评估方面的空白，揭示了当前聚合方法的局限性，为未来改进提供了重要基准

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [321] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: LEC-KG是一个双向协作框架，结合大语言模型的语义理解与知识图谱嵌入的结构推理，用于从非结构化文本构建领域特定知识图谱，在中文可持续发展目标报告上表现优异。


<details>
  <summary>Details</summary>
Motivation: 从非结构化文本构建领域特定知识图谱面临三大挑战：异构实体提及、长尾关系分布以及缺乏标准化模式。现有方法难以有效处理这些问题。

Method: 提出LEC-KG框架，包含三个关键组件：1）分层粗到细关系提取缓解长尾偏差；2）证据引导的思维链反馈将结构建议基于源文本；3）语义初始化支持对未见实体的结构验证。LLM和KGE模块通过迭代协作相互增强。

Result: 在中文可持续发展目标报告上评估，相比LLM基线有显著提升，特别是在低频关系上表现优异。通过迭代精炼，能够可靠地将非结构化政策文本转化为已验证的知识图谱三元组。

Conclusion: LEC-KG框架通过LLM的语义理解与KGE的结构推理双向协作，有效解决了领域特定知识图谱构建中的关键挑战，为从非结构化文本构建高质量知识图谱提供了可靠方法。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [322] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

TL;DR: Dicta-LM 3.0是一个为希伯来语等低资源语言开发的开源大语言模型系列，包含24B、12B和1.7B三种规模，支持65k上下文长度，并提供了基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 当前开源大语言模型主要针对英语，而希伯来语等低资源语言的高质量LLM供应不足但需求很高。训练这类语言的LLM面临独特挑战，需要专门解决方案。

Method: 基于Mistral-Small-3.1、NVIDIA Nemotron Nano V2和Qwen3-1.7B等基础模型进行适配，使用大量希伯来语和英语文本语料库训练。开发了包含翻译、摘要、Winograd、以色列常识和希伯来语注音等多种任务的基准测试套件。

Result: 发布了Dicta-LM 3.0模型系列，包含24B、12B和1.7B三种规模，每个模型都有基础版和带工具调用支持的聊天版。同时推出了专门用于评估希伯来语聊天LLM的基准测试套件。

Conclusion: 该工作不仅解决了低资源语言训练LLM的复杂性，还提出了一个可用于将其他LLM适配到各种非英语语言的框架，为多语言NLP领域做出了贡献。

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [323] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

TL;DR: OOMB是一个高效的内存优化训练系统，通过分块循环训练和即时激活重计算实现O(1)激活内存占用，结合分页内存管理、异步CPU卸载和页面级稀疏注意力等技术，显著降低长上下文LLM训练的GPU内存需求。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型在长上下文场景下面临GPU内存开销过大的问题，主要瓶颈是激活值的内存占用随序列长度线性增长，这严重限制了模型处理长上下文的能力。

Method: 采用分块循环训练框架配合即时激活重计算，保持恒定的激活内存占用；引入分页内存管理器消除KV缓存及其梯度的碎片化；使用异步CPU卸载隐藏数据传输延迟；实施页面级稀疏注意力降低计算复杂度和通信开销。

Result: Qwen2.5-7B模型每增加10K上下文token，端到端训练内存开销仅增加10MB；可在单个H200 GPU上训练4M-token上下文，而传统方法需要大规模集群使用上下文并行。

Conclusion: OOMB系统通过协同优化技术显著提升了长上下文LLM训练的资源效率，为处理超长序列提供了实用的单GPU解决方案，代表了该领域的重要进展。

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [324] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 研究发现LLM的拒绝行为由多个几何上不同的激活空间方向介导，而非单一方向，但线性调节这些方向会产生相似的拒绝-过度拒绝权衡


<details>
  <summary>Details</summary>
Motivation: 先前研究认为大语言模型的拒绝行为由单一激活空间方向介导，本文旨在验证这一假设的完整性，探索不同拒绝类别背后的神经机制

Method: 研究了11个拒绝和非合规类别（包括安全性、不完整请求、拟人化、过度拒绝等），分析这些拒绝行为在激活空间中的几何分布，并进行线性方向调节实验

Result: 发现不同拒绝行为对应几何上不同的激活空间方向，但线性调节任何拒绝相关方向都会产生几乎相同的拒绝-过度拒绝权衡，主要区别在于拒绝方式而非是否拒绝

Conclusion: LLM的拒绝机制比单一方向假设更复杂，存在多个几何不同的方向，但这些方向共享一维控制功能，主要影响拒绝的表达方式而非拒绝决策本身

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [325] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

TL;DR: GapEval是一个双向基准测试，用于量化统一多模态模型中理解与生成能力之间的差距，发现当前模型仅实现表面统一而非深度认知融合。


<details>
  <summary>Details</summary>
Motivation: 尽管统一多模态模型在理解和生成任务上取得了显著进展，但这两项能力是否真正在单一模型中实现对齐和整合仍不明确。需要量化评估这两种"统一"方向之间的认知一致性。

Method: 引入GapEval双向基准测试，每个问题都可以在图像和文本两种模态中回答，实现对模型双向推理能力和跨模态一致性的对称评估。同时从知识操作角度进行实证研究。

Result: 实验发现，在不同架构的统一多模态模型中，理解和生成两个方向之间存在持续差距，表明当前模型仅实现表面统一而非深度认知融合。模型内部知识往往保持分离，跨模态的能力涌现和知识发展不同步。

Conclusion: 当前统一多模态模型尚未实现理解和生成能力的深度整合，存在认知一致性差距。这为未来探索提供了方向，需要进一步研究如何实现真正的跨模态认知融合。

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [326] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: Focus-dLLM：针对扩散大语言模型的无训练注意力稀疏化框架，通过过去置信度引导的指示器和汇点感知剪枝策略，在32K上下文长度下实现超过29倍的无损加速


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在长上下文处理方面表现出色，但双向全注意力的计算成本限制了推理效率。现有稀疏注意力方法效果不佳，因为需要在解码前估计未解码token的注意力重要性，而扩散过程中未掩码token位置未知

Method: 1. 基于token置信度在相邻步骤间强相关的发现，设计过去置信度引导的指示器来预测未掩码区域；2. 提出汇点感知剪枝策略，准确估计并移除冗余注意力计算，同时保留高影响力的注意力汇点；3. 利用跨层一致性，在不同层间重用已识别的汇点位置以减少开销

Result: 实验结果表明，该方法在32K上下文长度下提供超过29倍的无损加速，代码已公开

Conclusion: Focus-dLLM是一个有效的训练免费注意力稀疏化框架，能够显著提升扩散大语言模型的长上下文推理效率，同时保持准确性

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [327] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: D-CORE框架通过任务分解和推理过程组合，解决大型推理模型在复杂工具使用场景中的懒惰推理问题，显著提升工具使用能力


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂工具使用场景中缺乏子任务分解能力，导致懒惰推理问题，需要提升模型的任务分解和推理能力

Method: 提出两阶段训练框架：1) 通过自蒸馏激励模型的任务分解推理能力；2) 使用多样性感知强化学习恢复模型的反思推理能力

Result: D-CORE-8B在BFCLv3上达到77.7%准确率，超越最佳8B模型5.7%；D-CORE-14B达到79.3%的SOTA，优于70B模型但体积小5倍

Conclusion: D-CORE框架能有效提升大型推理模型的工具使用能力，在不同规模模型和基准测试中均表现出色，为复杂问题解决提供了有效方案

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [328] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

TL;DR: AR-MAP是一种新颖的迁移学习框架，利用偏好对齐的自回归大语言模型作为隐式教师，通过简单的权重缩放来对齐扩散大语言模型，避免了直接对齐的高方差和计算开销。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归模型的替代方案，能够实现并行token生成，但其偏好对齐面临挑战，主要原因是基于ELBO的似然估计会引入高方差。

Method: 提出AR-MAP框架，利用偏好对齐的自回归大语言模型作为隐式教师，通过简单的权重缩放将对齐知识迁移到扩散大语言模型，利用了这两种不同生成范式之间的共享架构结构。

Result: 在多样化的偏好对齐任务实验中，AR-MAP相比现有的DLLM特定对齐方法，实现了竞争性或更优的性能，在所有任务和模型上平均得分达到69.08%。

Conclusion: AR-MAP框架有效解决了扩散大语言模型偏好对齐的挑战，通过利用自回归模型的已有对齐知识，避免了直接对齐的高方差和计算开销，为DLLM对齐提供了高效解决方案。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [329] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: LLMs的元语言知识有限，主要受数据可用性影响而非真正的跨语言语法能力。GPT-4o表现最佳但准确率仅0.367，所有模型都未能超过多数类基线，低资源语言表现显著较差。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在语言使用任务上被常规评估，但其对语言结构的理解仍不清楚。现有语言基准通常关注狭窄现象、强调高资源语言，很少评估元语言知识（对语言结构的显式推理而非语言使用）。

Method: 使用准确率和宏观F1分数，结合多数类和机会基线，分析整体性能并检查不同语言领域和语言相关因素的变化。创建开源基准数据集支持系统评估。

Result: 当前LLMs的元语言知识有限：GPT-4o表现最佳但准确率仅0.367，开源模型落后。所有模型表现高于机会但未能超过多数类基线。性能在不同语言领域有差异，词汇特征准确率最高，语音特征最低。语言层面，准确率与数字语言状态强相关：数字存在和资源可用性高的语言评估更准确，低资源语言表现显著较低。

Conclusion: LLMs的元语言知识是碎片化的，主要由数据可用性塑造而非跨世界语言的通用语法能力。需要更多全球语言多样性来改进未来LLMs。

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [330] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 首个僧伽罗语物理常识推理数据集，包含110个人工创建和验证的样本，每个样本包含提示、正确答案和错误答案，主要涉及斯里兰卡语境


<details>
  <summary>Details</summary>
Motivation: 为僧伽罗语（斯里兰卡官方语言）创建物理常识推理数据集，填补该语言在常识推理任务上的数据空白，支持全球多语言AI发展

Method: 人工创建和验证110个数据样本，每个样本包含提示、正确答案和错误答案，问题主要基于斯里兰卡文化和社会语境

Result: 成功创建了首个僧伽罗语物理常识推理数据集，作为Global PIQA项目的一部分，为僧伽罗语NLP研究提供了重要资源

Conclusion: 该数据集填补了僧伽罗语在常识推理任务上的空白，有助于促进僧伽罗语NLP研究和多语言AI系统的公平发展

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [331] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: AgriHubi是一个针对芬兰语农业决策支持的领域自适应检索增强生成系统，通过整合芬兰农业文档和开放模型，结合显式来源标注和用户反馈，在低资源语言环境下显著提升了回答完整性、语言准确性和可靠性感知。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型领域有潜力，但在农业应用中受到限制：缺乏领域基础、英语中心训练数据、有限的实际评估。这些问题在低资源语言环境中更加突出，虽然存在高质量的领域文档，但通用模型难以有效访问这些资源。

Method: 开发AgriHubi系统，整合芬兰农业文档与开放PORO系列模型，采用检索增强生成架构，结合显式来源标注和用户反馈机制支持迭代优化，经过8个迭代版本开发并通过两个用户研究进行评估。

Result: 系统在回答完整性、语言准确性和可靠性感知方面有明显提升，同时揭示了在部署更大模型时响应质量与延迟之间的实际权衡，为低资源语言环境下的领域特定RAG系统设计提供了实证指导。

Conclusion: 该研究为低资源语言环境下设计和评估领域特定RAG系统提供了实证指导，展示了通过领域自适应方法在农业决策支持中有效利用现有高质量文档的可行性。

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [332] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 研究发现基于评分标准的LLM评估存在位置偏差，提出平衡排列策略来缓解偏差并提高与人类评估的相关性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM作为评估者（LLM-as-a-judge）的研究主要集中在点对点和成对评估范式，但基于评分标准的评估（LLM从多个评分标准中选择分数）分析较少。本文发现这种评估存在位置偏差问题。

Method: 通过多个模型和数据集的控制实验证明位置偏差的存在，提出平衡排列策略，将每个分数选项均匀分布在各个位置，通过聚合平衡排列的分数来缓解偏差。

Result: 实验显示基于评分标准的LLM评估存在一致的位置偏差，平衡排列策略不仅揭示了潜在的位置偏差，还显著提高了LLM评估与人类评估的相关性。

Conclusion: 基于评分标准的LLM-as-a-Judge本质上不是点对点评估，简单的基于排列的校准可以显著提高其可靠性，位置偏差是影响评估质量的重要因素。

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [333] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

TL;DR: 提出了一种新的量化方法来衡量历史语言比较中的对应规律性，并开发了基于该方法的计算工具来识别不规则同源词集，在真实数据上达到85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 历史语言比较中虽然强调对应规律性，但通常依赖直觉判断而非量化评估，且不规则现象比新语法学派模型预期的更常见。随着计算方法和标准化词汇数据的发展，需要改进工作流程并提供定量评估。

Method: 提出了平衡平均对应模式重现率作为新的规律性度量标准，并开发了基于该度量的计算方法来识别对应模式不规则的同源词集。通过模拟和真实数据的实验验证，采用留一法验证来评估方法识别导致不规则性的词形的能力。

Result: 在基于真实数据的实验中，该方法整体准确率达到85%。研究还展示了使用大型数据集子样本的优势，以及数据中不规则性增加对结果的影响。

Conclusion: 新的规律性度量和基于此的不规则同源词识别方法在提高现有和未来计算机辅助语言比较数据集质量方面具有重要潜力。

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [334] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: OpenSeal：首个真正开源的东南亚语言大模型，仅用34.7B平行数据和180小时训练时间，性能媲美同类规模模型


<details>
  <summary>Details</summary>
Motivation: 当前多数大语言模型虽然支持多语言，但仍以英语为中心，在低资源语言上表现不佳。现有的东南亚语言模型都不是真正开源的，缺乏训练数据透明度。真正开源模型对于理解模型内部机制、偏见、泛化能力和多语言性至关重要。

Method: 通过受控和全面的实验研究平行数据在LLM持续预训练中的有效性。发现仅使用平行数据是将LLM扩展到新语言的最有效方法。使用34.7B tokens的平行数据，在8x NVIDIA H200 GPU上训练180小时。

Result: 开发了OpenSeal，这是第一个真正开源的东南亚语言大模型，其性能与现有相似规模的模型相当。证明了仅使用平行数据就能有效扩展LLM到新语言。

Conclusion: 平行数据是扩展大语言模型到新语言的有效方法，OpenSeal的成功展示了真正开源东南亚语言模型的可行性，为透明度研究和多语言AI发展提供了重要基础。

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [335] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: Kimi K2.5是一个开源的多模态智能体模型，通过文本-视觉联合优化和Agent Swarm并行框架，在编码、视觉、推理等任务上达到SOTA性能，推理延迟降低4.5倍。


<details>
  <summary>Details</summary>
Motivation: 推动通用智能体智能的发展，通过多模态联合优化提升智能体的综合能力，同时解决复杂任务执行效率问题。

Method: 采用文本-视觉联合预训练、零视觉SFT和联合文本-视觉强化学习；引入Agent Swarm自导向并行智能体编排框架，动态分解复杂任务为异构子问题并行执行。

Result: 在编码、视觉、推理和智能体任务等多个领域达到最先进水平；Agent Swarm相比单智能体基线将延迟降低高达4.5倍。

Conclusion: Kimi K2.5通过多模态联合优化和并行智能体框架显著提升了智能体性能和执行效率，开源模型检查点将促进智能体智能的研究和应用。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [336] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

TL;DR: 研究发现跨语言评估LLM时存在系统性排名不稳定性：表面指标保持稳定，但语用判断在不同语言间出现排名反转和接近零相关性，表明零-shot评估在形态丰富语言中不可靠。


<details>
  <summary>Details</summary>
Motivation: 当前跨语言LLM评估混淆了两个方差来源：真实模型性能差异和测量不稳定性。研究者希望分离这些因素，探究在生成条件相同、仅目标语言变化的情况下，评估方法是否能在形态丰富的芬兰-乌戈尔语系语言中保持稳定。

Method: 使用相同参数在爱沙尼亚语、芬兰语和匈牙利语中生成合成客户支持对话，测试自动指标和LLM-as-a-judge评分是否能在这些语言间产生稳定的模型排名。以少量爱沙尼亚语母语者标注为参考点，分析不同评估方法的稳定性。

Result: 发现系统性排名不稳定性：表面指标（词汇多样性、表面和语义相似性）保持跨语言稳定性，但语用判断（连贯性、指令遵循）出现排名反转和接近零相关性。由于生成条件受控，这些不一致反映了评分方法在不同语言间的行为差异而非真实模型差异。

Conclusion: 零-shot评估在形态丰富语言的语篇层面评估中不可靠，需要在部署前进行语言特定校准。研究提供了诊断性探测方法：在相同生成条件下无法保持稳定性的评估方法表明存在迁移失败。作者发布了受控生成协议、合成数据和评估框架以促进跨语言族复制。

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [337] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: StoryScore：一个评估AI生成科学故事的复合指标，整合语义对齐、词汇基础、叙事控制、结构保真度、冗余避免和实体级幻觉检测


<details>
  <summary>Details</summary>
Motivation: 生成式AI能将科学文章转化为适合不同受众的叙事，但评估这些故事具有挑战性。传统摘要指标无法捕捉故事讲述所需的抽象、简化和教学创造性，而幻觉检测器在科学背景下经常误判合法的叙事重构或在创造性内容中表现不稳定。

Method: 提出StoryScore复合指标，整合六个关键维度：语义对齐、词汇基础、叙事控制、结构保真度、冗余避免和实体级幻觉检测，形成一个统一的评估框架。

Result: 分析揭示了现有幻觉检测方法的局限性：虽然自动指标能有效评估与原始内容的语义相似性，但难以评估内容的叙述方式和控制质量，无法区分教学创造性和事实错误。

Conclusion: StoryScore为评估AI生成的科学故事提供了一个综合框架，解决了传统指标在捕捉叙事质量和区分创造性改编与事实错误方面的不足。

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [338] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 本文提出了模块化梯度手术（MGS）方法，通过解决Transformer模块级别的梯度冲突，有效缓解多领域强化学习中的跨域干扰问题，显著提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在强化学习训练中面临跨领域干扰问题。现有策略如顺序RL和混合RL在行为层面和梯度层面都存在显著的跨域干扰，导致整体性能提升有限。需要一种有效的方法来解决多领域RL训练中的干扰问题。

Method: 提出了模块化梯度手术（MGS）方法，该方法在Transformer内部模块级别解决梯度冲突。通过分析梯度冲突的根源，设计了一种能够缓解跨域干扰的梯度优化策略。

Result: 在Llama和Qwen模型上应用MGS，在三个代表性领域（数学、通用聊天和指令遵循）上分别实现了平均4.3分（16.6%）和4.5分（11.1%）的提升，优于标准多任务RL。进一步分析表明MGS在长时间训练中仍然有效。

Conclusion: 本研究阐明了多领域RL中干扰的来源，并提出了一种有效的解决方案MGS，为训练通用大型推理模型提供了重要方法。MGS通过模块级别的梯度冲突解决，显著提升了模型在多领域的性能表现。

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [339] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: LLMs通过上下文学习形成参数信念流形，线性干预会破坏几何结构，而几何感知的线性场探测能更好地保持信念流形


<details>
  <summary>Details</summary>
Motivation: 缺乏对LLMs中信念如何编码、更新和干预的机制性理解，需要研究模型如何隐式推断分布参数并在表示空间中形成信念结构

Method: 使用Llama-3.2在受控环境中从正态分布生成样本，仅通过上下文样本隐式推断分布参数（均值和标准差），研究信念流形的形成和变化，比较线性干预与几何感知干预的效果

Result: 发现LLMs通过足够上下文学习形成参数信念的弯曲"信念流形"，当分布突然变化时模型会适应；标准线性干预会将模型推离流形并导致耦合的分布外偏移，而几何和场感知的干预能更好地保持目标信念族

Conclusion: LLMs中自然涌现出丰富的结构，纯线性的概念表示通常是不充分的抽象；线性场探测（LFP）是一种简单方法来平铺数据流形并进行尊重底层几何的干预

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [340] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 提出全自动分子结构描述标注框架，通过解析IUPAC名称生成结构化XML元数据，指导LLM生成准确的自然语言描述，构建了16.3万分子-描述对数据集，验证精度达98.6%


<details>
  <summary>Details</summary>
Motivation: 分子功能主要由结构决定，准确对齐分子结构与自然语言对LLM推理化学任务至关重要。但人工标注成本高昂，难以构建大规模高质量的结构描述数据集。

Method: 基于规则化学命名解析器扩展，解释IUPAC名称并构建富结构化的XML元数据，明确编码分子结构信息，然后使用该元数据指导LLM生成准确的自然语言描述。

Result: 构建了约16.3万个分子-描述对的大规模数据集，通过结合LLM和专家人工评估的严格验证协议，在2000个分子子集上实现了98.6%的高描述精度。

Conclusion: 该数据集为未来分子-语言对齐提供了可靠基础，所提出的标注方法易于扩展到更大数据集和依赖结构描述的更广泛化学任务。

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [341] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 提出语言向量方法，通过激活差异引导模型行为，无需训练即可提升多语言上下文学习性能


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在非英语语言上的性能远低于英语，特别是在上下文学习中，使用英语演示但测试非英语输入时性能显著下降

Method: 提出语言向量方法，利用源语言和目标语言之间的激活差异来引导模型行为，在推理过程中将向量添加到中间模型激活中，使模型内部表示向目标语言空间转移，无需参数更新

Result: 在三个数据集和19种语言上测试三种不同模型，结果显示在所有任务和语言上都比基线有持续改进；语言向量的层次聚类揭示了与语言家族对齐的有意义语言结构；这些向量还能成功跨任务转移

Conclusion: 语言向量是一种无需训练的语言引导方法，能有效提升多语言上下文学习性能，揭示了模型内部的语言表示具有任务无关性和语言学意义

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [342] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 论文提出了一个统一框架，将不同LLM控制方法视为由控制信号诱导的动态权重更新，并引入偏好-效用分析来量化控制效果，发现偏好与效用之间存在权衡关系，最后提出了改进的SPLIT方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型控制方法（如本地权重微调、LoRA适配、基于激活的干预）通常被孤立研究，难以比较和建立联系，需要统一的框架来理解这些方法的共性和差异。

Method: 1. 提出统一视图：将各种干预方法框架化为由控制信号诱导的动态权重更新；2. 提出统一的偏好-效用分析：将控制效果分为偏好（对目标概念的倾向性）和效用（连贯且任务有效的生成），使用极性配对的对比示例在共享对数几率尺度上测量两者；3. 从激活流形角度解释控制行为；4. 提出新的引导方法SPLIT。

Result: 观察到不同方法中都存在一致的偏好-效用权衡：更强的控制会增加偏好，但可预测地降低效用。从激活流形视角看，控制会沿着目标概念方向移动表示以增强偏好，而当干预将表示推离模型的有效生成流形时，效用会下降。

Conclusion: 通过统一的框架和分析揭示了LLM控制方法中的基本权衡关系，并基于此提出了SPLIT方法，能够在提高偏好的同时更好地保持效用，为理解和改进LLM控制提供了新视角。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [343] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体提示框架，用于评估多站迷你面试中的软技能，通过结构化提示工程替代数据密集型微调，在复杂主观推理任务中表现优于专门微调的基线模型。


<details>
  <summary>Details</summary>
Motivation: 在竞争性选拔过程中评估共情、伦理判断和沟通等软技能至关重要，但人工评分往往不一致且存在偏见。虽然大型语言模型改进了自动作文评分，但现有基于推理的微调方法难以处理多站迷你面试的抽象性和上下文依赖性，无法捕捉候选人叙述中的隐含信号。

Method: 引入多智能体提示框架，将评估过程分解为转录本精炼和标准特定评分两个阶段。使用大型指令调优模型进行3次样本上下文学习，通过结构化提示工程替代传统的数据密集型微调方法。

Result: 该方法在MMI评估中显著优于专门微调的基线模型（平均QWK 0.62 vs 0.32），达到与人类专家相当的可靠性。在ASAP基准测试中也表现出良好的泛化能力，无需额外训练即可媲美领域特定的最先进模型。

Conclusion: 对于复杂的主观推理任务，结构化提示工程可能为数据密集型微调提供可扩展的替代方案，改变了大型语言模型在自动评估中的应用方式，特别是在处理抽象、上下文依赖的评估场景时具有优势。

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [344] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 该研究开发了一个可扩展的数据构建流程，利用LLM生成大量高质量的"问题-证明-检查"三元组数据，并训练了一个证明检查奖励模型，以增强LLM的数学证明能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型通过可验证奖励的强化学习展示了强大的数学推理能力，但许多高级数学问题是基于证明的，无法通过简单的答案匹配来确定证明的真实性。需要能够可靠评估完整证明过程的奖励模型来实现自动验证。

Method: 设计了一个可扩展的数据构建流程，以最小的人力成本利用LLM生成大量高质量的"问题-证明-检查"三元组数据。通过系统变化问题来源、生成方法和模型配置，创建了涵盖多个难度级别、语言风格和错误类型的多样化问题-证明对，并通过分层人工审查进行过滤。利用这些数据训练证明检查奖励模型，并加入过程奖励和令牌权重平衡来稳定强化学习过程。

Result: 实验验证了模型的可扩展性和强大性能，包括奖励准确性、泛化能力和测试时指导等多个角度，为增强LLM数学能力提供了重要的实践方法和工具。

Conclusion: 该研究提出的可扩展数据构建流程和证明检查奖励模型为增强LLM的数学证明能力提供了有效的解决方案，特别是在处理需要完整证明过程验证的高级数学问题时具有重要意义。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [345] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: 论文指出LLM从辅助转向决策支持时存在危险模式：流畅同意但缺乏校准判断，可能导致谄媚式AI，将验证成本转嫁给专家，而结果反馈太迟无法作为奖励信号。在深度不确定性决策中，这种模式会放大错误承诺。作者提出需要从答案生成转向协作前提治理，建立基于知识基底的差异驱动控制循环。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从辅助工具扩展到决策支持系统，出现了一个危险模式：AI表现出流畅的同意但缺乏校准的判断力。这种低摩擦的助手可能变得谄媚，隐含地接受用户的假设，将验证成本转嫁给专家，而决策结果反馈太迟无法作为有效的奖励信号。在深度不确定性决策（目标存在争议且逆转成本高昂）中，这种流畅同意的模式会放大错误承诺，而不是建立专业知识。

Method: 提出从答案生成转向协作前提治理的范式转变，建立基于知识基底的差异驱动控制循环。该方法包括：检测冲突、通过类型化差异（目的论、认识论、程序性）定位不对齐、通过决策切片触发有限协商。承诺门控机制阻止对未承诺的关键前提采取行动（除非在记录风险下被覆盖），价值门控挑战在交互成本下分配探测。信任建立在可审计的前提和证据标准上，而不是对话流畅度。

Result: 论文提出了一个理论框架，但未报告具体实验结果。作者以辅导场景为例说明了该方法的应用，并提出了可证伪的评估标准。核心贡献是概念性的：识别了LLM决策支持中的谄媚问题，并提出了基于前提治理和差异检测的解决方案框架。

Conclusion: 可靠的人机协作需要从答案生成转向协作前提治理，在知识基底上进行决策关键前提的协商。通过差异驱动控制循环、承诺门控和价值门控挑战等机制，将信任建立在可审计的前提和证据标准上，而不是对话流畅度。这种方法可以防止在深度不确定性决策中放大错误承诺，建立更可靠的决策支持系统。

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [346] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: ROG框架结合检索增强和LLM推理，通过分解复杂FOL查询为单算子子查询，在知识图谱上实现更鲁棒的逻辑推理


<details>
  <summary>Details</summary>
Motivation: 在不完整知识图谱上回答复杂的一阶逻辑查询（包含投影、交集、并集和否定）很困难，需要解决复杂查询结构和否定查询的挑战

Method: 提出ROG框架，结合查询感知的邻域检索和LLM链式思维推理，将多算子查询分解为单算子子查询序列，每个步骤基于紧凑的查询相关邻域证据，缓存和重用中间答案集

Result: 在标准知识图谱推理基准测试中，相比强嵌入基线取得一致提升，在高度复杂和否定密集的查询类型上改进最大

Conclusion: ROG通过用检索基础的逐步推理替代学习算子，为基于嵌入的逻辑推理提供了实用替代方案，减少了复合错误，在复杂和否定密集查询上实现更鲁棒的推理

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [347] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的学生-导师对话误解检测方法，通过生成-检索-重排流程提高误解识别准确性。


<details>
  <summary>Details</summary>
Motivation: 及时准确识别学生误解对改善学习成果至关重要，但目前主要依赖教师的努力和直觉，需要自动化解决方案。

Method: 采用两阶段方法：1) 使用微调LLM生成可能的误解；2) 通过嵌入相似性检索最有希望的候选误解；3) 使用另一个微调LLM评估和重排候选误解以提高相关性。

Result: 在真实教育辅导平台对话上评估，发现该方法优于基线模型，微调能提高生成误解质量，甚至可以超越更大的闭源模型。消融研究验证了生成和重排步骤的重要性。

Conclusion: 提出的基于LLM的误解检测方法能有效识别学生误解，微调策略显著提升性能，为教育领域的自动化误解检测提供了可行方案。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [348] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

TL;DR: 评估大型语言模型在多语言心理健康任务中的表现，比较专有和开源模型在不同设置下的性能，分析机器翻译质量对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在NLP任务中表现出色，但其在多语言环境下的表现，特别是在心理健康领域，尚未得到充分探索。需要评估LLMs在不同语言和翻译质量下的性能表现。

Method: 在八个多语言心理健康数据集及其机器翻译版本上评估专有和开源LLMs，采用零样本、少样本和微调设置，并与传统NLP基线模型比较。同时评估不同语系和语言类型学的翻译质量。

Result: 专有LLMs和微调后的开源LLMs在多个数据集上取得了有竞争力的F1分数，经常超过最先进的结果。但在机器翻译数据上的表现普遍较低，且下降程度因语言和类型学而异。

Conclusion: LLMs在处理英语以外的语言心理健康任务方面具有优势，但当翻译质量引入结构或词汇不匹配时存在局限性。翻译质量的变化凸显了LLMs的优缺点。

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [349] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: 本文提出了一种抽象引导推理框架，通过分离结构推理与词汇语义，减少大语言模型在演绎推理中的语义偏见，提高形式推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在演绎推理中经常混淆语义合理性与形式有效性（内容效应），即使生成逐步解释时，中间推理也会继承相同的语义捷径。现有方法通过增加推理时的结构约束来缓解此问题，但可靠抑制语义干扰仍是一个开放挑战。

Method: 构建配对的内容丰富和抽象三段论，利用模型在抽象输入上的激活定义抽象推理空间。学习轻量级抽象器，从内容条件残差流状态预测与该空间对齐的表示，并通过多层干预在前向传播中集成这些预测。

Result: 使用跨语言迁移作为测试平台，证明抽象对齐的引导减少了内容驱动的错误，提高了有效性敏感性能。

Conclusion: 激活级抽象作为一种可扩展机制，能够增强大语言模型形式推理对语义干扰的鲁棒性。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [350] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出使用混合因子分析器（MFA）作为语言模型激活分解的新方法，能够捕捉非线性、多维度的概念结构，相比现有基于线性可分假设的方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型激活分解方法通常假设概念在激活空间中线性可分，只寻找单个全局方向，这忽略了具有非线性或多维结构的复杂概念。需要一种能够捕捉复杂几何结构的方法。

Method: 采用混合因子分析器（MFA）作为可扩展的无监督替代方案，将激活空间建模为一组具有局部协方差结构的高斯区域。MFA将激活分解为两个组合几何对象：区域质心和局部变异。

Result: 在Llama-3.1-8B和Gemma-2-2B上训练大规模MFA，成功捕捉了激活空间中的复杂非线性结构。在定位和操控基准测试中，MFA优于无监督基线方法，与有监督定位方法竞争，并且在操控性能上通常优于稀疏自编码器。

Conclusion: 局部几何（通过子空间表达）是用于可扩展概念发现和模型控制的有前景的分析单元，能够捕捉孤立方向无法处理的复杂结构，为激活分解提供了新的几何视角。

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [351] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的端到端框架，通过分治推理策略增强大语言模型在复杂任务上的推理能力，相比传统的链式思维推理有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在大语言模型能力极限时往往不足，且其严格的顺序性限制了测试时的可扩展性。分治推理虽然有望解决复杂问题，但通用后训练与分治推理之间存在根本性不匹配，限制了模型充分发挥这种潜力。

Method: 提出了一个端到端的强化学习框架，增强大语言模型的分治推理能力。在每一步中，策略将问题分解为一组子问题，顺序解决它们，然后基于子问题解决方案处理原始问题，将分解和解决方案都整合到强化学习训练中。

Result: 在可比训练条件下，分治推理框架赋予模型更高的性能上限和更强的测试时扩展性，在竞赛级基准测试中，Pass@1指标超过链式思维推理8.6%，Pass@32指标超过6.3%。

Conclusion: 通过强化学习框架增强大语言模型的分治推理能力，能够有效解决复杂推理任务，相比传统链式思维推理具有显著优势，为提升模型在挑战性任务上的推理能力提供了新途径。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [352] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: 本文提出了RACO框架，一种无需奖励模型的多目标对齐方法，通过冲突规避梯度下降解决LLM对齐中的目标冲突问题，在多个任务和模型上实现了更好的帕累托权衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM对齐问题常涉及多个冲突目标，现有方法要么通过加权损失导致不稳定训练和差劲的权衡，要么依赖显式奖励模型增加了复杂性并扭曲了用户指定的偏好。

Method: 提出了RACO框架，直接利用成对偏好数据，通过一种新颖的裁剪变体冲突规避梯度下降来解决梯度冲突。该方法无需显式奖励模型，能够收敛到尊重用户指定目标权重的帕累托临界点。

Result: 在Qwen 3、Llama 3、Gemma 3等多个LLM家族的多目标摘要和安全对齐任务上，定性和定量评估均显示该方法相比现有基线实现了更好的帕累托权衡，且在双目标设置中裁剪能严格提高收敛速度。

Conclusion: RACO框架为多目标LLM对齐提供了一种有效且实用的解决方案，无需依赖显式奖励模型，能够直接处理偏好数据并解决目标冲突问题，在多个实际任务中表现优于现有方法。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>
